<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diacritics Restoration using BERT with Analysis on Czech language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>N?plava</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="department" key="dep2">Czech Republic Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="department" key="dep2">Czech Republic Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="department" key="dep2">Czech Republic Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diacritics Restoration using BERT with Analysis on Czech language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.14712/00326585.013</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new architecture for diacritics restoration based on contextualized embeddings, namely BERT, and we evaluate it on 12 languages with diacritics. Furthermore, we conduct a detailed error analysis on Czech, a morphologically rich language with a high level of diacritization. Notably, we manually annotate all mispredictions, showing that roughly 44% of them are actually not errors, but either plausible variants (19%), or the system corrections of erroneous data (25%). Finally, we categorize the real errors in detail. We release the code at https://github.com/ufal/bert-diacritics-restoration. Please cite the published version: https://ufal.mff.cuni.cz/pbml/116/art-naplava-straka-strakova.bib. Recently, the BERT model (Devlin et al., 2019) comprising of self-attention layers, was proposed and shown to reach remarkable results on a variety of tasks. As it uses no recurrent layers, its inference time is much shorter. We expect BERT to significantly improve the performance over current state-of-the-art diacritization architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head><p>The core of our system is a pre-trained multilingual BERT model that uses selfattention layers to create contextualized embeddings for tokenized text without diacritics. The contextual embeddings are fed into a fully-connected feed-forward neural network followed by a softmax layer. This outputs a vector with a distribution over</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Diacritics Restoration, also known as Diacritics Generation or Accent Restoration, is a task of correctly restoring diacritics in a text without any diacritics. Its main difficulty stems from ambiguity where context needs to be taken into account to select the most appropriate word variant, because diacritization removal creates new groups of homonymy.</p><p>Current state-of-the-art algorithms for diacritics restoration are mostly based on either recurrent neural networks combined with an external language model <ref type="bibr" target="#b9">(N?plava et al., 2018;</ref><ref type="bibr" target="#b1">AlKhamissi et al., 2020)</ref> or Transformer <ref type="bibr" target="#b8">(Mubarak et al., 2019)</ref>. Recently, BERT <ref type="bibr" target="#b6">(Devlin et al., 2019</ref>) was shown to outperform many models on many tasks while being much faster due to the fact that it uses simple parallelizable classification head instead of a slow auto-regressive approach.</p><p>In this work, we first describe a model for diacritics restoration based on BERT and evaluate it on multilingual dataset comprising of 12 languages <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref>.</p><p>We show that the proposed model outperforms the previous state-of-the-art system <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref> in 9 languages significantly.</p><p>We further provide an extensive analysis of our model performance in Czech, a language with rich morphology and a high level of diacritization. In addition to clean data from Wikipedia <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref>, the model was evaluated on data collected from other domains, including noisy data, and we show that stable performance holds even if the text contains spelling and other grammatical errors.</p><p>Sometimes, multiple plausible diacritization variants are possible, while only one gold reference exists, which comes from the original text before diacritization was automatically stripped to create test data. To assess the extent of these cases, we employed annotators to manually annotate all mispredictions and we found that 19% of errors are plausible variants and 25% of errors are system corrections of errors in data.</p><p>Finally, we further analyse the remaining errors by analysing characteristics of plausible variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Diacritics Restoration is an active area of research in many languages: Vietnamese <ref type="bibr" target="#b10">(Nga et al., 2019)</ref>, Romanian <ref type="bibr" target="#b12">(Nu?u et al., 2019)</ref>, Czech <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref>, Turkish <ref type="bibr" target="#b0">(Adali and Eryi?it, 2014)</ref>, Arabic <ref type="bibr" target="#b7">(Madhfar and Qamar, 2020;</ref><ref type="bibr" target="#b1">AlKhamissi et al., 2020</ref>) and many others.</p><p>There are three main architectures currently used in diacritics restoration: convolutional neural networks <ref type="bibr" target="#b2">(Alqahtani et al., 2019)</ref>, recurrent neural networks often combined with an external language model <ref type="bibr" target="#b4">(Belinkov and Glass, 2015;</ref><ref type="bibr" target="#b9">N?plava et al., 2018;</ref><ref type="bibr" target="#b1">AlKhamissi et al., 2020)</ref> and Transformer-based models <ref type="bibr" target="#b13">(Orife, 2018;</ref><ref type="bibr" target="#b8">Mubarak et al., 2019)</ref>. The convolutional neural networks are fast to train and also to infer. However, compared to the recurrent and Transformer-based architectures, they do generally achieve slightly worse results due to the fact that they model long-range dependencies worse. On the other hand, recurrent-and Transformer-based architectures are much slower. a set of instructions that define diacritization operation over individual characters of each input token. We select the instruction with maximum probability. The model is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Diacritization Instruction Set</head><p>To decrease the size of the final softmax layer, the output labels are not the diacritized variants of input subwords, as one would expect, but they are a set of instructions that provide prescription on how to restore diacritics. Specifically, one such instruction consists of index-diacritical mark tuples that define on what index of input subword a particular diacritical mark should be added.</p><p>An example of a diacritization instructions set can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>. Given an input subword dite (d?t?), with four characters indexed from 0 to 3, the appropriate diacritization instruction is 1:ACUTE;3:CARON, in which acute is to be added to i and caron is to be added to e resulting in a properly diacritized word d?t?. Obviously, the network can choose to leave the (sub)word unchanged, for which a special instruction &lt;KEEP&gt; is reserved. Should the network accidentally select an impossible instruction, no operation is carried out and the input (sub)word is also left unchanged.</p><p>To construct the set of possible diacritization instructions, we tokenize the undiacritized text of the particular training set and align each input token to the corresponding token in the diacritized text variant. The diacritical mark in each instruction is obtained from the Unicode name of the diacritized character. We keep only those input instruction  instructions that occurred at least twice in a training set to filter out extremely rare instructions that originate for example from foreign words or bad spelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Details</head><p>We train both the fully-connected network and BERT with AdamW optimizer which minimizes the negative log-likelihood. The learning rate linearly increases from 0 to 5e-5 over the first 10000 steps and then remains the same. We use HuggingFace implementation of BertForTokenClassification and initialize BERT-base values from bertbase-multilingual-uncased model.</p><p>We use the batch size of 2048 sentences and clip each training sentence on 128 tokens. We train each model for circa 14 days on Nvidia P5000 GPU and select the best checkpoint according to development set. with the previous state-of-the-art-results of <ref type="bibr" target="#b9">N?plava et al. (2018)</ref> are presented in <ref type="table">Table 1</ref>. Apart for alpha-word accuracy itself, we also report 95% confidential intervals computed using bootstrap resampling method. On 9 of 12 languages, our approach significantly outperforms previous state-ofthe-art combined recurrent neural networks with an external language model. The most significant improvements are achieved on Vietnamese and Latvian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Detailed Analysis on Czech</head><p>We further provide a detailed analysis of our model performance in Czech, a language with rich morphology and a high diacritization level: Of the 26 English alphabet letters, a half of them can have one or two kinds of diacritization marks <ref type="bibr" target="#b17">(Zeman, 2016)</ref>. Czech is also the 4-th most diacritized language of the 12 languages found in the diacritization corpus of <ref type="bibr" target="#b9">N?plava et al. (2018)</ref>.</p><p>Particularly, we are interested in the three following questions:</p><p>? How would our system perform outside the very clean Wiki domain? (Section 5.1) ? Is it possible that some of the labeled mispredictions are actually plausible variants? (Section 5.2) ? Is there an observable characteristics in the real errors made by the system?  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Additional Domains</head><p>The testing dataset of <ref type="bibr" target="#b9">N?plava et al. (2018)</ref> is composed of clean sentences originating from Wikipedia. It is, however, a well-known fact that the performance of the (deep neural) models may deteriorate substantially when the input domain is changed <ref type="bibr" target="#b3">(Belinkov and Bisk, 2017;</ref><ref type="bibr" target="#b14">Rychalska et al., 2019)</ref>. To test our system in other, more challenging domains, we used data from a new Czech dataset (unpublished, in annotation process) for grammatical-error-correction that contains data collected from 4 sources:</p><p>? Natives Formal -Essays of elementary school Czech pupils (decent Czech proficiency) ? Natives Informal -texts collected from web discussions ? Second Learners -essays of Czech second learners ? Romi -texts of Czech pupils with Romani ethnolect (low Czech proficiency) The dataset covers a wide range of Czech domains. It contains texts annotated in M2 format, a standard annotation format for grammar-error-correction corpora. In this format, each document contains original sentences with potential errors (e.g. spelling, grammatical or errors in diacritics) and a set of annotations describing what operations should be performed in order to fix each error.</p><p>To create target data for diacritics restoration, we apply all correcting edits that fix errors in diacritics and casing. We leave other errors intact, but do not evaluate on words that contain these errors, because they are not directly relevant to diacritics and in many cases, the errors are so severe that evaluation would be controversial. To rule out such words, we create a binary mask that distinguishes between evaluated and omitted words. Although the severely perturbed words are omitted from evaluation, they still remain in the sentence context and may still confuse the diacritization system, making the task potentially more difficult. See examples of such misleading sentence contexts in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>The basic statistics of the new dataset are presented in <ref type="table" target="#tab_4">Table 2</ref>. We display the number of sentences, the number of all words and the number of evaluated (unmasked) words. Compared to the Wikipedia dataset <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref>, our new dataset has half the number of sentences and one third of its number of words.</p><p>Pot?ebujeme nov? idea i novych lidi/lid?* , ktery je p?inesou .</p><p>Na ulic?ch vid?me ?asto nekter? lidi , kte?? nos? barevn?/barevn?* oble?eny , kter? jsou snad hezk? , ale ur?it? nejsou elegantn? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English translation (without ambiguities)</head><p>We need new ideas and also people to come up with them.</p><p>In the streets, we can see some people wearing colourful clothes, which may be nice but certainly not elegant. We evaluate our model on all the above introduced Czech domains and present the results in <ref type="table">Table 3</ref>. Despite our initial concern that the model would perform worse on these domains due to the noisy nature of the data, the results show that the model performance remains roughly stable on all domains. We suppose that although the writers produced quite noisy texts, they at the same time avoided foreign words that are generally harder to correctly diacritize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Error Annotation</head><p>Clearly, removing diacritics creates new groups of homonymy (dal/d?l, krize/k???e). In most cases, the correct diacritization variant can be inferred by a method which takes the sentence context into consideration. However, there are cases, in which more plausible variants are available, e.g., ?achu/?ach?, pradlena/p?adlena, pod?na/podan?, as illustrated in <ref type="figure" target="#fig_2">Figure 4</ref>. Furthermore, some variants can only be disambiguated in the context of the whole document, such as in: K nejv?znamn?j??m pat?? zmi?ovan? vily/v?ly. (more examples in <ref type="figure">Figure 6</ref>), not to mention other examples that can be only disambiguated by real-world knowledge such as in Povrch satelitu/satelit? Zem? u? zkoumalo n?kolik sond.</p><p>However, all our evaluation data are limited only to a single gold reference for each word without diacritics, given by the fact that the gold reference comes from the original text with diacritics. To explore both phenomena among the mispredictions, we hired annotators to examine: a) whether a word is correctly diacritized given the context of current sentence; and b) whether it is correct given a context of two previous sentences, current sentence and two following sentences (thus ruling out the words with even longer document dependencies).</p><p>While the evaluation of the clear Wiki data <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref> is straightforward, some of our newly introduced noisy data may become controversial to evaluate due Nebo z?m?na kapitol a jejich ?asov? posloupnost v knize je pak ve filmu pod?na/podan? rozd?ln? .</p><p>Hran? ?achu/?ach? , ale p?edev??m karetn?ch her , kritizoval tak? Petr Chel?ick? .</p><p>Jeho matka byla p?adlena/pradlena , kter? ke sklonku ?ivota propadla alkoholu .</p><p>Hororov? hudba slou?? p?edev??m pro dokreslen? film?/filmu .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English translation</head><p>The chapters and their chronological order in the book are then presented/given differently in the film.</p><p>Playing a game of chess/games of chess , but especially card games was criticized by Petr Chel?ick? .</p><p>His mother was a washerwoman/laundress who fell into alcoholism towards the end of her life .</p><p>Horror music is mainly used to complete a movie/movies . to erroneous words. Therefore, such words were also marked by the annotators and subsequently removed from our analysis. An example of a final annotation item presented to an annotator is illustrated in <ref type="figure">Figure 5</ref>.</p><p>To create the annotation items, we concatenated data from all domains, both the original Wikipedia data <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref> and other domains (Section 5.1) and we further considered those words in which the results of our system did not match target word. Before annotation, we automatically filtered out some cases:</p><p>? Predictions, in which the system and the target words are variants (as marked by MorphoDita <ref type="bibr" target="#b16">(Strakov? et al., 2014)</ref>) were automatically marked correct. ? Predictions, in which the target word was marked as non-existing by Mor-phoDiTa, while the system word was marked as Czech, were considered dubious and removed from our analysis.</p><p>For the remaining 4702 words, two annotation items were created: one with the predicted word and one with the gold reference word in the position of the annotated Current Word. The annotation process took circa 70 hours.</p><p>The basic analysis of the annotated system errors is the following: There are 4702 wrongly diacritized words in the all our data concatenated. Annotations revealed that 960 of the mispredicted words contain a non-diacritical error and we do not consider  <ref type="figure">Figure 5</ref>. Annotation item example. The annotator marks whether the word "rychlosti" is correct given a context of the current sentence, whether it is still correct in the context of two previous and two following sentences and whether it contains a typo. them further, as mentioned above. The remaining 3742 mispredicted words can be categorized as follows:</p><p>? System correct, Gold correct: 19% (694 of 3742) -plausible variants ? System correct, Gold wrong: 25% (964 of 3742) -system corrects data error ? System wrong, Gold wrong 1% (31 of 3742) -uncorrected error in data ? System wrong, Gold correct 55% (2 084 of 3742) -real errors Interestingly, the annotations revealed that about 44% of errors are not errors at all. In 694 cases (19%) both the system word and the gold word are correct, which is justified by the plausible variants. In 964 cases (25%) the original gold annotation was wrong whereas the system annotation was correct, which means that the system effectively corrected some of the errors in the original data. The remaining 31 cases are for neither the system nor the gold word being correct. Finally, the annotations confirmed 2084 real system errors, which we postpone for a more detailed analysis in the following Section 5.3.</p><p>Plausible variants, which constitute 19% of the annotated errors, are the most interesting item. Please note that our criterion for plausible variant was strict: only cases ambiguous both in the sentence and document context were marked as plausible variants. Circa 72% percent of these words share a common lemma. As <ref type="table" target="#tab_8">Table 4</ref>.a and <ref type="table" target="#tab_9">Table 5</ref>.a show, singular/plural ambiguities by far most often arise in inanimate masc. genitive (programu/program?, ?achu/?ach?). Another common ambiguity is passive participle vs. adjective (zalo?ena/zalo?en?), generally known to be difficult for diacritization disambiguation <ref type="bibr" target="#b17">(Zeman, 2016)</ref>. More interesting examples are given in <ref type="table" target="#tab_8">Table 4</ref>.a and <ref type="table" target="#tab_9">Table 5</ref>.a. To conclude, we use the collected annotations to refine our previous results, which we display in <ref type="table">Table 3</ref>. When considering all annotated words, including those preprocessed with MorphoDiTa, we achieve 35% to 67% error reduction. When omitting words newly marked by human annotators as containing another (non-diacritical) error, the error rate gets additionally reduced by up to 33%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of Real Errors</head><p>We follow with a morphological analysis of the remaining confirmed errors, which constitute 55% of the annotated mispredictions. To determine the morphological categories of the erroneously predicted words, we use UDPipe <ref type="bibr" target="#b15">(Straka et al., 2019)</ref> to generate morphological annotations for all words in model hypotheses and gold sentences. We then inspect the most frequent confusions between the system and the gold morphological annotations of words, using the Universal POS tags and Universal features <ref type="bibr" target="#b11">(Nivre et al., 2020)</ref>.</p><p>The annotations confirmed an interesting discourse phenomenon: a word can be correctly diacritized in multiple ways given the context of its sentence, however only a single correct diacritization variant exists if a wider context is taken into account. There are 50 such annotated cases; two examples are displayed in <ref type="figure">Figure 6</ref>. Although this phenomenon is interesting from a discourse perspective, its low proportion to actual errors (50 of 2084) indicates that it is quite rare. This implies that training models on longer texts (we currently train our model on examples comprising maximally 128 subwords -see Section 3.2) does not promise potential for overall improvement.</p><p>Finally, we offer a categorization of such ambiguities by means of the Universal POS tags and Universal features <ref type="bibr" target="#b11">(Nivre et al., 2020)</ref> in <ref type="table" target="#tab_8">Table 4.b and Table 5</ref>.b, respectively.</p><p>The remaining errors are a mix of complicated disambiguation cases or rare named entities. The most frequent errors bear similarity to plausible variants (compare <ref type="table" target="#tab_9">Table 5</ref>.a and <ref type="table" target="#tab_9">Table 5</ref>.c), only with a different order of appearance. Unlike plausible variants <ref type="table" target="#tab_9">(Table 5</ref>.a), most frequent mismatches occur already at the level of lemmas (st?t/sta?, ?e/ze, see <ref type="table" target="#tab_9">Table 5</ref>.c). Second most frequent cases are rare named entities (Sokrates/S?krat?s, Aristoteles/Aristotel?s, Diogen?s/D?ogen?s). Number is again often hard to disambiguate in inanimate masc. genitive <ref type="bibr">(milionu/milion?, reproduktoru/reproduktor?, dokumentu/dokument?)</ref>, followed by fem. case (ji/j?, ni/n?, zemi/zem?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We implemented a model for diacritics restoration based on BERT that outperforms previous state-of-the-art models. Further analysis on Czech data collected from additional, noisy domains shown that the model exhibits strong performance regardless the domain of the data.</p><p>We further annotated all reported mispredictions in Czech and found out that more than one correct variant is sometimes possible. Rarely, disambiguation on document level is necessary to distinguish between variants correct within the sentence context. We elaborated on these phenomena using morphological annotations and utilized them to further analyse real confirmed errors of the systems.</p><p>As for future work, we propose experimenting with a single joint model for a subset of languages, despite our initial unsuccessful attempts at training a single model for all languages, including an introduction of a larger XLM-Roberta model <ref type="bibr" target="#b5">(Conneau et al., 2020)</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Diacritization instructions examples for input "dite (d?t?)" with 4 characters, indexed from 0 to 3. Index-Instruction tuples generate diacritics for given input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Examples of misleading contexts in noisy texts. Correct diacritization (bold) can only be achieved by grammar corrections of the surrounding words (underlined).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of ambiguities, each illustrating two diacritization variants (bold), both valid in a given context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Basic statistics of new data for testing diacritics restoration in Czech.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Tento motiv m??e b?t ovlivn?n sibi?sk?m ?amanismem a pr?vodce pak m? funkci psychopompa .Tohoto ?lechtice s jeho len?ky a spojenci porazil 20. nebo 23. dubna v bitv? u Ho?ic , na?e? d?l pokra?oval v plen?n? jeho zbo?? .There are wood fairies, air fairies , mountain fairies , and also evil fairies in various forms . ?i?ka , together with the Orebits , waged war with King Zikmund's allies , especially in the Byd?ov region with Mr. ?en?k of Vartenberk .He defeated this nobleman with his feoffees and allies on April 20 or 23 at the Battle of Ho?ice , after which he continued to plunder his goods .Figure 6. Two examples of ambiguous diacritization determined by document context.</figDesc><table><row><cell>Krom? boh? znali pohan?t? Slovan? i celou ?adu ni???ch bytost? , naz?v?ny byly v?t?inou slovem b?s ?i div , kter? souvis? s indick?m d?va . K nejv?znamn?j??m pat?? zmi?ovan? v?ly/vily . V r?zn?ch pod?n?ch existuj? v?ly lesn? , vzdu?n? , horsk? a tak? v?ly zl? . Existuj? dal?? ?ensk? bytosti jim podobn? , pat?? mezi n? p?edev??m rusalky , div? ?eny nebo divo?enky doprov?zen? div?mi mu?i . Dal?? dokumenty t?kaj?c? se Jana ?i?ky z Kalichu jsou dva listy odeslan? z kl??tera ve Vil?mov? datovan? k 16. b?eznu a 1. dubnu 1423 . Slep? vojev?dce v nich vyz?v? sv? stran?ky z orebsk?ho svazu k porad? napl?novan? na 7. ?i 8. dubna do N?meck?ho Brodu . Z dopis?/dopisu je patrn? , ?e se pokou?el dokonaleji zorganizovat husitskou vojenskou moc , pro boj s dom?c?m i zahrani?n?m nep??telem . O ?trn?ct dn? pozd?ji ?i?ka spolu s orebity vedl v?lku se spojenci kr?le Zikmunda , zejm?na na By-d?ovsku s panem ?e?kem z Vartenberka . English translation This motif can be influenced by Siberian shamanism , and the guide then has the function of a psychopomp . Apart from the gods, the pagan Slavs knew a number of lower beings , mostly called Raver or Wonder , which is related to Indian deva . Among the most important are the mentioned fairies/villas. There are other female beings similar to them , they include mainly mermaids , wild women or witches accompanied by wild men . Other documents concerning Jan ?i?ka of the Kalich are two letters sent from the monastery in Vil?mov dated March 16 and April 1 , 1423 . In them , the blind military leader invites his party members from the Orebic Union to a meeting scheduled for April 7 or 8 in N?meck? Brod . The letter shows/letters show that he has tried to better organize Hussite military power , to fight both domestic and foreign enemies. Count Examples NOUN ? NOUN 406 program[u?], ?ach[u?], text[u?] ADJ ? ADJ 162 zn?m[?a], zalo?en[a?], schopn[i?] ADV ? ADJ 59 stejn[??], kr?sn[??], b??n[??] PROPN ? PROPN 31 Aristotel[e?]s, Sokrates/S?krat?s, J[a?]n VERB ? VERB 20 zam??l?m/zamysl?m, odr???/odraz?, os[i?]dluj? ADJ ? VERB 3 vznikl[?a], r?di/rad?, splaskl[?a] NOUN ? ADJ 2 p?esv?d?en[?i], o?i?t?n[?i] ADJ ? NOUN 2 veden[i?], pova?ov?n[i?] DET ? DET 2 jej[?i]ch, svoj[?i] (a) Plausible variants. Type Count Examples NOUN ? NOUN 32 st?t/sta?, objekt[u?], pulsar[u?] VERB ? VERB 4 naraz?/nar???, ?ekn[?e]te, ??[?i] DET ? DET 3 jej[i?]ch ADJ ? ADV 3 sou?asn[??], prav?/pr?v?, praktick[?y] ADJ ? ADJ 2 zn?m[?a], ??danou/zadanou ADV ? ADJ 2 stejn[?/?] NOUN ? VERB 1 mysl[i?] (b) Disambiguation from document context. Type Count Examples NOUN ? NOUN 1596 st?t/sta?, lid[?i], program[u?] PROPN ? PROPN 587 Aristotel[e?]s, Sokrates/S?krat?s, Kast[i?]lie ADJ ? ADJ 521 zn?m[a?], zalo?en[a?], ??d[i?]c? VERB ? VERB 193 m[?u]?e, M[a?]m, m[a?] ADJ ? ADV 134 kr?sn[??], hezk[?y], dobr?/dob?e PRON ? PRON 129 j[?i], n[?i], n[?i]? ADV ? ADJ 112 stejn[??], p?kn[??], Obvykl[e?] DET ? DET 59 jej[?i]ch, svoj[i?], na?[i?] NOUN ? ADJ 47 mobiln[i?], br?ny/bran?, ?e?ka/?esk? Fourteen days later , Type (c) Real errors.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Error categorization with universal POS. The context-dependent morphological annotations were obtained automatically using UDPipe.</figDesc><table><row><cell>Type</cell><cell>Count Examples</cell></row><row><cell>Number</cell><cell>325 program[u?], ?ach[u?], objekt[u?]</cell></row><row><cell>Passive participle / adjective + more features</cell><cell>116 zalo?en[a?], vzd?len[a?], naz?van[a?]</cell></row><row><cell>Lemma</cell><cell>82 l[e?]ty, mas[i?]vu, p[?e]rov?ch</cell></row><row><cell>Adj ? Adv</cell><cell>59 stejn[??], kr?sn[??]</cell></row><row><cell>Variant + more features</cell><cell>31 zn?m[?a], schopn[i?], spokojen[?i]</cell></row><row><cell>Case</cell><cell>25 dr[a?]hami, dr[a?]h?ch, ?[a?]rou</cell></row><row><cell>Lemma + more features</cell><cell>21 zam??l?m/zamysl?m, n?[s?], pacht[u?]</cell></row><row><cell>Lemma, NameType</cell><cell>20 Aristotel[e?]s, Sokrates/S?krat?s, [?I]lias</cell></row><row><cell>Case, Number</cell><cell>8 boh[?u], n?sobk[u?], funkc[?i]</cell></row><row><cell>Number, Person</cell><cell>5 pova?uj[?i], v?nuj[i?], kupuj[i?]</cell></row><row><cell></cell><cell>(a) Plausible variants.</cell></row><row><cell>Type</cell><cell>Count Examples</cell></row><row><cell>Lemma + more features</cell><cell>15 st?t/sta?, tv??/tvar, prav?/pr?v?</cell></row><row><cell>Number</cell><cell>15 objekt[?u], pulsar[u?], muzik?l[?u]</cell></row><row><cell>Lemma</cell><cell>6 ?azen?/ra?en?, v[i?]ly</cell></row><row><cell>Adj ? Adv</cell><cell>4 stejn[??], sou?asn[??], praktick[?y]</cell></row><row><cell>Case, Gender, Number</cell><cell>3 jej[i?]ch</cell></row><row><cell>Number, Person</cell><cell>2 naraz?/nar???</cell></row><row><cell cols="2">(b) Disambiguation from document context.</cell></row><row><cell>Type</cell><cell>Count Examples</cell></row><row><cell>Lemma + more features</cell><cell>924 st?t/sta?, [?c], [?z]e</cell></row><row><cell>Lemma, named entity + more features</cell><cell>382 D[i?]ogen?s, Hal/Hal, Dvo??k/Dvorak</cell></row><row><cell>Number</cell><cell>226 milion[u?], reproduktor[?u], dokument[?u]</cell></row><row><cell>Case</cell><cell>149 j[i?], n[?i], zem[?i]</cell></row><row><cell>Adj ? Adv</cell><cell>132 p?kn[??], ?esk[?y], sou?asn[??]</cell></row><row><cell>Passive participle / adjective + more features</cell><cell>37 spojen[a?], pojmenovan[?a], prodan?/prod?ny</cell></row><row><cell>Case, Number</cell><cell>27 referent[u?], Dvo??k[?u], akademi[?i]</cell></row><row><cell>Case, Gender, Number</cell><cell>16 jej[?i]ch, j[?i]m</cell></row><row><cell>Number, Person</cell><cell>15 p??[i?], pracuj[i?], ??[?i]</cell></row><row><cell>Variant + more features</cell><cell>8 zn?m[?a], schopn[?a], hodn[?a]</cell></row><row><cell></cell><cell>(c) Real errors.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Error categorization with extended Universal Features. The first column (Type) is the (primary) difference between the context-dependent feature sets of the system word and the gold word.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Automatic Evaluation on Diacritization Corpus with 12 Languages</head><p>We evaluate our approach on the dataset of <ref type="bibr" target="#b9">N?plava et al. (2018)</ref>. This dataset contains training and evaluation data for 12 languages: Vietnamese, Romanian, Latvian, Czech, Polish, Slovak, Irish, Hungarian, French, Turkish, Spanish and Croatian. We evaluate the model performance using a standard metric, the alpha-word accuracy. This metric omits words composed of non-alphabetical characters (e.g., punctuation).</p><p>For each language, we compute an independent set of operations and train a separate model. We use the concatenation of the Wiki and the Web training data of <ref type="bibr" target="#b9">(N?plava et al., 2018)</ref> both for computing a set of instructions and also as the training data for our model. The size of each instruction set and our results in comparison In Romanian Web data, ? (LATIN SMALL LETTER S WITH CEDILLA) is for historical reasons often used instead of s , (LATIN SMALL LETTER S WITH COMMA BELOW) and similarly ? (LATIN SMALL LETTER T WITH CEDILLA) is often used instead of t , (LATIN SMALL LETTER T WITH COMMA BELOW). We replace the occurrences of the previously-used characters (the former ones) with their standard versions (the latter ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work described herein has been supported by and has been using language resources stored by LINDAT/CLARIAH-CZ project of the Ministry of Education, Youth and Sports of the Czech Republic (project No. LM2018101) and also by OP VVV LINDAT/CLARIAH-CZ EXTENSION project of the Ministry of Education, Youth and Sports of the Czech Republic (project No. CZ.02.1.01/0.0/0.0/18_046/0015782). It has further been supported by the Grant Agency of the Czech Republic, project EXPRO LUSyD (GX20-16819X), and Mellon Foundation project No. G-1901-06505, by SVV project number 260 575 and by GAUK 578218 of the Charles University.</p><p>We are very grateful to our anonymous reviewers for their valuable comments and corrections.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vowel and diacritic restoration for social media texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K?bra</forename><surname>Adali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l?en</forename><surname>Eryi?it</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-1307</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)</title>
		<meeting>the 5th Workshop on Language Analysis for Social Media (LASM)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Diacritization: Efficient Hierarchical Recurrence for Improved Arabic Diacritization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alkhamissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">N</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elnokrashy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00538</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient Convolutional Neural Networks for Diacritic Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sawsan</forename><surname>Alqahtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-?CNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-?CNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1442" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Arabic diacritization with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1274</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2281" to="2285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Effective Deep Learning Models for Automatic Diacritization of Arabic Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mokhtar</forename><surname>Madhfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mustafa Qamar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Highly effective Arabic diacritization using sequence to sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younes</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2390" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diacritics restoration using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>N?plava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Stra??k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning Based Vietnamese Diacritics Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Nga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pao-Chi</forename><surname>Nguyen Khai Thinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISM46123.2019.00074</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="3313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Haji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.lrec-1.497" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="4034" to="4043" />
		</imprint>
	</monogr>
	<note>European Language Resources Association</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning for automatic diacritics restoration in romanian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nu?u</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Be?ta</forename><surname>L?rincz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Stan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCP48234.2019.8959557</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 15th International Conference on Intelligent Computer Communication and Processing (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="235" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attentive Sequence-to-Sequence Learning for Diacritic Restoration of Yor?B? Language Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iroro</forename><surname>Orife</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech</idno>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2060" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Models in the Wild: On Corruption Robustness of Neural NLP Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Rychalska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominika</forename><surname>Basaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicja</forename><surname>Gosiewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>Biecek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-36718-3_20</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="235" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Haji?</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-27947-9_12</idno>
	</analytic>
	<monogr>
		<title level="m">Text, Speech, and Dialogue</title>
		<editor>Ek?tein, Kamil</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Haji?</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-5003</idno>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5003.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Textu</surname></persName>
		</author>
		<title level="m">CzechEncy -Nov? encyklopedick? slovn?k ?e?tiny. Nakladatelstv? Lidov? noviny</title>
		<editor>Petr Karl?k, Marek Nekula, Jana Pleskalov?</editor>
		<meeting><address><addrLine>Praha, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
