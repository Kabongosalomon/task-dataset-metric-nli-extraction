<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Shin</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjae</forename><surname>Cho</surname></persName>
							<email>yoonjae.cho92@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungsoo</forename><surname>Ko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naver</forename><forename type="middle">/</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Kakao Brain</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">LINE Vision</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RTIC: Residual Learning for Text and Image Composition using Graph Convolutional Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the compositional learning of images and texts for image retrieval. The query is given in the form of an image and text that describes the desired modifications to the image; the goal is to retrieve the target image that satisfies the given modifications and resembles the query by composing information in both the text and image modalities. To remedy this, we propose a novel architecture designed for the image-text composition task and show that the proposed structure can effectively encode the differences between the source and target images conditioned on the text. Furthermore, we introduce a new joint training technique based on the graph convolutional network that is generally applicable for any existing composition methods in a plug-and-play manner. We found that the proposed technique consistently improves performance and achieves state-of-the-art scores on various benchmarks. To avoid misleading experimental results caused by trivial training hyper-parameters, we reproduce all individual baselines and train models with a unified training environment. We expect this approach to suppress undesirable effects from irrelevant components and emphasize the image-text composition module's ability. Also, we achieve the state-ofthe-art score without restricting the training environment, which implies the superiority of our method considering the gains from hyper-parameter tuning. The code, including all the baseline methods, are released 1 . * This work is done while at NAVER/LINE Vision. 1 https://github.com/nashory/rtic-gcn-pytorch</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Combining visual and contextual information, in which the desired items are retrieved from an image and a modification text that describes the desired modifications enables search engines to provide users with a powerful and intuitive experience for visual search. Among the various approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">33,</ref><ref type="bibr" target="#b28">42]</ref> that have been employed to <ref type="bibr">Figure 1</ref>. Our concept for encoding relationships between the nodes which are defined as image-text pairs in the graph. we consider a group of green-bordered pairs is highly correlated with one another because the pairs in the group describe visually similar target images. solve this problem, image-text composition is considered as the most direct and intuitive approach. The main principle behind this approach is to learn a non-linear mapping from a pair comprising the source image and the modification text to the target image. It indicates that the triplet of source image, target image, and text is required as the ground-truth for supervision. The task of image-text composition <ref type="bibr">[2,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b5">19,</ref><ref type="bibr" target="#b18">32,</ref><ref type="bibr" target="#b22">36]</ref> has a special constraints that the output representation is required to be embedded in the image feature space and not in an arbitrary space. Therefore, such property should be considered carefully in designing a composition module.</p><p>In light of this, we mainly focus on the use of the skip connection <ref type="bibr" target="#b2">[16]</ref> inspired by its powerfulness in learning representations. Despite its simplicity, little attention has been drawn to this idea in the image-text composition task. Since the skip connection helps to learn the residual, it is intuitive to use it for encoding the differences between the source image and target image in latent space. Based on this idea, we propose a novel architecture specialized for encoding the residual between the source and target image. Our method can be considered as an advanced variant of TIRG <ref type="bibr" target="#b22">[36]</ref> with the integration of the skip connection.</p><p>One challenge in image-text composition is data scarcity because collecting the ground-truth triplet pairs is extremely expensive. Therefore, it is inevitable that only a small amount of data is available for training, which causes the model to generalize poorly. To remedy this, we suggest a graph convolutional network (GCN)-based regularization technique that can be interpreted as a particular form of semi-supervised learning. We explore the possibility of utilizing graph information that encodes the similarity between image-text pairs as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The proposed graph-based technique, namely GCN-stream, can be attached to any existing composition method. We found that joint training with the GCN-stream consistently improves the results of all existing methods in a plug-and-play manner.</p><p>Our ultimate goal is <ref type="bibr" target="#b0">(1)</ref> to achieve the best score on the benchmark and (2) to compare the composition method itself as objectively as possible. The latter objective can be achieved by removing any unintended effects from the other components in the training pipeline but the composition method. We found the recent studies [2, 6, 7, 19] explore their best-performing training environment by tackling various parts in the training pipeline (e.g., word embedding, image/text encoder, composer and loss). Although such variants may improve the result significantly, the downside is that comparing the composition methods across the papers becomes less meaningful. Therefore, we consider both aspects in our experiments: (1) training with unified environment to ensure fairness and objectiveness in comparison, and (2) training with optimal environment to compare the best performance of each method. We experimentally show that our proposed method can achieve state-of-the-art performance in both aspects.</p><p>Main contributions. In summary, the main contributions of our work are threefold: Firstly, we introduce a simple but powerful image-text composition architecture, RTIC, that effectively uses skip connections for error encoding. Secondly, we propose a novel training technique that uses GCN as a good regularizer. The proposed technique is applicable for any existing methods in a plug-andplay manner. Lastly, we not only achieve the competitive result on various benchmarks but also provide a thorough and objective comparison between the composition methods by performing experiments with the unified and optimal environment both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image and Text Composition. Many studies <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b8">22,</ref><ref type="bibr" target="#b26">40,</ref><ref type="bibr" target="#b27">41]</ref> have investigated effective ways to combine more than two different modalities in feature fusion. The imagetext composition task can be seen as a unique form of feature fusion in that the textual information describes the target image to be retrieved. Inspired by the release of public datasets such as Fashion-IQ <ref type="bibr" target="#b1">[15]</ref>, many studies have explored this problem. In their pioneering work, Vo et al. <ref type="bibr" target="#b22">[36]</ref> proposed TIRG, which uses a gating mechanism to determine the channels of the image representation to be modified conditioned by a text. Based on their work, various methods <ref type="bibr">[2,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b5">19,</ref><ref type="bibr" target="#b18">32,</ref><ref type="bibr" target="#b22">36]</ref> that achieved state-of-the-art results on this task have been reported. VAL [7] employs a hierarchical feature matching that exploits the intermediate features from the image encoder. TRACE <ref type="bibr" target="#b5">[19]</ref> also uses a complicated hierarchical feature aggregation technique and applies three different loss functions simultaneously. ComposeAE <ref type="bibr" target="#b22">[36]</ref> suggests a novel embedding space that can semantically tie representations from the text and image modalities. The winning solutions <ref type="bibr" target="#b6">[20,</ref><ref type="bibr" target="#b18">32]</ref> for the competitions employed more exhaustive tricks to achieve the best performance in this task by using careful hyperparameter tuning and model ensemble with Bayes optimization <ref type="bibr" target="#b20">[34]</ref> to improve the results significantly. Although the methods mentioned above have focused on achieving the highest score on the benchmark, only a few studies have focused on the composition module itself <ref type="bibr">[2,</ref><ref type="bibr" target="#b22">36]</ref>. Since different aspects (e.g., image/text encoder or loss function) of image and text composition were addressed in recent related works <ref type="bibr">[2,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b5">19,</ref><ref type="bibr" target="#b18">32]</ref>, a naive comparison could lead to a misinterpretation. Therefore, we aim to achieve the highest score on the benchmark and, at the same time, analyze the ability of the composition module as objectively as possible. Graph Convolutional Network. The graph convolutional network (GCN) was first introduced in [23] to perform semi-supervised classification. The essential strength of the GCN is its ability to extract meaningful information encoded in the form of a structured graph from the relationships between the nodes. While most applications of GCN were limited to node classification in the early stage, many efforts <ref type="bibr">[5,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b11">25,</ref><ref type="bibr" target="#b12">26]</ref> have been made to use appropriately modified GCN as a joint training module. In the most closely related work to ours, Chen et al.</p><p>[8] applied GCN for multi-label classification by learning linear classifiers from the graph for projecting features into class labels. In that work, the word embeddings were used for the node feature matrix, and the label co-occurrence was measured to determine the values for the graph edges. Although the effectiveness of GCN has been proved in many other studies [9, <ref type="bibr" target="#b23">37,</ref><ref type="bibr" target="#b24">38]</ref>, applying GCN for a new task such as imagetext composition is not trivial. The main reason is that the GCN requires a high-quality graph. Building such mean- <ref type="figure">Figure 2</ref>. Alternative models are examined to justify our architecture design for the composition method. T and V indicate the representations extracted from the text and the image encoder, respectively. We conceptually illustrate the composition of layers for each blocks. The detailed composition is described in the appendix. ingful graphs is tricky because a proper definition of the graph nodes and a careful strategy for encoding the relationships between the nodes are needed. In this paper, we introduce the application of a GCN for the image-text composition task. We define the nodes as image-text pairs and assign higher correlations to nodes that have target images with high visual similarity. We explain our proposed approach in detail in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>This section introduces our proposed method called the Residual Text and Image Composer (RTIC). The goal of the image text composition task is to combine the source image I src and the text T by using a multi-modal composer ?(?, ?) to generate a desired representation close to the representation of the target image I trg . For this purpose, {I src , T , and I trg } triplets are required for supervision. Given a composition method such as RTIC, we denote the conventional training pipeline in which pairwise ranking loss (e.g., triplet loss) is applied on the ground-truth triplets as the mainstream. The proposed GCN-stream is an auxiliary module attached to the main-stream. Our approach is based on twostage training. In the first stage, the multi-modal composer, RTIC, is trained in the main-stream. Then, in the second stage, a graph is built using the trained RTIC to configure the GCN-stream and train the main-stream model jointly with the GCN-stream. The weights in the main-stream are initialized from a pre-trained composer or a scratch model for the composer. The advantage of training from scratch is that any type of composer different from the first stage can be chosen because the pre-trained composer is no longer required once the graph is constructed. On the other hand, more performance gain can be achieved when the weights of the pre-trained composer are transferred, while the com-poser architecture is restricted to be the same used at the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Text and Image Composer</head><p>RTIC is designed to learn the residual between the target and source image representations. The idea is that, given the images I src and I trg and the features v src and v trg of the source and the target, respectively, we consider the target v trg as an addition of the source v src and the residual h conditioned on the text feature t, which is represented as h = ?(v src ; t), where ? is a nonlinear mapping. Therefore, the final composed feature? is formulated as? = v src +h ?(v src ; t) = v trg , assuming that ?(?, ?) is an ideal composition module. In the RTIC, the residual h is considered as an error between the two different image features, v src and v trg , which is continuously distributed over the entire channel. The RTIC consists of three main components: a fusion block, an error encoding block, and a gating block. The fusion block F combines two modalities, t and v src , into a single representation x = F(v src , t). Then, x is fed to the error encoding block E and gating block G to obtain the final composed feature? as follows:</p><formula xml:id="formula_0">s = G(F(v src , t)) = G(x), X n = X n?1 + E n (X n?1 ), X 0 = x, v = (1 ? s) ? X n + s ? v src v trg ,<label>(1)</label></formula><p>where X n represents the output of the n-th error encoding block E n . Note that the output of the previous error encoding block X n?1 is added to the next output using the skip connection. <ref type="figure">Figure 2</ref> conceptually illustrates the flow to facilitate understanding. In all our experiments, n was set to four by default. For the next step, we follow the general pipeline of the other existing methods for the main-stream, in which a pairwise ranking loss is applied using? as the <ref type="figure">Figure 3</ref>. The pipeline for joint training with the GCN-stream. The blue region indicates the GCN module, which can be attached to any existing composition method in a plug-and-play manner. Note that the weights between the two composers in each stream are not shared. We use notations? for the GCN input and A for the batch sampling and pseudo-labeling.</p><p>anchor and v trg as the positive. Following <ref type="bibr" target="#b22">[36]</ref>, we use the DML loss (K=B) for the pairwise ranking loss. Although we observed that a better overall score was achieved using the batch hard triplet loss <ref type="bibr" target="#b3">[17]</ref>, we intentionally used the DML loss because such a classification-based objective can effectively suppress undesirable effects caused by the sampling strategy. We refer interested readers to <ref type="bibr" target="#b22">[36]</ref> for detailed definition of DML Loss. The structure finally chosen for the RTIC is architecture (d) in <ref type="figure">Figure 2</ref>. The details of our design choice are discussed in the later section.</p><p>In summary, RTIC has two main differences from TIRG. First, RTIC uses channel-wise linear interpolation between the source image feature v src and the residual h in which the sigmoid function is applied to the output of the gating block G to determine the ratio. Second, RTIC employs a deeper architecture for error encoding by stacking n learning blocks in series using skip connections. We demonstrate the potential of our idea by winning second place in the competition 2 . The following section explains our proposed graph-based approach, RTIC-GCN, a further improved version of RTIC. 2 https://sites.google.com/view/cvcreative2020/fashion-iq</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GCN as Good Regularizer</head><p>One challenge in the image-text composition is that an infinite number of perceptually acceptable image and text pairs exist, while the ground-truth triplets (I src , I trg , and T ) in the training set reflect only a tiny fraction of these pairs. Such a scarcity of training pairs causes poor generalization. To resolve this, we introduce a novel method of using a graph convolutional neural network (GCN) as a regularizer by propagating information between adjacent neighbors. Our intuition is that, because there exists an unlimited number of ways to describe I trg given I src , we consider {I src , T } pairs as a group that describes a visually similar target image I trg . The {I src , T } pairs in the same group are hence highly correlated with one another. For example, an image of a red coat with the text "it is blue" and an image of a blue shirt with the text "it is a coat" would both have an image of a blue coat as the target, which means that the two pairs are highly correlated. We model such relationships between pairs in the form of a graph by measuring the visual similarities between the target images of the triplets. We conceptually illustrate our intuition in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Revisiting GCN</head><p>Before moving into the details, we briefly review how a GCN works. The core idea in a GCN is that the node representation is updated by propagating information from the neighboring nodes based on graph-structured data. The goal of the GCN is to learn a nonlinear mapping f (?, ?) on the graph. The l-th layer of the GCN takes the node feature matrix H l ? R N ?D and the correlation matrix ? ? R N ?N as inputs, where N and D indicate the number of nodes and the dimensionality of the node features, respectively. The operation on each layer of the GCN can be represented a?</p><formula xml:id="formula_1">? = D 1 2 ?D ? 1 2 , H l+1 = h(?H l W l ),<label>(2)</label></formula><p>where D represents a diagonal matrix of ? and W l indicates the transformation matrix of the l-th layer to be learned. We used ReLU for the activation function h. For more details, we refer interested readers to <ref type="bibr" target="#b9">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Graph construction</head><p>Most importantly, we define each graph node as an imagetext pair, which means that the N ground-truth pairs in the training data create N graph nodes in total. Assuming that we have a learned image encoder ? pretrained trained in the first stage, we model the correlations between the nodes (image-text pairs) by measuring the cosine similarities between the target features of the corresponding nodes V = {v trg 1 , ..., v trg N }, where the target features of the i-th node are obtained by v trg i = ? pretrained (I trg i ). As a result, we obtain a symmetrical correlation matrix ? ? R N ?N because the cosine similarity measure has a commutative property, and the similarity with the self-node is always 1. Then, we binarize ? with the threshold ? to filter out the noisy edges:</p><formula xml:id="formula_2">? ij = 1, if ? i,j ? ? 0, if ? i,j &lt; ?,<label>(3)</label></formula><p>where ? is a hyperparameter set to the value that makes the average number of activated edges in each node 15% of N . Chen et al.</p><p>[8] addressed the over-smoothing problem that occurs when each node feature is repeatedly updated using its own features and those of the neighboring nodes by employing a re-weighting strategy. We follow the same scheme with a slight modification:</p><formula xml:id="formula_3">? ij = 1, if i = j 0.33/? N j=1,i =j ? ij , if i = j.<label>(4)</label></formula><p>Finally, ? is re-normalized:? = D 1 2 ? D ? 1 2 , as described in Equation 2. The full graph construction algorithm is given in Algorithm 1. </p><formula xml:id="formula_4">v src i ? ?(I src i ), t i ? ?(T ), v trg i ? ?(I trg i ) 6: H.append(v src i ? t i ) // node feature matrix 7:</formula><p>?.append(v trg ) // create feature DB 8: end for 9: ? ? cosine similarity(? ? ? T ) // correlation matrix 10: ? ? binarized(?) // thresholding 11: ? ? reweighted(? ) // to avoid over-smoothing 12:? ? normalized(? ) // to keep the scale 13: Output:</p><formula xml:id="formula_5">Normalized correlation matrix? ? R N ?N , node feature matrix H ? R N ?(D V +D T ) .</formula><p>Joint training with GCN-stream. Denoting the general training pipeline (e.g., training RTIC with ranking loss) as the main-stream, the GCN-stream can be defined as an auxiliary module that aims to improve the mainstream model by injecting additional signals in a semi-supervised manner. Once the graph is constructed, the GCN-stream can be configured and jointly trained with the main-stream. The composers in each stream in <ref type="figure">Figure 3</ref> are randomly initialized, and the graph is the only source of information that the model can benefit from during training. The GCN-stream is configured using the output correlation matrix? ? R N ?N and the node feature matrix H ? R N ?(D V +D T ) , where N is the number of {I src , T } pairs, and D V and D T are the output feature dimensions of the image and the text encoders, respectively. For the next step, the image and text feature matricesv ? R N ?D V ,t ? R N ?D T are split from H and fed to a randomly initialized new composer in the GCNstream that converts H into a matrix of composed features H ? R N ?D V . Note that the weights of the composers in each stream are not shared because the two composers will have different input distributions as the training proceeds.</p><p>Finally,H and? are forwarded by the GCN layers to obtain the GCN-stream output W ? R N ?D V . We inject information from the GCN-stream into the main-stream in the form of node classifications <ref type="bibr">[8]</ref>. Given a mini-batch B, we perform matrix multiplication between the output of the main-stream X = {? 1 , ..,? B } ? R B?D V and the output of the GCN-stream W ? R N ?D V to obtain the logits x = X ? W T ? R B?N . Then, we use the binary cross-entropy loss to perform a node classification on the N graph nodes. This process can be seen as finding the most relevant neighbors for a batch of queries among all the {I src , T } pairs that exist in the graph. For supervision, <ref type="table">Table 1</ref>. The performance evaluated on the Fashion-IQ. The scores in white-colored rows indicate the comparison between the different composition methods when trained and evaluated with our constrained environment. The scores with the optimal environment are brought from each method's paper to compare the best performance. ? mark indicates that the weights of the pre-trained composer are transferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average we sample B as the number of one-hot vectors c ? R B?N on the fly from ? according to the unique key assigned to the pairs in the mini-batch. The terminologies-batch sampling and pseudo labeling-in this process are represented in <ref type="figure">Figure 3</ref>. Finally, the loss for the joint training is obtained as the sum of the pairwise ranking loss in the mainstream and the binary classification loss in the GCN-stream, L f inal = ? pair ? L pair + ? bce ? L bce , where (? pair , ? bce ) = (1.0, 1.0) is used by default. Although the GCN-stream requires additional GPU memory, as shown in <ref type="figure" target="#fig_1">Figure 4</ref>, the GCN-stream is used only for the training and is removed for the inference. Importantly, it means that our method requires no additional model parameters for the inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>In this section, We mainly discuss our training environment. We also provide more explanations about the other trivial training settings in the appendix. While such preprocessings are not our primary focus, we claim that having an identical procedure is highly important in reproducing the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Training Environment</head><p>We train our model on two environments both: unified environment, optimal environment. For unified environment, we unified training conditions across the different composition methods. We keep each component (e.g., image/text encoder, loss function, image transform, etc.) in the pipeline as simple as possible to maximally suppress the misleading performance gain from parameter optimization. All the ablations in our experiments are performed in this environment. While such a constrained condition degrades the overall benchmark scores, it makes the results more reliable and objective for comparison. For example, we used the DML Loss <ref type="bibr" target="#b22">[36]</ref> for the ranking loss because it is independent of the sampling strategy. No initialization such as GloVe or BERT was used for the word embeddings because the effect of the initialization is highly coupled with the type of tokenizer used and the quality of the pre-trained word embeddings. We also used SGD with a fixed lr=0.01 for the optimizer with no parameter tuning because adjusting lr can increase the overall score significantly, as described in <ref type="table" target="#tab_4">Table 5</ref>. On the other hand, evaluating with unified environment only can overlook the fact that different methods need their optimization strategy. To remedy this, we also trained our model with optimal environment. The best parameters for training RTIC is found exhaustively and compared with the best scores of the other methods reported in their papers. We report the average score of eight trials for all our experiments to obtain more precise and more evident trends. The details of each training environment are described in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Evaluation</head><p>Comparison with the state-of-the-art. For evaluation, we measured the top-k recall (R@K) on different benchmarks with the unified environment. While following our settings led to relatively low scores, the composition methods can be compared fairly in this constrained environment. <ref type="table">Table 1</ref> and <ref type="table" target="#tab_1">Table 2</ref> show the results for the three benchmarks. We intentionally omitted some recent methods <ref type="bibr">[6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b5">19,</ref><ref type="bibr" target="#b25">39]</ref> because these works do not solely focus on the composition method itself but also on other aspects such as the image/text encoders and loss functions. Instead, we compared our method with other methods trained with our own best settings. Although this approach might not be sufficiently objective, it would still provide some naive insights. <ref type="table">Table 1</ref> shows that RTIC already achieves the highest score among the various methods and that joint training with the GCN-stream (RTIC-GCN) further improves the score by approximately 0.4%p. The scores for the methods marked by ? were obtained using the best settings for the methods reported in their respective papers. To achieve the state-of-the-art score, we gradually adjusted the detailed training parameters, as described in <ref type="table" target="#tab_4">Table 5</ref>. We did not employ trivial tricks that might improve the score, such as aggregating multiple local features or concatenating the intermediate features, because such tricks excessively increase the memory usage and model complexity. Nevertheless, we achieved a 2.19%p improvement compared to the previous state-of-the-art. Architecture verification. In <ref type="table" target="#tab_2">Table 3</ref>, we verify our architecture design by comparing the possible variants of the RTIC structure and analyzing the effect of each component. The best structure is (d), which consistently shows the best score, followed by (a) with a marginal gap. The main feature of architecture (d) is the use of a channel-wise gating mechanism that performs a linear interpolation between the image feature and the residual. Interestingly, the performance drops significantly without the skip connections inside the error encoding stack ((a) vs. (b)). The score degradation caused by removing the skip connections from the visual representation is relatively small ((a) vs. (c)). Adjusting the latent space of the image by attaching an additional projection layer is not helpful ((a) vs. (e)). Finally, (d) was chosen for the final architecture design of the RTIC. Effect of graph quality. We examine the effect of the graph quality on the joint training with the GCN-stream in Table 4. Assuming that the pre-trained RTIC can construct more precise graphs than the other methods based on the results in <ref type="table">Table 1</ref>, we compare the improvements of four existing methods trained with the GCN-stream (TIRG-GCN, MRN-GCN, ComposeAE-GCN, Param Hashing-GCN) using the graphs constructed by the methods themselves and those constructed by the RTIC. All the methods benefitted from training with the GCN-stream, which implies that the GCN-stream is a sufficiently general method that can be attached to any composition module. Moreover, the result shows that a higher graph quality consistently results in a more considerable gain, which implies that the singlemodel score can be improved with higher quality graphs. Ablation study. This section describes the optimization process used for the challenge step by step and discusses how the trivial modification of training parameters can affect the final score. The results are presented in <ref type="table" target="#tab_4">Table 5</ref>. Based on the initial baseline score of 33.24, the singlemodel performance could be improved by up to 13% by simply optimizing the training parameters. Following our optimization, we obtained a score of 37.18 for TIRG, which outperforms the more recent methods. It strongly implies that trivial training details can significantly affect the overall performance. The result strongly highlights the importance of removing such undesired effects for a fair comparison. Trade off. Although training with the GCN-stream is a well-generalized technique that can consistently improve the score of any existing method as shown in <ref type="table" target="#tab_3">Table 4</ref>, backpropagating gradients through the GCN layers requires additional GPU memory for training. <ref type="figure" target="#fig_1">Figure 4</ref> shows the additional GPU memory required for joint training with the GCN-stream. We gradually increased the number of graph nodes and measured the relative increase in GPU memory used when the GCN-stream was attached to the mainstream. We report the relative rather than the absolute increases because the latter vary with the batch size and network complexity. In the inference phase, we no longer require the GCN layers for inference, which means that the GCN-stream does not slow down the inference speed at all. <ref type="figure" target="#fig_2">Figure 5</ref> illustrates the t-SNE <ref type="bibr" target="#b21">[35]</ref> visualization of the error-encoding block outputs from the RTIC. We sample 250 images from each of the eight different color classes (yellow, black, red, blue, green, brown, orange, pink) and condition the images with the different class labels (e.g., blue image with the text "yellow") instead of the captions for the query. We then extract the output of the errorencoding block and visualize the distribution. Note that images in the same class could have different attributes. For example, the images in the "blue" class will have the same color but different patterns (e.g., striped, dotted, or floral). Nevertheless, <ref type="figure" target="#fig_2">Figure 5</ref> shows that the clusters are clearly formed, which indicates that the error encoding block successfully disentangles the other attributes from the representation when it is conditioned on the specific color-related text in the query. We also performed the same experiments with eight different pattern classes and observed consistent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel architecture, RTIC, specifically designed for the image-text composition task. We also introduced a novel graph-based regularization technique attached to any composer in a plug-and-play manner. The joint training with the GCN-stream is a well-generalized approach in that it consistently improves the scores of existing composition methods. Finally, we achieved a state-of-the-art score on the image-text composition task with the optimal environment without exploiting any ensemble-like tricks such as aggregating features from the different stages or combining multiple loss functions. We also provide a fair and objective comparison between the composition methods by training models with unified environment.</p><p>for flexible fashion search. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7708-7717, 2018. </p><formula xml:id="formula_6">Concat (dim=1) D T , D V D T + D V BatchNorm1d D T + D V D T + D V LeakyReLU D T + D V D T + D V Linear D T + D V D V</formula><p>Error encoding block (E)</p><formula xml:id="formula_7">Linear D V D V / 2 BatchNorm1d D V / 2 D V / 2 LeakyReLU D V / 2 D V / 2 Linear D V / 2 D V / 2 BatchNorm1d D V / 2 D V / 2 LeakyReLU D V / 2 D V / 2 Linear D V / 2 D V Gating block (G) Linear D V D V BatchNorm1d D V D V LeakyReLU D V D V Linear D V D V Sigmoid D V D V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Datasets</head><p>Fashion-IQ <ref type="bibr" target="#b1">[15]</ref> is a fashion product retrieval dataset that consists of images crawled from amazon.com and natural language-based descriptions of one or more visual characteristics relating to the source and target images. It contains three categories, namely, dresses, toptees, and shirts. Following the same protocol as <ref type="bibr" target="#b1">[15]</ref>, we used 46,609 images and 18,000 triplets for training, and 5,373 images and 6,016 triplets for evaluation.</p><p>Shoes <ref type="bibr">[14]</ref> is a dialog-based interactive retrieval dataset in which the images were originally crawled from like.com and additionally tagged with natural language-based relative captions. Following <ref type="bibr">[14]</ref>, we used 10,000 images and 8,990 triplets for training, and 4,658 images and 1,761 triplets for evaluation.</p><p>Birds-to-Words [11] consists of 3,520 bird images from iNaturalist along with human-annotated natural language captions that describe the differences between pairs of images. Each text description is longer (average of 31 words) than the previous two datasets and has a more detailed description. We used 2,835 images and 12,805 triplets for training and 361 images and 1,556 triplets for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning Curves</head><p>We compare the learning curves of RTIC when it is trained w/ or w/o the GCN stream. The result is shown in <ref type="figure" target="#fig_3">Figure 6</ref>. The graph shows that the GCN stream can achieve a clear performance improvement against the other. Such tendency is consistent in that similar results are also observed using the other composition methods (e.g., TIRG). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Image and Text Representation</head><p>Although the main focus of our work is on the composition of the two modalities, we observe that even a minor modification of the text and the image encoders dramatically affects the final performance. Therefore, we briefly describe our settings for learning the text and image representations.</p><p>For text representations, we learned sentence embedding from scratch using LSTM <ref type="bibr" target="#b4">[18]</ref>. Specifically, we tokenized the captions using a simple whitespace tokenizer and then built a word-to-vector vocabulary. Although applying GloVe <ref type="bibr" target="#b16">[30]</ref> initialization on the word vectors can improve the result, we intentionally omitted the use of this technique and retained our training configuration as the baseline. The 1-layered LSTM forwarded the word embeddings, and then the outputs were max-pooled to obtain the sentence embedding. Finally, the outputs were projected onto the desired dimensions using a fully connected layer.</p><p>Global average pooling (GAP) was applied to the output of the last convolutional layer, followed by a fully connected layer. ResNet-50 <ref type="bibr" target="#b2">[16]</ref> was used as the backbone for the image encoder. We did not use a shallower backbone such as ResNet-18 because this made the performance difference between the composition methods relatively trivial, mainly due to the limited network capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Study on Other Possible Variants</head><p>Our proposed GCN stream is a semi-supervised technique based on the structured graph. Since there exist various ways to utilize pseudo labels for training, we explore if other semi-supervised learning techniques can be employed for this task. We examine two different baselines to compare with GCN stream: Linear+BCE and Pseudo Pairs. Linear+BCE. Since the goal of the GCN stream is to learn a projection matrix W ? R D V ?N , it is equivalent to learning a single-layer perceptron with no bias. Therefore, one can attach a linear layer instead of GCN layers behind the main-stream and use binary cross-entropy (BCE) loss with pseudo labels sampled from A as the GCN stream. Pseudo Pairs. Using the graph, we can also generate pseudo {I src , T, I trg } pairs. The idea is that if two pairs have visually similar target image which has high cosine similarity above a certain threshold, we switch the I trg in two pairs. In this way, we double the number of {I src , T, I trg } pairs in the train split of Fashion-IQ and train RTIC in the main-stream without GCN stream.</p><p>The result is shown in <ref type="table" target="#tab_6">Table 7</ref>. The result shows that both Linear+BCE and Pseudo Pairs do not help training compared to the baseline that does not use any semi-supervised techniques. Only the GCN stream improves the result by 0.4%p. It implies that simply applying BCE loss as auxiliary supervision does not improve the score (Linear+BCE). Moreover, automatically generated pseudo pairs should be carefully considered because the noise degrades the score significantly (Pseudo Pairs). This experiment shows that the performance gain due to the GCN stream originated from the use of binary classification loss for auxiliary supervision and a carefully designed GCN module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis on Feature Distribution</head><p>No weight sharing between composers. The two composers in main-stream and GCN stream do not share the weights during the training. It is because the input for the GCN stream, H, is static values while the input for the main-stream, which is the representations learned by the image and text encoders, is updated dynamically as the training proceeds. Such discrepancy between the input distributions of two composers degrades the performance if the weights are shared. Although we train two composers independently, the GPU memory required for training trivially increases because the composer has a small number of parameters. Distribution before and after GCN layers. <ref type="figure">Figure 7</ref> shows the PCA analysis on the feature distribution in the GCN stream before and after it is forwarded by GCN layers. The GCN layer converts the original feature distribution (first row) extracted by the image and text encoders trained in the first stage into the more structured form of distribution (second row), which implies the features are projected to be more distinguishable from each other. As the training proceeds, the features after the GCN layers are spread into space in a broader range.</p><p>H. Naive comparison with feature fusion methods.</p><p>We apply RTIC to the feature fusion task in this section. The <ref type="table">Table 8</ref> shows the result on widely used benchmarks, VQA 2.0 [13] and VRD <ref type="bibr" target="#b14">[28]</ref>. We followed the training and testing environment with default parameters provided by the publicly released code 3 . The result shows that the methods designed for the image text composition task (TIRG and RTIC) are less effective for the feature fusion task (BLOCK vs. RTIC). Our interpretation is that since the image text composition methods are forced to embed the composed feature into the image feature space, it is considered as a redundant constraint in the feature fusion task, which degrades the performance. <ref type="figure">Figure 7</ref>. PCA visualization of the feature distribution in the GCN stream for each epoch. The first and second rows represent the feature distribution in the GCN stream before and after it is forwarded by GCN layers respectively. <ref type="table">Table 8</ref>. PP: Phrase Prediction, PD: Phrase Detection, RD: Relationship Detection task. The comparison with the feature fusion methods on two benchmarks (VQA 2.0 [13] and VRD <ref type="bibr" target="#b14">[28]</ref>). We measure the accuracy for VQA 2.0 and R@50 for VRD. I. Score Inconsistency across Papers <ref type="table" target="#tab_8">Table 10</ref> shows the inconsistency in reported scores across papers. We mainly investigate the score of TIRG <ref type="bibr" target="#b22">[36]</ref> because it is the most widely used method for comparison in various studies. We found a significant gap between the reported scores, which is the reason we address the need for a unified environment for training to ensure objectiveness and fairness in performing comparisons across papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VRD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Unified Training Environment</head><p>In <ref type="table" target="#tab_9">Table 9</ref>, we suggest a baseline for training to accomplish a fair comparison. We keep the image and text encoder as simple as possible while keeping the complexity of encoders high enough to represent the visual and contextual information. The image is resized to 256 ? 256 with padding while keeping the ratio, then cropped to 224 ? 224. We use DML loss (K=B) suggested in <ref type="bibr" target="#b22">[36]</ref>, mainly because it can suppress undesirable effects caused by the sampling strategy. According to the technical reports of the retrieval challenge, spell correction increases the result significantly. Moreover, initializing word embeddings with GloVe or BERT provides better generalization when the embeddings are fine-tuned. However, we omit such auxiliary techniques to keep our training environment simple. We strongly suggest using the metrics averaged over eight trials because the scores severely fluctuate even between models trained with the same training parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Qualitative Result</head><p>The image retrieval result of the top 8 candidates is illustrated in <ref type="figure" target="#fig_4">Figure 8</ref>. The examples show that even though a ground-truth I src T, I trg pair is defined, the text can not fully describe the fundamental aspects of the target image. Therefore, the answer can be multiple similarly-looking items that are perceptually acceptable. Ideally, the model is guided to obtain clues for the other attributes that are not described by the text based on the source image. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Graph construction algorithm for encoding the contextual similarity between the nodes represented by the image-text pairs 1: Input: Image encoder ?, text encoder ?, N triplets of image and text X = {(I src i , T i , I trg i ) : i ? (1, ..., N )}. 2: (H, ?) ? ([], []) 3: for i = 1 to N do 4: // extract image and text representations 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Additional GPU memory required for GCN-stream at training phase. It is worth noting that the proposed GCN-stream requires no additional GPU memory at inference phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5</head><label>5</label><figDesc>. t-SNE visualization of the feature distribution outputted by RTIC's error encoding block (E).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Learning curves of RTIC and RTIC-GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Top-8 retrieval examples of RITC, TIRG, and Parameter Hashing. The blue and the black border indicate the query and the ground-truth respectively. The red border means the answer is correctly found. The orange border is means the image is not the answer but perceptually acceptable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.58 32.83 ? 0.66 15.48 ? 0.53 39.13 ? 0.52 17.66 ? 0.39 41.89 ? 0.68 MRN [21] 28.83 ? 0.18 14.50 ? 0.43 35.43 ? 0.39 16.60 ? 0.47 40.50 ? 0.76 20.22 ? 0.55 45.74 ? 0.26 FiLM [31] 28.72 ? 0.32 13.56 ? 0.33 35.64 ? 0.94 16.78 ? 1.04 40.77 ? 0.98 19.91 ? 0.62 45.64 ? 0.60 TIRG [36] 30.71 ? 0.25 16.12 ? 0.39 37.69 ? 0.70 19.15 ? 0.61 43.01 ? 0.91 21.21 ? 0.70 47.08 ? 0.49 ComposeAE [2] 19.61 ? 0.41 9.96 ? 0.60 25.14 ? 0.88 10.77 ? 0.70 28.29 ? 0.49 12.74 ? 0.67 30.79 ? 0.62 VAL [7] 27.16 ? 0.24 13.62 ? 0.59 33.81 ? 0.95 16.03 ? 0.58 39.07 ? 0.64 18.02 ? 0.52 42.40 ? 0.44 Block [4] 27.84 ? 0.34 13.67 ? 0.69 35.35 ? 0.77 16.01 ? 0.54 39.61 ? 0.56 18.39 ? 0.74 44.03 ? 0.58 Mutan [3] 29.34 ? 0.22 15.20 ? 0.82 36.17 ? 0.42 17.46 ? 0.56 42.14 ? 0.46 20.04 ? 0.62 45.03 ? 0.62 MLB [22] 27.12 ? 0.49 13.30 ? 0.44 33.16 ? 0.43 15.33 ? 0.50 39.71 ? 0.82 17.75 ? 0.40 43.51 ? 1.10 MFB [40] 27.98 ? 0.21 13.94 ? 0.67 34.37 ? 0.38 16.60 ? 0.58 40.36 ? 0.90 18.36 ? 0.56 44.24 ? 0.65 MFH [41] 27.96 ? 0.27 13.87 ? 0.59 35.04 ? 0.22 15.95 ? 0.61 40.16 ? 0.84 18.52 ? 0.63 44.21 ? 0.75 MCB [12] 29.28 ? 0.29 14.77 ? 0.75 36.34 ? 0.48 17.43 ? 0.57 41.69 ? 0.63 19.96 ? 0.75 45.46 ? 1.01</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Shirt</cell><cell></cell><cell>Dress</cell><cell></cell><cell>Toptee</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(R@10 + R@50) / 2</cell><cell>R@10</cell><cell>R@50</cell><cell>R@10</cell><cell>R@50</cell><cell>R@10</cell><cell>R@50</cell></row><row><cell cols="4">Training with unified environment (objective comparison)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Param Hashing [29] 12.26 Training with optimal environment (best performance) 26.54 ? 0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JVSM [6]</cell><cell></cell><cell>19.26</cell><cell>12.00</cell><cell>27.10</cell><cell>10.70</cell><cell>25.90</cell><cell>13.00</cell><cell>26.90</cell></row><row><cell cols="2">TRACE w/ BERT [19]</cell><cell>34.38</cell><cell>20.80</cell><cell>40.80</cell><cell>22.70</cell><cell>44.91</cell><cell>24.22</cell><cell>49.80</cell></row><row><cell>VAL w/ GloVe [7]</cell><cell></cell><cell>35.38</cell><cell>22.38</cell><cell>44.15</cell><cell>22.53</cell><cell>44.00</cell><cell>27.53</cell><cell>51.68</cell></row><row><cell cols="2">CIRPLANT w/ OSCAR [27]</cell><cell>30.20</cell><cell>17.53</cell><cell>38.81</cell><cell>17.45</cell><cell>40.41</cell><cell>21.64</cell><cell>45.38</cell></row><row><cell>MAAF [10]</cell><cell></cell><cell>36.60</cell><cell>21.30</cell><cell>44.20</cell><cell>23.80</cell><cell>48.60</cell><cell>27.90</cell><cell>53.60</cell></row><row><cell>CurlingNet [39]</cell><cell></cell><cell>38.45</cell><cell>21.45</cell><cell>44.56</cell><cell>26.15</cell><cell>53.24</cell><cell>30.12</cell><cell>55.23</cell></row><row><cell>CoSMo [24]</cell><cell></cell><cell>39.45</cell><cell>24.90</cell><cell>49.18</cell><cell>25.64</cell><cell>50.30</cell><cell>29.21</cell><cell>57.46</cell></row><row><cell>RTIC w/ GloVe</cell><cell></cell><cell>39.22</cell><cell>22.81</cell><cell>45.97</cell><cell>27.81</cell><cell>53.15</cell><cell>30.04</cell><cell>55.53</cell></row><row><cell cols="2">RTIC-GCN w/ GloVe</cell><cell>39.55</cell><cell>23.26</cell><cell>45.68</cell><cell>27.96</cell><cell>52.70</cell><cell>29.98</cell><cell>57.73</cell></row><row><cell>RTIC-GCN w/ GloVe</cell><cell>?</cell><cell>40.64</cell><cell>23.79</cell><cell>47.25</cell><cell>29.15</cell><cell>54.04</cell><cell>31.61</cell><cell>57.98</cell></row></table><note>RTIC 31.28 ? 0.22 16.93 ? 0.45 38.36 ? 0.61 19.40 ? 0.50 43.51 ? 0.90 21.58 ? 0.70 47.88 ? 0.90 RTIC-GCN 31.68 ? 0.30 16.95 ? 0.59 38.67 ? 0.74 19.79 ? 0.41 43.55 ? 0.24 21.97 ? 0.71 49.11 ? 0.87</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Benchmark scores on Shoes and Birds-to-Words dataset.<ref type="bibr" target="#b22">36</ref>.37 ? 0.47 67.24 ? 0.88 27.54 ? 1.57 58.43 ? 1.59 MRN 37.51 ? 0.47 68.66 ? 0.75 37.33 ? 1.38 68.97 ? 1.35 FiLM 36.74 ? 0.56 68.47 ? 0.43 32.58 ? 1.59 65.80 ? 1.48 TIRG 40.40 ? 0.69 70.65 ? 0.65 33.83 ? 1.53 65.96 ? 1.56 ComposeAE 31.25 ? 0.67 60.30 ? 0.33 29.60 ? 0.99 59.82 ? 1.83 VAL 36.91 ? 0.75 67.16 ? 0.73 28.65 ? 1.48 60.22 ? 1.31 Block 36.82 ? 0.61 67.79 ? 0.51 26.19 ? 1.63 57.89 ? 1.37 Mutan 39.13 ? 0.73 69.86 ? 0.53 32.15 ? 1.83 62.36 ? 1.59 MLB 35.35 ? 0.88 67.30 ? 0.57 30.93 ? 1.77 63.22 ? 1.72 MFB 36.59 ? 1.54 67.58 ? 0.49 30.43 ? 1.92 64.03 ? 1.69 MFH 35.85 ? 1.20 68.07 ? 1.07 31.11 ? 2.18 64.18 ? 1.93 MCB 38.70 ? 0.66 68.96 ? 0.69 30.86 ? 1.44 62.80 ? 2.12 RTIC 43.66 ? 0.67 72.11 ? 0.51 37.40 ? 1.36 66.97 ? 1.70</figDesc><table><row><cell>Method</cell><cell>R@10</cell><cell>Shoes</cell><cell>R@50</cell><cell>Birds-to-Words R@10 R@50</cell></row><row><cell>Param Hashing</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>RTIC-GCN 43.38 ? 0.88 72.09 ? 0.48 37.56 ? 1.12 67.72 ? 1.46</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>30.44 ? 0.23 55.43 ? 0.60 48.38 ? 0.40 (d) 31.41 ? 0.41 57.73 ? 0.45 52.24 ? 1.06 (e) 30.02 ? 0.25 55.81 ? 0.54 48.42 ? 1.55</figDesc><table><row><cell cols="3">Comparison between alternative models (a-e) on image-</cell></row><row><cell cols="3">text composition task. The compared structures are illustrated in</cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(R@10 + R@50) / 2</cell></row><row><cell>Fashion IQ</cell><cell>Shoes</cell><cell>Birds-to-Words</cell></row><row><cell cols="2">(a) 31.28 ? 0.22 57.58 ? 0.40</cell><cell>51.73 ? 0.49</cell></row><row><cell cols="2">(b) 24.71 ? 0.69 53.07 ? 0.51</cell><cell>46.21 ? 1.84</cell></row><row><cell>(c)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The performance boost evaluated on Fashion-IQ when trained w/ or w/o the GCN-stream. w/ GCN, Self and w/ GCN, RTIC indicate that the graph of the GCN-stream is constructed using the compared method itself and using the RTIC, respectively.</figDesc><table><row><cell>Method</cell><cell>-</cell><cell cols="2">w/ GCN, Self w/ GCN, RTIC</cell><cell>max.?</cell></row><row><cell>TIRG</cell><cell>30.71</cell><cell>31.26</cell><cell>31.39</cell><cell>+2.21%</cell></row><row><cell>MRN</cell><cell>28.83</cell><cell>28.92</cell><cell>29.28</cell><cell>+1.56%</cell></row><row><cell>ComposeAE</cell><cell>19.61</cell><cell>29.70</cell><cell>29.43</cell><cell>+33.97%</cell></row><row><cell cols="2">Param Hashing 26.54</cell><cell>26.86</cell><cell>26.89</cell><cell>+1.31%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The variation of the single-model performance with different training settings. The optimizer (from SGD to AdamW), the text encoder (from LSTM to 2-layered LSTM+GRU), and the embedding feature dimension of the composed feature (from 512 to 2048) are changed gradually to maximize the final score.</figDesc><table><row><cell>Training details</cell><cell>Validation</cell></row><row><cell>Baseline (RTIC)</cell><cell>33.24</cell></row><row><cell>+ Replace optimizer</cell><cell>34.89 (+4.96%)</cell></row><row><cell cols="2">+ Increase embedding dimension 36.79 (+5.45%)</cell></row><row><cell>+ Replace text encoder</cell><cell>37.72 (+2.47%)</cell></row><row><cell>+ Spell correction</cell><cell>38.22 (+1.33%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Muhammad Umer Anwaar, Egor Labintcev, and Martin Kleinsteuber. Compositional learning of image-text query for image retrieval. In Proceedings of the IEEE/CVF Win-Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Schmidt Feris. Dialog-based interactive image retrieval. arXiv preprint arXiv:1805.00145, 2018. 11</figDesc><table><row><cell cols="4">1 [14] A. Notation</cell><cell>Vision and Pattern Recognition, pages 6904-6913, 2017. 12, 13</cell></row><row><cell cols="4">Symbol/Notation I src I trg T [2] ter Conference on Applications of Computer Vision, pages Definition Source image Target image Text description ?(?, ?) Text-Image multi-modal composer 1140-1149, 2020. 1, 2, 6, 13 v src Visual feature extracted from source image</cell></row><row><cell cols="4">[3] Hedi Ben-Younes, R?mi Cadene, Matthieu Cord, and Nico-v trg Visual feature extracted from target image</cell></row><row><cell cols="4">las Thome. Mutan: Multimodal tucker fusion for visual ?(?, ?), f (?, ?) Non-linear mapper</cell></row><row><cell cols="4">v question answering. In Proceedings of the IEEE interna-Final composed feature F Fusion block tional conference on computer vision, pages 2612-2620, E Error encoding block 2017. 2, 6 G Gating block</cell></row><row><cell cols="4">[4] Hedi Ben-Younes, Remi Cadene, Nicolas Thome, and n Number of error encoding blocks in stack</cell></row><row><cell cols="4">Matthieu Cord. Block: Bilinear superdiagonal fusion for X n Output of the n-th error encoding block in stack</cell></row><row><cell cols="4">visual question answering and visual relationship detection. K Number of negative samples in mini-batch</cell></row><row><cell cols="4">B In Proceedings of the AAAI Conference on Artificial Intelli-Batch size H Node feature matrix gence, volume 33, pages 8102-8109, 2019. 2, 6 ? Correlation matrix [5] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, N Number of nodes</cell></row><row><cell cols="4">Danny Z Chen, and Jian Wu. A hierarchical graph network D V Dimensionality of visual features</cell></row><row><cell cols="4">for 3d object detection on point clouds. In Proceedings of D T Dimensionality of text features</cell></row><row><cell cols="4">W the IEEE/CVF Conference on Computer Vision and Pattern Transformation matrix V Set of target image features Recognition, pages 392-401, 2020. 2 ? Text encoder [6] Yanbei Chen and Loris Bazzani. Learning joint visual se-? Image encoder</cell></row><row><cell cols="4">mantic matching embeddings for language-guided retrieval. ? Binarized correlation matrix</cell></row><row><cell cols="4">ECCV, 2020. 1, 2, 6, 7, 13 ? Threshold for correlation matrix binarization</cell></row><row><cell cols="4">[7] Yanbei Chen, Shaogang Gong, and Loris Bazzani. Image ? Re-weighted correlation matrix</cell></row><row><cell cols="4">? search with text feedback by visiolinguistic attention learn-Re-normalized correlation matrix ? Collection of image features ing. In Proceedings of the IEEE/CVF Conference on Com-X Output of main stream puter Vision and Pattern Recognition, pages 3001-3011, W Output of GCN stream</cell></row><row><cell>2020. 1, 2, 6, 7, 13 c</cell><cell>One hot vector</cell><cell></cell></row><row><cell cols="4">[8] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen</cell></row><row><cell cols="4">Guo. Multi-label image recognition with graph convolu-</cell></row><row><cell cols="4">tional networks. In Proceedings of the IEEE/CVF Confer-B. Architecture</cell></row><row><cell cols="4">ence on Computer Vision and Pattern Recognition, pages</cell></row><row><cell>5177-5186, 2019. 2, 5</cell><cell></cell><cell></cell></row><row><cell cols="4">[9] Ke Cheng, Yifan Zhang, Xiangyu He, Weihan Chen, Jian Table 6. Detailed configuration for RTIC architecture.</cell></row><row><cell cols="4">Cheng, and Hanqing Lu. Skeleton-based action recognition</cell></row><row><cell cols="4">with shift graph convolutional network. In Proceedings of Module Operation Input dim. Output dim.</cell></row><row><cell cols="4">the IEEE/CVF Conference on Computer Vision and Pattern</cell></row><row><cell cols="2">Recognition, pages 183-192, 2020. 2 Fusion block (F)</cell><cell></cell></row><row><cell cols="4">[10] Eric Dodds, Jack Culpepper, Simao Herdade, Yang Zhang,</cell></row><row><cell>and Kofi Boakye.</cell><cell cols="3">Modality-agnostic attention fusion</cell></row><row><cell cols="2">for visual search with text feedback.</cell><cell cols="2">arXiv preprint</cell></row><row><cell cols="2">arXiv:2007.00145, 2020. 6</cell><cell></cell></row><row><cell cols="4">[11] Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma,</cell></row><row><cell>and Serge Belongie.</cell><cell cols="2">Neural naturalist:</cell><cell>generat-</cell></row><row><cell cols="2">ing fine-grained image comparisons.</cell><cell cols="2">arXiv preprint</cell></row><row><cell cols="2">arXiv:1909.04101, 2019. 11</cell><cell></cell></row><row><cell cols="4">[12] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach,</cell></row><row><cell cols="4">Trevor Darrell, and Marcus Rohrbach. Multimodal com-</cell></row><row><cell cols="4">pact bilinear pooling for visual question answering and vi-</cell></row><row><cell cols="4">sual grounding. arXiv preprint arXiv:1606.01847, 2016. 2,</cell></row><row><cell>6</cell><cell></cell><cell></cell></row><row><cell cols="4">[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-</cell></row><row><cell cols="4">tra, and Devi Parikh. Making the v in vqa matter: Elevating</cell></row><row><cell cols="4">the role of image understanding in visual question answer-</cell></row><row><cell cols="4">ing. In Proceedings of the IEEE Conference on Computer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison with other semi-supervised learning techniques. We measure (R@10 + R@50) / 2 for the score.</figDesc><table><row><cell>Method</cell><cell>Score</cell></row><row><cell>Baseline</cell><cell>31.28 ? 0.22</cell></row><row><cell cols="2">Linear+BCE 27.44 ? 0.40</cell></row><row><cell cols="2">Pseudo Pairs 23.73 ? 0.18</cell></row><row><cell cols="2">GCN stream 31.68 ? 0.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>TIRG scores reported across papers.</figDesc><table><row><cell cols="4">(R@10+R@50)/2 Fashion-IQ Fashion200K Shoes</cell></row><row><cell>[7]</cell><cell>27.39</cell><cell>53.90</cell><cell>57.42</cell></row><row><cell>[2]</cell><cell>20.15</cell><cell>53.15</cell><cell>-</cell></row><row><cell>[19]</cell><cell>19.95</cell><cell>-</cell><cell>38.79</cell></row><row><cell>[6]</cell><cell>15.27</cell><cell>51.95</cell><cell>-</cell></row><row><cell>max.?</cell><cell>12.13</cell><cell>1.95</cell><cell>18.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>The detailed configuration of unified environment Configuration Image Encoder ResNet50 (pretrained on ImageNet, output shape: 32 x 1024, avg-pooled) Text Encoder LSTM (num layers: 1, input size: 1024, hidden size: 1024, output shape: 32 x 1024, max-pooled)</figDesc><table><row><cell>Image Size / Crop Size</cell><cell cols="2">256 / 224</cell><cell></cell></row><row><cell>Image Transform (Train)</cell><cell cols="3">ResizeWithPadding, RandomCrop, RandomHorizontalFlip, RandomAffine, Normalize</cell></row><row><cell>Image Transform (Test)</cell><cell cols="3">ResizeWithPadding, CenterCrop, Normalize</cell></row><row><cell>Optimizer</cell><cell>SGD</cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell cols="3">DML loss, K = B [36]</cell></row><row><cell>Training Epochs</cell><cell>100</cell><cell></cell><cell></cell></row><row><cell>Learning rate Learning rate decay</cell><cell>0.01 policy: StepLR, factor: 1/</cell><cell>?</cell><cell>2, step size: 10</cell></row><row><cell>Others</cell><cell cols="3">No spell correction, No initialization with GloVe</cell></row><row><cell>Metric</cell><cell cols="3">Average Recall@K over 8 trials</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/Cadene/block.bootstrap.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo Hwee</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><forename type="middle">Yew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tham</surname></persName>
		</author>
		<title level="m">Learning attribute representations with localization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12794</idno>
		<title level="m">Fashion iq: A new dataset towards retrieving images by natural language feedback</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Trace: Transform aggregate and compose visiolinguistic representations for image search with text feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surgan</forename><surname>Jandial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinkesh</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranit</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausoom</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01485</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11509</idno>
		<title level="m">Cycled compositional learning between images and text</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal residual learning for visual qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cosmo: Content-style modulation for image retrieval with text feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="802" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Guided similarity separation for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chundi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satya Krishna</forename><surname>Gorti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Od-gcn: Object detection boosted by knowledge gcn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zidong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image retrieval on real-life images with pretrained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2125" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image question answering using convolutional neural network with dynamic parameter prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fashioniq 2020 challenge 2nd place team&apos;s solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonjae</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongwuk</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06404</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semisupervised feature-level attribute manipulation for fashion image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minchul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05007</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.2944</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval-an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">L2-gcn: Layer-wise and learned efficient training of graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2127" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12299</idno>
		<title level="m">Curlingnet: Compositional learning between images and text for fashion iq data</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memory-augmented attribute manipulation networks for interactive fashion search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
