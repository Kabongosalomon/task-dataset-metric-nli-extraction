<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Morley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename></persName>
						</author>
						<title level="a" type="main">Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks on 3D point cloud data have been widely used in the real world, especially in safety-critical applications. However, their robustness against corruptions is less studied. In this paper, we present ModelNet40-C, the first comprehensive benchmark on 3D point cloud corruption robustness, consisting of 15 common and realistic corruptions. Our evaluation shows a significant gap between the performances on ModelNet40 and ModelNet40-C for state-of-the-art (SOTA) models. To reduce the gap, we propose a simple but effective method by combining PointCutMix-R and TENT after evaluating a wide range of augmentation and testtime adaptation strategies. We identify a number of critical insights for future studies on corruption robustness in point cloud recognition. For instance, we unveil that Transformer-based architectures with proper training recipes achieve the strongest robustness. We hope our in-depth analysis will motivate the development of robust training strategies or architecture designs in the 3D point cloud domain. Our codebase and dataset are included in https://github. com/jiachens/ModelNet40-C.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point clouds are one of the most acknowledged data format in 3D computer vision tasks, as they are inherently flexible representations and can be retrieved from a variety of sensors and computer-aided design (CAD) models. Because of these strengths, point clouds have been increasingly utilized in real-world applications, particularly in safety-critical areas like self-driving cars <ref type="bibr" target="#b64">(Yin et al., 2021)</ref>, robotics <ref type="bibr" target="#b42">(Pomerleau et al., 2015)</ref>, medical imaging , and virtual and augmented reality (VR/AR) <ref type="bibr" target="#b36">(Maloca et al., 2018)</ref>. Processing of point clouds is thus crucial un-Paper Under Review. der these circumstances. For instance, autonomous vehicles rely on correct recognition of LiDAR point clouds to perceive the surroundings. Similar to 2D image classification, recent efforts demonstrate that deep learning models has dominated the point cloud recognition task.</p><p>As opposed to stellar progress on model architectures in 2D computer vision, deep 3D point cloud recognition is emerging where various architectures and operations are being proposed. Classic approaches discretize the point cloud into 3D cells, which causes cubic complexity. PointNet <ref type="bibr" target="#b43">(Qi et al., 2017a)</ref> innovates to achieve end-to-end learning on point clouds. A few studies optimize the convolutional operation to be preferable for 3D point cloud learning <ref type="bibr" target="#b35">Liu et al., 2019b)</ref>. Transformer <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> blocks are also applied as backbones in point cloud recognition architectures . The most extensively utilized benchmark for comparing methods for point cloud recognition is ModelNet40 <ref type="bibr" target="#b61">(Wu et al., 2015)</ref>. Although the accuracy on ModelNet40 over the past several years has been steadily improved, it merely shows a single perspective of model performance on the clean data. Given the importance of 3D point cloud in the safety-critical application, a comprehensive robustness benchmark for point cloud recognition models is necessary.</p><p>In the literature, the vast majority of research on robustness in 3D point cloud recognition has concentrated on the critical difficulties of robustness against adversarial examples. Adversarial training has been adapted to defend against various threats to point cloud learning <ref type="bibr">(Sun et al., 2020b;</ref><ref type="bibr" target="#b48">2021a)</ref>. However, we find that the inevitable sensor inaccuracy and physical constraints will result in a number of common corruption on point cloud data. For example, occlusion is a typical corruption for LiDAR and other scanning devices, rendering partially visible point clouds. Deformation is also ubiquitous in AR/VR games. Such corruptions pose a even bigger threat in most real-world application scenarios. Therefore, it is imperative to study the corruption robustness of 3D point cloud recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Our Contributions:</head><p>In this paper, we create, to our knowledge, the first systematic corruption robustness benchmark, ModelNet40-C, for arXiv:2201.12296v1 <ref type="bibr">[cs.</ref>LG] 28 Jan 2022 <ref type="figure">Figure 1</ref>. Despite the impressive results on clean inputs (i.e., Mod-elNet40), state-of-the-art point cloud recognition models cannot deliver good performance on corrupted inputs (i.e., ModelNet40-C). The error rate is 3? larger on ModelNet40-C than ModelNet40.</p><p>3D point cloud recognition and present an in-depth analysis. To construct the dataset, we meticulously design and formulate 75 corruptions (15 types with 5 severity levels) that cover the majority of real-world point cloud distortion cases. We further provide a taxonomy of these corruptions into three categories (i.e, density, noise and transformation) and discuss their application scenarios. We anticipate that ModelNet40-C will serve as a first step towards 3D point cloud corruption-resistant models.</p><p>We conduct extensive evaluation on our ModelNet40-C. Specifically, we compare six representative models including PointNet <ref type="bibr" target="#b43">(Qi et al., 2017a)</ref>, PointNet++ <ref type="bibr" target="#b44">(Qi et al., 2017b)</ref>, DGCNN , RSCNN <ref type="bibr" target="#b35">(Liu et al., 2019b)</ref>, PCT , and SimpleView <ref type="bibr" target="#b19">(Goyal et al., 2021)</ref>. We find that current models are vulnerable to our created corruptions. As shown in <ref type="figure">Fig. 1</ref>, there are nearly 3? error rate gaps between model performances on Model-Net40 and ModelNet40-C. Our results reveal that there is still considerable room for point cloud recognition models to improve on robustness against common corruptions.</p><p>To mitigate such gaps, we propose a simple but effective strategy by combing PointCutMix-R and TENT, after evaluating a wide range of data augmentation and test-time adaptation methods. Our method on average achieves the lowest error rate of 15.4%. Specifically, we try augmentation (or regularization) strategies including PointCutMix-R, PointCutMix-K, PointMixup, RSMix, and adversarial training based strategy. Additionally, we employ test-time adaptation methods (i.e., BN <ref type="bibr" target="#b46">(Schneider et al., 2020)</ref> and TENT <ref type="bibr" target="#b55">(Wang et al., 2020)</ref>) to show their potential in improving corruption robustness. We examine every feasible combinations of architectures and methodologies, a total of 3,180 different configurations of experiments.</p><p>We summarize four conclusive insights below and eleven detailed findings in ? 5.</p><p>? Insight 1. Occlusion corruptions, rotation transformation, and background noises pose significant challenges for most point cloud recognition models ( ? 5.1).</p><p>? Insight 2. Different architectures are vulnerable to different corruption types, which can be attributed to their design principles. ( ? 5.2).</p><p>? Insight 3. Different data augmentation strategies are especially advantageous for certain types of corruptions, which also correlate well with their design choices ( ? 5.3).</p><p>? Insight 4. Test-time adaptations (BN and TENT) are beneficial for enhancing the corruption robustness, particularly for hard corruptions like occlusions and rotations <ref type="bibr">( ? 5.4)</ref>.</p><p>We hope our comprehensive benchmark and in-depth analysis will shed light on the robustness of point cloud recognition, and facilitate the development of robust architectures and training-and test-time robustness strategies on ModelNet40-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adversarial &amp; Corruption Robustness of 2D Images. Deep neural networks are known to be vulnerable to adversarial examples and common corruptions <ref type="bibr" target="#b3">(Bulusu et al., 2020)</ref>. <ref type="bibr" target="#b22">Hendrycks &amp; Dietterich (2019)</ref>; <ref type="bibr" target="#b24">Hendrycks et al. (2021)</ref> developed corruption robustness benchmarking datasets CIFAR-10/100-C, ImageNet-C, and ImageNet-R to facilitate robustness evaluations of CIFAR and ImageNet classification models. <ref type="bibr" target="#b38">Michaelis et al. (2019)</ref> extended this benchmark to object detection models. <ref type="bibr" target="#b39">Mintun et al. (2021)</ref> further proposed ImageNet-C dataset that is comprised of a set of corruptions that are perceptually dissimilar to ImageNet-C. Recently, <ref type="bibr" target="#b51">Sun et al. (2021b)</ref> proposed a comprehensive benchmarking suite CIFAR-10/100-F that contains corruptions from different regions in the spectral domain. <ref type="bibr" target="#b27">(Koh et al., 2021)</ref> presented WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications. ; <ref type="bibr" target="#b12">Cubuk et al. (2018)</ref>; <ref type="bibr" target="#b4">Calian et al. (2021)</ref> proposed augmentation methods to improve the corruption robustness in 2D vision tasks. On the adversarial robustness benchmarking front, <ref type="bibr" target="#b6">Carlini et al. (2019)</ref> discussed the methodological foundations, reviewed commonly accepted best practices, and suggested new methods for evaluating defenses to adversarial examples.  proposed a standardized leaderboard called RobustBench, which evaluates the adversarial robustness with AutoAttack , a comprehensive ensemble of white-and black-box attacks.</p><p>3D Point Cloud Deep Learning. Deep learning models are increasingly being proposed to process point cloud data. Early works attempted to use 3D voxel grids for perception, which have cubic complexity <ref type="bibr" target="#b37">(Maturana &amp; Scherer, 2015;</ref><ref type="bibr" target="#b56">Wang &amp; Posner, 2015)</ref>. PointNet <ref type="bibr" target="#b43">(Qi et al., 2017a)</ref> pioneered to leverage shared multi-layer perceptrons and a global pooling operation to achieve permutation-invariance and thus enable end-to-end training. <ref type="bibr" target="#b44">Qi et al. (2017b)</ref> further proposed PointNet++ to hierarchically stack PointNet for multi-scale local feature encoding. PointCNN and RSCNN refactor the traditional pyramid CNN to improve the local feature learning for point cloud recognition <ref type="bibr" target="#b31">(Li et al., 2018b;</ref><ref type="bibr" target="#b35">Liu et al., 2019b)</ref>. The graph data structure is also heavily used in point cloud learning <ref type="bibr">(Landrieu &amp; Simonovsky, 2018;</ref><ref type="bibr" target="#b50">Shen et al., 2018)</ref>. For example, DGCNN built a dynamic graph of point cloud data for representation learning . PointConv and KPConv improve the convolution operation for point cloud learning <ref type="bibr" target="#b60">(Wu et al., 2019;</ref><ref type="bibr" target="#b52">Thomas et al., 2019)</ref>. Recent work demonstrated that ResNet <ref type="bibr" target="#b21">(He et al., 2016)</ref> on multi-view 2D projections of point clouds could also achieve high accuracy <ref type="bibr" target="#b19">(Goyal et al., 2021)</ref>. PointTransformer and PCT advance Transformer <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> blocks into point cloud learning and achieve state-of-the-art performance <ref type="bibr" target="#b67">(Zhao et al., 2021;</ref><ref type="bibr" target="#b20">Guo et al., 2021)</ref>.</p><p>Robustness Enhancements for 3D Point Cloud. Several recent efforts tackle improving the robustness of 3D point cloud learning <ref type="bibr">(Sun et al., 2020a)</ref>. <ref type="bibr" target="#b62">Xiang et al. (2019)</ref> and <ref type="bibr" target="#b32">Liu et al. (2019a)</ref> first demonstrated that point cloud recognition is vulnerable to adversarial attacks. <ref type="bibr" target="#b69">Zhou et al. (2019)</ref> and <ref type="bibr" target="#b15">Dong et al. (2020)</ref> proposed to leverage input randomization techniques to mitigate such vulnerabilities. <ref type="bibr">Sun et al. (2020b)</ref> conducted adaptive attacks on existing defenses and analyzed the application of adversarial training on point cloud recognition. <ref type="bibr" target="#b68">Zhao et al. (2020)</ref> discovered that adversarial rotation greatly degrades the perception performance. <ref type="bibr">Sun et al. (2021a)</ref> further showed that pre-training on self-supervised tasks enhances the adversarial robustness of point cloud recognition. Recent studies presented a framework that uses the Shapley value <ref type="bibr" target="#b45">(Roth, 1988)</ref> to assess the quality of representations learned by different point cloud recognition models <ref type="bibr" target="#b48">(Shen et al., 2021a;</ref>. Recent efforts also proposed certified adversarial defenses . However, little attention has been paid to the common corruption robustness of point cloud recognition. In this work, we aim to present a systematic benchmark and rigorously analyze the corruption robustness of representative deep point cloud recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D Point Cloud Corruption Robustness</head><p>In this section, we introduce the design principles of our 3D corruption benchmark. As mentioned in ? 1, 3D point clouds are being utilized in various safety-and securitycritical real-world applications <ref type="bibr" target="#b18">(Geiger et al., 2012;</ref><ref type="bibr" target="#b0">Adallah et al., 2019;</ref><ref type="bibr" target="#b65">Yu et al., 2021;</ref><ref type="bibr" target="#b9">Cheng et al., 2020)</ref>. Extensive studies have been carried out to improve both architectures and training strategies for point cloud recognition on in-distribution data <ref type="bibr" target="#b43">(Qi et al., 2017a;</ref><ref type="bibr" target="#b8">Chen et al., 2020;</ref><ref type="bibr" target="#b29">Lee et al., 2021)</ref>. However, there has not been any systematic study on the model robustness against common corruption. To bridge this gap, we design 15 common corruptions for benchmarking corruption robustness of point cloud recognition models. It is worth noting that such designs are non-trivial since the manipulation space of 3D point clouds is completely different from 2D images where the corruptions come from the RGB modification <ref type="bibr" target="#b22">(Hendrycks &amp; Dietterich, 2019)</ref>. In particular, we have three principles to design our benchmarks: i) Since we directly manipulate the position of points, we need to take extra care to preserve the original semantics of point clouds <ref type="figure">(Fig. 2)</ref>. ii) we should ensure the constructed corruptions are realistic in various applications. iii) We should take diversity as an important factor to emulate a wide range of natural corruptions for 3D point clouds.</p><p>Our 15 corruption types can be naturally grouped into three categories (i.e., density, noise, and transformation) , and we will introduce them in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Density Corruption Patterns</head><p>3D point cloud can be collected from various sensors like VR scanning devices and LiDAR for autonomous driving, or generated from computer-aided design (CAD) models. Therefore, the testing point clouds may have different density patterns from the training samples. For example, VR scanning (in indoor scenes) and LiDAR sensors may suffer from occlusion, so that only a portion of the point cloud is visible <ref type="bibr" target="#b18">(Geiger et al., 2012;</ref><ref type="bibr" target="#b13">Dai et al., 2017)</ref>. Besides, the direct reflection of lasers on metal materials will cause local missing points in LiDAR point clouds <ref type="bibr" target="#b34">(Liu et al., 2018)</ref>. The local density of 3D scanned point clouds rely on how frequently the device passes that area <ref type="bibr" target="#b40">(Nguyen &amp; Le, 2013)</ref>. We hence formulate five corruption types to cover the density corruption patterns: {Occlusion, LiDAR, Local Density Inc, Local Density Dec, Cutout}. Specifically, Occlusion and LiDAR both simulate occlusion patterns using ray tracing on the original meshes <ref type="bibr" target="#b71">(Zhou et al., 2018)</ref>, and LiDAR additionally incorporates the vertically line-styled pattern of LiDAR point clouds <ref type="bibr" target="#b34">(Liu et al., 2018)</ref>. Local Density Inc and Local Density Dec will randomly select several local clusters of points using k-nearest neighbors (kNN) to increase and decrease their density, respectively. Similarly, Cutout discards several randomly chosen local clusters of points using kNN (DeVries &amp; Taylor, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Noise Corruption Patterns</head><p>Noise evidently exists in all real-world point cloud applications. For example, the inevitable digital noise of scanning sensors (e.g., medical imaging) <ref type="bibr" target="#b59">(Wolff et al., 2016)</ref> and the random reflections and inaccuracy of LiDAR lasers <ref type="bibr" target="#b18">(Geiger et al., 2012)</ref> will contribute to a substantial variation of points. Compression and decompression will potentially result in noisy point clouds as well <ref type="bibr" target="#b5">(Cao et al., 2019)</ref>. Besides, real-time rendering in VR games is another source of noise <ref type="bibr" target="#b2">(Bonatto et al., 2016)</ref>. Although noise is a common corruption pattern for both 2D and 3D data, the manip- <ref type="figure">Figure 2</ref>. Visualizations of Our Constructed ModelNet40-C. Our ModelNet40-C dataset consists of 15 corruption types that represent different out-of-distribution shifts in real-world applications of point clouds. Similar to ImageNet-C <ref type="bibr" target="#b22">(Hendrycks &amp; Dietterich, 2019)</ref>, each corruption type has five severity levels. We carefully examine the generated point clouds and ensure they preserve their original semantics. More visualization samples are shown in Appendix A ulation space is larger for point clouds since their numbers of points are adjustable. We thus formulate five noise perturbations: {Uniform, Gaussian, Impulse, Upsampling, Background}. As their names indicate, Uniform and Gaussian apply different distributional noise to each point in a point cloud. Impulse applies deterministic perturbations to a subset of points. Upsampling assigns new perturbation points around the existing points. Background randomly adds new points in the bounding box space of the pristine point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transformation Corruptions Patterns</head><p>We use both linear and non-linear 3D transformations to formulate the corruptions. For the linear ones, we leverage 3D Rotation and Shear as our corruption types and exclude translation and scale transformations since they can be easily restored by normalization (i.e., the inverse transformation matrix). Rotation of point clouds is common in the real world and the robustness against adversarial rotations has been investigated by a few studies <ref type="bibr" target="#b68">(Zhao et al., 2020;</ref><ref type="bibr" target="#b48">Shen et al., 2021a)</ref>. We here do not use aggressive rotations that might affect human perception as well, but instead enable a milder rotation (? 15 ? ) along xyz axes. We consider Shear on the xy plane to represent the motion distortion in 3D point clouds <ref type="bibr" target="#b63">(Yang et al., 2021)</ref>. We utilize free-form deformation (FFD) <ref type="bibr" target="#b47">(Sederberg &amp; Parry, 1986)</ref> and radial basis function (RBF)-based deformation <ref type="bibr" target="#b17">(Forti &amp; Rozza, 2014)</ref> for non-linear transformations. Such deformations are also common in VR/AR games and point clouds from generative models (GAN) <ref type="bibr" target="#b30">(Li et al., 2018a;</ref><ref type="bibr" target="#b70">Zhou et al., 2021)</ref>. Specifically, we use multi quadratic (?(x) = ? x 2 + r 2 ) and inverse multi quadratic splines (?(x) = (x 2 + r 2 ) ? 1 2 ) as the representative RBFs to cover a wide range of deformation types. As a result, we in total formulate {Rotation, Shear, FFD, RBF, Inv RBF} as our transformation-based corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ModelNet40-C Robustness Benchmark</head><p>ModelNet40 is the most popular dataset for benchmarking point cloud recognition performance, containing 12,308 point clouds from 40 classes <ref type="bibr" target="#b61">(Wu et al., 2015)</ref>. Point clouds from ModelNet40 are extracted from CAD models, rendering a perfectly clean dataset. <ref type="bibr" target="#b53">Uy et al. (2019)</ref> recently proposed ScanObjectNN consisting of point clouds scanned from real-world objects to show the performance gap between models trained on synthetic and real-world data. However, quantifying how models trained on the clean dataset perform under common corruptions encountered during the test time remains challenging. To this end, we use the Mod-elNet40 as our base dataset to construct ModelNet40-C. It is worth noting that our devised corruptions are general to be applied to other dataset, like ShapeNet <ref type="bibr" target="#b7">(Chang et al., 2015)</ref>.</p><p>Setup. We create ModelNet40-C with five severity levels for each corruption type, the same as ImageNet-C. <ref type="figure">Fig. 2</ref> illustrates samples from ModelNet40-C with severity level four, and they clearly still preserve the semantics of the "airplane" class. Since it is hard to qualify and quantify the corruption severity for LiDAR and Occlusion, we instead leverage five different view angles to create their corrupted point clouds. The detailed construction of ModelNet40-C 15 c=1 ER f c . We will release our leaderboard publicly to facilitate future studies on robustness of point cloud learning. The goal of ModelNet40-C is to evaluate the general robustness of point cloud learning models in various real-world scenarios. ModelNet40-C is not designed for training-time optimizations but augmentations with other/similar corruptions are allowed and should be explicitly stated. The corruption robustness of trained models can be assessed by their performance on ModelNet40-C using above metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we elaborate our comprehensive evaluation and rigorous analysis in detail. We benchmark corruption robustness of point cloud recognition from the perspectives of corruption types and model architectures. Moreover, we examine the effectiveness of data augmentations and test-time adaptation methods as mitigation solutions against common corruptions.</p><p>Setup. We leverage six representative model architectures: PointNet <ref type="bibr" target="#b43">(Qi et al., 2017a)</ref>, PointNet++ <ref type="bibr" target="#b43">(Qi et al., 2017a)</ref>, DGCNN , RSCNN <ref type="bibr" target="#b35">(Liu et al., 2019b)</ref>, PCT , and SimplView <ref type="bibr" target="#b19">(Goyal et al., 2021)</ref>. These six models stand for distinct architecture designs, and have achieved good accuracy on the clean dataset. They are also well-recognized by the 3D vision community, and have been extensively applied to complex tasks like semantic segmentation <ref type="bibr" target="#b40">(Nguyen &amp; Le, 2013)</ref> and object detection <ref type="bibr">(Shi et al., 2019;</ref>. As suggested by <ref type="bibr" target="#b19">Goyal et al. (2021)</ref>, we adopt the same training strategy for all models. We utilize smoothed cross-entropy  as the loss function as it has been demonstrated to improve the recognition performance. We take 1024 points as input size in the train-  <ref type="table">airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet  tv_stand  vase</ref>  ing phase and use the Adam optimizer <ref type="bibr" target="#b26">(Kingma &amp; Ba, 2014)</ref> with the ReduceLROnPlateau scheduler implemented in PyTorch <ref type="bibr" target="#b41">(Paszke et al., 2019)</ref>. We train 300 epochs and pick the best performant model for our further evaluation and follow  to use random translation and scaling as our default data augmentation. All training and testing experiments are done on a GeForce RTX 2080 GPU. We report the class-wise mean ER in Appendix B due to space constraints.</p><p>Besides, we try data augmentation and test-time adaption strategies and evaluate their effectiveness against our created corruptions. In particular, we leverage PointCutMix-R, PointCutMix-K , PointMixup , RSMix <ref type="bibr" target="#b29">(Lee et al., 2021)</ref>, and PGD-based adversarial training <ref type="bibr">(Sun et al., 2021a)</ref> as additional data augmentation strategies. We adopt the original hyper-parameter settings from their official implementations in our study. Detailed introduction can be found in Appendix B. We only enable adversarial training for PointNet, DGCNN, and PCT since PointNet++ and RSCNN leverage ball queries to find neighboring points and SimpleView projects the pristine point cloud into multi-view 2D images. Both methods will hinder the gradients from backward propagating to the original point cloud, making adversarial training inapplicable. We leave them as future work.</p><p>Clean Performance. <ref type="table" target="#tab_0">Table 1</ref> shows the ER clean of different model architectures with the adopted training strategies. All models achieve 90+% accuracy with standard training. As <ref type="bibr" target="#b19">Goyal et al. (2021)</ref> indicate, auxiliary factors will obscure the effect of architectures, and the performance gaps among different models are not significant on the Mod-elNet40 validation set. Data augmentation strategies like PointMixup claim to enhance the general model performance. However, with these factors controlled, we also do not find tangible improvements over these augmentation recipes. Besides, adversarially trained models are expected to perform slightly worse on the clean dataset compared to others <ref type="bibr">(Sun et al., 2021a)</ref>. It also initiates that model performances on ModelNet40 tend to saturate. Thus, it is a necessary to evaluate the model effectiveness from other perspectives (e.g., robustness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison Among Corruption Types</head><p>Validity. We first benchmark robustness of six standard trained models to verify the validity of our ModelNet40-C.</p><p>The detailed results are presented in <ref type="figure">Fig. 9</ref> and 10 in Appendix B. We find that the ER s,c gradually increases as the severity level increasing for each corruption, which justifies our hyper-parameter setting. As shown in <ref type="figure">Fig. 1</ref>, there is a ? 3? performance degradation between the overall ER cor and ER clean for all benchmarked models. <ref type="figure" target="#fig_1">Fig. 3</ref> shows the average confusion matrix over six models and 15 corruptions, where the values in the diagonal are still high, further validating the semantic maintenance of ModelNet40-C. ModelNet40-C fullfills its goal as a robustness benchmark for future studies. <ref type="table" target="#tab_1">Table 2</ref> presents the detailed ER c of the six models evaluated on ModelNet40-C with standard training. As mentioned before, there is a significant increase in ER c on each corruption compared to ER clean . The gap ranges from 17.6% to 89.7% among different corruptions. From the perspective of the corruption, we obtain several interesting observations.</p><p>? Insight 1.1. Occlusion and LiDAR corruptions pose a major threat for 3D point cloud recognition models.</p><p>From <ref type="table" target="#tab_1">Table 2</ref>, ER Occlusion and ER LiDAR reach 55.0% and 71.6% on average, respectively. Occlusion happens in most real-world application of 3D point clouds. Moreover, we find the models have poor performance regardless of the occlusion directions, suggesting a general vulnerability of point cloud recognition.</p><p>? Insight 1.2. Rotation is still a challenging corruption for 3D point cloud recognition models even with small angles.</p><p>Rotation is a well-known threat for point cloud recognition by several recent studies <ref type="bibr" target="#b68">(Zhao et al., 2020;</ref><ref type="bibr" target="#b48">Shen et al., 2021a)</ref>. Existing studies allow a rotation angle (e.g., ? 45 ? ). However, such rotated point clouds confuse human perception without RGB information. In our study, we find that a small rotation (? 15 ? ) still causes a high ER on point cloud recognition models ranging from 18.1% to 36.8%.</p><p>? Insight 1.3. Impulse and Background corruptions are surprisingly troublesome to 3D point cloud recognition.</p><p>We find ER Impulse (33.2%) and ER Background (48.0%) are abnormally high for most architectures. Although they are even less perceptible than Gaussian and uniform noise since only a small portion of points are affected. However, the magnitudes of Impulse and Background noises are high, suggesting that a small portion of outliers will greatly affect point cloud recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison Among Model Architectures</head><p>As presented in <ref type="table" target="#tab_1">Table 2</ref>, there is no overarching model that dominates our ModelNet40-C dataset, unlike robustness benchmarking in 2D vision <ref type="bibr" target="#b22">(Hendrycks &amp; Dietterich, 2019)</ref>. Point cloud recognition models have various designs and no consensus has been reached as deep learning in the 3D space is a relatively nascent field. The model performances on ModelNet40-C are found to be in good alignment with their design attributes.</p><p>? Insight 2.1. PointNet achieves strong performance on density corruptions, but fails on transformation corruptions.</p><p>PointNet does not encode local feature, and several publications have studies it from the perspectives of adversarial robustness <ref type="bibr">(Sun et al., 2021a</ref>) and representation quality <ref type="bibr" target="#b48">(Shen et al., 2021a)</ref>. Such a design has been regarded as a main drawback of PointNet. However, we find it robust against the variations in density. As mentioned in ? 5.1, Background is a challenging corruption to point cloud recognition. However, we find that PointNet++ and RSCNN are specially robust against it, which have outperform other models by 70.7%. We discover that the ball query of neighboring points is the key to such robustness. Compared to kNN that has deterministic k points to cluster, ball query fixes the radius to reject faraway points in the bounding box space. This design helps models tackle the root cause of the Background corruption.</p><p>? Insight 2.3. Transformer-based architectures are robust against transformation corruptions.</p><p>Transformer <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> has recently reformed the 2D vision <ref type="bibr" target="#b16">(Dosovitskiy et al., 2021)</ref>. PCT leverages multiple Transformer blocks as its backbone, which leverage self-attention modules to embed robust global features. PCT reaches the ER of 13.5% on transformation corruptions, consistently achieving the best model on all corruption types. In comparison to density and noise corruptions, transformation corruptions are mild and have a minor effect on the local smoothness. Transformer has been demonstrated to have large capacity and a global receptive field, and we believe this design contribute to its resilience to global corruption of point clouds.</p><p>Besides above, we find that SimpleView cannot achieve better robustness under common corruptions than other architectures, despite it high performance on clean data (Table 2), suggesting point cloud-specific designs are indeed desired. Due to its good results on Background, Point-Net++ on average performs the best with standard training, achieving an ER cor of 23.6%, and PCT has a more balanced performance across corruptions, making it the runner-up architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Data Augmentation Strategies</head><p>As mentioned earlier, we use five additional data augmentation strategies to train the six models. In this section, we examine how these training recipes combined with different models perform on ModelNet40-C, and <ref type="table" target="#tab_3">Table 3</ref> presents the overall results. Due to the space limit, we group the evaluation results per corruption patterns and the detailed results are shown in <ref type="figure">Fig. 9</ref> and 10 in Appendix B. Several interesting insights can be concluded from our experiments.</p><p>? Insight 3.1 Data augmentation strategies generally improve the corruption robustness of 3D point cloud recognition.</p><p>As As <ref type="table" target="#tab_3">Table 3</ref> presents, PointCutMix-R performs the best on noise corruptions (ER = 12.7%), PointMixup specializes the transformation corruptions (ER = 14.1%), and RSMix is especially robust against density corruptions (ER = 26.8%). Such results also relate to the design of augmentation strategies. In details, given two point cloud samples x a ,x b from class a and b, PointCutMix-R simply merges (?) two randomly selected ( ) subsets together based on hyper-parameter ? (</p><formula xml:id="formula_0">x aug = ? x a ?(1??) x b ).</formula><p>The two subsets will overlap in the resulting point cloud x aug . Each point cloud subset can be regarded as a special noise by the other. Thus, it naturally includes noise corruptions with mixing into data augmentations. PointMixup leverages interpolation-based mixing that the transition between two point clouds (</p><formula xml:id="formula_1">x aug = ?x a + (1 ? ?)?(x a , x b ), where ?(x a , x b )</formula><p>finds the shortest path for every pair in x a and x b ). The augmented point cloud is thus locally smooth, which aligns with the transformation corruptions.</p><p>In contrast, RSMix acts similarly with PointCutMix-K but guarantee a rigid mixing of two partial point clouds. There will be no overlaps and each point cloud subset is clustered and isolated in the 3D space. Such patterns correspond to density corruptions in point cloud data. Adversarial training improves robustness on noise corruptions since we rely on point shifting attacks in the inner maximization stage. <ref type="bibr" target="#b48">Shen et al. (2021a)</ref> suggest that adversarial rotation training improves the robustness against random rotations. We here motivate future research to present general methods that improve both adversarial and corruption robustness for point cloud learning.</p><p>Moreover, with more data augmentation and we find PCT outperforms PointNet++ with augmentation sophisticated training recipes. Such results align with recent studies in corruption robustness of 2D vision tasks as well <ref type="bibr" target="#b1">(Bai et al., 2021)</ref>, suggesting the superiority of Transformer-based design in 3D point cloud learning. Surprisingly, the simplest augmentation, PointCutMix-R, achieves the best overall robustness (ER corrup =18.7%). As <ref type="figure">Fig. 4</ref> shows, it is especially helpful on corruptions with high severity levels. We hope our analysis will facilitate future research on designing effective and robust training recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Test-Time Adaptation Methods</head><p>Besides the introduced training-time strategies, we evaluate test-time adaptation methods on our ModelNet40-C. Specifically, we for the first time adapt the BN <ref type="bibr" target="#b46">(Schneider et al., 2020)</ref> and TENT <ref type="bibr" target="#b55">(Wang et al., 2020)</ref> to point cloud recognition. BN updates the statistics of BatchNorm <ref type="bibr" target="#b25">(Ioffe &amp; Szegedy, 2015)</ref> layers (i.e., ? and ?.) based on the incoming batch of testing point clouds. TENT updates both the statistics and weight parameters (i.e., ? and ?.) of Batch-Norm layers to minimize the cross-entropy of the output layer. <ref type="table" target="#tab_5">Table 4</ref> presents the evaluation results.</p><p>? Insight 4.1. Test-time adaptations overall perform worse than data augmentation strategies in improving robustness.</p><p>BN and TENT consistently help enhance the corruption robustness for point cloud recognition. However, we find they overall are not as effective as data augmentation strategies. Such observations are different from 2D vision tasks, and we attribute the reason to the nature of corruptions in 3D space. Corruption robustness benchmarks for 2D images are created by changing the RGB values. Corruptions in the 3D space directly modify both the numbers and positions of points. The distributional shift is thus large between corrupted and clean point clouds.</p><p>? Insight 4.2. Test-time adaptation is surprisingly useful on tough corruptions.</p><p>We find that TENT on average helps achieve the strongest robustness on Occlusion (ER=47.6%), LiDAR (ER=54.1%), and Rotation (ER=19.8%) corruptions, outperforming the best augmentation method by 6.7%, 1.9%, and 7.9% respectively. Especially, we find test-time adaptation methods achieve the best ER rotation,5 = 35.6%, which is a 27.1 % improvement over the best augmentation strategy. Augmentation strategies cannot handle these difficult corruptions, but test-time adaptation methods deliver a more consistent improvement. We have so far demonstrated that PointCutMix-R and TENT obtain the best among the training-and test-time methods, in terms of the overall ER cor . We here evaluate the performance of the combination of PointCutMix-R and TENT as they do not conflict with each other. As presented in <ref type="table" target="#tab_6">Table 5</ref>, we find that the combined solution further improves the corruption robustness by 14.7%. To our best knowledge, there is no test-time adaptation designs specific for point cloud learning, and we hope our study will shed light on future research on corruption robustness in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>Through our systematic benchmarking and analysis, we found that the performance discrepancies of different point cloud recognition models across different corruptions are much larger than 2D architectures. This suggests future studies on a universal architecture design for 3D point cloud to be a worthwhile direction. In the future, we plan to extend our current benchmark to complex tasks like point cloud segmentation and object detection to facilitate future research on robustness in the 3D domain.</p><p>To conclude, we have presented ModelNet40-C, the first comprehensive benchmark for corruption robustness of point cloud recognition models. We have unveiled the massive performance degradation on our ModelNet40-C for six representative models. We also provided critical insights on how different architecture and data augmentation designs affect model robustness on different corruptions. For example, Transformer appears to be a promising architecture in improving robustness in 3D vision tasks. Our study on test-time adaptation in point cloud recognition shows its potential as a robustness strategy. We hope that our ModelNet40-C benchmark will benefit future research in developing robust 3D point cloud models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ModelNet40-C</head><p>We elaborate the creation of ModelNet40-C in this section. The detailed implementation can be found in our codebase, which is included in the supplementary materials.</p><p>Occlusion and LiDAR share similar general corruption features. We leverage five viewing angles to construct these two corruptions on ModelNet40, as shown in <ref type="figure">Fig. A</ref>. Specifically, we utilize ray tracing algorithms on the original meshed from ModelNet40 to generate the point cloud. Let the facing direction of the object as 0 ? pivoting the z axis, we use 0 ? , 72 ? , 144 ? , 216 ? , and 288 ? as our viewing angles, the viewing angles between the xy plane are randomly sampled from is 30 ? ? 60 ? . For LiDAR, we additionally render the generated point cloud into the vertically multi-line style to simulate the pattern of the LiDAR sensor. For Local Density Inc and Local Density Dec, we first sample a number of anchor points based the severity level. We further find the kNN of the anchor points and up-sample or down-sample them to increase and decrease their local density, respectively. Similarly, Cutout discards the full kNN (k = 50) subsets of the anchor points to simulate the sensor limitations of LiDAR and other scanning devices.</p><p>Gaussian and Uniform noises are sampled from Gaussian and uniform distributions with different ? and based on the severity level. For the Background noise, we randomly sample different numbers of points in the edge-length-2 cube that bounds the point cloud based on the severity level. For Impulse noise, we first sample different numbers of points based on the severity level and assign the maximum magnitude of perturbation ? = 0.05 to them. For the Upsampling noise, we first choose different numbers of points based on the severity level and generate new points around the selected anchors, bounded by ? = 0.05.</p><p>For Rotation and Shear, we have introduced their construction in ? 3. As mentioned, we allow relatively small transformations since we find larger ones will affect the human perception of the object class as well.</p><p>For deformation-based corruptions FFD, RBF, and Inv RBF, we assign 5 control points along each xyz axis, resulting in 125 control points in total. We choose the deformation distance based on the severity level and randomly assign their directions in the 3D space. The deformations then are formulated based on the interpolation functions that we choose in ? 3.</p><p>We visualize two additional groups of sample point clouds from ModelNet40-C in <ref type="figure" target="#fig_3">Fig. 6</ref>. and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Data Augmentation Setups</head><p>We introduce the detailed setting of our experiments and analysis in this section. For all mixing-based data augmentation strategies, we have a hyper-parameter ? to determine the weight of two samples to mix, as well as the weight of the virtual label vector:  where y a and y b are the label vectors for point cloud x a and x b . We set ? = 0.5, which has shown to achieve good results reported in <ref type="bibr" target="#b8">Chen et al., 2020;</ref><ref type="bibr" target="#b29">Lee et al., 2021)</ref>. For adversarial training, we use the point shifting attack in the adversarial inner maximization:</p><formula xml:id="formula_2">y aug = ? ? y a + (1 ? ?) ? y b<label>(1</label></formula><formula xml:id="formula_3">x s+1 = ? x+S (x s + ? ? sign(? x s L(x s , y; f ))); x 0 = x + U(? , )<label>(2)</label></formula><p>where = 0.05, ? = 0.01 and we use seven steps PGD, as suggested by <ref type="bibr">Sun et al. (2021a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Evaluation Results</head><p>We illustrate the confusion matrices of all six models with standard training in <ref type="figure" target="#fig_10">Fig. 8</ref> to show the validity of ModelNet40-C. We present our detailed evaluation results containing 3,180 data points. <ref type="figure">Fig. 9</ref> and 10 shows the model comparison on all data augmentation strategies. The class-wise mean error rates (mER) are shown in <ref type="figure" target="#fig_13">Figure 11</ref> and 12. The detailed results of test-time adaptation methods (ER and mER) are shown in <ref type="figure" target="#fig_1">Fig. 13, 14</ref>   <ref type="table">Prediction   airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet</ref>   <ref type="table">airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet</ref>   <ref type="table">Prediction   airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet  tv_stand  vase</ref>   <ref type="table">airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet  tv_stand  vase</ref>   <ref type="table">Prediction   airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet  tv_stand  vase</ref>   <ref type="table">Prediction   airplane  bathtub  bed  bench  bookshelf  bottle  bowl  car  chair  cone  cup  curtain  desk  door  dresser  flower_pot  glass_box  guitar  keyboard  lamp  laptop  mantel  monitor  night_stand  person  piano  plant  radio  range_hood  sink  sofa  stairs  stool  table  tent  toilet  tv_stand  vase</ref>          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Confusion Matrix on ModelNet40-C Averaged over Six Models with Standard Training. Confusion matrices for each model are shown in Figure 8 in Appendix B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of Occlusion and LiDAR Corruption Generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of Samples from ModelNet40-C -"Toliet" Class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Visualization of Samples from ModelNet40-C -"Monitor" Class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>a in d e s k d o o r d re s s e r fl o w e r_ p o t g la s s _ b o x g u it a r k e y b o a rd la m p la p to p m a n te l m o n it o r n ig h t_ s ta n d p e rs o n p ia n o p la n t ra d io ra n g e _ h o o d s in k s o fa s ta ir s s to o l ta b le te n t to il e t tv _ s ta n d v a s e w a rd ro b e x b o x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>a in d e s k d o o r d re s s e r fl o w e r_ p o t g la s s _ b o x g u it a r k e y b o a rd la m p la p to p m a n te l m o n it o r n ig h t_ s ta n d p e rs o n p ia n o p la n t ra d io ra n g e _ h o o d s in k s o fa s ta ir s s to o l ta b le te n t to il e t tv _ s ta n d v a s e w a rd ro b e x b o x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Confusion Matrices for Each Model on ModelNet40-C with Standard Training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Error Rates of Different Models with Different Data Augmentation Strategies on ModelNet40-C. Error Rates of Different Models with Different Data Augmentation Strategies on ModelNet40-C (Cont'd).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Class-wise Mean Error Rates of Different Models with Different Data Augmentation Strategies on ModelNet40-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .Figure 15 .Figure 16 .</head><label>141516</label><figDesc>Error Rates of Different Models with Different Test-time Adaptation Methods on ModelNet40-C (Cont'd). Class-wise Mean Error Rates of Different Models with Different Test-time Adaptation Methods on ModelNet40-C. Class-wise Mean Error Rates of Different Models with Different Test-time Adaptation Methods on ModelNet40-C (Cont'd).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Error Rates of Different Models with Training Strategies on the ModelNet40 (ERclean). Bold and underline denote the best and runner-up results throughout this paper, respectively.</figDesc><table><row><cell cols="7">Model (%) ? Standard PointCutMix-R PointCutMix-K PointMixup RSMix PGD</cell></row><row><cell>PointNet</cell><cell>9.3</cell><cell>9.4</cell><cell>9.0</cell><cell>8.9</cell><cell>9.8</cell><cell>11.8</cell></row><row><cell>PointNet++</cell><cell>7.0</cell><cell>7.1</cell><cell>6.7</cell><cell>7.1</cell><cell>6.6</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell>7.4</cell><cell>7.4</cell><cell>6.8</cell><cell>7.8</cell><cell>7.1</cell><cell>8.1</cell></row><row><cell>RSCNN</cell><cell>7.7</cell><cell>7.6</cell><cell>7.1</cell><cell>7.2</cell><cell>7.6</cell><cell>-</cell></row><row><cell>PCT</cell><cell>7.1</cell><cell>7.2</cell><cell>6.9</cell><cell>7.4</cell><cell>6.9</cell><cell>8.9</cell></row><row><cell>SimpleView</cell><cell>6.1</cell><cell>7.9</cell><cell>7.4</cell><cell>7.2</cell><cell>7.9</cell><cell>-</cell></row><row><cell cols="7">is introduced in Appendix A. These designed corruptions</cell></row><row><cell cols="7">are applied to the validation set of ModelNet40, resulting in</cell></row><row><cell cols="7">ModelNet40-C a 75? larger dataset to test the corruption</cell></row><row><cell cols="7">robustness of pre-existing models. Note that ModelNet40-</cell></row><row><cell cols="7">C should be only used in the test time rather than in the</cell></row><row><cell cols="2">training phase.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Metrics. We use the error rate (ER) and class-wise mean</cell></row><row><cell cols="7">error rate (mER) as the main metrics for ModelNet40-C</cell></row><row><cell cols="7">benchmarking. We denote ER f clean as the error rate for a</cell></row><row><cell cols="7">classifier f on the clean dataset (i.e., ModelNet40) and</cell></row><row><cell cols="7">ER f s,c as the error rate for f on corruption c with severity</cell></row><row><cell cols="2">s. Similarly, ER f c =</cell><cell cols="3">5 s=1 ER f s,c and ER f cor =</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Error Rates of Different Model Architectures on ModelNet40-C with Standard Training. ? ER cor Occlusion LiDAR Density Inc. Density Dec. Cutout Uniform Gaussian Impulse Upsampling Background Rotation Shear FFD RBF Inv. RBF</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Density Corruptions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Noise Corruptions</cell><cell></cell><cell cols="3">Transformation Corruptions</cell></row><row><cell>Model (%) PointNet</cell><cell>28.3</cell><cell>52.3</cell><cell>54.9</cell><cell>10.5</cell><cell>11.6</cell><cell>12.0</cell><cell>12.4</cell><cell>14.4</cell><cell>29.1</cell><cell>14.0</cell><cell>93.6</cell><cell>36.8</cell><cell>25.4 21.3 18.6</cell><cell>17.8</cell></row><row><cell>PointNet++</cell><cell>23.6</cell><cell>54.7</cell><cell>66.5</cell><cell>16.0</cell><cell>10.0</cell><cell>10.7</cell><cell>20.4</cell><cell>16.4</cell><cell>35.1</cell><cell>17.2</cell><cell>18.6</cell><cell>27.6</cell><cell>13.4 15.2 16.4</cell><cell>15.4</cell></row><row><cell>DGCNN</cell><cell>25.9</cell><cell>59.2</cell><cell>81.0</cell><cell>14.1</cell><cell>17.3</cell><cell>15.4</cell><cell>14.6</cell><cell>16.6</cell><cell>24.9</cell><cell>19.1</cell><cell>53.1</cell><cell>19.1</cell><cell>12.1 13.1 14.5</cell><cell>14.0</cell></row><row><cell>RSCNN</cell><cell>26.2</cell><cell>51.8</cell><cell>68.4</cell><cell>16.8</cell><cell>13.2</cell><cell>13.8</cell><cell>24.6</cell><cell>18.3</cell><cell>46.2</cell><cell>20.1</cell><cell>18.3</cell><cell>29.2</cell><cell>17.0 18.1 19.2</cell><cell>18.6</cell></row><row><cell>PCT</cell><cell>25.5</cell><cell>56.6</cell><cell>76.7</cell><cell>11.8</cell><cell>14.3</cell><cell>14.5</cell><cell>12.1</cell><cell>13.9</cell><cell>39.1</cell><cell>17.4</cell><cell>57.9</cell><cell>18.1</cell><cell>11.5 12.4 13.0</cell><cell>12.6</cell></row><row><cell cols="2">SimpleView 27.2</cell><cell>55.5</cell><cell>82.2</cell><cell>13.7</cell><cell>17.2</cell><cell>20.1</cell><cell>14.5</cell><cell>14.2</cell><cell>24.6</cell><cell>17.7</cell><cell>46.8</cell><cell>30.7</cell><cell>18.5 17.0 17.9</cell><cell>17.2</cell></row><row><cell>Average</cell><cell>26.1</cell><cell>55.0</cell><cell>71.6</cell><cell>13.8</cell><cell>13.9</cell><cell>14.4</cell><cell>16.4</cell><cell>15.6</cell><cell>33.2</cell><cell>17.6</cell><cell>48.0</cell><cell>26.9</cell><cell>16.3 16.2 16.6</cell><cell>15.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>presents that PointNet achieves an ER of 28.3% on density corruptions, and overall outperforms the runner-up by 11.7%. Such results can be attributed to the locality of density corruptions. Compared to other models that embeds complex local features, PointNet is less sensitive to local changes of the input point cloud. On the other hand, PointNet indeed fails on other corruptions, rendering itself the worst performant model on average. Our analysis complements the existing understanding of PointNet, we believe the usage of PointNet should be determined by the application scenarios.</figDesc><table /><note>? Insight 2.2. Ball query-based clustering operation is robust against Background noise.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Error Rates of Different Model Architectures on ModelNet40-C with Different Data Augmentation Strategies. ERcor ERcor Density Noise Trans. ERcor Density Noise Trans. ERcor Density Noise Trans. ERcor Density Noise Trans. ERcor Density Noise Trans.</figDesc><table><row><cell></cell><cell>Standard</cell><cell></cell><cell cols="2">PointCutMix-R</cell><cell cols="2">PointCutMix-K</cell><cell cols="2">PointMixup</cell><cell cols="2">RSMix</cell><cell></cell><cell cols="2">PGD</cell><cell></cell></row><row><cell>Model (%) ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointNet</cell><cell>28.3</cell><cell>21.8</cell><cell>30.5</cell><cell>18.0 16.9 21.3</cell><cell>26.8</cell><cell>21.8 15.4 25.4</cell><cell>28.3</cell><cell>28.9 19.0 22.5</cell><cell>24.8</cell><cell cols="2">27.3 15.5 25.9</cell><cell>28.8</cell><cell cols="2">28.4 20.5</cell></row><row><cell>PointNet++</cell><cell>23.6</cell><cell>19.1</cell><cell>28.1</cell><cell>12.2 17.0 20.2</cell><cell>26.3</cell><cell>16.9 17.3 19.3</cell><cell>30.8</cell><cell>14.3 12.9 23.3</cell><cell>27.0</cell><cell>19.3 23.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DGCNN</cell><cell>25.9</cell><cell>17.3</cell><cell>28.9</cell><cell>11.4 11.5 17.3</cell><cell>29.1</cell><cell>11.9 10.9 20.4</cell><cell>32.1</cell><cell>16.8 12.3 18.1</cell><cell>28.8</cell><cell cols="2">13.0 12.6 20.7</cell><cell>36.8</cell><cell cols="2">13.8 11.5</cell></row><row><cell>RSCNN</cell><cell>26.2</cell><cell>17.9</cell><cell>25.0</cell><cell>13.0 15.8 21.6</cell><cell>28.3</cell><cell>19.0 17.6 19.8</cell><cell>29.7</cell><cell>15.5 14.1 21.2</cell><cell>26.8</cell><cell>17.4 19.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PCT</cell><cell>25.5</cell><cell>16.3</cell><cell>27.1</cell><cell>10.5 11.2 16.5</cell><cell>25.8</cell><cell>12.6 11.1 19.5</cell><cell>30.3</cell><cell>16.7 11.5 17.3</cell><cell>25.0</cell><cell cols="2">12.0 15.0 18.4</cell><cell>29.3</cell><cell cols="2">14.7 11.1</cell></row><row><cell>SimpleView</cell><cell>27.2</cell><cell>19.7</cell><cell>31.2</cell><cell>11.3 16.5 20.6</cell><cell>29.1</cell><cell>15.6 17.0 21.5</cell><cell>32.7</cell><cell>17.1 14.8 20.4</cell><cell>28.4</cell><cell>14.6 18.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Average</cell><cell>26.1</cell><cell>18.7</cell><cell>28.5</cell><cell>12.7 14.8 19.6</cell><cell>27.6</cell><cell>16.3 14.9 21.0</cell><cell>30.6</cell><cell>18.2 14.1 20.5</cell><cell>26.8</cell><cell>17.3 17.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>indicates, all the augmentation recipes enhanceFigure 4. Average Error Rates over Six Models and 13 Corruptions (except for Occlusion and LiDAR). We exclude Occlusion and LiDAR since they do not have different severity levels.the overall corruption robustness from 19.6% to 28.4%. Data augmentation methods enrich the training set the resulting model more general. Combined with results in Table 1, our paper suggests different conclusions from the original claims: mixing-based augmentations have little gain on clean performance but help generalize to common corruptions.</figDesc><table><row><cell>Average Error Rate (%)</cell><cell>5 10 15 20 25 30</cell><cell>Severity-1 Severity-2</cell><cell>Severity-3 Severity-4</cell><cell>Severity-5</cell></row><row><cell></cell><cell>0</cell><cell cols="2">Standard PointCutMix-R PointCutMix-K PointMixup</cell><cell>RSMix</cell></row><row><cell cols="5">? Insight 3.2. No single data augmentation can rule them all.</cell></row><row><cell cols="5">Different augmentation methods have expertise on distinct</cell></row><row><cell cols="3">corruption patterns.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Error Rates of Different Model Architectures on ModelNet40-C with Different Test-time Adaptation Methods. BN TENT Model (%) ? ER cor Density Noise Trans. ER cor Density Noise Trans.</figDesc><table><row><cell>PointNet</cell><cell>25.9</cell><cell>26.8</cell><cell>29.6 21.2 27.3</cell><cell>27.8</cell><cell>31.0 23.0</cell></row><row><cell>PointNet++</cell><cell>16.9</cell><cell>24.2</cell><cell>12.7 13.8 16.7</cell><cell>24.1</cell><cell>12.3 13.6</cell></row><row><cell>DGCNN</cell><cell>21.1</cell><cell>31.4</cell><cell>19.5 12.5 20.9</cell><cell>30.7</cell><cell>19.4 12.5</cell></row><row><cell>RSCNN</cell><cell>20.0</cell><cell>26.4</cell><cell>16.7 17.0 19.4</cell><cell>26.0</cell><cell>15.9 16.4</cell></row><row><cell>PCT</cell><cell>19.5</cell><cell>27.9</cell><cell>18.2 12.4 18.0</cell><cell>26.9</cell><cell>15.4 11.7</cell></row><row><cell cols="2">SimpleView 19.8</cell><cell>29.8</cell><cell>13.8 15.8 16.9</cell><cell>28.1</cell><cell>9.9 12.8</cell></row><row><cell>Average</cell><cell>20.5</cell><cell>27.7</cell><cell>18.4 15.4 19.9</cell><cell>27.3</cell><cell>17.3 15.0</cell></row><row><cell cols="6">? Insight 3.3. Adversarial training does not show superiority</cell></row><row><cell cols="6">on corruption robustness for 3D point cloud recognition.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Error Rates of Different Model Architectures on ModelNet40-C with PointCutMix-R and TENT.</figDesc><table><row><cell cols="7">Corruption (%) ? PointNet PointNet++ DGCNN RSCNN PCT SimpleView</cell></row><row><cell>Density</cell><cell>24.8</cell><cell>21.8</cell><cell>24.7</cell><cell cols="2">22.7 22.8</cell><cell>23.5</cell></row><row><cell>Noise</cell><cell>15.7</cell><cell>9.5</cell><cell>9.2</cell><cell>10.7</cell><cell>9.0</cell><cell>9.7</cell></row><row><cell>Trans.</cell><cell>15.2</cell><cell>12.1</cell><cell>10.4</cell><cell cols="2">12.9 10.1</cell><cell>13.3</cell></row><row><cell>ERcor</cell><cell>18.5</cell><cell>14.5</cell><cell>14.8</cell><cell cols="2">15.4 13.9</cell><cell>15.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>General blackbox adversarial sensor attack and countermeasures. In 29th {USENIX} Security Symposium ({USENIX} Security 20), pp. 877-894, 2020a. Sun, J., Koenig, K., Cao, Y., Chen, Q. A., and Mao, Z. M. On adversarial robustness of 3d point cloud classification under adaptive attacks. arXiv preprint arXiv:2011.11922, 2020b.</figDesc><table /><note>Shi, S., Wang, X., and Li, H. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 770-779, 2019. Shi, S., Guo, C., Jiang, L., Wang, Z., Shi, J., Wang, X., and Li, H. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10529-10538, 2020. Sun, J., Cao, Y., Chen, Q. A., and Mao, Z. M. Towards robust lidar-based perception in autonomous driving:Sun, J., Cao, Y., Choy, C. B., Yu, Z., Anandkumar, A., Mao, Z. M., and Xiao, C. Adversarially robust 3d point cloud recognition using self-supervisions. Advances in Neural Information Pro- cessing Systems, 34, 2021a.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, 15, and 16.</figDesc><table><row><cell>Occlusion</cell><cell>LiDAR</cell><cell>Local_Density_Inc</cell><cell>Local_Density_Dec</cell><cell>Cutout</cell></row><row><cell>Uniform</cell><cell>Gaussian</cell><cell>Impulse</cell><cell>Upsampling</cell><cell>Background</cell></row><row><cell>Rotation</cell><cell>Shear</cell><cell>FFD</cell><cell>RBF</cell><cell>Inv_RBF</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Error Rates of Different Models with Different Test-time Adaptation Methods on ModelNet40-C.</figDesc><table><row><cell>15 20 25 30 35 15 20 25 30 35</cell><cell>1 1</cell><cell>PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 PointCutMix-R training and RBF corruption 3 4 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 PointCutMix-R training and Inv_RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 Standard testing and Occlusion corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 Standard testing and LiDAR corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView Standard testing and Upsampling corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 Standard testing and Local_Density_Inc corruption 1 1 1 1 1 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Local_Density_Dec corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Cutout corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Uniform corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Gaussian corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Impulse corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 1 2 3 4 5 Standard testing and Background corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Rotation corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 Standard testing and Shear corruption Figure 13. 1 3 4 5 Standard training and RBF corruption 2 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 12.5 15.0 17.5 20.0 22.5 25.0 27.5 2 3 4 5 Standard training and Inv_RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 12.5 15.0 17.5 20.0 22.5 25.0 27.5 50 60 65 55 50 55 60 65 70 75 80 85 20 25 30 35 15 10.0 12.5 15.0 17.5 20.0 22.5 25.0 10 15 20 25 10 15 20 25 30 35 10 15 20 25 30 35 40 10 15 20 25 30 20 30 40 50 60 70 20 40 60 80 10 20 30 40 50 60 70 10 15 20 25 30 35 40 PointNet PointNet++ DGCNN RSCNN PCT SimpleView</cell><cell>5 5</cell><cell>15 20 25 30 10 15 20 25 30 10 15 20 25 30 50 55 60 40 45 40 45 50 55 60 65 70 14 16 18 12 10 8 10 12 14 16 7.5 10.0 12.5 15.0 17.5 20.0 22.5 7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 7.5 10.0 12.5 15.0 17.5 20.0 22.5 8 10 12 14 16 15 20 25 30 35 20 40 60 80 10 20 30 40 50 60 10 15 20 25 30 35</cell><cell>1 1 1 1 1 1 BN testing and Local_Density_Inc corruption 2 3 4 PointCutMix-K training and FFD corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 PointCutMix-K training and RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 PointCutMix-K training and Inv_RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView BN testing and Occlusion corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 2 3 4 5 BN testing and LiDAR corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView BN testing and Upsampling corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Local_Density_Dec corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Cutout corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Uniform corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Gaussian corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Impulse corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 1 2 3 4 5 BN testing and Background corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Rotation corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 BN testing and Shear corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView</cell><cell>5 5 5</cell><cell>15 20 25 30 12.5 15.0 17.5 20.0 22.5 25.0 27.5 12 14 16 18 20 22 24 50 55 60 40 45 40 45 50 55 60 65 14 16 12 70 10 10 12 14 16 8 10 12 14 16 18 10 12 14 16 8 10 12 14 16 18 20 22 8 10 12 14 16 10 15 20 25 30 8 20 40 60 80 10 20 30 40 50 60</cell><cell>1 1 1 1 1 1 TENT testing and Local_Density_Inc corruption 2 3 4 PointMixup training and FFD corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 PointMixup training and RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 PointMixup training and Inv_RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView TENT testing and Occlusion corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 2 3 4 5 TENT testing and LiDAR corruption TENT testing and Upsampling corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 5 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Local_Density_Dec corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Cutout corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Uniform corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Gaussian corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Impulse corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 1 2 3 4 5 TENT testing and Background corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 1 2 3 4 5 TENT testing and Rotation corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView</cell><cell>5 5 5</cell><cell>15 20 25 30 35 10 15 20 25 30 35 40 45 15 20 25 30 35 40</cell><cell>1 1 1</cell><cell>2 RSMix training and FFD corruption 3 4 PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 RSMix training and RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView 2 3 4 RSMix training and Inv_RBF corruption PointNet PointNet++ DGCNN RSCNN PCT SimpleView</cell><cell>5 5 5</cell></row></table><note>Figure 12. Class-wise Mean Error Rates of Different Models with Different Data Augmentation Strategies on ModelNet40-C (Cont'd).</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">University of Michigan, Ann Arbor 2 Lawrence Livermore National Laboratory 3 NVIDIA 4 ASU. Correspondence to: Jiachen Sun &lt;jiachens@umich.edu&gt;.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d point cloud analysis for automatic inspection of aeronautical mechanical assemblies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Adallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Orteu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolives</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jovan?evi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth International Conference on Quality Control by Artificial Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11172</biblScope>
		</imprint>
	</monogr>
	<note>111720U. International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Are transformers more robust than cnns? In NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explorations for real-time point cloud rendering of natural scenes in virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bonatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ercek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lafruit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on 3D Imaging (IC3D)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomalous example detection in deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="132330" to="132347" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Calian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gyorgy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01086</idno>
		<title level="m">S. Defending against image corruptions through adversarial augmentations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d point cloud compression: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Preda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th International Conference on 3D Web Technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">On evaluating adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="345" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III 16</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A morphing-based 3d point cloud reconstruction framework for medical image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods and programs in biomedicine</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105495</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2206" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Debenedetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09670</idno>
		<title level="m">Robustbench: a standardized adversarial robustness benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-robust 3d point recognition via gather-vector guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11513" to="11521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient geometrical parametrisation techniques of interfaces for reduced-order modelling: application to fluid-structure interaction coupling problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rozza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Fluid Dynamics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="158" to="169" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05304</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pct</surname></persName>
		</author>
		<title level="m">Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-ofdistribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8340" to="8349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions Landrieu, L. and Simonovsky, M. Large-scale point cloud semantic segmentation with superpoint graphs</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularization strategy for point cloud via rigidly mixed sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15900" to="15909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point Cloud Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05795</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
		<title level="m">Convolution on x-transformed points. Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extending adversarial attacks and defenses to deep 3d point cloud classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2279" to="2283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Provably robust 3d point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointguard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6186" to="6195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tof lidar development in autonomous vehicle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 3rd Optoelectronics Global Conference (OGC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-performance virtual reality volume rendering of original optical coherence tomography point-cloud data enhanced with real-time ray casting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Maloca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E R</forename><surname>De Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mushtaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mon-Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Scholl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Balaskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tufail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Translational vision science &amp; technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voxnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Benchmarking robustness in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07484</idno>
	</analytic>
	<monogr>
		<title level="m">Autonomous driving when winter is coming</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11273</idno>
		<title level="m">On interaction between augmentations and corruptions in natural corruption robustness</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d point cloud segmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 6th IEEE conference on robotics, automation and mechatronics (RAM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A review of point cloud registration algorithms for mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<title level="m">Deep hierarchical feature learning on point sets in a metric space</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The Shapley value: essays in honor of Lloyd S. Shapley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improving robustness against common corruptions by covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16971</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Free-form deformation of solid geometric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Parry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 13th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Interpreting representation quality of dnns for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Verifiability and predictability: Interpreting utilities of network architectures for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="10703" to="10712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00659</idno>
		<title level="m">Certified adversarial defenses meet out-of-distribution corruptions: Benchmarking robustness and simple baselines</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10726</idno>
		<title level="m">Fully test-time adaptation by entropy minimization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="15607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep closest point: Learning representations for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3523" to="3532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Point cloud noise and outlier removal for image-based 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating 3d adversarial point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9136" to="9144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Lidar with velocity: Motion distortion correction of point clouds from oscillating scanning lidars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09497</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Center-based 3d object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11784" to="11793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">3d medical point transformer: Introducing convolution to attention networks for medical point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04863</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcutmix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01461</idno>
		<title level="m">Regularization strategy for point cloud classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Point</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transformer</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On isometry robustness of deep 3d point cloud models under adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dup-net: Denoiser and upsampler network for 3d adversarial point clouds defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">3d shape generation and completion through point-voxel diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03670</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Open3d</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<title level="m">A modern library for 3D data processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
