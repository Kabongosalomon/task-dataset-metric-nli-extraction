<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Shavrina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Research University Higher School of Economics / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alena</forename><surname>Fenogenova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Emelyanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Moscow Institute of Physics and Technology / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Shevelev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Artemova</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National Research University Higher School of Economics / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Malykh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Mikhailov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Research University Higher School of Economics / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tikhonova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Research University Higher School of Economics / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Chertok</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Evlampiev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sberbank / Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>4 Huawei Noah&apos;s Ark lab/ Moscow, Russia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce an advanced Russian general language understanding evaluation benchmark -RussianGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general intellectual skills -detection of natural language inference, commonsense reasoning, ability to perform simple logical operations regardless of text subject or lexicon. For the first time, a benchmark of nine tasks, collected and organized analogically to the SuperGLUE methodology (Wang et al., 2019), was developed from scratch for the Russian language. We provide baselines, human level evaluation, an opensource framework for evaluating models and an overall leaderboard of transformer models for the Russian language. Besides, we present the first results of comparing multilingual models in the adapted diagnostic test set and offer the first steps to further expanding or assessing state-of-the-art models independently of language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of technologies for text processing and then deep learning methods for obtaining better text representation, language models went through the increasingly advanced stages of natural language modelling.</p><p>Modern scientific methodology is beginning to gradually explore universal transformers as an independent object of study -furthermore, such models show the ability to extract causal relationships in texts (natural language inference), common sense and world knowledge and logic (textual entailment), to generate coherent and correct texts. An actively developing field of model interpretation develops testing procedures comparing their performance to a human level and even the ability to reproduce some mechanisms of human brain functions.</p><p>NLP is gradually absorbing all the new areas responsible for the mechanisms of thinking and the theory of artificial intelligence.</p><p>Benchmark approaches are being developed, testing general intellectual "abilities" in a text format, including complex input content, but having a simple output format. Most of these benchmarks (for more details see Section 2) make the development of machine intelligence anglo-centric, while other, less widespread languages, in particular Russian, have other characteristic linguistic categories to be tested.</p><p>In this paper, we expand the linguistic diversity of the testing methodology and present the first benchmark for evaluating universal language models and transformers for the Russian language, together with a portable methodology for collecting and filtering the data for other languages.</p><p>The contribution of RussianGLUE is two-fold. First, it provides nine novel datasets for the Russian language covering a wide scope of NLU tasks. The choice of the tasks are justified by the design of prior NLU benchmarks <ref type="bibr" target="#b20">(Wang et al., 2018</ref><ref type="bibr" target="#b19">(Wang et al., , 2019</ref>. Second, we evaluate two widely used deep models to establish baselines.</p><p>The remainder is structured as follows. We overview multiple prior works on developing NLU benchmarks, including those designed for languages other than English, in Section 2. Section 3.1 lists the tasks and novel datasets, proposed for the Russian NLU. Section 4 presents with the baselines, established for the tasks, including a human level baseline. We overview compare achieved results in Section 2 to the current state of English NLU. We discuss future work directions and emphasize the importance of NLU benchmarks for languages other than English in Section 6. Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several benchmarks have been developed to evaluate and analyze word and sentence embeddings over the past few years.</p><p>SentEval <ref type="bibr" target="#b1">(Conneau and Kiela, 2018)</ref> is one of the first frameworks intended to evaluate the quality of sentence embeddings. A twofold set of transfer tasks is used to assess the generalization power of sentence embedding models. The transfer tasks comprise downstream tasks, in which the sentence embedding is used as a feature vector, and probing tasks, which are aimed to evaluate the capability of sentence embeddings to encode linguistic properties. The choice of the downstream tasks is limited to sentiment classification, natural language inference, paraphrase detection and image captioning tasks. The probing tasks are meant to analyse morphological, syntactical and semantical information encoded in sentence embeddings.</p><p>The General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b20">(Wang et al., 2018)</ref> benchmark is a collection of tools for evaluating the performance of language models across a diverse set of existing natural language understanding (NLU) tasks, adopted from different sources. These tasks are divided into two parts: single sentence classification tasks and sentence pair classifications tasks subdivided further into similarity and inference tasks. GLUE also includes a hand-crafted diagnostic test, which probes for complex linguistic phenomena, such as the ability of the model to express lexical semantics and predicate-argument structure, to pose logical apparatus and knowledge representation. GLUE is recognized as a de-facto standard benchmark to evaluate transformer-derived language models. Last but not least GLUE informs on human baselines for the tasks, so that not only submitted models are compared to the baseline, but also to the human performance. The SuperGLUE <ref type="bibr" target="#b19">(Wang et al., 2019)</ref> follows GLUE paradigm for language model evaluation based on NLU tasks, providing with more complex tasks, of which some require reasoning capabilities and some are aimed at detecting ethical biases. A few recent projects reveal that GLUE tasks may be not sophisticated enough and do not require much tasks-specific linguistic knowledge <ref type="bibr" target="#b7">(Kovaleva et al., 2019;</ref><ref type="bibr" target="#b21">Warstadt et al., 2019)</ref>. Thus SuperGLUE benchmark, being more challenging, becomes much more preferable for evaluation of language models. decaNLP <ref type="bibr" target="#b13">(McCann et al., 2018)</ref> widens the scope for language model evaluation by introducing ten disparate natural language tasks. These tasks comprise not only text classification problems, but sequence tagging and sequence transformation problems. The latter include machine translation and text summarization, while the former include semantic parsing and semantic role labelling. Although decaNLP along with the associated research direction focuses on multi-task learning as a form of question answering, it supports zero-shot evaluation.</p><p>To evaluate models for languages other than English, several monolingual benchmarks were developed, such as FLUE <ref type="bibr" target="#b9">(Le et al., 2019)</ref> and CLUE <ref type="bibr" target="#b11">(Liang, 2020)</ref>, being French and Chinese versions of GLUE. These benchmarks include a variety of tasks, ranging from part-of-speech tagging and syntax parsing to machine reading comprehension and natural language inference.</p><p>To the best of our knowledge, LINSPECTOR <ref type="bibr" target="#b4">(Eichler et al., 2019</ref>) is a first multi-lingual benchmark for evaluating the performance of language models. LINSPECTOR offers 22 probing tasks to analyse for a single linguistic feature such as case marking, gender, person, or tense for 52 languages. A part of these 22 probing tasks are static, i.e. are aimed at evaluation of word embeddings, and the rest are contextual and should be used to evaluate language models. Released in early 2020 two multilingual benchmarks,  and XTREME <ref type="bibr" target="#b5">(Hu et al., 2020)</ref>, aim at evaluation of cross-lingual models. XGLUE includes 11 tasks, which cover both language understanding and language generation problems, for 19 languages. XGLUE provides with several multilingual and bilingual corpora that allow of cross-lingual model training. As for the Russian language, XGLUE provides with four datasets for POS tagging, a part of XNLI  and two datasets, crawled from commercial news website, used for news classification and news headline generation. XTREME consists of nine tasks which cover classification, sequence labelling, question answering and retrieval problems for 40 languages. Almost a half of the datasets were translated from English to the target languages with the help of professional translators. XTREME offers for the Russian language five datasets, including NER and two question-answering datasets. Both XGLUE and XTREME offer tasks that are much simpler than SuperGLUE and are aimed at evaluation of cross-lingual models rather than at comparison of mono-lingual models in similar setups. Thus the need for novel datasets targeted at mono-lingual model evaluation for languages other than English is still not eliminated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RussianGLUE Overview</head><p>We have intenooed to have the same task set in the framework as one in the SuperGLUE. There is no one-to-one mapping, but the corpora we use could be considered close to the specified tasks in the SuperGLUE framework.</p><p>We divided the tasks into six groups, covering the general diagnostics of language models and different core tasks: common sense understanding, natural language inference, reasoning, machine reading and world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks</head><p>The tasks description is provided below. The samples from the tasks are presented at figs. 1 to 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Diagnostics</head><p>LiDiRus: Linguistic Diagnostic for Russian is a diagnostic dataset that covers a large volume of linguistic phenomena, while allowing you to evaluate information systems on a simple test of textual entailment recognition. This dataset was translated from English to Russian with the help of professional translators and linguists to ensure that the desired linguistic phenomena remain. This dataset corresponds to AX-b dataset in SuperGLUE benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Common Sense RUSSE:</head><p>Word in context is a binary classification task, based on word sense disambiguation problem. Given two sentences and a polysemous word, which occurs in both sentences, the task is to determine, whether the word is used in the same sense in both sentences, or not. For this task we used the Russian word sense disambiguation dataset RUSSE <ref type="bibr" target="#b14">(Panchenko et al., 2015)</ref> and converted it into WiC dataset format from SuperGLUE. PARus: The choice of Plausible Alternatives for Russian language evaluation provides researchers with a tool for assessing progress in open-domain commonsense causal reasoning. Each question in PARus is composed of a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. The correct alternative is randomized so that the expected performance of randomly guessing is 50%. PARus is constructed as a translation of COPA dataset from SuperGLUE and edited by professional editors. The data split from COPA is retained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Natural Language Inference</head><p>TERRa: Textual Entailment Recognition for Russian is a dataset which is devoted to capture textual entailment. The task of textual entailment has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text. The corresponding dataset in SuperGLUE is RTE, which in its place is constructed from NIST RTE challenge series corpora. To collect TERRa we filtered out the large scale Russian webcorpus, Taiga <ref type="bibr" target="#b18">(Shavrina and Shapovalova, 2017)</ref> with a number of rules to extract suitable sentence pairs and manually corrected them. The rules had the following structures: there should be a mental verb in the first sentence and the second sentence should be attached to the first one by a subordinate conjunction. To ensure the literary language of the extracted sentences, we processed only news and fiction parts of Taiga and made sure, that the sentences contain only frequently used words (i.e. number instances per million, IPM is higher than 1). The word frequencies were estimated according to Russian National Corpus 1 .</p><p>RCB: The Russian Commitment Bank is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional). Similarly to the design of TERRa dataset, we filtered out Taiga with a number of rules and manually post processed the extracted passages. Final labelling was conducted by three of the authors. This dataset corresponds to CommonBank dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Machine Reading</head><p>MuSeRC: Russian Multi-Sentence Reading Comprehension is a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills. The task is actually a binary classification, whether the answer to the question is correct or not. Each example consists of numerated passage, question and answers. Our dataset contains approximately 6000 questions for more than 800 paragraphs across 5 different domains, namely: 1) elementary school texts, 2) news, 3) fiction stories, 4) fairy tales, 5) brief annotations of TV series and books. First, we have collected open sources data from different domains and automatically preprocessed them, filtered only those paragraphs that corresponds to the following parameters: 1) paragraph length 2) number of named entities 3) number of coreference relations. Afterwords we have checked the correct splitting on sentences and numerate each of them. Next, in Toloka 3 we have generated the crowd sourcing task to get the following information: 1) generate questions 2) generate answers 3) check that to solve every question a human needs more than one sentence in the text. Collecting the dataset we adhere the principles of MultiRC <ref type="bibr" target="#b6">(Khashabi et al., 2018)</ref>: a) We exclude any question that can be answered based on a single sentence from a paragraph; b) Answers are not written in the full match form in the text; c) Answers to the questions are independent from each other. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RuCoS: Russian reading comprehension with</head><p>Commonsense reasoning is a large-scale dataset for machine reading comprehension requiring commonsense reasoning. The dataset construction is based on ReCoRD methodology <ref type="bibr" target="#b22">(Zhang et al., 2018)</ref>. RuCoS consists of passages and cloze-style queries automatically generated from Russian news articles, namely Lenta 4 and Deutsche Welle 5 . Each sample from the dev and test sets was validated by crowd workers. The answer to each query is a text span that corresponds to one or more referents of the answer entity in the context. The answer entity may be expressed by an abbreviation, an acronym or a set of surface forms. Hence, the task requires understanding of rich inflectional morphology and lexical variability of Russian. The goal of RuCoS is to test a machine's ability to infer the answer based on the commonsense reasoning and knowledge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">World Knowledge</head><p>DaNetQA: This question-answering corpus follows BoolQ <ref type="bibr" target="#b0">(Clark et al., 2019)</ref> design: it comprises natural yes/no questions. Each question is paired with a paragraph from Wikipedia and an answer, derived from the paragraph. The task is to take both the question as input and a paragraph and come up with a yes/no answer, i.e. to produce a binary output. DaNetQA was collected in a few steps: 1) we used crowd workers to compose candidate yes/no questions; 2) we used Google API to retrieve relevant Wikipedia pages by treating each question as a search query; 3) we queried a pretrained BERT-based model for SQuAD <ref type="bibr" target="#b8">(Kuratov and Arkhipov, 2019)</ref> to extract relevant paragraphs from Wikipedia pages, using candidate questions; 4) finally, we used crowd workers to evaluate each question and paragraph pair and provide the desired yes/no answers. We ensure high quality of the dataset by using a high overlap for annotation at the last step and a number of control gold-standard control questions, labelled by two of the authors. The core difference of DaNetQA to BoolQ is that some question may occur multiple times in the dataset, as at the step 3) we may retrieve more than one relevant paragraph. To make the dataset more challenging, we admit contradictory answers to a question if these answers are implied from the passages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Statistics for the Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring</head><p>Following <ref type="bibr" target="#b19">(Wang et al., 2019)</ref>, we calculate scores for each of the tasks based on their individual metrics. All metrics are scaled by 100x (i.e., as percentages). These scores are then averaged to get the final score. For the tasks with multiple metrics, the metrics are averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>In this section, we provide a two-step baseline design. At first we have developed a na?ve baseline based on the TF-IDF model (section 4.1.1), and then evaluate state-of-the-art models for Russian language (section 4.1.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Na?ve Baseline</head><p>We used Scikit-learn package <ref type="bibr" target="#b15">(Pedregosa et al., 2011)</ref> to train a TF-IDF model. We used a 20 thousand sample from Wikipedia, from Russian and English sites equally. We restricted a vocabulary to 10 thousand most common words. Then for each task set a logistic regression was trained to predict an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics RuBERT</head><p>MultiBERT  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Advanced Baselines</head><p>We leverage two BERT-derived models as baseline. Multilingual BERT (MultiBERT), released by <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, is a single language model pre-trained from monolingual corpora in 104 languages, Russian texts being a part of training data.</p><p>MultiBERT uses a shared vocabulary for all languages. The capabilities of MultiBERT for zeroshot cross-lingual tasks have been recently studied by <ref type="bibr" target="#b16">(Pires et al., 2019)</ref>. Russian BERT (RuBERT) was trained on large-scale corpus of news and Wikipedia in Russian. To alleviate the training all weights except sub-word embeddings were borrowed from MultiBERT. The sub-word vocabulary was obtained from the same training corpus and the new mono-lingual embeddings were transformed from the multi-lingual ones. This allowed to incorporate longer Russian sub-word units into the vocabulary. This model is part of DeepPavlov framework <ref type="bibr" target="#b8">(Kuratov and Arkhipov, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Evaluation</head><p>We include human performance estimates for all provided benchmark tasks, including the diagnostic set. We estimate human performance by hiring crowd workers via Toloka platform to re-annotate a sample from each task test set. We suggest a two step procedure: 1) a crowd worker is provided with an instruction and completes a short training phase before proceeding to the annotation phase, 2) a crowd worker that passed through the training phase solves the original test set.</p><p>For the annotation phase we ask crowd workers to annotate the full test sets except for the RUSSE and the RuCoS datasets, where we randomly sampled only 5000 and 1000 examples from the tasks' test sets, respectively. For each sample, we collect annotations from three to five crowd workers and take a majority vote to estimate human performance. In annotation phase we add control questions to prevent the crowd workers from cheating. As a result, we reject the annotations from the crowd workers that fail the training phase and do not include the results of those who achieved low performance on the control tasks. The results of human evaluation are presented in <ref type="table" target="#tab_3">Table 2</ref>. The example of a Toloka task is provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The analysis of <ref type="table" target="#tab_3">Table 2</ref> can give an exact representation of the baseline model performance, which still remains significantly different from the human level. Nevertheless, the task of resolving the ambiguity of the word meaning in context (RUSSE) was solved by both monolingual and multilingual BERT at a level significantly exceeding the human one (0.89 vs 0.74). Besides, the monolingual model is showing a slightly higher quality than that of the multilingual one, especially prevailing textual entailment tasks (RCB, TERRA, PARus), disambiguating word meaning (RUSSE) and reading comprehension (MuSeRC). The multilingual model shows the most excellent result on the smallest dataset on commonsense QA task (DaNetQA) and also on commonsense-related task on machine reading (RuCoS).</p><p>We hope that our benchmark will help to excel the performance of models for the Russian language in the future, and will favour achieving comparably high results.</p><p>Can the results of a multilingual BERT on Russian and English data be considered analo-gous? Based on the results of the assessment, MultiBERT in English gets an overall score of 60.8 6 , while on RussianGLUE task set an overall score of 54.2 is achieved-6% lower, but noting that the English benchmark includes additionally Winograd Gender Parity <ref type="bibr" target="#b10">(Levesque et al., 2012)</ref> dataset, giving SOTA models from 90 to 93% of accuracy added to the overall assessment. In the next section, a detailed comparison of the multilingual model performance is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to SuperGLUE</head><p>As mentioned in Section 3.1, the diagnostic dataset has been obtained by professional translation with preservation of the original linguistic features mentioned. Thus being said, this diagnostic data is the first of its kind that allows drawing a multilingual analogy of comparable models.  <ref type="bibr">vlin et al., 2019)</ref>, we conducted sequential model pretraining in English and Russian using the RTE dataset, and then tested the models on the diagnostic set, as long as the task requires exactly the same format. Predictions were further scored using 6 Jiant, full SuperGLUE task set Matthews' correlation (MCC), and correlation for different linguistic features was computed. The results are presented in <ref type="figure" target="#fig_7">Figure 8</ref>.</p><p>First of all, it could be noticed that the English variant of the model performs slightly better and shows a higher overall correlation of 0.2 compared to 0.15 for the Russian variant. This could be due to an asymmetry of the quality of the multilingual model and its better understanding of the English language in general.</p><p>As for the models' performance in the context of different linguistic features, the results generally coincide. For those categories for which correlation is low in English, the result in Russian is in most cases poor as well (for instance, Redundancy, Nominalization, Intervals/Numbers). However, there exist several categories which are much better solved in English than in Russian such as PA Structure, Ellipsis/Implicits, Genetives/Partitives, Prepositional phrases, Datives -mostly low-level and/or syntactically driven categories, that may indicate that optimal hyperparameters of BERT architecture are much more suitable for English syntax and may not be linguistically universal. Similarly, we could find categories which show an extremely high correlation in Russian and low correlation in English (Factivity, Coordination scope, Restrictivity and Existential) -high-level logical and semantic categories.</p><p>These numbers compared to the ones for English could be explained by the fact that the language features now included in the diagnostics are not exactly linguistically universal in different languages and are mostly focused on the English language (at least those syntactic ones). Thus, for the comprehensive cross-linguistic typological analysis of possible linguistic features should be reviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We hope that our project will give a start to new research in the application of universal language models and transformers, including multilingual ones. Our example of an analysis of translated diagnostics shows that even in languages of the same European family (which Russian and English belong to), significant differences in the influence of linguistic categories on model performance are possible. One of the directions of the next studies, we consider detailed experiments on the influence of model parameters and language categories in data on the quality of the model in different languages.</p><p>An independent problem for the English original leaderboard is that a gradual improvement in the quality of models allows us to exceed the human performance level in individual tasks, as happened with the T5 <ref type="bibr" target="#b17">(Raffel et al., 2019)</ref> model. We expect that a similar situation will soon happen on Russian data, which means that when releasing straight off with complex SuperGLUE tasks, we will still be focused on adding tasks of a higher level of complexity in the future. Such tasks can become those that are obviously inaccessible to models for the "understanding" of long texts and documents, seq2seq tasks, tasks that require knowledge graphs.</p><p>In the further development of our leaderboard, we also see the possibility of adding an industrial assessment of models: for fair ranking and ease of use, all models could receive an estimate of the required memory resources, an estimate of performance, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we present the first benchmark on general language understanding evaluation for the Russian language. The benchmark including nine task sets is aimed to test BERT-like models for their ability to perform entailment recognition, commonsense reasoning and machine reading while denoising various linguistic features added on the level of semantics, logical and syntactic structure.</p><p>We invite developers, researchers, and AI experts to join our project. Further development of the benchmark includes areas such as evaluation of industrial performance of models on the leaderboard and multilingual diagnostics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A sample from RUSSE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A sample from PARus dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A sample from TERRa dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A sample from RCB dataset.3.1.4 ReasoningRWSD:Winograd Schema task is devoted to coreference resolution in specifically designed experiment, where reference could be resolved only using the common sense. The Russian Winograd Schema Dataset (RWSD) is constructed as translation of the Winograd Schema Challenge 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>A sample from RWSD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>A sample from MuSeRC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>A sample from RuCoS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Russian and English Diagnostic Evaluation on Multilingual transformer, scored using Matthews' correlation (MCC). Procedure: using the original MultiBERT (De</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>below presents the characteristics of the collected datasets -examples partitioning by train/val/test, as well as the total volume in tokens and sentences. As one can see, the size of the Ru-CoS task significantly exceeds the rest of the tasks due to the articles included in the task.</figDesc><table><row><cell>Task</cell><cell>Samples</cell><cell>Sents</cell><cell>Tokens</cell></row><row><cell>LiDiRus</cell><cell>0/0/1104</cell><cell>2210</cell><cell>3.6 ? 10 4</cell></row><row><cell></cell><cell cols="2">Common Sense</cell><cell></cell></row><row><cell>RUSSE</cell><cell cols="2">19845/8508/12151 90862</cell><cell>1.1 ? 10 6</cell></row><row><cell>PARus</cell><cell>500/100/400</cell><cell>1000</cell><cell>5.4 ? 10 3</cell></row><row><cell></cell><cell>NLI</cell><cell></cell><cell></cell></row><row><cell>TERRa</cell><cell>2616/307/3198</cell><cell>13706</cell><cell>2.53 ? 10 5</cell></row><row><cell>RCB</cell><cell>438/220/348</cell><cell>2715</cell><cell>3.7 ? 10 4</cell></row><row><cell></cell><cell>Reasoning</cell><cell></cell><cell></cell></row><row><cell>RWSD</cell><cell>606/204/154</cell><cell>1541</cell><cell>2.3 ? 10 3</cell></row><row><cell></cell><cell cols="2">Machine Reading</cell><cell></cell></row><row><cell>MuSeRC</cell><cell>500/100/322</cell><cell>12805</cell><cell>2.53 ? 10 5</cell></row><row><cell>RuCoS</cell><cell>72193/4370/4147</cell><cell cols="2">583930 1.2 ? 10 7</cell></row><row><cell></cell><cell cols="2">World Knowledge</cell><cell></cell></row><row><cell cols="2">DaNetQA 392/295/295</cell><cell>6231</cell><cell>1.31 ? 10 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Cumulative task statistics.</figDesc><table><row><cell>The size</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of the human benchmark and the baseline models. MCC stands for Matthews Correlation Coefficient; Acc -Accuracy; EM -Exact Match.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.ruscorpora.ru/new/en/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://cs.nyu.edu/faculty/davise/ papers/WinogradSchemas/WS.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://toloka.yandex.ru 4 https://lenta.ru/ 5 https://www.dw.com/ru/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Ekaterina Artemova works within the framework of the HSE University Basic Research Program and funded by the Russian Academic Excellence Project "5-100".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Senteval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05449</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating crosslingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LINSPECTOR WEB: A multilingual probing suite for word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?zde</forename><surname>G?l ? Ahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-3022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11080</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Looking beyond the surface:a challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4356" to="4365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adaptation of deep bidirectional multilingual transformers for russian language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Arkhipov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07213</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Alexandre Allauzen, Beno?t Crabb?, Laurent Besacier, and Didier Schwab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibril</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Flaubert: Unsupervised language model pre-training for french</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://github.com/chineseGLUE/chineseGLUE" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<title level="m">Xglue: A new benchmark dataset for cross-lingual pretraining, understanding and generation. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2004</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The natural language decathlon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
	</analytic>
	<monogr>
		<title level="m">Multitask learning as question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Russe: the first workshop on russian semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ustalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Konstantinova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="89" to="105" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python. the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual bert?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">To the methodology of corpus construction for machine learning:&quot;taiga&quot;. syntax tree corpus and parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Shavrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Shapovalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus linguitics-2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Investigating bert&apos;s knowledge of language: Five analysis methods with npis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Blix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Alsop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2870" to="2880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
