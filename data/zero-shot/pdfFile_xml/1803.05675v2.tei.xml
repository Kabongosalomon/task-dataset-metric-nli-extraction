<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Meletis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Dubbelman</surname></persName>
						</author>
						<title level="a" type="main">Training of Convolutional Networks on Multiple Heterogeneous Datasets for Street Scene Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a convolutional network with hierarchical classifiers for per-pixel semantic segmentation, which is able to be trained on multiple, heterogeneous datasets and exploit their semantic hierarchy. Our network is the first to be simultaneously trained on three different datasets from the intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and is able to handle different semantic levelof-detail, class imbalances, and different annotation types, i.e. dense per-pixel and sparse bounding-box labels. We assess our hierarchical approach, by comparing against flat, nonhierarchical classifiers and we show improvements in mean pixel accuracy of 13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB classes. Our implementation achieves inference rates of 17 fps at a resolution of 520 x 706 for 108 classes running on a GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Semantic classification is a key task in the perception sub-system of an autonomously driving vehicle <ref type="bibr" target="#b0">[1]</ref>. The segmentation task, posed as per-pixel classification, has seen great progress in the past years <ref type="bibr" target="#b1">[2]</ref> due to deep learning techniques. However, two critical challenges that still need to be addressed are: 1) to utilize as much and diverse training data as possible, and 2) to increase the number of recognizable classes from a few dozens to virtually anything that a scene can contain.</p><p>In this work, we take steps towards solving both challenges and we present a method that leverages multiple heterogeneous datasets, i.e. datasets with different classes and annotation types, to train a fully convolutional network for per-pixel semantic segmentation. This approach facilitates better use of available datasets, thereby reducing annotation effort, and increases the number of classes that can be recognized. The datasets that we are using in the context of Highly Automated Driving (HAD) are Cityscapes <ref type="bibr" target="#b2">[3]</ref>, Mapillary Vistas <ref type="bibr" target="#b3">[4]</ref>, and GTSDB <ref type="bibr" target="#b4">[5]</ref>.</p><p>The first challenge, i.e. training for semantic segmentation with diverse annotations, is tackled in previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> by external components to the network, in order to generate pseudo per-pixel ground truth. Our method, in contrast, is self-inclusive and uses the networks' own outputs to refine non-compatible, diverse annotations for supervision.</p><p>The second challenge, i.e. to increase the number of recognizable classes, can be accomplished in two ways: 1) continue per-pixel annotating an existing dataset with the extra (sub)classes, e.g. <ref type="bibr" target="#b7">[8]</ref>  Our hierarchical classification convolutional network during inference. The input image is transformed to a shared feature representation, which is connected to a hierarchy of classifiers though adaptation subnetworks. The Level-1 classifier outputs predictions for every pixel of the image, while each subsequent classifier infers only about its own set of classes. The output of all levels is combined to form the final fine-grained per-pixel segmentation. datasets only for the new (sub)classes. The first approach can be very costly for big datasets and mainly unnecessary, as a plethora of datasets with fine-grained (sub)classes exist (e.g. traffic sign types, car models, pedestrians). In our work, we research the second approach. For this, the heterogeneity, i.e. different label spaces and annotations types, of datasets poses challenges for combining them with traditional "flat", i.e. non-hierarchical, classifiers. Therefore, we propose the use of hierarchical classifiers, which explicitly take advantage of the semantic relationships between the datasets, and we compare against flat classifiers. Our hierarchy is comparable to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, but differs in the scalability it offers.</p><p>In Sec. II, we describe the exact challenges that are addressed by our hierarchical approach. An example is combined training on Cityscapes and GTSDB. In that case, all classes of GTSDB are subclasses of the traffic sign class in Cityscapes. The straightforward approach of combining classes from both datasets in a conventional flat classifier is infeasible, since a traffic sign pixel cannot have different labels depending on the dataset it comes from. This poses challenges for end-to-end training and inference for flat classifiers, which are addressed by our hierarchical classification scheme.</p><p>The fundamentals of our general hierarchical approach are provided in Sec. III and in Sec. IV we provide the specifics of our implementation. In Sec. V, we demonstrate the performance gain of hierarchical classifiers with the three heterogeneous datasets, over flat, non-hierarchical classifiers. Furthermore, we show that multi-dataset training of a common feature representation, using our proposed method, can improve performance across all datasets regardless of their structural differences.</p><p>To summarize, the contributions of this work to per-pixel semantic segmentation are:</p><p>? A methodology for combined training on datasets with disjoint, but semantically connected, label spaces. ? A modular architecture of hierarchical classifiers that can replace the classification stage in modern convolutional networks. Our system implementation is made available to the research community <ref type="bibr" target="#b10">[11]</ref>. Additionally, we provide our perpixel annotations of the Cityscapes dataset for the GTSDB traffic sign subclasses, which we use for validation purposes, but is not required for training. We refer to this dataset as Cityscapes Extended throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CHALLENGES FROM MULTIPLE DATASET TRAINING</head><p>End-to-end supervised training on multiple datasets can face many challenges due to the structural differences of the datasets. The most important challenges can be categorized in the following groups:</p><p>Semantic level-of-detail: Each dataset is labeled with a set of semantic classes. In the naive flat classification approach, the output of the classifier will be the union of classes from all datasets. It is highly probable that the semantics of a class in one dataset contain, or are contained in, the semantics of a class of another dataset. If those classes are placed in the same level, as in the case of a flat classifier, a conflict of supervision happens, since some pixels belonging to the same semantic class will be labeled with different classes.</p><p>For our three datasets, this challenge appears in three cases: 1) Cityscapes defines its road class as: "part of ground on which cars usually drive", which includes lane markings, bicycle lanes, potholes, etc. In Vistas, these fine-grained subclasses are separately labeled, in addition to the a road class, leading to conflicts in the semantic level-of-detail of labels, 2) Cityscapes and Vistas contain one high-level traffic sign class, while GTSDB has 43 traffic sign subclasses, and 3) Cityscapes has only one rider class, while Vistas differentiates between three different rider subclasses. Introducing a hierarchy of labels, as shown in <ref type="figure">Fig. 2</ref>, effectively solves this challenge, and is discussed in more detail in Sec. III-A.</p><p>Annotation types: Semantic segmentation is by definition a per-pixel problem, thus supervision must be provided at the pixel level. Unfortunately, many existing datasets have bounding-box or per-image annotations, which are incompatible for per-pixel training. The direct approach to make them compatible would be to convert those annotations to masks. However, these masks will include pixels that don't belong to the object of interest, for example bounding boxes might include many non-relevant background pixels that will be assigned to the foreground class. Eventually, during training, supervision to the network will flow from incorrectly labeled pixels, leading to weights confusion.</p><p>In our case, Cityscapes and Vistas have per-pixel annotations, but GTSDB has only bounding box annotations. In order to include GTSDB during training, we propose a novel hierarchical loss, provided in III-D, which uniformly handles supervision from different annotation types.</p><p>Training sample imbalances: Batch-wise training suffers from class imbalances, especially when there are limited examples per batch. In our case, we face strong intra-dataset and inter-dataset imbalances. Imbalances between datasets happen due to the big different of annotated pixels, i.e. in the order of 10 3 (see <ref type="table" target="#tab_3">Table I</ref>). Imbalances between the classes of the same dataset are common for street scene datasets, since most of the pixels belong to classes of big surfaces, like road and buildings. Our method deals with imbalances by placing classes with the similar order of examples in the same classifier and thus all classes have bigger probability to be represented in the same batch. This strategy is highly beneficial, as shown in Sec. V-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TRAINING AND INFERENCE ON HETEROGENEOUS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATASETS USING SEMANTIC HIERARCHIES</head><p>In this Section, we describe the components of our general hierarchical classification methodology for an arbitrary number of heterogeneous datasets. These components provide solutions to the challenges of Sec. II and for each one we elaborate on the specific cases for our selected datasets. Our current experiments, detailed in Sec. V, are based on an implementation with a 3-level hierarchy using 3 datasets. The specifics of this implementation are provided in Sec. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic hierarchy of label spaces</head><p>Multiple dataset training requires a common label space for all selected datasets. We propose to combine individual label spaces into the common space, containing labels from all datasets, by a hierarchical manner into a semantic tree of labels. This approach solves any conflict in the semantic definition of labels, by introducing the necessary parent or intermediate nodes and/or grouping of existing labels. <ref type="figure">Figure 2</ref>, depicts the 3-level hierarchy using all labels for the three selected datasets of this paper. The challenges that arise from combining these three label spaces were described in Sec. II and are solved as follows: 1) A new high-level driveable class is introduced to solve Cityscapes and Vistas road class semantic conflict, 2) A superclass of traffic signs and an intermediate node for differentiating Vistas back from front traffic signs are added, and 3) A rider superclass is introduced to include the Cityscapes rider class and the 3 Vistas rider subclasses. The semantic hierarchy of the labels induces a corresponding hierarchy of classifiers. Each classifier classifies the children labels of a node and the whole tree of classifiers is trained, in an end-to-end, fully convolutional manner, over a shared feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convolutional network architecture</head><p>The proposed network architecture (see <ref type="figure" target="#fig_0">Fig. 1</ref> for an example) consists of a fully convolutional feature extractor for computing a dense, shared representation, and a set of classifiers, each corresponding to an inner class node of the semantic hierarchy. Every classifier can be connected with classifiers one level down in the hierarchy, in order to pass its predictions for inference, and annotation type independent training, as described in Sec. III-C, III-D. Each classifier may be preceded by a shallow adaptation network, which adapts the common representation, its depth, and receptive field to the needs of the classifier. This gives the network designer the opportunity to select different features dimensions and receptive fields for each of the classifiers. For example, discriminating between e.g. traffic signs is easier <ref type="bibr" target="#b11">[12]</ref>, as less features are needed, compared to high-level discrimination, like road vs. sidewalk and bushes vs. trees <ref type="bibr" target="#b2">[3]</ref>. The flexibility of applying a different field-of-views to different classifiers, enables more or less context aggregation, depending on the classifier's object average size: e.g. traffic signs appear generally in smaller scales than buildings or cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference: hierarchical decision rule</head><p>Inference is carried out per-pixel, in a hierarchical manner across the tree of softmax classifiers. Each classifier j computes a per-pixel normalized vector ? j,p of class probabilities for its own set of pixels p ? P j and set of classes C j = {0, 1, ...}, and outputs per-pixel decision? y j,p = argmax i ? j,p i , where? j,p ? C j . The set of pixels P j , for which every classifier must produce decisions, is generated by its parent according to its own decisions. Every pixel of the input is labeled with the desired level of detail, from the available set of labels {? j,p } j?J , where J are the classifiers that produced decisions for this specific pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training: hierarchical classification loss</head><p>The annotations type of many datasets are not compatible with the required per-pixel supervision for semantic segmentation, as outlined in Sec. II. Our proposed method treats incompatible annotations with a unified approach, without the need of external components, as in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and with negligible computational load to the system. The flexibility for handling diverse ground truth, is exchanged with the only constraint that classes on the root classifier, should have perpixel annotated examples. Annotations for any other level can be of any type or even mixed.</p><p>We propose a hierarchical classification loss, which separates supervision according to annotations type at the pixel level. Each classifier j is trained on all labeled pixels P j = P j 1 + P j 2 that correspond to its respective node in the label hierarchy. Pixels P j 1 with per-pixel annotations are trained using the standard one-hot cross-entropy loss. Pixels P j 2 with non-per-pixel annotations are trained with generated perpixel ground truth using a modified cross-entropy loss. To achieve this, our method uses the online, per-pixel decisions of the parent classifier during training, to refine the pseudo, per-pixel labels. The process is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. First, non-compatible annotations are converted to per-pixel pseudo ground truth. Then, in every training step, decisions of parent classifiers are intersected with this pseudo ground truth to generate the per-pixel ground truth used for supervision.</p><p>Both losses are accumulated per classifier to the so called hierarchical loss:  where |?| is the cardinality of the pixel's set, and y j,p ? C j selects the element of ? that corresponds to the ground truth class of pixel p for classifier j. Finally, losses from all classifiers are collected and weighted with different hyperparameters ? j to obtain the total objective to minimize:</p><formula xml:id="formula_0">L j = ? 1 P j 1 p?P j 1 log ? j,p y j,p ? 1 P j 2 p?P j 2 log ? j,p y j,p ,<label>(1)</label></formula><formula xml:id="formula_1">L total = j ? j ? L j + regularizer .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THREE-LEVEL LABEL HIERARCHY WITH CITYSCAPES, MAPILLARY VISTAS AND GTSDB</head><p>In this section, we outline implementation details, to improve the repeatability of our experiments.</p><p>Convolutional network architecture: The network is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. The feature extractor consists of the feature layers of the ResNet-50 architecture <ref type="bibr" target="#b12">[13]</ref>, followed by an 1x1 convolutional layer (with ReLU and Batch Normalization), to decrease feature dimensions to 256. The stride on the input is reduced from 32 to 8, using dilated convolutions. The representation has depth 256, spatial dimensions 1/8 of the input, and is shared among 5 branches. Each branch has an extra bottleneck module <ref type="bibr" target="#b12">[13]</ref> and ends at a softmax classifier, which includes a hybrid upsampling module. We choose the feature dimensions and the field-of-view of the per-classifier adaptation subnetworks to be the same for all branches. After experimenting with different upsampling techniques (fractional strided convolution, bilinear, convolutional), we concluded that the best performance and reduction of artifacts, is obtained by hybrid upsampling, which consists of one 2x2 learnable fractional strided convolutional layer, followed by bilinear upsampling to reach input dimensions.</p><p>Implementation details: We use Tensorflow <ref type="bibr" target="#b13">[14]</ref> and a Titan X (Pascal architecture) GPU for training and inference. Due to limited memory we set batch size to 4 (Cityscapes:Vistas:GTSDB = 1:2:1) and training dimensions to 512x706 (the average of Vistas images scaled to the smaller Cityscapes dimension). During training images are downscaled, respecting their aspect ratio, and then randomly cropped. The network is trained for 17 Vistas epochs (early stopping) with Stochastic Gradient Descent and momentum of 0.9, L2 weight regularization with decay of 0.00017, initial learning rate 0.01 that is halved three times, and batch normalization and exponential moving averages decay are both set to 0.9. The hyperparameters ? j of Eq. (2) are chosen to be 1.0, 0.1 and 0.1 for the three levels of the hierarchy respectively. For inference, we currently achieve a frame rate of 17 fps, i.e. 58 milliseconds per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head><p>We conduct the following experiments to evaluate our hierarchical classification approach: 1) Baselines for flat classification: Sets the baselines of flat classifiers for single and multiple datasets training. 2) Hierarchical classification on three heterogeneous datasets: Demonstrates the benefits of our complete method for combined training on three heterogeneous datasets (Cityscapes, GTSDB, Vistas) with disjoint label spaces and different annotation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Hierarchical versus flat classification on Cityscapes</head><p>Extended: Validates the effectiveness of our hierarchical approach against extremely imbalanced classes, by isolating it on the per-pixel annotated Cityscapes Extended dataset with a two-level label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We summarize the datasets that we use in <ref type="table" target="#tab_3">Table I</ref>. Next we describe the extra annotations needed for our experiments. Please note that these annotations are only used for validation purposes and not for training the networks.</p><p>1) Labeling Cityscapes with traffic sign classes: We extend the label space of Cityscapes with 43 traffic sign classes of GTSDB. Cityscapes provides only per-pixel traffic sign annotations without differentiating between instances. We design an automatic segmentation algorithm based on the 8neighborhood distance, for separating connected traffic sign instances in the ground truth traffic sign mask, and a GUI application, which proposes image areas for labeling. We pack original and new annotations under the name Cityscapes Extended. This dataset contains 2778 and 380 traffic signs in the train and validation splits respectively.</p><p>2) Annotating GTSDB with per-pixel labels: Only for specific experiments that involve the flat classifier, we converted the GTSDB bounding box annotations to fine per-pixel annotations, using the traffic signs shapes (circle, triangle, hexagon) and inscribing them into their bounding box. This procedure can be problematic with in-plane rotation of traffic signs, but after dataset inspection, we observed that only a negligible amount of in-plane rotations are present.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metrics and evaluation conventions</head><p>We use multi-class mean Pixel Accuracy (mPA) and mean Intersection over Union (mIoU), which are relevant in the context of automated driving, and they represent good local and area criteria, following the definitions given in <ref type="bibr" target="#b14">[15]</ref>. For Cityscapes, we report results on 27 classes (19 of the official benchmark + 8 common with Vistas). For the traffic sign classes we evaluate on a subset of the 43 traffic signs that satisfy both conditions: 1) have less than 10 3 pixels in GTSDB train set, and 2) have less than 10 3 pixels on both GTSDB and Cityscapes Extended validation sets. Please note that 10 3 pixels limit is chosen, as it is 2 orders of magnitude smaller than the least represented class in Cityscapes. For Vistas, we report results on the official 65 classes benchmark. Finally, we evaluate the performance of the model every one epoch and we report the average over the last 2 runs.</p><p>A new evaluation protocol for fair comparisons is introduced only for the experiments of Sec. V-C, which trains a flat classifier on two datasets. It solves the semantic conflict of the high-level traffic sign class being in the same level with traffic sign subclasses (Sec. II). The decision for a traffic sign pixel is deemed correct: 1) if it is correctly labeled with any traffic sign subclass, or 2) if it is labeled as traffic sign and the second most probable choice is the correct traffic sign subclass. To be clear, we do not use this evaluation scheme for the hierarchical classifier but only for the flat classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baselines for flat classification</head><p>In <ref type="table" target="#tab_3">Table II</ref>, we set the same-and cross-dataset baselines for the conventional flat classification approach, using the same input dimensions and batch size as described in the implementation details of Sec. IV, in order to be able to fairly compare with the hierarchical results of Table III. In columns 1-3, we train three models on three datasets independently and we provide results for the evaluated classes of <ref type="table" target="#tab_3">Table I</ref>. In column 4, we provide cross-dataset results on Cityscapes Extended of combined training on Cityscapes and GTSDB.</p><p>For fair comparisons, the models of third and fourth column were trained with the generated per-pixel annotations of the GTSDB dataset (see Sec. V-A.2 for the details). Training on 43 classes of GTSDB does not converge, due to the limited number of training pixels per image, so we included the unlabeled pixels as an extra class, to solve this issue. It can be observed that simultaneous training on Cityscapes and GTSDB, fails to achieve satisfactory cross-dataset results on the traffic sign classes of Cityscapes Extended. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hierarchical classification on 3 heterogeneous datasets</head><p>This experiment evaluates our complete hierarchical classification approach on three heterogeneous datasets (Cityscapes, Mapillary Vistas, and GTSDB).</p><p>In <ref type="table" target="#tab_3">Table III</ref>, we present evaluation results on the validation splits of the three datasets that the model is trained on (columns 1-3) and results for traffic sign subclasses on Cityscapes Extended (column 4), which was not used during training. In Figures 4, 5, 6 qualitative results are depicted.</p><p>By comparing <ref type="table" target="#tab_3">Table II columns 1-3 and Table III</ref> columns 1-3, we achieve significant performance increases in mean PA (in the range +2.4% to +32.3%) and IoU (in the range +2.3% to +24.3%) for all datasets. By comparing <ref type="table" target="#tab_3">Table II  column 4 and Table III</ref> column 4, we also observe an increase in cross-dataset performance on traffic sign subclasses. It is important to note that the model was not trained on any examples from Cityscapes Extended traffic sign classes, and the +10.6% increase in mean PA is solely due to our hierarchical multiple dataset training scheme.</p><p>We conclude that hierarchical classification is highly advantageous for combined heterogeneous datasets training, when datasets have different classes, different annotation types and in-and between-dataset(s) imbalances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hierarchical vs flat classification on Cityscapes Extended</head><p>In this experiment we evaluate the hierarchical classification method on Cityscapes Extended with per-pixel annotations and a two-level label space. The goal is to isolate our method on a single dataset, in order to show its effectiveness on highly imbalanced datasets against flat classification. We used 512 x 1024 input dimensions and batch size of 2. From <ref type="table" target="#tab_3">Table IV</ref>, we observe that hierarchical classification significantly increases mPA (+26.0%) and mIoU (+16.1%) for L2 classes (i.e. GTSDB traffic sign subclasses) with respect to the flat classifier, while for L1 classes (i.e. Cityscapes classes) the increase is above +6% for both mPA and IoU.</p><p>We conclude that hierarchical classification is robust against class imbalances, even when applied on a single dataset with per-pixel annotations, since it places in each level classes with the same order of examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we considered the challenge of simultaneously training on three heterogeneous, but semantically connected datasets, for the problem of per-pixel semantic segmentation. The main motives are to maximally reuse resources (datasets and computation) and eliminate human labeling effort. In order to achieve this, we leverage the semantic relationships between datasets' labels to construct a hierarchy of classifiers and we introduce the respective hierarchical training and inference rules. Our final network can per-pixel segment an input image into 108 classes from 28.8 trained on</p><p>Cityscapes Extended L1 and L2 classes 8 high-level street scene categories. The results clearly show the benefit of using our hierarchical classification scheme for multiple heterogeneous dataset training. In future work, we will extend our implementation to include more datasets with more divergent characteristics, to demonstrate the scalability of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our hierarchical classification convolutional network during inference. The input image is transformed to a shared feature representation, which is connected to a hierarchy of classifiers though adaptation subnetworks. The Level-1 classifier outputs predictions for every pixel of the image, while each subsequent classifier infers only about its own set of classes. The output of all levels is combined to form the final fine-grained per-pixel segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Online procedure during training for generating per-pixel ground truth from bounding box labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Mapillary Vistas validation split image examples. The network predictions include decisions from L1-L3 levels of the hierarchy. Note that the ground truth does not include traffic sign subclasses. GTSDB test split image examples. The network predictions include decisions from L1-L3 levels of the hierarchy. Note that the ground truth includes only traffic sign bounding boxes, since rest pixels are unlabeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, or 2) use existing auxiliary Panagiotis Meletis (p.c.meletis@tue.nl) and Gijs Dubbelman (g.dubbelman@tue.nl) are with the Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 688099.</figDesc><table><row><cell></cell><cell cols="3">Shared Fully Convolutional Feature Extractor</cell><cell></cell></row><row><cell>adapt. subnet</cell><cell>adapt. subnet</cell><cell>adapt. subnet</cell><cell>adapt. subnet</cell><cell>adapt. subnet</cell></row><row><cell>L1 classifier</cell><cell>L2 rider classifier</cell><cell>L2 drivreable classifier</cell><cell>L2 traffic sign classifier</cell><cell>L3 traffic sign front classifier</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I DATASET</head><label>I</label><figDesc>STATISTICS. IMAGES CONTAIN TRAINING AND VALIDATION SPLITS. IN PARENTHESIS THE NUMBER OF EVALUATED CLASSES ARE SHOWN.</figDesc><table><row><cell></cell><cell cols="2">Cityscapes Cityscapes (fine) Extended</cell><cell>Mapillary Vistas</cell><cell>GTSDB</cell></row><row><cell>resolution</cell><cell cols="2">1024 x 2048</cell><cell cols="2">0.5 -25 MP 800 x 1360</cell></row><row><cell>images</cell><cell></cell><cell>2975 / 500</cell><cell cols="2">18000 / 2000 600 / 300</cell></row><row><cell>annot. type</cell><cell></cell><cell>per-pixel</cell><cell>per-pixel</cell><cell>bound. box</cell></row><row><cell>annot. pixels</cell><cell></cell><cell>1.6 ? 10 9</cell><cell cols="2">156.2 ? 10 9 0.003 ? 10 9</cell></row><row><cell>classes</cell><cell></cell><cell>34 (27)</cell><cell>66 (65)</cell><cell>43 (28)</cell></row><row><cell>t. sign classes</cell><cell>-</cell><cell>43 (18)</cell><cell>-</cell><cell>43 (28)</cell></row><row><cell>traffic signs</cell><cell>-</cell><cell>3158</cell><cell>-</cell><cell>900</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="5">FLAT CLASSIFICATION PERFORMANCE BASELINES ON PER-PIXEL</cell></row><row><cell></cell><cell></cell><cell cols="2">ANNOTATED DATASETS.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Same dataset</cell><cell></cell><cell>Cross-dataset</cell></row><row><cell cols="4">Tested on Cityscapes Vistas GTSDB</cell><cell>Cityscapes Extended traffic sign classes</cell></row><row><cell>mPA (%)</cell><cell>53.6</cell><cell>36.5</cell><cell>25.4</cell><cell>19.1</cell></row><row><cell>mIoU (%)</cell><cell>46.2</cell><cell>29.6</cell><cell>17.2</cell><cell>3.0</cell></row><row><cell cols="4">Trained on Cityscapes Vistas GTSDB</cell><cell>Cityscapes + GTSDB</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF OUR COMPLETE HIERARCHICAL CLASSIFICATION APPROACH ON 4 DATASETS.</figDesc><table><row><cell></cell><cell></cell><cell>Same dataset</cell><cell></cell><cell>Cross-dataset</cell></row><row><cell>Tested on</cell><cell cols="3">Cityscapes Vistas GTSDB classes classes classes</cell><cell>Cityscapes Extended traffic sign classes</cell></row><row><cell>mPA (%)</cell><cell>66.6</cell><cell>38.9</cell><cell>57.7</cell><cell>29.7</cell></row><row><cell>mIoU (%)</cell><cell>57.3</cell><cell>31.9</cell><cell>41.5</cell><cell>8.3</cell></row><row><cell>Trained on</cell><cell></cell><cell cols="3">Cityscapes + Vistas + GTSDB</cell></row><row><cell>Image Predictions</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>truth</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Fig. 4. Cityscapes val split image examples. The network predictions</cell></row><row><cell cols="5">include decisions from L1-L3 levels of the hierarchy. Note that the ground</cell></row><row><cell cols="5">truth includes only one traffic sign superclass (yellow) and no road attribute</cell></row><row><cell>markings.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV FLAT</head><label>IV</label><figDesc>VERSUS PROPOSED HIERARCHICAL CLASSIFICATION PERFORMANCE ON CITYSCAPES EXTENDED. (IN PARENTHESIS PERFORMANCE FOR TRAFFIC SIGN L1 CLASS.)</figDesc><table><row><cell></cell><cell cols="2">flat classifier</cell><cell cols="2">hierarchical classifiers</cell></row><row><cell></cell><cell>L1 classes</cell><cell>L2 classes</cell><cell>L1 classes</cell><cell>L2 classes</cell></row><row><cell>mPA (%)</cell><cell>69.4 (73.0)</cell><cell>23.0</cell><cell>75.6 (74.8)</cell><cell>49.0</cell></row><row><cell>mIoU (%)</cell><cell>60.4 (65.2)</cell><cell>12.7</cell><cell>66.7 (65.7)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computer vision for autonomous vehicles: Problems, datasets and state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1704.05519</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on deep learningbased fine-grained object classification and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="135" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning Semantic Segmentation with Diverse Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00509</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weaklyand semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-automatic image annotation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petrovai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical semantic classification and attribute relations analysis with clothing region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Multimedia and Ubiquitous Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical cnn for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hijazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Full implementation code for training, evaluation and inference, and the extra annotated datasets will be made publicly available at https: //github.com/pmeletis/hierarchical-semantic-segmentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A review on deep learning techniques applied to semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orts-Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Villena-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garcia-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
