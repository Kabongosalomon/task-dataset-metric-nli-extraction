<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
							<email>yunqinghu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
							<email>jinxuan.jx@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
							<email>zhangyin98@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Hong</surname></persName>
							<email>honghaiwen96@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<email>hui.xueh@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><forename type="middle">Xue</forename></persName>
						</author>
						<title level="a" type="main">RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition</title>
					</analytic>
					<monogr>
						<meeting> <address><addrLine>Chengdu, Sichuan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<note>ACM Reference Format: 2021. RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition. In Chengdu &apos;21: ACM Symposium on Neural Gaze Detection, October 20-24, 2021, Chengdu, Sichuan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445.1122456 * Corresponding author. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In fine-grained image recognition (FGIR), the localization and amplification of region attention is an important factor, which has been explored a lot by convolutional neural networks (CNNs) based approaches. The recently developed vision transformer (ViT) has achieved promising results on computer vision tasks. Compared with CNNs, Image sequentialization is a brand new manner. However, ViT is limited in its receptive field size and thus lacks local attention like CNNs due to the fixed size of its patches, and is unable to generate multi-scale features to learn discriminative region attention. To facilitate the learning of discriminative region attention without box/part annotations, we use the strength of the attention weights to measure the importance of the patch tokens corresponding to the raw images. We propose the recurrent attention multi-scale transformer (RAMS-Trans), which uses the transformer's self-attention to recursively learn discriminative region attention in a multi-scale manner. Specifically, at the core of our approach lies the dynamic patch proposal module (DPPM) guided region amplification to complete the integration of multiscale image patches. The DPPM starts with the full-size image patches and iteratively scales up the region attention to generate new patches from global to local by the intensity of the attention weights generated at each scale as an indicator. Our approach requires only the attention weights that come with ViT itself and can be easily trained end-to-end. Extensive experiments demonstrate that RAMS-Trans performs better than concurrent works, in addition to efficient CNN models, achieving state-of-the-art results on three benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Fine-grained image recognition (FGIR) has been a challenging problem. Most of the current methods are dominated by convolutional neural networks (CNNs). Unlike conventional image classification problems, FGIR has the problem of large intra-class variance and small inter-class variance. Therefore, FGIR methods need to be able to identify and localize region attention in an image that is critical for classification. There is a class of methods called part-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">36]</ref> for FGIR, and some of them use additional supervised information such as bounding box/part annotations to locate key regions. However, labeling bounding boxes/part annotations is a labor-intensive task that requires a lot of resources. How to be able to use the effective information generated by the model itself for region attention localization and amplification is one of the research directions that FGIR has to face.</p><p>The effectiveness of CNNs needs no further explanation here. However, we need to emphasize again that one of the key aspects of CNNs that make them effective is their translation invariance and local feature representation capability. CNNs are continuously downsampled as their depth increases, yet the receptive field of the model increases, so that both global and local information of the feature map can be utilized. For example, in networks such as VggNet <ref type="bibr" target="#b28">[29]</ref> and ResNet <ref type="bibr" target="#b16">[17]</ref>, the receptive field of underlying convolutional is smaller and has more local information, while the receptive field of higher convolutional is larger and has more global information. The works <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b19">[20]</ref> use this characteristic for FGIR. Some works exploit the attention properties of the feature maps of CNN itself, such as <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b34">[35]</ref> that exploit the attention maps of image features to select region attentions. Transformer <ref type="bibr" target="#b31">[32]</ref> has gradually transformed from a research hotspot in NLP <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38]</ref> to CV tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> in recent years. The proposal of vision transformer (ViT) has brought a new shock to computer vision and aroused the research interest in image sequentialization within the community. ViT flattens the segmented image patches and transforms them into patch tokens. Similar to character sequences in NLP, those tokens will be sent to the multi-head self-attention mechanism for training. Since patch tokens are position-agnostic, position embedding will be added to serve the purpose of adding spatial information. However, when ViT encounters FGIR, there are two main problems need to be solved. First, The model processes all the patch tokens at once, and when the complexity of the dataset increases, such as when the image resolution is high or when an image has a cluttered background, the model may not be able to effectively capture the region attention carried in the patch tokens.</p><p>Conversely, when the image resolution is low, this fixed patch size is more likely to make the model lose local information. Second, ViT differs from CNNs in that the length of patch tokens does not change as its encoder blocks increases, thus the receptive field of the model cannot be effectively extended. Therefore, for FGIR, we can do much more than just feed the flattened raw image patches into the Transformer. If we refer to the characteristics of CNNs and introduce attention to local regions in the model, that is, extend the effective receptive field, the recognition performance of the model is likely to be further improved. So then we encounter a very important question, how to explore and discover local information in ViT which focuses on using global information? The latest TransFG <ref type="bibr" target="#b15">[16]</ref> gives us a pretty good answer, which is to take advantage of ViT's inherent attention weights. Both in the NLP domain of transformer and ViT training, most of the works simply require the use of the last layer of classification token information, discarding the seemingly accessory attention weights. TransFG multiplies all the attention weights before the last transformer layer to get the importance ranking of patch tokens, and then concatenate the selected tokens along with the global classification token as input sequence to the last transformer layer. However, this kind of hard attention filtering is easy to fail in two cases, one is in the case of small image resolution, and the other is in the case of the high complexity of the dataset. In the former case, a lot of important local information is not easily available, and if most of the tokens information has to be filtered out at this time, it is likely to lose classification performance. In the latter case, a model can easily make wrong judgments based on improper token information when the attention mechanism fails. Through preliminary visualization experiments, We find that the strength of the attention weights can be intuitively correlated with the extent to which the patches contain the target object. To this end, we propose the recurrent attention multi-scale transformer (RAMS-Trans), which uses the transformer's self-attention mechanism to recursively learn discriminative region attention in a multi-scale manner. Specifically, at the core of our approach lies the proposed dynamic patch proposal module (DPPM) which aims to adaptively select the most discriminative region for each image. The DPPM starts with the complete image patches and scales up the region attention to generate new patches from global to local by the intensity of the attention weights generated at each scale as an indicator. The finer scale network takes as input the tendency regions scaled up from the previous scale in a cyclic manner. The following contributions are made in this paper:</p><p>? We reformulate the FGIR problem from a sequence to sequence learning perspective and design a new visual Transformer architecture namely recurrent attention multi-scale transformer (RAMS-Trans). It combines the advantages of CNNs in expanding receptive field, strengthening locality, and the advantages of Transformers in utilizing global information. ? As an instantiation, we exploit the transformer framework, specifically, to the use of multi-head self-attention weights to locate and zoom in on regions of interest, to implement our fully attentive feature representation by sequentializing images.</p><p>? Extensive experiments show that our RAMS-Trans model can learn superior feature representations as compared to traditional CNNs and concurrent work on three popular FGIR benchmarks (CUB-200-2011, Stanford Dogs, and iNatu-ralist2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 CNN based Fine-grained image recognition</head><p>FGIR can be divided into the following three directions, localizationclassification sub-networks, end-to-end feature encoding, and external information, of which the first two directions are the main content of this section. The first method is classified as strongly <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref> or weakly supervised <ref type="bibr" target="#b14">[15]</ref> according to whether it utilizes bounding box/part annotations information. This class of methods locates key component regions by training supervised or weakly supervised localization sub-networks. Then, the classification subnetwork uses the fine-grained region information captured by the localization sub-network to further improve the classification capability. Mask-CNN <ref type="bibr" target="#b35">[36]</ref> is based on part annotations and uses FCN to localize key parts (head, torso) to generate a mask with weighted object/part. However, the acquisition of part annotation can add additional and high markup costs. Many methods use attention mechanisms to set specific sub-network structures so that classification can be done using only image-level annotations. The second type of approach usually designs end-to-end models that encode discriminative features as higher-order information. From Bilinear-Pooling <ref type="bibr" target="#b23">[24]</ref> to compact kernel pooling <ref type="bibr" target="#b12">[13]</ref>, many works use different methods such as designing kernel modules <ref type="bibr" target="#b4">[5]</ref> or special loss functions <ref type="bibr" target="#b41">[42]</ref> to reduce the dimensionality of higherorder features. However, these methods have difficulty in obtaining fine variance from the global feature view and hardly surpass the previous method. An approach very close to our work is RA-CNN <ref type="bibr" target="#b11">[12]</ref>, and the common denominator is the learning of regional features under the action of two scales. However, we have the following two key differences from RA-CNN. First, we don't need additional parameters to learn the coordinates of the regions, and we only need to rely on the attention weights attached to the transformer training for the region attention learning. Second, we do not need to force the accuracy of scale 2 to be higher than scale 1, and we are letting the two scales learn from each other and jointly improve the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer in Vision</head><p>Inspired by the Transformer <ref type="bibr" target="#b31">[32]</ref> for NLP tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38]</ref>, a large number of models have recently emerged that rely heavily on the Transformer for computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b45">[46]</ref> are the earlier works to apply transformer to object detection. ViT <ref type="bibr" target="#b8">[9]</ref> is the first work to transform a 2D image into an 1D patch tokens, which will be fed into the subsequent transformer layers for training, achieving an accuracy rate comparable to that of CNNs for image recognition. DeiT <ref type="bibr" target="#b30">[31]</ref> enhances ViT <ref type="bibr" target="#b8">[9]</ref> by introducing a simulation token and employs knowledge distillation to simulate the output of CNN teachers to obtain satisfactory results without training on large-scale datasets. SETR <ref type="bibr" target="#b43">[44]</ref> proposes a pure self-attention-based encoder to perform semantic segmentation.</p><p>The most related work is TransFG <ref type="bibr" target="#b15">[16]</ref> which also leverages attention for FGIR. However, there is a key difference. We take the use of attention weights for amplification and reuse of region attentions, while TransFG only filters the patch tokens in the last layer of the Transformer. Second, we propose a recurrent structure to extract and learn multi-scale features for better visual representation. Our model is superior in various image resolutions and large-scale FGIR datasets (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our approach (RAMS-Trans) is built on top of the vision transformer (ViT <ref type="bibr" target="#b8">[9]</ref>), so we first present a brief overview of ViT and then describe our approach for learning multi-scale features for FGIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: Vision Transformer</head><p>Image Tokenization. The innovation and key to ViT are that it processes a 2D image into a string-like 1D sequence and then feeds it into blocks stacked by the standard Transformer's encoder. Specifically, ViT reshapes the image , ? R ? ?3 , with certain patch size, into a 2D sequence of patches , ? R ?( ? ?3) , where H, W are the height and width of the raw image respectively, 3 is the number of channels of the raw RGB image, and P is the artificially set patch size used to split the image. In ViT, the size of is usually 16 or 32. is the total number of patches split into, = ? / 2 . Then ViT maps the vectorized patches into a latent -dimensional embedding space using a trainable linear projection, to obtain the patch tokens ? , where ? ? R ? . Similar to BERT, ViT also initializes the class tokens (CLS) for final classification in the tokenization phase, which will be concatenated with the patch tokens and then sent to the subsequent transformer layers. In addition, since the patch tokens input to the subsequent transformer are position-agnostic, and the image processing depends on the spatial information of each pixel, ViT adds the position embedding to each patch, which can be continuously learned in the subsequent training process:</p><formula xml:id="formula_0">0 = [ || ? ] +<label>(1)</label></formula><p>where ? R 1? and ? R (1+ )? are the CLS and the position embedding respectively. However, image tokenization in FGIR with a fixed patch size may have two problems: (1) The model processes all the patch tokens at once, and when the complexity of the dataset increases, e.g., with a cluttered background, the model may not be able to effectively capture the region attention carried in the patch tokens. (2) This kind of fixed patch size makes it easier for the model to lose local information when the image resolution is low.</p><p>Encoder Blocks. The main structure of ViT are Blocks, consists of a stack of Transformer's standard encoder. Each block consists of a multi-head self-attention (MSA) and a feed-forward network (FFN), which consists of two fully connected layers. The output of the ? layer can be expressed as:</p><formula xml:id="formula_1">= ? 1 + ( ( ? 1)) (2) = + ( ( ))<label>(3)</label></formula><p>where (?) indicates the Layer Normalization operation <ref type="bibr" target="#b0">[1]</ref>. The uniqueness of CNN's image processing lies in the fact that as the depth of the model increases, the raw images are continuously downsampled, while the receptive field of the model keeps getting larger so that both global and local information of the images can be utilized. What makes ViT different from CNNs is with the increasing number of encoder blocks, the length of patch tokens does not change, and the receptive field of the model cannot be effectively extended, which may affect the accuracy of the model on the FGIR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Network</head><p>Inspired by SCDA <ref type="bibr" target="#b34">[35]</ref> and RA-CNN <ref type="bibr" target="#b11">[12]</ref>, we propose the recurrent attention multi-scale transformer (RAMS-Trans) to solve the above problems. We take two scales in <ref type="figure" target="#fig_1">Figure 2</ref> as an example. First, the model accepts the raw input image 1 and then outputs cross-entropy loss1 and the multi-head self-attention weights of each transformer layer after the computation. Then is fed into DPPM, which firstly outputs the corresponding patches mask matrix on the raw image 1 , that is, the binary patch mask matrix, and then gets the coordinate value of the region attention on 1 by the maximum connected region search algorithm according to the matrix. In the second stage, we get the local image input 2 by bilinear interpolation algorithm to zoom in from 1 , which will be recomputed through L-layer encoder blocks to get cross-entropy loss2 and multi-head self-attention weights 2 .</p><p>It is important to emphasize that the core of our approach is how to use the characteristics of ViT itself to find the region attention in the raw image, to break the receptive field limitation of ViT due to the fixed size of patches, and then use the multi-scale image representation to achieve more effective recognition of objects. In CNNs, SCDA uses the fusion of multiple channel feature maps to achieve the localization of objects, from which we are inspired that since ViT processes the raw images into patch tokens for subsequent MSA and FFN calculations, can we thus obtain the importance of each patch corresponding to the raw image 1 ? Since one of the cores of the Transformer is MSA, then it is natural to think of using self-attention weights to try to accomplish this. We first perform a visual evaluation:</p><p>Visual Evaluation. Relying only on the attention weights incidental to the pre-training of ViT, it is possible to accurately locate the object region in the raw images and discard the irrelevant and noisy regions. In <ref type="figure" target="#fig_0">Figure 1</ref>., we show some images from three datasets CUB-200-2011, Stanford Dogs, and iNaturalist2017. We extract their attention weights using a ViT model pre-trained on ImageNet21k without fine-tuning the target dataset at all, and then visualize them employing CAM <ref type="bibr" target="#b44">[45]</ref>. In <ref type="figure" target="#fig_0">Figure 1</ref> we can see that using only the raw attention weights we can well localize the objects in the raw images and mask the background and noise regions. The above visualization process illustrates that the self-attention weights generated by ViT in the calculation of its MSA mechanism can be correlated to some extent with the positions of the target objects in the raw images.</p><p>Dynamic Patch Proposal Module. Our goal is to adaptively select a varying number of patches from 1 with 1/2 ? 1/2 patches to recompose 2 . We first take out the attention weights of each transformer layer as:</p><formula xml:id="formula_2">= ( 1/2 ) = [ 0 , 1 , ..., ]<label>(4)</label></formula><formula xml:id="formula_3">= [ 1 , 2 , ..., ] ? 1, 2, ...,<label>(5)</label></formula><formula xml:id="formula_4">= [ 1 , 2 , ..., ] ? 1, 2, ...,<label>(6)</label></formula><p>where Q, K are Query and Key vectors respectively. Then we regularize the</p><formula xml:id="formula_5">= 1 =1 +<label>(7)</label></formula><p>where is the regularization factor and is the diagonal matrix.</p><formula xml:id="formula_6">= 1 ?? =1 ( 1 ?? =1 + )<label>(8)</label></formula><p>Then we propose to integrate attention weights of all previous layers and recursively apply a matrix multiplication to the modified attention weights in all the layers as:</p><formula xml:id="formula_7">= =1<label>(9)</label></formula><p>We calculate the mean value of all the positions in as the threshold to determine the localization position of the object. In particular, to improve the localization ability and further determine the region attention, we design a magnification factor as a hyperparameter to increase the threshold:</p><formula xml:id="formula_8">( , ) = 1 ( , ) &gt; 0 ?<label>(10)</label></formula><p>where?( , ) is the patch mask matrix and (x, y) is a particular position in these 1/2 ? 1/2 positions. Finally, we employ Algorithm 1 to extract the largest connected component of?to localize and zoom in the region attention in the raw image 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>We present our implementation details on loss function as well as scale-wise class token. Loss Function In the training phase, our loss function is represented as a multi-task loss consisting of classification 1 and guided 2 :</p><formula xml:id="formula_9">= 1 + 2<label>(11)</label></formula><p>which are complementary to each other. is the coefficient to balance the weight between the two losses, which we take as 1.0 in the experiments. <ref type="bibr" target="#b0">1</ref> represents the fine-grained classification loss of scale1 and 2 is the guided loss which is designed to guide the mode to select the more discriminative regions. These two losses work together in the backpropagation process to optimize the performance of the model. It enables the final convergent model to make classification predictions based on the overall structural characteristics of the object or the characteristics of region attention. During the testing phase, we removed the scale 2 to reduce a large number of calculations, so our approach will not take too long to predict in practical applications.</p><p>Scale-wise Class Token In Sec 3.1 we have described how the class token is generated and its role, which is mainly to exchange information with the patch tokens and finally to feed the class information to the classification layer. However, in our framework, the region attention of the raw image will be positioned and enlarged, and thus the patch tokens will be different between scales, which may affect the final classification performance if the class token is shared between scales. We, therefore, propose the scale-wise class token, i.e., different class-tokens are used to adapt patch tokens of different scales:</p><formula xml:id="formula_10">0 = [ 1 || 2 || ? ] +<label>(12)</label></formula><p>We demonstrate the effectiveness of this design in subsequent experiments with different resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe our experiments and discuss the results. We first present three datasets and then present our specific experimental details and results for each dataset respectively. Finally, we conduct detailed ablation experiments on our approach to investigate the impact of the components on FGIR in more depth. Note that all our results are reported as accuracies and are compared with the latest methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Finding Connected Components in Binary Patch Mask Marix</head><p>Require: A binary matrix: ; 1: Select a patch as the starting point; <ref type="bibr">2:</ref> while True do 3:</p><p>Use a flood-fill algorithm to label all the patches in the connected component containing ; <ref type="bibr">4:</ref> if All the patches are labeld then Search for the next unlabeled patch as ; 8: end while 9: return Connectivity of the connected components, and their corresponding size (patches numbers)</p><p>Datasets. A total of three benchmark datasets are presented in our experiments, namely CUB-200-2011 <ref type="bibr" target="#b32">[33]</ref>, Stanford Dogs <ref type="bibr" target="#b21">[22]</ref> and iNaturalist2017 <ref type="bibr" target="#b17">[18]</ref>. CUB-200-2011 is a fine-grained dataset on bird classification. In addition to labels, it also contains bounding box/part annotations which are useful for classification. Stanford Dogs contains images of 120 breeds of dogs from around the world. iNaturalist2017 is a large-scale FGIR dataset containing over 5,000 species of animals and plants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on CUB-200-2011</head><p>Implementation Details. We load the model weights from the official ViT-B_16 model pre-trained on ImageNet21k. In all experiments, we used the SGD optimizer to optimize with an initial learning rate of 0.03 and a momentum of 0.9. We use weight decay 0. We use cosine annealing to adjust the learning rate with batch size 16. The model is trained for a total of 10,000 steps, of which the first 500 steps are warm-up. We resize the input images by scaling the shortest side to 600 and randomly crop a region of 448 ? 448 for training. In the test, we use center crop to change the image size to 448 ? 448. We split the image into patches as in the ViT, with the patch size is 16 ? 16. The hyperparameter is chosen to be 1.3. We complete the construction of the whole model using Pytorch and run all experiments on the Tesla V-100 GPUs.</p><p>Comparison with state-of-the-art methods. The classification accuracies of CUB-200-2011 are summarized in <ref type="table" target="#tab_0">Table 1</ref>. All previous FGIR methods test their performance on this dataset. As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, our approach outperforms all CNN-based methods and TransFG's PSM module and achieves state-of-the-art performance. Although ViT itself achieves good performance on this dataset, with the addition of our DPPM module, it achieves a further 0.7% improvement, which is rare for the Vit-B16 model which has been adequately pre-trained on a large-scale dataset.</p><p>Visualization. In order to visually analyze the selected regions from raw images for our DPPM, we present the amplified regions in the left part of <ref type="figure" target="#fig_3">Figure 3</ref>. The first, third, and fifth rows are the raw images, and the second, fourth and sixth rows are the visualization of the local images after the proposed patches have been amplified. From <ref type="figure" target="#fig_3">Figure 3</ref> we can clearly see that DPPM has amplified the most discriminative regions of birds in each category, such as the head and the bill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on iNaturalist2017</head><p>Implementation Details. To fully validate the effectiveness of our RAMS-Trans, we load the officially provided ViT-B16 model pre-trained on ImageNet21k. In all experiments, we use the SGD optimizer to optimize with an initial learning rate of 0.005 and a momentum of 0.9. We use weight decay 0. We use cosine annealing to adjust the learning rate with batch size 16. The model is trained for a total of 1e6 steps, of which the first 500 steps are warm-up. To align with TransFG, we resize the input images by scaling the shortest side to 448 and randomly crop a region of 304 ? 304 for training. In the test, we use center crop to change the image size to 304 ? 304. We still split the image into 16 ? 16 patches with non-overlap and use a hyperparameter alpha of 1.2.</p><p>Comparison with state-of-the-art methods. <ref type="table" target="#tab_1">Table 2</ref> summarizes our results and compares them with the CNN-based state-ofthe-art methods and TransFG. Our approach outperforms ResNet152 by 9.5% and outperforms all CNN-based methods. With the pretrained model loaded, our method can achieve an improvement of 1.5% higher than the baseline. It is worth note that for a fair comparison, we report in <ref type="table" target="#tab_1">Table 2</ref> both the result obtained when we run the PSM module of TransFG in our code environment under the non-overlap setting. It can be seen that the DPPM module of our approach outperforms the PSM module of TransFG by 2.4% with the same loading of the ViT-B_16 pre-trained model.</p><p>Visualization. In order to visually analyze the selected regions from raw images for our DPPM, we present the amplified regions in the right part of <ref type="figure" target="#fig_3">Figure 3</ref>. The first, third, and fifth rows are the raw images, and the second, fourth, and sixth rows are the visualization of the local images after the proposed patches have been amplified. From <ref type="figure" target="#fig_3">Figure 3</ref> we can clearly see that our RAMS-Trans successfully captures the most discriminative regions for an object, i.e., head, eyes for Amphibia; fins for Actinopterygii; thallus for Protozoa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Stanford Dogs</head><p>Implementation Details. We load the model weights from the official ViT-B_16 model pre-trained on ImageNet21k. In all experiments, we use the SGD optimizer to optimize with an initial learning rate of 0.003 and a momentum of 0.9, aligning with TransFG. We use weight decay 0. We use cosine annealing to adjust the learning rate with batch size 16. The model is trained for a total of 20,000 steps, of which the first 500 steps are warm-up. We resize the input images by scaling the shortest side to 448 and randomly crop a region of 224 ? 224 for training. In the test, we use center crop to change the image size to 224 ? 224. We split the image into patches as in the ViT, with the patch size is 16 ? 16. The hyperparameter is chosen to be 1.0.</p><p>Comparison with state-of-the-art methods. The classification results of Stanford Dogs are summarized in <ref type="table" target="#tab_2">Table 3</ref>. As can be seen, our approach outperforms all CNN-based methods and TransFG and achieves state-of-the-art results. It is important to note that for TransFG, the DPPM module of ours is aligned with its PSM module. Although they are both hard attention mechanisms, our approach is softer than its simple token filtering way because our approach extends the effective receptive field of the raw images, thus the classification performance is also better. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Acc.(%) ResNet-50 <ref type="bibr" target="#b16">[17]</ref> ResNet-50 84.5 RA-CNN <ref type="bibr" target="#b11">[12]</ref> VGG-19 85.3 GP-256 <ref type="bibr" target="#b36">[37]</ref> VGG-16 85.8 MaxExt <ref type="bibr" target="#b10">[11]</ref> DenseNet-161 86.6 DFL-CNN <ref type="bibr" target="#b33">[34]</ref> ResNet-50 87.4 NTS-Net <ref type="bibr" target="#b38">[39]</ref> ResNet-50 87.5 Cross-X <ref type="bibr" target="#b25">[26]</ref> ResNet-50 87.7 DCL <ref type="bibr" target="#b3">[4]</ref> ResNet-50 87.8 CIN <ref type="bibr" target="#b13">[14]</ref> ResNet-101 88.1 DBTNet <ref type="bibr" target="#b41">[42]</ref> ResNet-101 88.1 ACNet <ref type="bibr" target="#b20">[21]</ref> ResNet-50 88.1 S3N <ref type="bibr" target="#b7">[8]</ref> ResNet-50 88.5 FDL <ref type="bibr" target="#b24">[25]</ref> DenseNet-161 89.1 PMG <ref type="bibr" target="#b9">[10]</ref> ResNet-50 89.6 API-Net <ref type="bibr" target="#b46">[47]</ref> DenseNet-161 90.0 StackedLSTM <ref type="bibr" target="#b14">[15]</ref> GoogleNet 90.4 MMAL-Net <ref type="bibr" target="#b39">[40]</ref> ResNet-50 89.6 ViT <ref type="bibr" target="#b8">[9]</ref> ViT-B_16 90.6 TransFG &amp; PSM <ref type="bibr" target="#b15">[16]</ref> ViT-B_16 90.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAMS-Trans</head><p>ViT-B_16 91.3 ViT-B_16 67.0 TransFG &amp;PSM <ref type="bibr" target="#b15">[16]</ref> ViT-B_16 66.6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAMS-Trans</head><p>ViT-B_16 68.5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Stanford Dogs MaxEnt <ref type="bibr" target="#b10">[11]</ref> DenseNet-161 84.9 FDL <ref type="bibr" target="#b24">[25]</ref> DenseNet-161 84.9 RA-CNN <ref type="bibr" target="#b11">[12]</ref> VGG-19 873 SEF <ref type="bibr" target="#b26">[27]</ref> ResNet-50 88.8 Cross-X <ref type="bibr" target="#b25">[26]</ref> ResNet-50 88.9 API-Net <ref type="bibr" target="#b46">[47]</ref> ResNet-101 90.3 ViT <ref type="bibr" target="#b8">[9]</ref> ViT-B_16 92.2 TransFG &amp; PSM <ref type="bibr" target="#b15">[16]</ref> ViT-B_16 90.0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAMS-Trans</head><p>ViT-B_16 92.4</p><p>Visualization. In order to visually analyze the selected regions from raw images for our DPPM, we present the amplified regions in the center part of <ref type="figure" target="#fig_3">Figure 3</ref>. The first, third and fifth rows are the raw images, and the second, fourth and sixth rows are the visualization of the local images after the proposed patches have been amplified. <ref type="figure" target="#fig_3">Figure 3</ref> conveys that the proposed regions do contain more finegrained information of dogs in each category such as the ears, eyes, and fur. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Experiments</head><p>We conduct ablation experiments on our RAMS-Trans framework to analyze how its variants affect the FGIR results. All ablation studies are done on CUB-200-2011 dataset while the same phenomenon can be observed on other datasets as well.</p><p>Hyperparameters. Since DPPM in our approach requires the choice of hyperparameter , we experimentally investigate the effect of the threshold on the classification performance. We set 4 sets of alpha values between 1.1 and 1.4 with 0.1 intervals, and the results of all experiments are summarized in <ref type="table" target="#tab_4">Table 5</ref>. It can be seen that as the value increases from 1.1 to 1.4, the recognition accuracy first increases and then decreases. The best performance is obtained when is 1.3. Based on this, we can get the following analysis: when the is small, the DPPM will crop as many patches in the raw images as possible, resulting in many non-critical regions being fed into the model again, while when the is large, the DPPM will crop as few patches in the raw images as possible, losing many critical regions to some extent. Both of these situations will lead to a decrease in classification accuracy, thus it is important to choose a suitable threshold value.</p><p>Compared to bounding box annotations. In order to further investigate the effectiveness of RAMS-Trans in selecting region attention, we replace our DPPM with the bounding box coordinates that come with the CUB-200-2011 dataset, keeping other settings   unchanged, and conduct a comparison experiment. In <ref type="table" target="#tab_6">Table 7</ref>, it can be seen that the experiments with a bounding box that utilized the supervised information are instead lower than the baseline. We also compare and analyze these two sets of comparative experiments by zooming in on the local images obtained from the raw images. From <ref type="table" target="#tab_6">Table 7</ref>, we can analyze and obtain that using object annotations limits the performance since human annotations only give the coordinates of important parts rather than the accurate discriminative region location. <ref type="figure">Figure 4</ref> visualizes the change of the feature maps before and after training. It can be seen that the activation region tends to be localized from the original global. Influence of image resolution. Image resolution is a critical factor for image classification, and there are very many applications in real industrial scenarios. In order to investigate the performance improvement of RAMS-Trans for different image resolutions, we set five different image resolutions for comparison experiments, which are 224 ? 224, 256 ? 256, 288 ? 288 and 320 ? 320. Meanwhile, we conduct experiments on the PSM of TransFG with the same settings. As shown in <ref type="table" target="#tab_3">Table 4</ref>, our method exceeds the baseline at each set of resolutions, while the PSM module loses performance.</p><p>Compared to random proposal method. In order to prove the effectiveness of our proposed DPPM module, we add the experiment of random sampling. Instead of the DPPM module, we use randomly generated coordinate points. As shown in <ref type="table" target="#tab_6">Table 7</ref>, the DPPM can improve 0.4% over the random sampling method.</p><p>Influence of scale-wise class token. To verify the validity of the scale-wise class token, we add scale-shared class token experiments to experiments 4. As can be seen from <ref type="table" target="#tab_5">Table 6</ref>, the experimental results with scale-wise class token are better than those scale-wise class token in most resolution cases.</p><p>Influence of patch size To analyze the effect of different patch sizes on FGIR and the effectiveness of our approach at different patch sizes, we load the official ViT-B_32 model provided for pretraining on ImageNet21k. The experimental results are summarized in <ref type="table" target="#tab_7">Table 8</ref>. For a fair comparison, we report in <ref type="table" target="#tab_7">Table 8</ref> both the results obtained when we run the PSM module of TransFG in our code environment under the non-overlap setting. It can be seen that  <ref type="figure">Figure 4</ref>: An illustration of learning discriminative details by RAMS-Trans. The first row is the original images, the second raw shows the attention maps generated from raw attention weights, and the third raw shows the attention maps generated from trained attention weights.</p><p>when the patch size is 32, the performance of ViT as a baseline drops by 2.1% compared to when the patch size is 16. We can analyze that the larger the patch size is, the more sparsely the raw image is split, and the local information is not well utilized, so the performance of FGIR is degraded, yet our approach still exceeds the baseline by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a new recurrent attention multi-scale transformer (RAMS-Trans) architecture that combines the advantages of CNNs in expanding the effective receptive field, strengthening locality, and the advantages of Transformers in utilizing global information. Without bounding box/part annotations and additional parameters, RAMS-Trans uses the transformer's selfattention weights to measure the importance of the patch tokens corresponding to the raw images and recursively learns discriminative region attention in a multi-scale manner. Last but not least, our approach can be easily trained end-to-end and achieves state-of-theart in CUB-200-2011, Stanford Dogs, and the large-scale iNatural-ist2017 datasets. The future work is how to locate region attention more precisely to further improve the classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization results of Attention weights on CUB-200-2011, iNaturalist2017 and Stanford dogs datasets. The first, the third and the fifth rows are original images, while the second, the fourth and the sixth raws show the raw attention maps. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The framework of recurrent attention multi-scale transformer (RAMS-Trans). The inputs are from global full-size images to local region attention (from left to right). The attention weights of all transformer layers are aggregated to generate the patch mask matrix, of which red 1 indicates an activated patch. The red box indicates the selected patch. Note that the linear projection, the transformer layers, and Fc (Fully Connection) layers are parameter-sharing, while CLS tokens do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Attention localization at the local scale for CUB-200-2011, Stanford Dogs and iNaturalist2017. The regions (in second row of each category) learned from multiple image samples, represent consistent region attention for a specific fine-grained category, which are discriminative to classify this category from others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our RAMS-Trans with existing state of the arts methods on CUB-200-2011.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our RAMS-Trans with existing state of the arts methods on iNaturalist2017.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>iNaturalist2017</cell></row><row><cell>Resnet152 [17]</cell><cell>ResNet152</cell><cell>59.0</cell></row><row><cell>SSN [28]</cell><cell>ResNet101</cell><cell>65.2</cell></row><row><cell>Huang et al. [19]</cell><cell>ResNet101</cell><cell>66.8</cell></row><row><cell>IncResNetV2 [30]</cell><cell>InResNetV2</cell><cell>67.3</cell></row><row><cell>TASN [43]</cell><cell>ResNet101</cell><cell>68.2</cell></row><row><cell>ViT [9]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our RAMS-Trans with existing state of the arts methods on Stanford Dogs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation experiments on DPPM with different input resolutions.</figDesc><table><row><cell>Resolution</cell><cell>192</cell><cell>224</cell><cell>256</cell><cell>288</cell><cell>320</cell></row><row><cell>ViT</cell><cell cols="5">81.5 85.3 87.4 88.3 88.9</cell></row><row><cell cols="6">TransFG &amp; PSM [16] 81.6 84.9 86.8 88.0 89.1</cell></row><row><cell>RAMS-Trans</cell><cell cols="5">82.2 86.3 88.1 88.9 89.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Impact of Threshold on CUB-200-2011 dataset.</figDesc><table><row><cell>Approach</cell><cell>Value of</cell><cell>Accuracy(%)</cell></row><row><cell>RAMS-Trans</cell><cell>1.1</cell><cell>90.9</cell></row><row><cell>RAMS-Trans</cell><cell>1.2</cell><cell>91.2</cell></row><row><cell>RAMS-Trans</cell><cell>1.3</cell><cell>91.3</cell></row><row><cell>RAMS-Trans</cell><cell>1.4</cell><cell>91.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation experiments on CLS token with different input resolutions</figDesc><table><row><cell>Resolution</cell><cell>192</cell><cell>224</cell><cell>256</cell><cell>288</cell><cell>320</cell><cell>448</cell></row><row><cell cols="7">scale-sharing 82.2 85.9 87.8 88.5 89.8 91.2</cell></row><row><cell>scale-wise</cell><cell cols="6">82.2 86.3 88.1 88.9 89.7 91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation experiments on patch proposal way.</figDesc><table><row><cell>Method</cell><cell cols="2">Patch Proposal Accuracy(%)</cell></row><row><cell>ViT [9]</cell><cell>Bounding Box</cell><cell>89.2</cell></row><row><cell cols="2">RAMS-Trans Bounding Box</cell><cell>90.5</cell></row><row><cell>RAMS-Trans</cell><cell>Random</cell><cell>90.9</cell></row><row><cell>RAMS-Trans</cell><cell>DPPM</cell><cell>91.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Ablation experiments on patch size.</figDesc><table><row><cell>Method</cell><cell cols="2">Patch Size Accuracy(%)</cell></row><row><cell>ViT [9]</cell><cell>16 ? 16</cell><cell>90.6</cell></row><row><cell>TransFG &amp; PSM [16]</cell><cell>16 ? 16</cell><cell>90.9</cell></row><row><cell>RAMS-Trans</cell><cell>16 ? 16</cell><cell>91.3</cell></row><row><cell>ViT</cell><cell>32 ? 32</cell><cell>88.4</cell></row><row><cell>TransFG &amp; PSM [16]</cell><cell>32 ? 32</cell><cell>88.9</cell></row><row><cell>RAMS-Trans</cell><cell>32 ? 32</cell><cell>89.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<ptr target="http://arxiv.org/abs/1607.06450" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bird Species Categorization Using Pose Normalized Deep Convolutional Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<ptr target="http://arxiv.org/abs/1406.2952" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<ptr target="https://arxiv.org/abs/2005.12872" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Destruction and Construction Learning for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00530</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00530" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel Pooling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.325</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.325" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="3049" to="3058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1285</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1285" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<editor>Long Papers, Anna Korhonen, David R. Traum, and Llu?s M?rquez</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/n19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selective Sparse Sampling for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00670</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00670" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="6598" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. CoRR abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11929</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-Grained Visual Classification via Progressive Multi-granularity Training of Jigsaw Patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58565-5_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58565-5_10" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">12365</biblScope>
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XX</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum-Entropy Fine Grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Montr?al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/0c74b7f78409a4022a2c4c5a5ca3ee19-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>Kristen Grauman, Nicol? Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="page" from="635" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.476</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.476" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4476" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compact Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.41</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.41" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel Interaction Networks for Fine-Grained Image Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6712" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10818" to="10825" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weakly Supervised Complementary Parts Models for Fine-Grained Image Classification From the Bottom Up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00315</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00315" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TransFG: A Transformer Architecture for Fine-grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<ptr target="https://arxiv.org/abs/2103.07976" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The INaturalist Species Classification and Detection Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00914</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00914" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretable and Accurate Fine-grained Recognition via Region Grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00869</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00869" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8659" to="8669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learn to Pay Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyzbhfWRW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention Convolutional Binary Neural Tree for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01048</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.01048" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10465" to="10474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-Grained Recognition as HSnet Search for Informative Image Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.688</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.688" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6497" to="6506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilinear CNN Models for Fine-Grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.170</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.170" />
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Computer</title>
		<imprint>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="2015-12-07" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Filtration and Distillation: Enhancing Region Attention for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6822" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11555" to="11562" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-X Learning for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00833</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00833" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27" />
			<biblScope unit="page" from="8241" to="8250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Semantically Enhanced Feature for Fine-Grained Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/LSP.2020.3020227</idno>
		<ptr target="https://doi.org/10.1109/LSP.2020.3020227" />
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1545" to="1549" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to Zoom: A Saliency-Based Sampling Layer for Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01240-3_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01240-3_4" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08" />
			<biblScope unit="volume">11213</biblScope>
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IX</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>Satinder P. Singh and Shaul Markovitch</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<ptr target="https://arxiv.org/abs/2012.12877" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Alexandre Sablayrolles, and Herv? J?gou</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00436</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00436" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2688133</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2688133" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06878</idno>
		<ptr target="http://arxiv.org/abs/1605.06878" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grassmann Pooling as Compact Homogeneous Bilinear Pooling for Fine-Grained Visual Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01219-9_22" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="365" to="380" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Navigate for Fine-Grained Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01264-9_26" />
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part XIV (Lecture Notes in Computer Science</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting>Part XIV (Lecture Notes in Computer Science<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-08" />
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="438" to="454" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2018 -15th European Conference</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisheng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-67832-6_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-67832-6_12" />
	</analytic>
	<monogr>
		<title level="m">Mul-tiMedia Modeling -27th International Conference, MMM 2021</title>
		<editor>Klaus Schoeffmann, Vasileios Mezaris, Xirong Li, Stefanos Vrochidis, and Ioannis Patras</editor>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-06-22" />
			<biblScope unit="volume">12572</biblScope>
			<biblScope unit="page" from="136" to="147" />
		</imprint>
	</monogr>
	<note>Jakub Lokoc, Tom?s Skopal</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Multiattention Convolutional Neural Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.557</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.557" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Deep Bilinear Transformation for Fine-grained Image Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/959ef477884b6ac2241b19ee4fb776ae-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="4279" to="4288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-Grained Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00515</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00515" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<ptr target="https://arxiv.org/abs/2012.15840" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Discriminative Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.319</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.319" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<ptr target="https://arxiv.org/abs/2010.04159" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/7016" />
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13130" to="13137" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
