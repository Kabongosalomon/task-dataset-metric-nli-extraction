<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pooled Motion Features for First-Person Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
							<email>mryoo@jpl.nasa.gov</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Jet Propulsion Laboratory</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Rothrock</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Jet Propulsion Laboratory</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Matthies</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Jet Propulsion Laboratory</orgName>
								<orgName type="institution">California Institute of Technology</orgName>
								<address>
									<settlement>Pasadena</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pooled Motion Features for First-Person Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN).</p><p>We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>First-person videos, also called egocentric videos, are videos taken from an actor's own viewpoint. The volume of egocentric video is rapidly increasing due to the recent ubiquity of small wearable devices. The main difference between conventional 3rd-person videos and 1stperson videos is that, in 1st-person videos, the person wearing the camera is actively involved in the events being recorded. Strong egomotion is observed in first-person videos, which makes them visually very unique <ref type="figure">(Figure 1</ref>). Automated understanding of such videos (i.e., first-person activity recognition) is crucial for many societal applications including quality-of-life systems to support daily liv-ing and video-based life-logging. Applications also include robot perception and human-robot interactions, since videos from the robot's viewpoint naturally are in first-person.</p><p>Despite a massive amount of first-person videos becoming available, approaches to semantically understand such videos have been very limited. This is particularly true for research on 'motion features' for first-person videos, which serves as a fundamental component for visual grounding of actions and events. Even though there has been previous works on extraction of first-person-specific semantic features like hand locations <ref type="bibr" target="#b10">[11]</ref> and human gaze <ref type="bibr" target="#b12">[13]</ref>, features and representations designed to capture motion dynamics of first-person videos have been lacking. Representing this egomotion is very essential for recognition of sports activities, accident activities for patient/health monitoring (e.g., a person collapsing), activities for surveillance/military (e.g., another person assaulting), and many others from first-person videos. Most of the previous firstperson activity recognition works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> focused on the use of existing features and representations designed for conventional 3rd-person videos, without tailoring motion features for the first-person case. This paper introduces a new feature representation named pooled time series (PoT). Our PoT is a general representation framework based on time series pooling of feature descriptors, which is particularly designed to capture motion information in first-person videos. Given a sequence of per-frame feature descriptors (e.g., HOF or CNN features) from a video, PoT abstracts them by computing short-term/long-term changes in each descriptor element. The motivation is to develop a new feature representation that captures 'details' of entire scene dynamics displayed in first-person videos, thereby obtaining better video recognition performances. Capturing egomotion information is crucial for recognition of ego-actions and interactions from first-person videos, and our PoT representation allows the system to do so by keeping track of very detailed changes in feature descriptor values while suppressing noise. Multiple novel pooling operators are introduced, and are combined with temporal filters to handle the temporal structure of human activities.  <ref type="figure">Figure 1</ref>. Conceptual comparison between 1st-person videos and 3rd-person videos. Example snapshots from public first-person datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref> taken with human/animal wearable cameras and those from public 3rd-person dataset <ref type="bibr" target="#b16">[17]</ref> are also illustrated.</p><p>We experimentally confirm that our proposed PoT representation clearly outperforms previous feature representations such as bag-of-visual-words and improved Fisher vector <ref type="bibr" target="#b13">[14]</ref> on first-person activity recognition. Both the global motion aspect and local motion aspect of first-person videos are captured with our PoT by taking advantage of different types of descriptors, and we illustrate recognition accuracies of our PoT with each of the descriptors as well as their combinations. Furthermore, we demonstrate that our combined PoT representation has superior performance to the best-known motion feature designed for 3rd-person videos <ref type="bibr" target="#b20">[21]</ref>, when handing 1st-person videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related works</head><p>Recognition from first-person videos is a topic with an increasing amount of attention. There are works focusing on first-person-specific features, including hand locations in first-person videos <ref type="bibr" target="#b10">[11]</ref> and human gaze estimation based on first-person videos <ref type="bibr" target="#b12">[13]</ref>. There also have been works on object recognition from first-person videos <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>However, study on motion features for first-person videos has been relatively limited, particularly those for first-person activity recognition. Most of the works focused on temporal segmentation of videos using optical flow-based features, without taking advantage of highdimensional image features for detailed recognition of highlevel activities. Kitani et al. <ref type="bibr" target="#b8">[9]</ref> worked on unsupervised learning of ego-actions and segmentation of videos based on it. A simple histogram based on optical flow direction/magnitude and frequency was constructed as a feature representation, which can be viewed as an extension of HOF. Poleg et al. <ref type="bibr" target="#b15">[16]</ref> introduced the use of displacement vectors similar to optical flows for long-term temporal segmentation, but they only focused on segmentation of relatively simple egomotion such as walking and wheeling. <ref type="bibr" target="#b17">[18]</ref> investigated the first-person activity recognition scenarios by combining multiple features, while particularly focusing on recognition of interaction-level activities. Still, they used very conventional HOF and local spatio-temporal features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref> together with general bag-of-visual-words representation, without any attempt to develop first-personspecific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Pooled times series representation</head><p>In this section, we introduce our new feature representation named pooled time series (PoT), which is specifically designed for first-person videos. The role of a feature 'representation' is to abstract a set of raw feature descriptors (e.g., histogram of oriented gradients) extracted from each video into a single vector representing the video. It converts a large number of high-dimensional descriptors into a single vector with a tractable dimensionality, allowing its result to serves as an input vector for classifiers (e.g., activity classification). Existing feature representations include bag-of-visual-words (BoW) and improved Fisher vector (IFV), which converts a set of raw descriptors into a lowdimensional histogram. What we introduce in this section is a new feature representation that better abstracts motion displayed in first-person videos.</p><p>The overall pipeline of our PoT representation is as follows. Given a first-person video (i.e., a sequence of image frames), our approach first extracts appearance/motion descriptors from each frame. As a result, a sequence of ndimensional descriptor vectors is obtained where n is the size of the vector from each frame. Our approach interprets this as a set of n time series. The idea is to keep track of how each element of the descriptor vector is changing over time (i.e., it becomes a function of time), and summarize such information to represent the activity video. Next, temporal pooling is performed: a set of temporal filters (i.e., time intervals) is applied to each time series and the system performs multiple types of pooling operations (e.g., max, sum, gradients, ...) per filter. Finally, the pooling results are concatenated to form the final representation of the video. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the overall process.</p><p>Let each per-frame feature descriptor obtained at frame t be denoted</p><formula xml:id="formula_0">as V t = [v t 1 , v t 2 , ..., v t n ]</formula><p>. Our PoT representation framework interprets this sequence of vectors V 1 , ..., V m (m is the number of video frames) as a set of time series, {f 1 (t), ..., f n (t)}. That is, each of our time series f i (t) corresponding to the ith feature descriptor value is defined as f i (t) = v t i . For each time series, temporal pooling is performed with a set of k temporal filters, which essentially is a set of time intervals to make the system focus on each local time window: {[t s 1 , t e 1 ], ..., [t s k , t e k ]}. A temporal pyramid structure <ref type="bibr" target="#b1">[2]</ref> is used in our implementation to obtain filters, but any number of filters with (overlapping) intervals can be used by our framework in principle.</p><p>Finally, multiple pooling operators are applied for each filter and their results are concatenated to obtain the final  PoT feature representation of the video:</p><formula xml:id="formula_1">x = [x op1 1 [t s 1 , t e 1 ], x op2 1 [t s 1 , t e 1 ], ? ? ? , x opr n [t s k , t e k ]]<label>(1)</label></formula><p>where x op j i specifies that it is applying the jth pooling operator to the ith time series f i (t). Our PoT representation takes advantage of four different types of pooling operators including two newly introduced temporal pooling operators, which we discuss more in Subsection 2.2.</p><p>Our framework of (i) extracting per-frame descriptors, (ii) interpreting them as a set of time series, and (iii) applying various types of time series pooling and concatenating them provides the following three important abilities:</p><p>First, (1) it preserves detailed dynamics displayed in each descriptor element as a time series, and allows the representation to capture both long-term motion and short-term information with multiple temporal filters. That is, depending on the nature of the time series, our representation is able to capture subtle short-term motion by pooling from a filter with a small time interval as well as long-term motion by performing pooling with a large time interval. Such flexibility is in contrast to previous bag-of-visual-words representation for global motion descriptors (e.g., the one used in <ref type="bibr" target="#b17">[18]</ref>) that abstracts all descriptor values in one frame (or a subsequence of few frames) into a single discretized 'visual word'. In addition, (2) our representation explicitly imposes temporal structure of the activity by decomposing the entire time interval to multiple subintervals, which is very important for representing high-level activities. Finally, (3) it allows the system to take advantage of multiple types of pooling operators so that the representation captures different aspects of the data.</p><p>As a result of our framework, each video is represented with one single vector having a tractable dimensionality. Activity recognition is performed by training/testing standard classifiers (e.g., SVM classifiers) with these vectors. Our representation is able to cope with any type of generative and discriminative classifiers in principle, and we show its superiority over others in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Handling high-dimensional feature descriptors</head><p>The proposed representation framework is very general in the aspect that it is able to cope with any types of per-frame image/motion descriptors such as histogram of oriented gradients (HOG) or histogram of optical flows (HOF). Furthermore, it is particularly designed to handle high-dimensional per-frame image descriptors: imagebased deep learning features which are also called convolutional neural network (CNN) features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. These deep learning image features are obtained by snatching intermediate outputs from internal convolutional layers of a CNN, pre-trained on image datasets. They can also be viewed as cascades of automatically learned image filters. These image descriptors are trained from large scale image datasets and have obtained highly successful results on image classification/detection <ref type="bibr" target="#b3">[4]</ref> as well as video classification <ref type="bibr" target="#b7">[8]</ref>, performing superior to state-of-the-art hand-designed image descriptors such as HOG even without re-training the networks.</p><p>Our motivation was to design a general representation that best takes advantage of such high-dimensional descriptors and confirm that these CNN features are able to increase first-person activity recognition performance significantly together with other features. Each element of a CNN feature vector abstracts particular local/global appearance for a single frame, and (by extension) its time series models how this local/global appearance is changing over time. As a human in the scene moves (e.g., changes his/her posture) and the camera changes its viewpoint because of egomotion, certain CNN feature values will become activated/deactivated and our idea is to keep track of such changes to represent the activity video. In our experiments, we explicitly confirm this while comparing our representation with the conventional representations. When using CNN features as our base per-frame descriptors, we get a 4096-dimensional feature vector (i.e., n=4096) for each image frame by obtaining outputs of the last convolutional layer (e.g., stage 7 in <ref type="bibr" target="#b18">[19]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Temporal pooling operators</head><p>Our PoT representation is constructed by applying multiple types of temporal pooling operators over each temporal filter (i.e., time interval). In this paper, we take advantage of four different types of pooling operators: conventional max pooling and sum pooling, and two types of our new 'histogram of time series gradients' pooling.</p><p>Our max pooling and sum pooling operators are defined as follows:</p><formula xml:id="formula_2">x max i [t s , t e ] = max t=ts..te f i (t), x i [t s , t e ] = te t=ts f i (t).<label>(2)</label></formula><p>In addition to these traditional pooling operators, we newly introduce the concept of 'histogram of time series gradients' pooling. The idea is to count the number of positive (and negative) gradients within the temporal filter.</p><formula xml:id="formula_3">x ? + 1 i [t s , t e ] = |{t | f i (t) ? f i (t ? 1) &gt; 0 ? t s ? t ? t e }|, x ? ? 1 i [t s , t e ] = |{t | f i (t) ? f i (t ? 1) &lt; 0 ? t s ? t ? t e }|.</formula><p>(3) Furthermore, we propose a variation of the above new pooling operator, which sums the amount of positive (or negative) gradients instead of simply counting their numbers. It is defined as: </p><formula xml:id="formula_4">x ? + 2 i [t s , t e ] = te t=ts h + i (t), x ? ? 2 i [t s , t e ] = te t=ts h ? i (t) (4) where h + i (t) = f i (t) ? f i (t ? 1) if (f i (t) ? f i (t ? 1)) &gt; 0 0 otherwise, h ? i (t) = f i (t ? 1) ? f i (t) if (f i (t) ? f i (t ? 1)) &lt; 0 0 otherwise.<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental settings</head><p>Datasets: We conducted our experiments with two different public first-person video datasets: DogCentric activity dataset <ref type="bibr" target="#b4">[5]</ref> and UEC Park dataset <ref type="bibr" target="#b8">[9]</ref>. These are very challenging datasets with strong camera egomotion, which are different from conventional 3rd-person datasets. <ref type="figure">Figure 1</ref> shows sample images. DogCentric dataset was recorded with wearable cameras mounted on dogs' back. UEC Park dataset was collected by a human wearing a camera. Dog-Centric dataset contains ego-actions of the dog as well as interactions between the dog and other humans (e.g., a human throws a ball and the dog chases it). UEC Park dataset contains ego-actions of the person (wearing a camera) involved in various types of physical activities (e.g., climbing a ladder) at a park. Dogcentric dataset consists of 10 activity classes, while UEC dataset consists of 29 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation implementation: We implemented our</head><p>PoT representations with four different types of per-frame feature descriptors: histogram of optical flows (HOF), motion boundary histogram (MBH) used in <ref type="bibr" target="#b20">[21]</ref>, Overfeat CNN image feature <ref type="bibr" target="#b18">[19]</ref>, and Caffe CNN image feature <ref type="bibr" target="#b6">[7]</ref>. The first two descriptors (i.e., HOF and MBH) are optical flow based motion descriptors, and the last two (i.e., Overfeat and Caffe) are deep learning based image appearance descriptors from CNNs pre-trained on ImageNet. Our HOF descriptors are in 200-D (5-by-5-by-8), MBH descriptors are in 400-D (two 5-by-5-by-8), and Overfeat and Caffe are in 4096-D. L1 normalization was applied for each descriptor. As a result, four different versions of our PoT representations were implemented as well as the final representation combining all four representations. As described in Section 2, pyramid temporal filters with level 4 were used and four different types of pooling operators were applied.</p><p>Classifiers: In all our experiments, we used the same nonlinear SVM with a ? 2 kernel. It showed better performance compared to linear SVM. When combining representations with multiple descriptors, a multi-channel kernel was used.</p><p>Evaluation setting: We followed the standard evaluation setting of the DogCentric dataset: we performed repeated random training/testing splits 100 times, and averaged the performance. We randomly selected half of videos per activity class as training videos, and used the others for the testing. If the number of total videos per class is odd, we made the testing set to contain one more video. Once training videos are selected, they are used across the entire experiments for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature representation evaluation</head><p>We conducted experiments to confirm superiority of our proposed PoT representation over conventional feature rep-  resentations. The idea is to evaluate the performances of our PoT, and compare it with those of other widely-used stateof-the-art representations while making them use exactly the same feature descriptors. More specifically, we compared our PoT representations with bag-of-visual-words (BoW) and Improved Fisher Vector (IFV) <ref type="bibr" target="#b13">[14]</ref> while using four types of feature descriptors (i.e., HOF, MBH, Overfeat, and Caffe) and their combinations.</p><p>BoW and IFV are very commonly used feature representations in the activity recognition literature <ref type="bibr" target="#b0">[1]</ref>. BoW represents a video as a histogram of 'visual words' by clustering all feature descriptors, making each descriptor assigned to a specific visual word. IFV can be viewed as a 'soft' version of that, in the aspect that it represents each descriptor as a set of soft assignments to cluster centers. We tested tens of different parameter settings for each feature type, and chose the best setting per feature. This includes the tuning of the number of visual words (e.g., IFV has 4000-D for HOF and 40960-D for Caffe). In addition, we implemented BoW and IFV in conjunction with the temporal pyramid pooling identical to the one used in our PoT, so that they also consider temporal structure among features. We explicitly compared all these different representations with our PoT. Also, since the clustering processes of BoW and IFV contain randomness, we report their 95% confidence interval together with the median performance by testing them 10 times. Here, we are showing the accuracies of the PoT representation with the best combination of pooling operators. As described in Section 2.2, there are four different pooling operators our PoT representation can take advantage of. We conducted experiments with all possible combinations of pooling operators for PoT (which can be found in our supplementary Appendix), and selected the best performing combination. In general, concatenations of all pooling operators (e.g., +max+? 1 ) obtained the best results, or results very close to the best. 'PoT (base)' is the basic version of our feature representation, which is constructed by applying the pooling operator to a single time interval that covers the entire activity video (i.e, no temporal pyramid structure).</p><p>We are able to observe that our PoT representations perform superior to BoW and IFV in all cases, except for MBH descriptors where all representations showed similar performances. Even when we add temporal pyramid pooling (identical to the one used in our representation) to BoW and IFV, their performances were clearly inferior to our PoT. The mean accuracy of the combined IFV representation (with pyramid) was 0.666, while our PoT obtained the accuracy of 0.730. Previous state-of-the-art is 0.605 <ref type="bibr" target="#b4">[5]</ref>.</p><p>Particularly, in the case of using high-dimensional deep learning features (i.e., 4096-D in Overfeat and Caffe), we confirmed that our representation significantly improves the performance over both BoW and IFV. We believe this is due to the fact that per-frame abstraction (i.e., clustering) performed in BoW and IFV fails to capture subtle local cues while our representation is particularly designed to handle such high-dimensionality descriptors. BoW abstracts each high-dimensional per-frame descriptor into a single 'visual word' (or a few soft assignments in case of IFV). As a result, subtle changes in a small number of descriptor values are ignored in general, which is particularly harmful in the case of high-dimensional descriptors. This is unlike our PoT that tries to explicitly capture such changes with time series pooling. The result suggests that PoT is the better representation to take advantage of modern high-dimensional feature descriptors.</p><p>Another important observation is that our PoT benefitted greatly by considering temporal structures among features, much more compared to BoW and IFV. We discuss this more in Subsection 3.3.</p><p>In addition, since dynamic time warping (DTW) is a traditional approach to deal with time series data, we also tested a basic DTW-based template matching (using the same time series with PoT) as a baseline. The best performance of DTW was 0.288, as opposed to 0.730 of ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UEC Park dataset:</head><p>We also performed the same experiments described above with one more public first-person video dataset: UEC Park dataset. As described in the previous subsection, the dataset contains video segments labeled with 29 different classes. Labels are very rough and the number of videos per activity class are very unbalanced in this dataset (e.g., there is a class with only 1 video), making the classification task challenging.</p><p>Also, videos of this dataset were obtained by segmenting a long video every 2 seconds, and each segment was labeled based on the most dominant activity observed in the segment. As a result, activities in these videos are not temporally aligned (e.g., a video segment may not even contain the initial part of the activity at all) and using pooling with temporal structures only harms the recognition performances. We confirmed this with all representations: BoW, IFV, and PoT. Thus, here, we only show the results of representations without any temporal structure consideration. PoT is at a disadvantage for this dataset, since it benefits greatly using pooling with temporal structures while BoW and IFV do not, as we discuss more in Subsection 3.3. <ref type="figure" target="#fig_5">Figure 3</ref> (bottom) shows the result. Our PoT obtained the best performance on all feature descriptors, similar to the case of the DogCentric dataset. PoT obtained the best result using all four feature descriptors and obtained particularly higher performances for high-dimensional CNN features. Even though PoT was not able to fully take advantage of activites' temporal structures, it still performed superior to BoW and IFV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal structure evaluation</head><p>We conducted further experiments to confirm the advantage of PoT: it benefits more compared to other representations when considering temporal structure among features. DogCentric activity dataset was used for this experiment. We illustrate classification accuracies of BoW, IFV, and PoT with and without consideration of the temporal pyramid structure. 'Without pyramid' means that the feature representation is constructed by applying the pooling operator on one single time interval that covers the entire activity video. 'With pyramid' means that a set of temporal filters were used. For PoT, we also compare results of different time series pooling operators (and their combinations) with and without temporal pyramids. <ref type="figure" target="#fig_7">Figure 4</ref> shows the results. We are able to confirm that consideration of temporal structure benefited the recognition with our PoT while it did not benefit the other representations much. This is particularly true for the representations with combinations of all four descriptors. We believe this observation is caused by the following characteristic: as mentioned in Subsection 3.2, abstraction/discretization of per-frame observations in BoW (or IFV) completely ignores subtle local descriptor changes. This makes them have less chance to capture short-term local motion even when we temporally split the video using a pyramid. On the other hand, PoT does not suffer from such abstraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison to state-of-the-art features</head><p>We explicitly compared activity classification accuracies of our PoT representations with other state-of-the-art features, including well-known local spatio-temporal features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref> and recent trajectory-based local video features <ref type="bibr" target="#b20">[21]</ref>. Notably, INRIA's improved trajectory feature (ITF) <ref type="bibr" target="#b20">[21]</ref> is the one that obtained the best performance in the ICCV 2013 challenge on UCF101 dataset <ref type="bibr" target="#b19">[20]</ref>. ITF internally takes advantage of three different feature descriptors similar to ours: HOG, HOF, and MBH. IFV representations were used for all these features. For the DogCentric dataset experiments, the temporal pyramid structure was considered by all of these previous features, since using temporal pyramid improved their classification accuracies by 0.03?0.05. What we show is that our new feature representations (together with frame-based descriptors) are more suitable for representing motion in first-person videos compared to the previous features designed for 3rd-person videos.</p><p>In addition, we implemented the approach of combining ITF with CNN descriptors <ref type="bibr" target="#b5">[6]</ref>, which won the ECCV 2014 classification challenge on UCF101 dataset. Both Overfeat <ref type="table">Table 1</ref>. A table comparing performances of the proposed approach with state-of-the-arts on DogCentric dataset <ref type="bibr" target="#b4">[5]</ref>: F1-scores per class and the final 10-class classification accuracies. Approaches with our representations are colored blue. The performances are split into three categories: representations with only one descriptor, representations with multiple descriptors, and combinations of multiple different features representations. The best performance per category is indicated with bold. The overall best performance is indicated with bold+red.  <ref type="bibr" target="#b2">[3]</ref> 0   and Caffe were used, and mean of per-frame CNN vectors were computed and added to ITF. <ref type="table">Table 1</ref> shows the results with the DogCentric dataset. In addition to the final 10-class classification accuracies of the approaches, we are also reporting per-class F1-scores of them. The motivation is to analyze which feature descriptor/representation is better for recognition of which activity class. Instead of simply reporting per-class classification accuracies that do not take false positives into account, we computed a pair of precision and recall values per class from the confusion matrix and obtained F1-scores based on them.</p><p>The result clearly illustrate that our PoT obtains the best result, outperforming local spatio-temporal features as well <ref type="table">Table 2</ref>. A table comparing performances of the proposed approach with state-of-the-arts using UEC Park dataset <ref type="bibr" target="#b8">[9]</ref>: 29-class classification accuracies are shown.</p><p>Final accuracy STIP (with IFV) <ref type="bibr" target="#b9">[10]</ref> 0.6913438 ? 0.003 Cuboid (with IFV) <ref type="bibr" target="#b2">[3]</ref> 0.7233332 ? 0.002 BoW -all 0.7649616 ? 0.002 IFV -all 0.7640002 ? 0.002 Inria ITF (with IFV) <ref type="bibr" target="#b20">[21]</ref> 0.7662412 ? 0.002 ITF + CNN <ref type="bibr" target="#b5">[6]</ref> 0.757359 ? 0.002 PoT -all (best) 0.793641 PoT + ITF 0.794897 ? 0 as the previously reported results <ref type="bibr" target="#b4">[5]</ref>. Particularly, our PoT performed significantly superior to the state-of-the-art ITF approach. Even with the temporal pyramid pooling added to the original ITF, our PoT performed much better than the ITF: 0.676 vs. 0.730. The ITF performance without pyramid was 0.638. Our PoT also showed the best per-class recognition accuracies in most of the classes. ITF showed slightly better performances for 'turn head' classes, since these videos are actually more similar to 3rd-person videos: the camera was mounted on the back of the dog (i.e., not head) and these 'turn head' videos do not involve much camera motion unlike the others. Furthermore, our PoT performed superior to the conventional method <ref type="bibr" target="#b5">[6]</ref> of combining ITF and mean per-frame CNN: 0.692 vs. 0.730. We are able to observe similar results with the UEC Park dataset. <ref type="table">Table 2</ref>   performance, and we were able to (slightly) increase the performance further by combining PoT with ITF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation of appearance-based features</head><p>Taking advantage of CNN descriptors: We explicitly compared the recognition accuracies of our approach with and without CNN-based appearance descriptors. The idea was to confirm 'how much benefit our PoT representation is able to get from CNN descriptors' when representing firstperson videos for activity recognition. The result is that, with our PoT, CNN-based appearance descriptors capture information different from motion descriptors and combining them with others really benefits the entire system, while the degree of their effectiveness is dependent on the dataset and activities. <ref type="figure" target="#fig_10">Figure 6</ref> shows the results.</p><p>For the DogCentric dataset, using CNN descriptors greatly benefited the overall recognition accuracy. Notice that CNN descriptors themselves showed superior performance compared to all other motion-based descriptors (e.g., HOF) with our PoT, as described in <ref type="figure" target="#fig_5">Figure 3</ref>. DogCentric dataset contains activity videos taken at various environments (indoor, outdoor, ...), and certain activities are highly correlated with such environment/background information (e.g., there will not be 'ball chasing' activity in an indoor environment). As a consequence, capturing appearance information is very important for these activities/videos, and CNN descriptors showed very good results on them with our PoT. On the other hand, all UEC Park video sequences are taken at a same environment (i.e., a park), and thus CNN-based appearance descriptors were not as effective as motion descriptors. Nevertheless, in both cases, combining CNN features with other descriptors benefited the overall recognition performances, suggesting that our PoT is properly taking advantage of them.</p><p>Appearance descriptors: CNN vs. HOG: We tested another appearance descriptor, histogram of oriented gradients (HOG), and compared it with the CNN descriptors we are using. For this experiment, we extracted pure histogram of oriented gradients similar to our HOF from images. For each frame, a HOG descriptor was constructed with 8 different gradient directions and 5-by-5 spatial bins. Then, each sequence of these HOG descriptors was represented using our PoT representation. We not only compared our PoT representation only based on HOG with those only based on CNN descriptors, but also tested our final 'combined' PoT representing using both appearance descriptors and motion descriptors (i.e., HOF and MBH) by replacing CNN with HOG.</p><p>The idea was to compare two appearance descriptors (CNN vs. HOG) in representing first-person videos, and confirm that our PoT is appropriately taking advantage of CNN descriptors which is supposed to perform superior to HOG. <ref type="figure" target="#fig_11">Figure 7</ref> illustrates the results obtained with the two datasets we use. We are able to observe that, with our PoT, CNN descriptors are significantly outperforming HOG descriptors under identical settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We introduced the new feature representation designed for first-person videos: pooled time series (PoT). Our PoT was designed to capture entire scene dynamics as well as local motion in first-person videos by representing longterm/short-term changes in high-dimensional feature descriptors, and it was combined with four different types of per-frame descriptors including CNN features. We evaluated our PoT using two public first-person video datasets, and confirmed that our PoT clearly outperforms previous feature representations (i.e., BoW and IFV) as well as the other state-of-the-art video features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>dimensional data (e.g., n = 4096 features, m = 1000 frames) n time series (e.g., 4096) k temporal filters (e.g., 15) n*k-D vector (e.g., 61440) ? max pooling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overall representation framework of our pooled time series (PoT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Each of our time series gradients pooling operation generates a pair of values (i.e., x ? + i and x ? ? i ) instead of a single value like max pooling. These two values are concatenated in our PoT representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Classification accuracies of feature representations with each descriptor (and their combination). Representations that utilize randomness are drawn with 95% confidence intervals. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>DogCentric activity dataset: Figure 3 (top) describes the 10-class activity classification accuracies of our representation (and BoW and IFV) for each of the base descriptors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Feature performance using BoW and IFV compared with various PoT pooling operators with and without a temporal pyramid on the DogCentric dataset. Y-axis is classification accuracy, and X-axis shows different representations. PoT generally benefits much more from the temporal pyramid than BoW and IFV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>.646436 0.777357 0.535933 0.471589 0.252507 0.068375 0.550717 0.854224 0.720194 0.682976 0.5961668 ? 0.007 IFV -HOF 0.613862 0.619561 0.421567 0.308094 0.345337 0.281042 0.548307 0.747027 0.535438 0.662877 0.5281018 ? 0.01 IFV -MBH 0.641111 0.863174 0.261308 0.470158 0.332133 0.284172 0.443836 0.674957 0.61996 0.570858 0.549259 ? 0.006 IFV -Overfeat 0.614632 0.815432 0.452256 0.576355 0.311339 0.318194 0.676456 0.465434 0.642253 0.449071 0.5567592 ? 0.004 IFV -Caffe 0.660997 0.860932 0.58083 0.55149 0.269409 0.229007 0.639273 0.446267 0.649675 0.411165 0.5453332 ? 0.014 PoT -HOF (? + ?1) 0.790159 0.79585 0.586986 0.431446 0.499128 0.468511 0.501162 0.832549 0.590227 0.671908 0.618426 PoT -MBH (? + max + ?1) 0.644583 0.709702 0.320233 0.386772 0.441355 0.567058 0.299373 0.873127 0.574558 0.627518 0.556759 PoT -Overfeat (? + max + ?1) 0.74655 0.895397 0.640212 0.594052 0.291462 0.355681 0.783395 0.726989 0.755053 0.564199 0.649907 PoT -Caffe (max + ?2) 0.738111 0.900798 0.725664 0.626889 0.33498 0.338351 0.705605 0.592798 0.773768 0.545347 0.639352 Multi-descriptor features Inria ITF (with IFV) [21] 0.691291 0.893157 0.545962 0.579966 0.495152 0.589468 0.625639 0.708337 0.778854 0.676454 0.6757592 ? 0.006 IFV -all 0.753374 0.876573 0.580573 0.597431 0.368204 0.305966 0.725102 0.789129 0.742027 0.659151 0.6657036 ? 0.008 PoT -all (? + max + ?1) 0.820552 0.932507 0.68982 0.59662 0.45 0.472542 0.758327 0.854455 0.817615 0.778485 0.727685 PoT -all (? + max + ?2) 0.820359 0.93047 0.714618 0.584604 0.439639 0.45971 0.756757 0.870967 0.82741 0.788683 0.73 Combinations of multiple feature representations Iwashita et al. 2014 [5] 0.618939 0.818613 0.383081 0.510749 0.397806 0.41918 0.544725 0.86418 0.698887 0.779148 0.605 STIP + Cuboid (with IFV) 0.685341 0.788416 0.471734 0.519026 0.395602 0.23537 0.540914 0.837125 0.74858 0.738611 0.6291759 ? 0.008 ITF + STIP + Cuboid 0.714646 0.871098 0.533088 0.591699 0.47348 0.423435 0.650097 0.827838 0.813176 0.753727 0.6912039 ? 0.006 ITF + CNN [6] 0.696641 0.928735 0.703593 0.651387 0.434422 0.415808 0.778585 0.726934 0.808843 0.608596 0.692315 ? 0.004 PoT + STIP + Cuboid 0.804031 0.925272 0.712457 0.591944 0.460633 0.433242 0.742753 0.866876 0.836603 0.797134 0.73137 ? 0.001 PoT + ITF 0.826126 0.933284 0.71304 0.597523 0.477482 0.515245 0.754544 0.87818 0.851537 0.795619 0.7447038 ? 0.001 PoT + ITF + STIP + Cuboid 0.819623 0.92363 0.703833 0.594038 0.479739 0.50065 0.733664 0.873437 0.848916 0.809405 0.7406666 ? 0.001 PoT (Combined</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Confusion matricies on the DogCentric dataset, comparing our PoT (combined) and the state-of-art INRIA ITF feature (with IFV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Performance gain from combining CNN features with conventional motion features for both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Performance comparison of CNN features as a replacement for HOG, showing consistent gains on both datasets with and without motion features included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>shows the results. PoT obtained the best</figDesc><table><row><cell>DogCentric</cell><cell>0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76</cell><cell>Without CNN features</cell><cell>With CNN features</cell></row><row><cell>UEC Park</cell><cell>0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.82 0.80</cell><cell>HOF+MBH</cell><cell>HOF+MBH+ITF</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: The research described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. This research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-10-2-0016.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatio-temporal pyramid matching for sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on VS-PETS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">First-person animal activity recognition from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Takamine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kurazume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">University of amsterdam at thumos challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">This hand is my hand: A probabilistic approach to hand disambiguation in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Franchak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving the Fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
