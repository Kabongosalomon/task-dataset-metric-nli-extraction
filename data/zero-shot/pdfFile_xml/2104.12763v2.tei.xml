<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MDETR -Modulated Detection for End-to-End Multi-Modal Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">NYU Center for Data Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
							<email>mannatsingh@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<email>yann.lecun@nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<email>imisra@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NYU Courant Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MDETR -Modulated Detection for End-to-End Multi-Modal Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MDETR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Architecture</head><p>We depict the architecture for MDETR in <ref type="figure">Fig. 2</ref>. As in DETR, the image is encoded by a convolutional backbone and flattened. In order to conserve the spatial information, 2-D positional embeddings are added to this flattened vec-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection forms an integral component of most state-of-the-art multi-modal understanding systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>, typically used as a black-box to detect a fixed vocabulary of concepts in an image followed by multi-modal alignment. This "pipelined" approach limits co-training with other modalities as context and restricts the downstream model to only have access to the detected objects and not the whole image. In addition, the detection system is usually frozen, which prevents further refinement of the model's perceptive capability. In the vision-language setting, it implies restricting the vocabulary of the resulting system to the categories and attributes of the detector, and is often a bottleneck for performance on these tasks <ref type="bibr" target="#b73">[72]</ref>. As a result, such a system cannot recognize novel combinations of concepts expressed in free-form text.</p><p>A recent line of work <ref type="bibr" target="#b67">[66,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b12">13]</ref> considers the problem of text-conditioned object detection. These methods extend mainstream one-stage and two-stage detection architectures to achieve this goal. However, to the best of our knowledge, it has not been demonstrated that such detectors can improve performance on downstream tasks that require reasoning over the detected objects, such as visual question answering (VQA). We believe this is because these detectors are not end-to-end differentiable and thus cannot be trained in synergy with downstream tasks.</p><p>Our method, MDETR, is an end-to-end modulated detector based on the recent DETR <ref type="bibr" target="#b1">[2]</ref> detection framework, and performs object detection in conjunction with natural language understanding, enabling truly end-to-end multimodal reasoning. MDETR relies solely on text and aligned boxes as a form of supervision for concepts in an image. Thus, unlike current detection methods, MDETR detects nuanced concepts from free-form text, and generalizes to unseen combinations of categories and attributes. We showcase such a combination as well as modulated detection in The features of both modalities are projected to a shared embedding space, concatenated and fed to a transformer encoder-decoder that predicts the bounding boxes of the objects and their grounding in text. <ref type="figure" target="#fig_0">Fig. 1</ref>. By design, our predictions are grounded in text, which is a key requirement for visual reasoning <ref type="bibr" target="#b66">[65]</ref>. When pre-trained using a dataset of 200,000 images and aligned text with box annotations, we achieve best reported results on the Flickr30k dataset for phrase grounding, Re-fCOCO/+/g datasets for referring expression comprehension, and referring expression segmentation on Phrase-Cut, as well as competitive performance on the GQA and CLEVR benchmarks for visual question answering. Our contributions are as follows:</p><p>? We introduce an end-to-end text-modulated detection system derived from the DETR detector.</p><p>? We demonstrate that the modulated detection approach can be applied seamlessly to solve tasks such as phrase grounding and referring expression comprehension, setting new state of the art performance on both these tasks using datasets having synthetic as well as real images.</p><p>? We show that good modulated detection performance naturally translates to downstream task performance, for instance achieving competitive performance on visual question answering, referring expression segmentation, and on few-shot long-tailed object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section we first briefly summarize the object detection pipeline <ref type="bibr" target="#b1">[2]</ref> based on which we build our model in ?2.1 and then describe how we extend it for modulated detection in ?2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Background</head><p>DETR Our approach to modulated detection builds on the DETR system <ref type="bibr" target="#b1">[2]</ref>, which we briefly review here. We refer the readers to the original paper for additional details. DETR is an end-to-end detection model composed of a backbone (typically a convolutional residual network <ref type="bibr" target="#b11">[12]</ref>), followed by a Transformer Encoder-Decoder <ref type="bibr" target="#b59">[59]</ref>.</p><p>The DETR encoder operates on 2D flattened image features from the backbone and applies a series of transformer layers. The decoder takes as input a set of N learned embeddings called object queries, that can be viewed as slots that the model needs to fill with detected objects. All the object queries are fed in parallel to the decoder, which uses cross-attention layers to look at the encoded image and predicts the output embeddings for each of the queries. The final representation of each object query is independently decoded into box coordinates and class labels using a shared feed-forward layer. The number of object queries acts as a de facto upper-bound on the number of objects the model can detect simultaneously. It has to be set to a sufficiently large upper-bound on the number of objects one may expect to encounter in a given image. Since the actual number of objects in a particular image may be less than the number of queries N , an extra class label corresponding to "no object" is used, denoted by ?. The model is trained to output this class for every query that doesn't correspond to an object.</p><p>DETR is trained using a Hungarian matching loss, where a bipartite matching is computed between the N proposed objects and the ground-truth objects. Each matched object is supervised using the corresponding target as groundtruth, while the un-matched objects are supervised to predict the "no object" label ?. The classification head is supervised using standard cross-entropy, while the bounding box head is supervised using a combination of absolute error (L1 loss) and Generalized IoU <ref type="bibr" target="#b48">[48]</ref>.</p><p>tor. We encode the text using a pre-trained transformer language model to produce a sequence of hidden vectors of same size as the input. We then apply a modality dependent linear projection to both the image and text features to project them into a shared embedding space. These feature vectors are then concatenated on the sequence dimension to yield a single sequence of image and text features. This sequence is fed to a joint transformer encoder termed as the cross encoder. Following DETR, we apply a transformer decoder on the object queries while cross attending to the final hidden state of the cross encoder. The decoder's output is used for predicting the actual boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Training</head><p>We present the two additional loss functions used by MDETR, which encourage alignment between the image and the text. Both of these use the same source of annotations: free form text with aligned bounding boxes. The first loss function that we term as the soft token prediction loss is a non parametric alignment loss. The second, termed as the text-query contrastive alignment is a parametric loss function enforcing similarity between aligned object queries and tokens.</p><p>Soft token prediction For modulated detection, unlike in the standard detection setting, we are not interested in predicting a categorical class for each detected object. Instead, we predict the span of tokens from the original text that refers to each matched object. Concretely, we first set the maximum number of tokens for any given sentence to be L = 256. For each predicted box that is matched to a ground truth box using the bi-partite matching, the model is trained to predict a uniform distribution over all token positions that correspond to the object. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an example where the box for cat is trained to predict a uniform distribution over the first two words. In <ref type="figure" target="#fig_3">Fig. 6</ref>, we show a simplified visualization of the loss for this example, in terms of a distribution over words for each box, but in practice we use token spans after tokenization using a BPE scheme <ref type="bibr" target="#b52">[52]</ref>. Any query that is not matched to a target is trained to predict the "no object" label ?. Note that several words in the text could correspond to the same object in the image, and conversely several objects could correspond to the same text. For example, "a couple" referred to by two boxes in the image, could further be referred to individually in the same caption. By designing the loss function in this way, our model is able to learn about co-referenced objects from the same referring expression.</p><p>Contrastive alignment While the soft token prediction uses positional information to align the objects to text, the contrastive alignment loss enforces alignment between the embedded representations of the object at the output of the decoder, and the text representation at the output of the cross encoder. This additional contrastive alignment loss ensures that the embeddings of a (visual) object and its corresponding (text) token are closer in the feature space compared to embeddings of unrelated tokens. This constraint is stronger than the soft token prediction loss as it directly operates on the representations and is not solely based on positional information. More concretely, consider the maximum number of tokens to be L and maximum number of objects to be N . Let T + i be the set of tokens that a given object o i should be aligned to, and O + i be the set of objects to be aligned with a given token t i . The contrastive loss for all objects, inspired by InfoNCE <ref type="bibr" target="#b39">[40]</ref> is normalized by number of positive tokens for each object and can be written as follows:</p><formula xml:id="formula_0">l o = N ?1 i=0 1 |T + i | j?T + i ? log exp(o ? i t j /? ) L?1 k=0 exp(o ? i t k /? )<label>(1)</label></formula><p>where ? is a temperature parameter that we set to 0.07 following literature <ref type="bibr" target="#b64">[63,</ref><ref type="bibr" target="#b47">47]</ref>. By symmetry, the contrastive loss for all tokens, normalized by the number of positive objects for each token is given by:</p><formula xml:id="formula_1">l t = L?1 i=0 1 |O + i | j?O + i ? log exp(t ? i o j /? ) N ?1 k=0 exp(t ? i o k /? )<label>(2)</label></formula><p>We take a the average of these two loss functions as our contrastive alignment loss. Combining all the losses In MDETR, a bipartite matching is used to find the best match between the predictions and the ground truth targets just as in DETR. The main difference is that there is no class label predicted for each object -instead predicting a uniform distribution over the relevant positions in the text that correspond to this object (soft token predictions), supervised using a soft cross entropy. The matching cost consists of this in addition to the L1 &amp; GIoU loss between the prediction and the target box as in DETR. After matching, the total loss consists of the box prediction losses (L1 &amp; GIoU), soft-token prediction loss, and the contrastive alignment loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section we describe the data and training used for pre-training MDETR, and provide details and results on the tasks that we use to evaluate our approach. Results on the CLEVR dataset are reported in <ref type="table">Table 1</ref>. For a discussion on the CLEVR results and further details on data preparation and training, please see Appendix B. Experimental details for pre-training and downstream tasks on natural images are detailed in ?3.1 and ?3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-training Modulated Detection</head><p>For pre-training, we focus on the task of modulated detection where the aim is to detect all the objects that are re- ferred to in the aligned free form text. We create a combined dataset using images from the Flickr30k <ref type="bibr" target="#b46">[46]</ref>, MS COCO <ref type="bibr" target="#b29">[30]</ref> and Visual Genome (VG) <ref type="bibr" target="#b23">[24]</ref> datasets. Annotations from the referring expressions datasets, VG regions, Flickr entities and GQA train balanced set are used for training. An image may have several text annotations associated with it. Details on the datasets can be found in Appendix C. Data combination For each image, we take all annotations from these datasets and combine the text that refers to the same image while ensuring that all images that are in the validation or testing set for all our downstream tasks are removed from our train set. The combination of sentences is done using a graph coloring algorithm which ensures that only phrases having boxes with GIoU ? 0.5 are combined, and that the total length of a combined sentence is less than 250 characters. In this way, we arrive at a dataset having 1.3M aligned image -text pairs. This combination step is important for two reasons: 1) data efficiency, by packing more information into a single training example and 2) it provides a better learning signal for our soft token prediction loss since the model has to learn to disambiguate between multiple occurrences of the same object category, as depicted in <ref type="figure" target="#fig_2">Fig 3.</ref> In the single sentence case, the soft token prediction task becomes trivial since it can always predict the root of the sentence without looking at the image. Experimentally, we find that such dense annotations translate to better grounding between text and image and subsequently to better downstream performance.</p><p>Model We use a pre-trained RoBERTa-base <ref type="bibr" target="#b31">[32]</ref> as our text encoder, having 12 transformer encoder layers, each with hidden dimension of 768 and 12 heads in the multihead attention. We use the implementation and weights from HuggingFace <ref type="bibr" target="#b61">[61]</ref>. For the visual backbone, we explore two options. The first is a ResNet-101 <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet with frozen batchnorm layers, taken from in examples such as the following: "the person in the grey shirt with a watch on their wrist. the other person wearing a blue sweater. the third person in a gray coat and scarf." We show the predictions from our model for this caption. It is able to pay attention to all the objects in the image and then disambiguate between them based on the text. The model is trained to predict the root of the phrase as the positive token span, which as we can see in this figure, correctly refers to the three different people.</p><p>Torchvision. This is to be comparable with current literature in the space of multi-modal understanding, where the popular approach is to use the BUTD object detector with a Resnet-101 backbone from <ref type="bibr" target="#b0">[1]</ref> trained on the VG dataset. In our work, we are not limited by the existence of pre-trained detectors, and inspired by its success in object detection <ref type="bibr" target="#b58">[58]</ref>, we choose to explore the EfficientNet family <ref type="bibr" target="#b57">[57]</ref> for our backbone. We use a model which was trained on large amounts of unlabelled data in addition to ImageNet, using a pseudo-labelling technique called Noisy-Student <ref type="bibr" target="#b65">[64]</ref>. We choose the EfficientNetB3, which achieves 84.1% top 1 accuracy on ImageNet with only 12M weights and EfficientB5 which achieves 86.1% using 30M weights. We use the implementation provided by the Timm library [?], and freeze the batchnorm layers. We pre-train our model for 40 epochs on 32 V100 gpus with an effective batch size of 64, which takes approximately a week to train. Training hyperparameters are detailed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Downstream Tasks</head><p>We evaluate our method on 4 downstream tasks: referring expression comprehension and segmentation, visual question answering and phrase grounding. Training hyperprameters for all tasks can be found in Appendix A.</p><p>Phrase grounding Given one or more phrases, which  <ref type="table">Table 2</ref>: Accuracy results on referring expression comprehension. *As mentioned in UNITER <ref type="bibr" target="#b5">[6]</ref>, methods using box proposals from the BUTD detector <ref type="bibr" target="#b0">[1]</ref> suffer from a test set leak, since the detector was trained on images including the validation and test set of the RE comprehension datasets. We report numbers for these methods from their papers using these "contaminated features" but we would like to stress that all of our pre-training excluded the images used in the val/test of any of the downstream datasets including for RE comprehension. CC refers to Conceptual Captions <ref type="bibr" target="#b53">[53]</ref>, VG to Visual Genome <ref type="bibr" target="#b23">[24]</ref>, SBU refers to the SBU Captions <ref type="bibr" target="#b40">[41]</ref> and COCO to Micosoft COCO <ref type="bibr" target="#b29">[30]</ref>.  <ref type="table">Table 3</ref>: Results on the phrase grounding task on Flickr30k entities dataset <ref type="bibr" target="#b46">[46]</ref>. Models with ? are pre-trained on COCO, models with * are also pre-trained on VG and Flickr 30k. Our models (MDETR) use a RoBERTa text encoder while other models use RNNs, word2vec-based features, or BERT (comparable to RoBERTa) text encoders. All models use a ResNet101 backbone, except MDETR-ENB3 which uses EfficientNet-B3 and MDETR-ENB5 with an EfficientNet-B5. may be inter-related, the task is to provide a set of bounding boxes for each phrase. We use the Flickr30k entities dataset for this task, with the train/val/test splits as provided by <ref type="bibr" target="#b46">[46]</ref> and evaluate our performance in terms of Recall@k. For each sentence in the test set, we predict 100 bounding boxes and use the soft token alignment prediction to rank the boxes according to the score given to the token positions that correspond to the phrase. We evaluate under two protocols which we name ANY-BOX <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> and MERGED-BOXES <ref type="bibr" target="#b44">[44]</ref>. Please see Appendix D for a discussion on the two protocols. We compare our method to existing state-ofthe-art results from two types of approaches -the text conditioned detection models <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b67">66]</ref> and a transformer based vision-language pre-training model <ref type="bibr" target="#b25">[26]</ref>. In the ANY-BOX setting, we obtain a 8.5 point boost over current state of the art on this task as measured in terms of Recall@1 on the validation set, without using any pre-training (no additional data). With pre-training, we further obtain a 12.1 point boost over the best model's performance on the test set, while using the same backbone.</p><formula xml:id="formula_2">Method Val Test R@1 R@5 R@10 R@1 R@5 R@10 ANY-BOX-PROTOCOL BAN [22] - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring expression comprehension</head><p>Given an image and a referring expression in plain text, the task is to localize the object being referred to by returning a bounding box around it. The approach taken by most prior work <ref type="bibr" target="#b70">[69,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b69">68]</ref> on this task has been to rank a set of preextracted bounding boxes associated with an image, that are obtained using a pre-trained object detector. In this paper, we solve a much harder task -we train our model to directly predict the bounding box, given a referring expression and the associated image. There are three established datasets for this task called RefCOCO, RefCOCO+ <ref type="bibr" target="#b71">[70]</ref> and Ref-COCOg <ref type="bibr" target="#b35">[36]</ref>. Since during pre-training we annotate every object referred to within the text, there is a slight shift in the way the model is used in this task. For example, during pre-training, given the caption "The woman wearing a blue dress standing next to the rose bush.", MDETR would be trained to predict boxes for all referred objects such as the woman, the blue dress and the rose bush. However, for referring expressions, the task would be to only return one bounding box, which signifies the woman being referred to by the entire expression. For this reason, we finetune the  <ref type="table">Table 4</ref>: Following <ref type="bibr" target="#b63">[62]</ref>, we report the mean intersection-overunion (IoU) of our masks with the ground-truth masks. We also report the precision Pr@I of our model, where success is marked when our proposed mask has an IoU with the ground-truth higher than the threshold I. With a comparable ResNet backbone, we observe consistent gains across all metrics over HULANet <ref type="bibr" target="#b63">[62]</ref>, the current state-of-the-art. The EfficientNet backbone further improves on those results. model on the task specific dataset for 5 epochs. At inference time, we use the ? label to rank the 100 detected boxes. Let P (?) be the probability assigned to the "no object" label, we rank by decreasing order of 1 ? P (?). We report results in <ref type="table">Table 2</ref>, showing large improvements over state-of-theart across all datasets.</p><p>Referring expression segmentation Similarly to DETR, we show that our approach can be extended to perform segmentation by evaluating on the referring expression segmentation task of the recent PhraseCut <ref type="bibr" target="#b63">[62]</ref> dataset which consists of images from VG, annotated with segmentation masks for each referring expression. These expressions comprise a wide vocabulary of objects, attributes and relations, making it a challenging benchmark. Contrary to other referring expression segmentation datasets, in PhraseCut the expression may refer to several objects. The model is expected to find all the corresponding instances. Our training occurs in two stages. In the first step, we take our pre-trained model after 40 epochs and fine-tune it for 10 epochs on this dataset, supervising the model to output correct boxes for the referred expressions. We use the box AP on the validation set for early stopping. In the second stage, following <ref type="bibr" target="#b1">[2]</ref>, we freeze the weights of the network, and only train a segmentation head for 35 epochs, with a learning rate drop at 25 epochs, supervised using a combination of the Dice/F1 loss <ref type="bibr" target="#b37">[38]</ref> and the Focal loss <ref type="bibr" target="#b28">[29]</ref>. At inference-time, we assign a confidence to each predicted box equal to 1 ? P (?) where P (?) is the probability assigned to the "no-object" token (see ?2). We then filter the boxes with a confidence lower than 0.7. Finally, we merge the masks corresponding to each of these boxes into one binary mask corresponding to this referring expression. The results are collected in <ref type="table">Table 4</ref>. Our model is able to produce clean masks for a wide variety of long tailed-concepts covered by PhraseCut. Example predictions from our model on this dataset are given in Appendix A.</p><p>Visual Question Answering We evaluate our hypothesis that modulated detection is a useful component for multi-  modal reasoning by fine-tuning our pre-trained model on the GQA dataset. To train MDETR, we use the scene graph provided in GQA to obtain the alignment between question words and the boxes. Our model architecture is depicted in <ref type="figure">Fig 4.</ref> Object queries are learned embeddings input to the decoder, each of which can be used to a detect an object. Apart from the 100 queries that are used for detection, we use additional queries that specialize in the type of question as well as one that is used to predict the type of question, where the types are defined in the GQA annotations as REL, OBJ, GLOBAL, CAT and ATTR. We take our pre-trained model trained for 40 epochs on our combined dataset, and initialise these queries as well as the heads for each of them randomly, and fine-tune first for 125 epochs on the unbalanced all GQA split, followed by 10 epochs on the balanced split similar to what is done in prior work <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>. During the first 125 epochs, we train the modulated detection losses along with the question answering, but put a weight on question answering loss that encourages the model to focus more on this task. For the balanced split finetuning, we only use the question answering loss. During inference, the type head predicts the type of question and the answer is taken from that head. Using our model with a Resnet-101 backbone, we not only outperform LXMERT <ref type="bibr" target="#b55">[55]</ref> and VL-T5 <ref type="bibr" target="#b6">[7]</ref> which use comparable amount of data, but also OSCAR <ref type="bibr" target="#b27">[28]</ref> which uses magnitude more data in their pre-training. MDETR with the EfficientNet-B5 backbone is able to push performance even higher as reported in <ref type="table" target="#tab_5">Table 5</ref>. The NSM model makes use of an external scene graph generation model, while the MMN model makes use of the scene graph and functional programs during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Few-shot transfer for long-tailed detection</head><p>Inspired by the success of CLIP <ref type="bibr" target="#b47">[47]</ref>, on zero-shot transfer for image classification, we explore the opportunity to construct a useful detector over a given label set from a pre- <ref type="figure">Figure 4</ref>: During MDETR pre-training, the model is trained to detect all objects mentioned in the question. To extend it for question answering, we provide QA specific queries in addition to the object queries as input to the transformer decoder. We use specialized heads for different question types. <ref type="figure">Figure 5</ref>: MDETR provides interpretable predictions as seen here. For the question "What is on the table?", MDETR fine-tuned on GQA predicts boxes for key words in the question, and is able to provide the correct answer as "laptop". Image from COCO val set. trained MDETR model. Unlike CLIP, we do not ensure our pre-training dataset contains a balanced representation of all the target classes. By construction, our dataset has no training instances where there are zero boxes aligned to the text, biasing the model to always predict boxes for a given text. This prevents evaluating in a true zero-shot transfer setting, so we turn instead to a few-shot setting, where the model is trained on a fraction of the available labelled data. We conduct our experiments on the LVIS dataset <ref type="bibr" target="#b10">[11]</ref>, a detection dataset with a large vocabulary of 1.2k categories, with a long-tail that contains very few training samples, making it a challenging dataset for current approaches. Federated datasets often pose problems to standard detectors, and require developing specific loss functions <ref type="bibr" target="#b56">[56]</ref>. However this  <ref type="table">Table 6</ref>: Box AP fixed results on LVIS-v1. Since the validation set of LVIS contains some training images from MSCOCO, we report results on the subset of 5k validation images that our model has never seen during training. We call this subset minival. All models use a Resnet 101 as backbone. Mask-RCNN can be regarded as a strong representative of the detection performance of current approaches on this dataset, using bells and whistles such as Repeat Factor Sampling (RFS) to address class imbalance. We use a vanilla DETR pretrained on MSCOCO as a few-shot transfer baseline, and show that our pre-training on natural text improves performance significantly, especially on rare categories.</p><p>property makes it well suited to train MDETR: for each positive category, we create a training instance composed of the image and a text version of the class name, and provide as annotations all the instances of this category. For each negative category, we provide the class name and an empty set of annotations. For inference on a given image, we query each possible class name, then merge the sets of boxes detected on each of the text prompts. This inference scheme costs about 10s/image on a GPU.</p><p>We fine-tune MDETR on three subsets of the LVIS train set, each containing respectively 1%, 10% and 100% of the images. We ensure a balanced sampling of the cate-gories, such that our 1% set contains at least one positive and one negative examples from each category. We compare to two baselines: the first one is Mask-RCNN trained exclusively on the full training set of LVIS. The other is a DETR model pre-trained on MSCOCO then fine-tuned on the various subsets of the LVIS training set. Our results are shown in <ref type="table">Table 6</ref>. Following recent recommendation <ref type="bibr" target="#b7">[8]</ref> on AP evaluation in the context of large vocabulary, we report the box AP fixed, obtained by limiting the number of detections per category instead of per image. Even with as little as 1 example per class, MDETR leverages the text pre-training and outperforms a fully fine-tuned DETR on rare categories. We note however that under full fine-tuning on the whole training set, the performance on rare objects drops significantly from 20.9 AP with 10% data to 7.5 with 100%, likely due to the extreme class imbalance. We expect that common techniques such as Repeat Factor Sampling will improve the situation in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>The CLEVR dataset <ref type="bibr" target="#b19">[20]</ref> is a popular vision-language benchmark for reasoning on objects, their relations, and the composition of such relations. A prominent line of work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15]</ref> makes use of the functional programs annotations that are part of the CLEVR dataset. Such approaches tend to dominate on the question answering benchmark, but fail to generalize beyond synthetic data. Conversely, many approaches <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b16">17]</ref> learn directly from images or pre-detected objects, with varying amounts of inductive bias tailored to the QA task. Our method can be seen as an in-between: while not explicitly using the program supervision, it is trained to detect objects that are required for performing intermediate reasoning steps.</p><p>Recent progress in multi-modal understanding has been mainly powered by pre-training large transformer models to learn generic multi-modal representations from enormous amounts of aligned image-text data <ref type="bibr" target="#b53">[53]</ref>, then finetuning them on downstream tasks. These methods can be divided into single stream <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b25">26]</ref> and two-stream <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">54]</ref> architectures depending on whether the text and images are processed by a single combined transformer or two separate transformers followed by some cross attention layers. For both these types, the prevalent approach is to extract visual and textual features independently and then use the attention mechanism of the transformers to learn an alignment between the two. While this approach has improved state of the art results on a wide variety of tasks such as image-text retrieval <ref type="bibr" target="#b73">[72]</ref>, phrase grounding <ref type="bibr" target="#b25">[26]</ref>, image captioning <ref type="bibr" target="#b27">[28]</ref> and visual question answering <ref type="bibr" target="#b24">[25]</ref>, it leaves opportunity for a more tightly knit architecture, such as MDETR, in which information flows between the two modalities at an even earlier stage of the model. Some previous attempts at achieving this using modulated architec-tures such as <ref type="bibr" target="#b42">[42]</ref> and <ref type="bibr" target="#b38">[39]</ref> show improvements on counting tasks and visual question answering.</p><p>The visual features used by the current state-of-the-art models are extracted using an external pre-trained detector <ref type="bibr" target="#b0">[1]</ref>, which outputs regions that are noisy, often oversampled and ambiguous. <ref type="bibr" target="#b27">[28]</ref> attempts to alleviate the problem of noisy image features by using tags as anchors between the text and images. This is still a weaker form of supervision than in MDETR where we have explicit alignment between words or phrases in text and the objects in the images. To alleviate the constraints implied by fixed vocabulary of concepts, <ref type="bibr" target="#b73">[72]</ref> trains on a collection of much larger object detection datasets in pursuit of better coverage. <ref type="bibr" target="#b8">[9]</ref> conduct adversarial training on top of existing high performing models pushing performance even higher. Other approaches <ref type="bibr" target="#b69">[68]</ref> attempt to incorporate scene graph prediction as part of their pre-training to learn more robust representations. Some recent work also attempts to build multipurpose multi-modal architectures that are able to tackle a variety of vision-language <ref type="bibr" target="#b6">[7]</ref> as well as pure language tasks in a single architecture <ref type="bibr" target="#b15">[16]</ref>. A separate line of work that attacks a similar problem to ours but with a much more task specialized model architectures are the single <ref type="bibr" target="#b67">[66,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref> and two stage <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b12">13]</ref> referring expression segmentation and phrase detection models which are designed specifically for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented MDETR, a fully differentiable modulated detector. We established its strong performance on multimodal understanding tasks on a variety of datasets, and demonstrated its potential in other downstream applications such as few-shot detection and visual question answering. We hope that this work opens up new opportunities to develop fully integrated multi-modal architectures, without relying on black-box object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model details and hyperparameters</head><p>Pre-training hyperparameters MDETR follows the pre-train then fine-tune strategy by first training on our constructed combined dataset for 40 epochs followed by fine-tuning on the respective downstream task. We train our model using AdamW <ref type="bibr" target="#b32">[33]</ref>, a variant of Adam <ref type="bibr" target="#b22">[23]</ref> better suited for weight decay. We use exponential moving average (EMA) with a decay rate of 0.9998, and a weight-decay of 1e ?4 . The backbone and the transformer have a constant learning rate of respectively 1e ?4 and 1e ?5 for 35 epochs, after which their learning rate is reduced by a factor of 10. For the language model's learning rate, we use a linear decay with warmup schedule, increasing linearly to 5e ?5 during the first 1% of the total number of steps, then decreasing linearly back to 0 for the rest of the training. Flickr30k Our results on Flickr30k are evaluated using the pre-trained model, without any additional fine-tuning as we found that it brings no additional gains. For evaluation, we must rank the boxes associated with each phrase. Since there might be several phrase in the same sentence, we must provide a ranking for each and every such phrase. To that end, we use the prediction from the soft-token classification loss. In the example depicted in <ref type="figure" target="#fig_3">Fig 6:</ref> to rank the boxes for the phrase "a cat", we use the probability mass that each query assigns to the positions that correspond to "a cat" in the sentence "a cat with white paws jumps over a fence in front of a yellow tree" (in this example, the first few tokens). Through this approach, the red box is found to be the highest-ranked box. On the other hand, if we want the boxes corresponding to "the fence", we sort them according to the corresponding token positions, and in this case we find the green box as the top-scoring one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Referring Expression Comprehension</head><p>For the Referring Expression Comprehension task, there is a stark difference in how the data is presented to the model in terms of density of annotation. For all other datasets that we use in our pre-training, each noun phrase in the sentence is annotated with its respective box, if available. On the other hand, in RE comprehension, the task is to align the whole referring expression with the corresponding box, possibly by needing to disambiguate between different occurrences of the same category of object. In other words, our model now needs to predict one box per expression. A problem with this setting is that our box-token contrastive alignment as well as soft token prediction losses get very diluted signal if we align the whole sentence to the box. To alleviate this, we pre-process the text using Spacy <ref type="bibr" target="#b13">[14]</ref> to extract the root of the sentence using a dependency parser. The tokens from this root phrase are used to align to the box. Fine-tuning on this dataset is therefore crucial for good performance. We fine-tune for 5 epochs on the RefCOCO, RefCOCO+ and RefCOCOg datasets with a learning rate of 1e ?5 for the backbone and 5e ?5 for the rest of the network. We use a learning rate drop by a factor of 10 after 3 epochs. For the text encoder we use a learning rate of 1e ?5 , with a linear decay with warmup schedule, warming up over the first 1% of steps and then decaying to 0 linearly. At inference time, to detect a given expression, we feed it to the model alongside the image. We then rank the 100 detected boxes according to the probability that the box corresponds to an actual object (as opposed to a "no object"). If P (?) is the probability mass assigned to the "no object" label, then we rank by increasing order of P (?), or equivalently by decreasing order of 1 ? P (?). We show an example in <ref type="figure" target="#fig_0">Fig 10 of</ref> the box predicted by our model for the corresponding referring expressions. In addition there is quite some variety in the type of text annotations from the three datasets. Both RefCOCO and RefCOCO+ were collected in a timed game setting whereas RefCOCOg was not. This led to differences in the length and diversity of language used in the different datasets. RefCOCO+ disallowed usage of location words to describe objects or disambiguate between multiple occurrences   We also fine-tuned the EfficientNetb5 model on these datasets but did not see much improvement over the EfficientNetB3 model, and we believe this is due to the smaller size of these datasets causing the larger model to overfit.   PhraseCut Detection: For this phase, we use a batch size of 64, a learning rate of 1e ?5 for the text encoder and backbone and 5e ?5 for the rest of the network, and exponential moving average (EMA) of the network weights with a decay of 0.9998. segmentation: For this stage, we use a lr of 5e ?4 and no EMA. See <ref type="bibr" target="#b1">[2]</ref> for additional details. The CLEVR dataset consists of 3D-rendered scenes containing between 3 and 10 objects of various shapes, material, size and color. Each of these scenes is associated with about 10 questions that are formulated about the visible objects, generated from a fixed set of templates. Each question is guaranteed to be answerable, and the annotations further provide a functional program that describe how to compute the answer using elementary reasoning steps. The total training set contains 70k images and slightly less than 700k questions. Overall, the visual aspect of this task, ie the scene parsing, is not really challenging by modern standards, since the set of objects is limited, unambiguous, and there are no visual distractors. The only challenging cases occur in the event of heavy occlusion, where it might be hard to make out the shape of the occluded object, or in some cases where the question requires comparing ambiguous spatial relations (eg. asking which object is the closest to the camera in a setting where they are visually nearly tied). On the other hand, the text understanding aspect is more involved, since the questions can be quite complex, involving up to 20 reasoning steps. Unlike several successful approaches to CLEVR <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b68">67]</ref> , MDETR doesn't incorporate any special inductive bias to cope with such complex reasoning tasks. In this section, we show that despite its relatively straight-forward formulation, our approach competes with state-of-the-art models on the question answering task.</p><p>The first ingredient required for training MDETR is bounding box annotations for objects in the image. The original CLEVR dataset doesn't provide any, so we use the scene graphs from the dataset to re-create the original scene in the 3Drenderer Blender, then use some of its functionalities to extract the segmentation masks of the visible parts of the objects, and deduce the bounding boxes from that. The main complication is that the original rendering involved some non-deterministic jittering of the camera's position and rotation, leading to some potential discrepancies in the computed boxes. To minimize the error, we use the known 3D position of each object, as well as their known 2D location in the rendered image to optimize the camera parameters using a gradient-based approach. The final boxes obtained using this approach are accurate within a 10-pixel error margin, which we deem appropriate for our purposes.</p><p>The second ingredient required is the alignment between bounding boxes and tokens in the question. MDETR is trained to predict only objects that are referred to in the question. For example, in the question "What is the color of the cube in front of the small cylinder?", we provide an annotation for both the small cylinder (an intermediate step) and the cube (the main subject), and none of the other objects present in the scene. We use the functional programs that are part of the original CLEVR annotations to extract this set of objects, along with their corresponding text tokens in the original question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training details</head><p>Model We use a ResNet-18 <ref type="bibr" target="#b11">[12]</ref> from Torchvision, pre-trained on ImageNet <ref type="bibr" target="#b49">[49]</ref> as the convolutional backbone. For the text-encoder, we use a pre-trained DistilRoberta <ref type="bibr" target="#b50">[50]</ref> from HuggingFace <ref type="bibr" target="#b61">[61]</ref>. The final transformer is the same as DETR, with 6 encoder layers as well as 6 decoder layers, and 8 attention heads in the attention layers. We reduce the number of object queries to 25, since the maximum number of objects to be detected is low.</p><p>Pre-training We first train the model only on the modulated detection objective, on our CLEVR-Medium subset, for 30 epochs. Following DETR training procedure, the transformer and the backbone use a learning rate of respectively 1e ?4 and  <ref type="figure" target="#fig_0">Figure 11</ref>: During MDETR pre-training, the model is trained to detect all objects mentioned in the question. To extend it for question answering, we provide QA specific queries in addition to the object queries as input to the transformer decoder. We use specialized heads for different question types.</p><p>1e ?5 , and we reduce them by a factor of 10 at the 20th epoch. The text encoder uses a linear decay with warmup schedule, with a warm-up to 5e ?5 over the first 1% of the training steps.</p><p>QA-finetuning For question answering, we take our pre-trained checkpoint, add the (untrained) question queries and their corresponding heads, then train on the full CLEVR dataset for 30 epochs, following the exact same learning rate schedule, with both the modulated detection as well as question answering losses. As depicted in <ref type="figure" target="#fig_0">Fig.12</ref>, we use additional queries in the transformer decoder to answer each type of question in CLEVR: numerical, binary and attributes. We supervise each of these heads using a standard cross-entropy loss. We monitor the accuracy on the validation set to apply early-stopping. Finally, for CLEVR-Humans, we further fine-tune for 60 epochs, with the learning-rate drop occuring at epoch 40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Results and discussion</head><p>The results are collected in <ref type="table" target="#tab_7">Table 7</ref>. On CLEVR, we closely match the performance of NS-VQA <ref type="bibr" target="#b68">[67]</ref>, a method that uses external supervision in the form of ground-truth program annotations, and clearly surpass the performance of methods which like us, don't use this extra supervision signal. We then evaluate the generalization capability of our model. <ref type="bibr" target="#b20">[21]</ref> is a dataset of human-generated questions on CLEVR images. It tests the robustness of the model to new vocabulary and and different reasoning primitives. In a zero-shot setting, we improve substantially over the best competing model. We credit this improvement to our pre-trained language model. After fine-tuning, the gap narrows, suggesting that additional developments may be required to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEVR-Humans</head><p>CoGenT is a test for compositional generalization. The evaluation protocol consists in training on a set A, where the spheres can be any color but the cubes are either gray, blue, brown or yellow, and the cylinders are red, green, purple or cyan. We then evaluate in a zero-shot manner on a split B which has the opposite color-shape pairs for cubes and cylinders. Similar to other models, we observe a significant generalization gap. On closer inspection, the biggest drop in accuracy occurs on questions querying the shape of an object (from 99.98% on testA to 34.68% on testB), suggesting that the model has learnt strong spurious biases between shape and color.</p><p>CLEVR-REF+ Finally, we evaluate our model on CLEVR-REF+ <ref type="bibr" target="#b30">[31]</ref>, a referring expression comprehension dataset built on CLEVR images. For each object query, we train an additional binary head to predict whether or not the query corresponds to an object being referred to (as opposed to an auxiliary object in the sentence, that we detect as well). Following <ref type="bibr" target="#b30">[31]</ref>, we evaluate accuracy on the subset of expressions that refer to a unique object, measured as whether the top ranked box has an IoU of at least 0.5 with the target box. Using the aforementioned binary prediction to rank the boxes, our model correctly ranks in first position a valid box for each of the examples of the validation set, leading to an accuracy of 100%, greatly outperforming prior work.  <ref type="table">Table 8</ref>: Ablation results on modulated detection on CLEVR-Medium. We report the class-agnostic AP. See text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Ablations</head><p>We use CLEVR as a test bed to ablate several aspects of our model. Depending on the ablation, we report either the accuracy on the question answering task on the validation set of CLEVR, and/or the detection performance on this dataset, measured as a class-agnostic Average Precision (AP). When inspecting the modulated detection capabilities of the model, we use class agnostic Average Precision (AP) to evaluate the model. In the unconditional detection case, DETR is able to detect all boxes perfectly. When evaluated on the task of modulated detection, the AP metric therefore captures the model's capability for text understanding since now only the boxes relevant to the query must be detected. To put this in context, a model that detects all boxes even when given a text query (thereby ignoring the text completely) gets an AP of around 60. The goal is to achieve an AP close to 100 which would imply the model only finds the relevant boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Loss ablations</head><p>We first ablate the various parts of our loss. The results are summarized in <ref type="table">Table 8</ref> We report modulated detection results on the CLEVR-Medium subset, that we constructed by removing data-points from CLEVR where the same object is referred to by distinct parts of the question. In these ablations, we consider only the performance of the detector, and not the question answering capability. As a result, the queries related to question answering are not present and we do not propagate any QA related loss.</p><p>Contrastive loss We first ablate the impact of the contrastive loss, by training a model without it. In this situation, the alignment must occur solely through the soft-token classification loss. As shown in <ref type="table">Table 8</ref>, removing this loss results in a drastic drop in AP. More specifically, when evaluating the model, it becomes apparent that it is able to filter the objects based on some attributes (in particular their shape and size) but not others (in particular color and texture). It is unclear what drives the model in this local sub-optima, nor what statistical shortcut it is leveraging to correctly identify shapes and sizes. However, it shows that solely predicting the spans of the text query associated with each object is not sufficient to learn proper alignment. The contrastive loss, which forces object-queries to be similar to their corresponding text-token, is thus necessary.</p><p>Soft-token classification loss We now study whether predicting the text-spans associated with each object is necessary, provided that we propagate the contrastive loss. Instead of predicting a distribution over span, we construct a simplified version of MDETR which only predicts a binary label for each object query: "object" or "no-object" (?). This formulation is equivalent to the vanilla DETR classification loss, with one object class. We observe similar results as the previous ablation, namely a sharp decline in AP and a model that only understands half of the attributes correctly. We thus conclude that both ingredients of our loss are indeed required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Question answering ablations</head><p>Finally in <ref type="table">Table 9</ref> we ablate two aspects of our training recipe that differ with previous approaches:</p><p>? Curriculum: We evaluate a model trained directly on the full CLEVR training set, without our modulated detection pretraining on CLEVR-medium. Similarly as the previous section, the model learns to detect only a subset of attributes, leading to poor QA accuracy.</p><p>? Single QA head In our approach, we train a specialized head for each type of question (numerical, binary, or categorical over the attributes). This differs from previous approaches that usually cast it as a single classification over all possible answers. As shown in <ref type="table">Table 9</ref>, this separation has a big impact on the final accuracy. We hypothesize that it enables the attention pattern for each question type to specialize accordingly to the task, there-by yielding better performance.  <ref type="table">Table 9</ref>: Ablation results on the validation set of CLEVR. We report the class-agnostic AP and the question answering accuracy. See text for details.</p><p>(a) Current object detection pipeline outputs, predicting all possible objects in the image. This extensive annotation is essential to multi-modal understanding systems that treat detection as a black box.</p><p>(b) MDETR predicts boxes relevant to the caption and labels them with the corresponding spans from the text. Here we use the caption: "blond boy wearing blue shorts. a black surf-board." <ref type="figure" target="#fig_0">Figure 12</ref>: Modulated detection using MDETR vs detection output for a current state-of-the-art multi-modal understanding system. Image taken from <ref type="bibr" target="#b73">[72]</ref> C. Dataset constructions MS COCO On the COCO dataset, we include annotations from the referring expressions datasets (RefCOCO <ref type="bibr" target="#b71">[70]</ref>, Ref-COCO+ <ref type="bibr" target="#b71">[70]</ref> and RefCOCOg <ref type="bibr" target="#b35">[36]</ref> datasets). By construction, in this dataset, each referring expression is a whole sentence that describes one object in the image, where the constituent noun phrases from the sentences are not themselves annotated. For example, in <ref type="figure" target="#fig_2">Figure 3</ref>, the caption would be "the person in the grey shirt with a watch on their wrist.", where only the person would be annotated and not the grey shirt or their watch. To avoid ambiguity, we perform some text pre-processing using SpaCy <ref type="bibr" target="#b13">[14]</ref> to extract the root of the referring expression. This is used in our soft token prediction as well as the contrastive alignment loss for aligning to the referred box. The auxiliary objects (in this example the shirt and the watch) are ignored altogether.</p><p>Visual-Genome We use annotations from VG regions, a dataset having diverse descriptions of a wide variety of objects, often having a very high degree of descriptive detail and covering several concepts. By construction, the VG dataset comprises a lot of redundant annotations. We detect redundant sentences by normalizing them (removing all punctuation, stop-words, and lower-casing), then testing for equality. Once we found a pair of equivalent sentences, two cases arise:</p><p>? The corresponding boxes are highly overlapping (IoU &gt; 0.7). In this case, we consider both annotations to be redundant, and we keep only one of them.</p><p>? The boxes are non-overlapping. The most likely explanation is that the sentence is under-specified and actually corresponds to several distinct objects in the image. In this case, we merge the two data-points together, and the resulting annotations comports two boxes for this sentence.</p><p>We iterate recursively this process until no equivalent sentences remain. In some cases, the VG annotations provide information about the object referred to in the sentence. For example, if the region is tagged "the cat on the white table", in some cases the individual boxes for the cat and the table are available. In this case, we discard the region box and use the individual boxes instead. We note that despite our merging strategy, it may remain some region description that do not canonically refer to a unique object in the image, but for which we don't have ground-truth annotations for the other objects that also match the said region description. Despite the noise it introduces in the training process, we don't pursue extra efforts to try and fix these situations. In addition, we also use questions from the GQA dataset <ref type="bibr" target="#b17">[18]</ref>, where bounding box annotations are provided for key phrases in the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluating grounded detection</head><p>The main evaluation metric proposed to evaluate grounded detection in datasets like Flickr30K entities <ref type="bibr" target="#b46">[46]</ref> is Recall@k, that is measuring whether a given model is able to rank the "correct" box amongst the top k it produces. The correctness of a box is decided by computing the Intersection-over-Union (IoU) between the proposed box and the ground-truth box, and deemed correct if the IoU is above a predetermined threshold, generally 0.5. While this kind of evaluation is well-suited for tasks where there is a clear one-to-one mapping between phrases and boxes, for example in Referring Expression Comprehension tasks, we argue that in general grounded detection tasks, they fall short of properly evaluating the performance of the models. Specifically, they run into the following issues:</p><p>1. Multiple boxes for a given phrase: Since the recall@k metric implies a single box per phrase, it is not clear how to extend it to situations where a given phrase refers to several distinct objects in the image . In the absence of clear guidelines, various authors have adopted divergent approaches to deal with that. Specifically, some <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b21">22]</ref> consider the predicted box to be correct if it has an IoU&gt; 0.5 with any of the ground-truth boxes. We refer to this protocol as the ANY-Protocol. Others <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b67">66]</ref> first merge all the ground-truth boxes associated to the phrase by considering the smallest enclosing box. Then they proceed to compute the IoU as usual, using this merged box as the ground-truth one. We refer to this protocol as the MERGED-BOXES-Protocol.</p><p>Arguably, both methods have drawbacks: the first one keeps the atomicity of each instance but fails to evaluate whether the model found all the referred instances. The second one looses the fine-grained instance in favor of a box that may be unreasonably bloated if the instances are spread apart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Multiple phrases for a given box. In some cases, the same object is referred to multiple times in the sentence. Sometimes, the corresponding phrases are exact duplicates of each other, or close synonyms (eg "a guy" and "a man"), but sometimes the co-references are more subtle. One such example include referring alternatively to a group (eg "a couple") or to a sub-constituent (eg "the woman").</p><p>Under the current evaluation protocol, each phrase is evaluated independently, even if they refer to the same object. As such, it does not test the model's understanding of co-references, which is arguably an important aspect of learning grounded representations.</p><p>Recognising the discrepancy in the evaluation procedures in the literature and the difficulties it creates in comparing various approaches, we also evaluate MDETR under the merged-box protocol, as described in Sect D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Evaluation under the MERGED-BOXES-Protocol</head><p>In MDETR, the dataset creation process operates under the assumption that a phrase is associated to all the individual boxes that correspond to it. As a result, MDETR does not naturally predict the merged-boxes that would be required by the MERGED-BOXES-Protocol. For that reason, it is necessary to fine-tune the model on a version of Flickr30k entities where the boxes have been merged appropriately. We note that the ANY-Protocol did not require such fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Error Analysis</head><p>To better understand the failures of the model on the grounding task, we provide a small-scale error analysis. We evaluate our best model, the EfficientNetB5 variant, on the validation set of Flickr30k. We manually inspect the first 100 errors made by the model. The results are summarized in <ref type="figure" target="#fig_0">Fig 14 and</ref> we detail the types of errors we uncovered in the following:</p><p>? Issues with the ground truth annotations Ambiguous GT location This corresponds to phrases that don't have a canonical localization. They mostly correspond to scene elements, such as "beach" or "tree".</p><p>Inconsistent annotations (when several objects are referred) In the ANY-Protocol, if a phrase corresponds to several objects, then in principle every single instance should be individually annotated. However, we find some inconsistencies in the annotations, for example some instances that are missed, or some distinct instances that are annotated using the same box.</p><p>Imprecise GT box This corresponds to cases where the provided box is not adequate. It is either too big (not tight), or too small, cutting out a part of the object.</p><p>Wrong GT In those cases, the annotated box(es) don't correspond to the correct referred object at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Grounding mistakes</head><p>Wrong instance The model picks an instance of the correct type (including adjective modifiers, if any), however it is not the correct instance when taking into account context from the rest of the sentence.</p><p>Wrong object The model picks the wrong object, and it is not of the correct type. This usually occurs on long-tail concepts that the model doesn't seem to know about.</p><p>OCR The phrase refers to written text, thus requiring OCR abilities from the model, which it doesn't have.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Detection issues</head><p>Imprecise box prediction The model is clearly selecting the right object, however the predicted box isn't quite precise enough. This happens on small objects as well as elongated objects, where a relatively small L1 error can cause a low IoU with the ground truth.</p><p>Overall, we find that on the analyzed subset, more than half the errors stem from issues in the ground-truth annotations. Extrapolating to the rest of the dataset, this would imply a label noise of around 10%, which might be detrimental to make significant further progress on the task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiments on VQA2</head><p>In our visual question answering experiments on the GQA dataset, we always had access to bounding box information for the questions. Only during fine-tuning on the balanced set for 10 epochs, we do not supervise the detection losses. On the other hand, for datasets such as VQA2 <ref type="bibr" target="#b9">[10]</ref>, we do not have access to any box annotations and the supervision comes solely from the question answering loss. We fine-tune two of our models -pre-trained on the joint dataset, as well as the model fine-tuned on the all-split of GQA -on the VQA v2 dataset for 25 epochs. The results are reported in <ref type="table" target="#tab_11">Table 10</ref>. While these results are not state-of-the-art on the VQA2 benchmark, they are still quite reasonable with respect to current literature. This show that our method can be extended to tasks where we do not have the dense supervision (in the form of bounding boxes and their alignment to the text) that we otherwise expect in tasks reported in this paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Output of MDETR for the query "A pink elephant". The colors are not segmentation masks but the real colors of the pixels. The model has never seen a pink nor a blue elephant in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"Figure 2 :</head><label>2</label><figDesc>A cat with white paws jumps over a fence in front of a yellow tree" white paws jumps over a fence in front of a yellow tree ? MDETR uses a convolutional backbone to extract visual features, and a language model such as RoBERTa to extract text features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Our combination of annotations results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the soft-token classification loss. For each object, the model predicts a distribution over the token positions in the input sequence. The weight of the distribution should be equally spread over all the tokens that refer to the predicted box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>(a) "one small boy climbing a pole with the help of another boy on the ground" (b) "A man talking on his cellphone next to a jewelry store" (c) "A man in a white t-shirt does a trick with a bronze colored yo-yo" Examples of phrase grounding on the Flickr30k dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>Query: "street lamp" (b) Query: "major league logo" (c) Query: "zebras on savanna"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative segmentation examples on the phrasecut dataset of the same object, focusing more on appearance based descriptions. RefCOCOg consists of expressions more than twice the length (on average) of the others and with more flowery and descriptive language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>Query: "Any other things that are the same color as the partially visible thing(s)" (b) Query: "Any other things that are the same material as the first one of the object(s) from right" (c) Query: "The second one of the shiny object(s) from front that are to the left of the second one of the metallic thing(s) from front"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Qualitative example from the CLEVR-REF+ dataset. When the model predicts a box that is referred to, we display it in green. The other boxes are intermediate reasoning steps and are depicted in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>(a) "brown bear" (b) "zebra facing away" (c) "the man in the red shirt carrying baseball bats" (d) "the front most cow to the right of the other cows" Examples from RefCOCO, RefCOCO+ and RefCOCOg datasets. Fig(a) taken from RefCOCO, Fig(b) from RefCOCO+ and Fig(c) and (d) are taken from RefCOCOg, in which the expressionsare much longer on average and contain more descriptive language than in RefCOCO and RefCOCO+. Even when the expressions are long, we train our model to align the box to the root of the phrase, for eg. "the man" in (c). The model however, still has access to the whole text and uses it to disambiguate between the two men in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( a )Figure 13 :</head><label>a13</label><figDesc>Text prompt: "A pink elephant." (b) Text prompt: "A blue elephant." (c) Text prompt: "A normal elephant." (d) Text prompt: "A pink elephant. A blue elephant. A normal elephant" Qualitative results on unseen attributes combinations. While the model correctly singles out the pink elephant (a), it incorrectly includes the pink elephant when prompted about the blue one (b). In (c), we show that the model does not understand what a "normal elephant" looks like. However, in (d), when prompted about all three elephants at once, it is able to assign the correct label to each of them, by process of elimination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Break down of the first 100 errors from the EfficientNetB5 model on the Flickr30k validation set. Yellow shade corresponds to mistakes in the ground truth annotations. Blue shade corresponds to errors made by the model in grounding. Green shade corresponds to errors made by the model in accurate localization. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>81.14 69.99 65.33 71.62 56.02 66.58 67.27 R101 CC, SBU, COCO, VG (4.6M) 81.41 87.04 74.17 75.90 81.45 66.70 74.86 75.77 VILLA L[9] * R101 CC, SBU, COCO, VG (4.6M) 82.39 87.48 74.84 76.17 81.54 66.84 76.18 76.71 Flickr30k (200k) 86.75 89.58 81.41 79.52 84.09 70.62 81.64 80.89 MDETR ENB3 COCO, VG, Flickr30k (200k) 87.51 90.40 82.67 81.13 85.52 72.96 83.35 83.31</figDesc><table><row><cell>Method</cell><cell>Detection</cell><cell>Pre-training</cell><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell>RefCOCO+</cell><cell cols="2">RefCOCOg</cell></row><row><cell></cell><cell>backbone</cell><cell>image data</cell><cell>val</cell><cell cols="2">testA testB</cell><cell>val</cell><cell>testA testB</cell><cell>val</cell><cell>test</cell></row><row><cell cols="4">MAttNet[69] 76.65 ViLBERT[34] R101 None R101 CC (3.3M) -</cell><cell>-</cell><cell>-</cell><cell cols="2">72.34 78.52 62.61</cell><cell>-</cell><cell>-</cell></row><row><cell>VL-BERT L [54]</cell><cell>R101</cell><cell>CC (3.3M)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">72.59 78.57 62.30</cell><cell>-</cell><cell>-</cell></row><row><cell>UNITER L[6]  ERNIE-ViL L[68]</cell><cell>R101</cell><cell>CC, SBU (4.3M)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">75.95 82.07 66.88</cell><cell>-</cell><cell>-</cell></row><row><cell>MDETR</cell><cell>R101</cell><cell>COCO, VG,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>R101 ? * 82.5 92.9 94.9 83.4 93.5 95.3 MDETR-ENB3 ? * 82.9 93.2 95.2 84.0 93.8 95.6 MDETR-ENB5 ? * 83.6 93.4 95.1 84.3 93.9 95.8</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">69.7 84.2 86.4</cell></row><row><cell>VisualBert[26]</cell><cell cols="3">68.1 84.0 86.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">VisualBert ?[26] 70.4 84.5 86.3 71.3 85.0 86.5</cell></row><row><cell>MDETR-R101</cell><cell cols="3">78.9 88.8 90.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">MDETR-MERGED-BOXES-PROTOCOL</cell><cell></cell></row><row><cell>CITE [43]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.9</cell><cell>-</cell><cell>-</cell></row><row><cell>FAOG [66]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.7</cell><cell>-</cell><cell>-</cell></row><row><cell>SimNet-CCA [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DDPN [71]</cell><cell>72.8</cell><cell>-</cell><cell>-</cell><cell>73.5</cell><cell>-</cell><cell>-</cell></row><row><cell>MDETR-R101</cell><cell cols="3">79.0 86.7 88.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">MDETR-R101 ? *  82.3 91.8 93.7 83.8 92.7 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Visual question answering on the GQA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results on CLEVR-based datasets. We report accuracies on the test set of CLEVR, including the detail by question type. On CLEVR-Humans, we report accuracy on the test set before and after fine-tuning. On CoGenT, we report performance when the model is trained in condition A, without finetuning on condition B. On CLEVR-Ref+, we report the accuracy on the subset where the referred object is unique. *indicates method uses external program annotations</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>CLEVR</cell><cell></cell><cell></cell><cell cols="2">CLEVR-Humans</cell><cell cols="2">CoGenT</cell><cell>CLEVR-Ref+</cell></row><row><cell></cell><cell cols="10">Overall Count Exist Comp. Num Query Comp. Att Before FT After FT TestA TestB</cell><cell>Acc</cell></row><row><cell>MAttNet[69]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.9</cell></row><row><cell>MGA-Net[73]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.1</cell></row><row><cell>FiLM[42]</cell><cell>97.7</cell><cell cols="2">94.3 99.1</cell><cell>96.8</cell><cell>99.1</cell><cell>99.1</cell><cell>56.6</cell><cell>75.9</cell><cell cols="2">98.3 78.8</cell><cell>-</cell></row><row><cell>MAC [17]</cell><cell>98.9</cell><cell cols="2">97.1 99.5</cell><cell>99.1</cell><cell>99.5</cell><cell>99.5</cell><cell>57.4</cell><cell>81.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NS-VQA[67]  *</cell><cell>99.8</cell><cell cols="2">99.7 99.9</cell><cell>99.8</cell><cell>99.8</cell><cell>99.8</cell><cell>-</cell><cell>67.8</cell><cell cols="2">99.8 63.9</cell><cell>-</cell></row><row><cell>OCCAM [60]</cell><cell>99.4</cell><cell cols="2">98.1 99.8</cell><cell>99.0</cell><cell>99.9</cell><cell>99.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MDETR</cell><cell>99.7</cell><cell cols="2">99.3 99.9</cell><cell>99.4</cell><cell>99.9</cell><cell>99.9</cell><cell>59.9</cell><cell>81.7</cell><cell cols="2">99.8 76.7</cell><cell>100</cell></row><row><cell cols="3">B. CLEVR Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>B.1. Dataset details</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>VQA v2 results</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Kyunghyun Cho, Ethan Perez, Sergey Zagoruyko and Francisco Massa for helpful discussions and feedback at various points of this project. We would also like to thank Alex Kirillov and Ross Girshick for their help with the LVIS evaluations, Justin Johnson for test set evaluation on CLEVR, Bryan Plummer for discussions on best evaluation practices for phrase grounding and finally Runtao Liu and Chenxi Liu for their feedback on dataset construction and evaluation for CLEVR referring expressions.</p><p>Aishwarya Kamath was supported in part by AFOSR award FA9550-19-1-0343 and Nicolas Carion by a grant from NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="7453" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="7453" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta module network for compositional visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Uniter: Learning universal image-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unifying visionand-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno>abs/2102.02779</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno>abs/2102.01066</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale adversarial training for visionand-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>De</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>cember 6-12, 2020, virtual, 2020. 5, 8</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>2019. 7</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative learning of open-vocabulary object retrieval and localization by negative phrase augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2605" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sofie</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriane</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unit: Multimodal multitask learning with a unified transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno>2019. 20</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="5901" to="5914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<editor>Nicol? Cesa-Bianchi, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>Samy Bengio,</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations. ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>abs/1602.07332</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semvlp: Vision-language pretraining by aligning semantics at multiple levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language. ArXiv preprint, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1908" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Can</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5745" to="5753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clevr-ref+: Diagnosing visual reasoning with referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>2019. 17</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4185" to="4194" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10434" to="10443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transparency by design: Closing the gap between performance and interpretability in visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mascharka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Duy-Kien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11883</idno>
		<idno>arXiv: 2004.11883</idno>
		<title level="m">MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<editor>John Shawe-Taylor, Richard S. Zemel, Peter L</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held 12-14 December 2011</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conditional image-text embedding networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paige</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kordas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="249" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="1946" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting image-language networks for open-ended phrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bryan Allen Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ArXiv preprint, abs/2103.00020, 2021. 3, 6</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2020. 5, 8</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11659" to="11668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientdet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Interpretable Visual Reasoning via Induced Symbolic Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<pubPlace>Sam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Online, 2020. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Phrasecut: Language-based image segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10213" to="10222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Object-Centric Diagnosis of Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno>abs/2012.11587</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A fast and accurate onestage approach to visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural-symbolic VQA: disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing</title>
		<meeting><address><addrLine>Nicol? Cesa-Bianchi, and Roman Garnett; NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Samy Bengio,</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ArXiv preprint, abs/2006.16934, 2020. 5, 8</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Modeling Context in Referring Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00272</idno>
		<idno>arXiv: 1608.00272 version: 3. 5</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Rethinking diversified and discriminative proposal generation for visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1114" to="1120" />
		</imprint>
	</monogr>
	<note>J?r?me Lang</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Vinvl: Making visual representations matter in visionlanguage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2101.00529, 2021. 1, 6, 8</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Modular graph attention network for complex visual relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiquan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
