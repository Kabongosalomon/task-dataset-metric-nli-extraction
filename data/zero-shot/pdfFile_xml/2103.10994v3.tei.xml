<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Classification Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Classification Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Self-Supervised Classification, Representation Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Self-Classifier -a novel self-supervised end-toend classification learning approach. Self-Classifier learns labels and representations simultaneously in a single-stage end-to-end manner by optimizing for same-class prediction of two augmented views of the same sample. To guarantee non-degenerate solutions (i.e., solutions where all labels are assigned to the same class) we propose a mathematically motivated variant of the cross-entropy loss that has a uniform prior asserted on the predicted labels. In our theoretical analysis, we prove that degenerate solutions are not in the set of optimal solutions of our approach. Self-Classifier is simple to implement and scalable. Unlike other popular unsupervised classification and contrastive representation learning approaches, it does not require any form of pre-training, expectationmaximization, pseudo-labeling, external clustering, a second network, stop-gradient operation, or negative pairs. Despite its simplicity, our approach sets a new state of the art for unsupervised classification of Ima-geNet; and even achieves comparable to state-of-the-art results for unsupervised representation learning. Code is available at https://github. com/elad-amrani/self-classifier.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-supervised visual representation learning has gained increasing interest over the past few years <ref type="bibr">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">2,</ref><ref type="bibr">23]</ref>. The main idea is to define and solve a pretext task such that semantically meaningful representations can be learned without any human-annotated labels. The learned representations are later transferred to downstream tasks, e.g., by fine-tuning on a smaller dataset. Current state-of-the-art self-supervised models are based on contrastive learning (Sec. 2.1). These models maximize the similarity between two different augmentations of the same image while simultaneously minimizing the similarity between different images, subject to different conditions. Although they attain impressive overall performance, for some downstream tasks, such as unsupervised classification (Sec. 6.1), the objective of the various proposed pretext tasks might not be sufficiently well aligned. For example, instance discrimination methods, such as <ref type="bibr">[18,</ref><ref type="bibr" target="#b6">7]</ref> used for pre-training in the current state-of-the-art unsupervised classification method <ref type="bibr">[28]</ref>, decrease similarity between all instances, even <ref type="figure" target="#fig_0">Fig. 1</ref>: Self-Classifier architecture. Two augmented views of the same image are processed by a shared network comprised of a backbone (e.g. CNN) and a classifier (e.g. projection MLP + linear classification head). The cross-entropy of the two views is minimized to promote same class prediction while avoiding degenerate solutions by asserting a uniform prior on class predictions. The resulting model learns representations and discovers the underlying classes in a single-stage end-to-end unsupervised manner.</p><p>between those that belong to the same (unknown during training) class, thus potentially working against the set task. In contrast, in this paper we propose a classification-based pretext task whose objective is directly aligned with the end goal in this case. Knowing only the number of classes C we learn an unsupervised classifier (Self-Classifier ) such that two different augmentations of the same image are classified similarly. In practice, such a task is prone to degenerate solutions, where all samples are assigned to the same class. To avoid them, we assert a uniform prior on the standard cross-entropy loss function, such that a solution with an equipartition of the data is an optimal solution. In fact, we show that the set of optimal solutions no longer includes degenerate ones.</p><p>Our approach can also be viewed as a form of deep unsupervised clustering (Section 2.2) <ref type="bibr">[30,</ref><ref type="bibr">31,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">17,</ref><ref type="bibr">20,</ref><ref type="bibr">28,</ref><ref type="bibr">32]</ref> combined with contrastive learning. Similarly to deep clustering methods, we learn the parameters of a neural network and cluster (class) assignments simultaneously. Recently, clustering has been combined with contrastive learning in <ref type="bibr">[32,</ref><ref type="bibr">2]</ref> with great success, yet in both studies clustering was employed as a separate step used for pseudo-labeling. In contrast, in this work we learn representations and cluster labels in a single-stage end-to-end manner, using only minibatch SGD.</p><p>The key contributions of this paper are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A simple yet effective self-supervised single-stage end-to-end classification and representation learning approach. Unlike previous unsupervised classification works, our approach does not require any form of pre-training, expectation-maximization algorithm, pseudo-labeling, or external clustering. Unlike previous unsupervised representation learning works, our approach does not require a memory bank, a second network (momentum), external clustering, stop-gradient operation, or negative pairs.</p><p>2. Although simple, our approach sets a new state of the art for unsupervised classification on ImageNet with 41.1% top-1 accuracy, achieves results comparable to state of the art for unsupervised representation learning, and attains a significant (? 2% AP) improvement in transfer to COCO det/seg compared to other self. sup. methods. 3. We are the first to provide quantitative analysis of self-supervised classification predictions alignment to a set of different class hierarchies (defined on ImageNet and its subpopulations), and show significant (up to 3.4% AMI) improvement over previous state of the art in this new metric.</p><p>2 Related Work 2.1 Self-Supervised Learning</p><p>Self-supervised learning methods learn compact semantic data representations by defining and solving a pretext task. In such tasks, naturally existing supervision signals are utilized for training. Many pretext tasks were proposed in recent years in the domain of computer vision, including colorization <ref type="bibr">[35]</ref>, jigsaw puzzle <ref type="bibr">[24]</ref>, image inpainting <ref type="bibr">[25]</ref>, context prediction <ref type="bibr" target="#b8">[9]</ref>, rotation prediction <ref type="bibr" target="#b13">[14]</ref>, and contrastive learning <ref type="bibr">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">2,</ref><ref type="bibr">23]</ref> just to mention a few. Contrastive learning has shown great promise and has become a de facto standard for self-supervised learning. Two of the earliest studies of contrastive learning are Exemplar CNN <ref type="bibr" target="#b9">[10]</ref>, and Non-Parametric Instance Discrimination (NPID) <ref type="bibr">[29]</ref>. Exemplar CNN <ref type="bibr" target="#b9">[10]</ref>, learns to discriminate between instances using a convolutional neural network classifier, where each class represents a single instance and its augmentations. While highly simple and effective, it does not scale to arbitrarily large amounts of unlabeled data since it requires a classification layer (softmax) the size of the dataset. NPID <ref type="bibr">[29]</ref> tackles this problem by approximating the full softmax distribution with noise-contrastive estimation (NCE) <ref type="bibr">[16]</ref> and utilizing a memory bank to store the recent representation of each instance to avoid computing the representations of the entire dataset at each time step of the learning process. Such approximation is effective since, unlike Exemplar CNN, it allows training with large amounts of unlabeled data. However, the proposed memory bank by NPID introduces a new problem -lack of consistency across representations stored in the memory bank, i.e., the representations of different samples in the memory bank are computed at multiple different time steps. Nonetheless, Exemplar CNN and NPID have inspired a line of studies of contrastive learning <ref type="bibr">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">21,</ref><ref type="bibr">2]</ref>.</p><p>One such recent study is SwAV <ref type="bibr">[2]</ref> which resembles the present work the most. SwAV takes advantage of contrastive methods without requiring to compute pairwise comparisons. More specifically, it simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or "views") of the same image, instead of comparing features directly. To avoid a trivial solution where all samples collapse into a single cluster, SwAV alternates between representation learning using back propagation, and a separate clustering step using the Sinkhorn-Knopp algorithm. In contrast to SwAV, in this work we propose a model that allows learning both representations and cluster assignments in a single-stage end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Unsupervised Clustering</head><p>Deep unsupervised clustering methods simultaneously learn the parameters of a neural network and the cluster assignments of the resulting features using unlabeled data <ref type="bibr">[30,</ref><ref type="bibr">31,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">17,</ref><ref type="bibr">20,</ref><ref type="bibr">28,</ref><ref type="bibr">32]</ref>. Such a task is understandably vulnerable to degenerate solutions, where all samples are assigned to a single cluster. Many different solutions that were proposed to avoid the trivial outcome are based on one or few of the following: a) pre-training mechanism; b) Expectation-Maximization (EM) algorithm (i.e., alternating between representation learning and cluster assignment); c) pseudo-labeling; and d) external clustering algorithm such as k-means.</p><p>Two of the earliest studies of deep clustering are DEC [30] and JULE <ref type="bibr">[31]</ref>. <ref type="bibr">DEC [30]</ref> initializes the parameters of its network using a deep autoencoder, and its cluster centroids using standard k-means clustering in the feature space. It then uses a form of EM algorithm, where it iterates between pseudo-labeling and learning from its own high confidence predictions. JULE <ref type="bibr">[30]</ref>, similarly to DEC, alternates between pseudo-labeling and learning from its own predictions. However, unlike DEC, JULE avoids a pre-training step and instead utilizes the prior on the input signal given by a randomly initialized ConvNet together with agglomerative clustering.</p><p>More recent approaches are SeLa <ref type="bibr">[32]</ref> and IIC <ref type="bibr">[20]</ref>. SeLa [32] uses a form of EM algorithm, where it iterates between minimization of the cross entropy loss and pseudo-labeling by solving efficiently an instance of the optimal transport problem using the Sinkhorn-Knopp algorithm. IIC [20] is a single-stage end-toend deep clustering model conceptually similar to the approach presented in this paper. IIC maximizes the mutual information between predictions of two augmented views of the same sample. The two entropy terms constituting mutual information -the entropy of a sample and its negative conditional entropy given the other sample compete with each other, with the entropy being maximal when the labels are uniformly distributed over the clusters, and the negative conditional entropy being maximal for sharp one-hot instance assignments.</p><p>In this work, we follow a similar rationale for single-stage end-to-end classification without the use of any pseudo-labeling. Unlike IIC, our proposed loss is equivalent to the cross-entropy classification loss under a uniform label prior that guarantees non-degenerate, uniformly distributed optimal solution as explained in Sec. 3. Although many deep clustering approaches were proposed over the years, only two of them <ref type="bibr">(SCAN [28]</ref> and SeLa <ref type="bibr">[32]</ref>) have demonstrated scalability to large-scale datasets such as ImageNet. In fact, the task of unsupervised classification of large-scale datasets remains an open challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Self-Classifier</head><p>Let x 1 , x 2 denote two different augmented views of the same image sample x. Our goal is to learn a classifier y ? f (x i ) ? [C], where C is the given number of classes, such that two augmented views of the same sample are classified similarly, while avoiding degenerate solutions. A naive approach to this would be minimizing the following cross-entropy loss:</p><formula xml:id="formula_0">?(x 1 , x 2 ) = ? y?[C] p(y|x 2 ) log p(y|x 1 ),<label>(1)</label></formula><p>where p(y|x) is a row softmax with temperature ? row [29] of the matrix of logits S produced by our model (backbone + classifier) for all classes (columns) and batch samples (rows). However, without additional regularization, an attempt to minimize (1) will quickly converge to a degenerate solution in which the network predicts a constant y regardless of the x. In order to remedy this, we propose to invoke Bayes and total probability laws, obtaining:</p><formula xml:id="formula_1">p(y|x 2 ) = p(y)p(x 2 |y) p(x 2 ) = p(y)p(x 2 |y) ??[C] p(x 2 |?)p(?) ,<label>(2)</label></formula><formula xml:id="formula_2">p(y|x 1 ) = p(y)p(y|x 1 ) p(y) = p(y)p(y|x 1 ) x1?B1 p(y|x 1 )p(x 1 ) ,<label>(3)</label></formula><p>where B is a batch of N samples (B 1 are the first augmentations of samples of B), and p(x|y) is a column softmax of the aforementioned matrix of logits S with the temperature ? col . Now, assuming that p(x 1 ) is uniform (under the reasonable assumption that the training samples are equi-probable), and, since we would like all classes to be used, assuming (an intuitive) uniform prior for p(y), we obtain:</p><formula xml:id="formula_3">?(x1, x2) = ? y?[C] p(x2|y) ? p(x2|?) log N C p(y|x1) x 1 p(y|x1) ,<label>(4)</label></formula><p>where p(y) and p(?) cancel out in <ref type="formula" target="#formula_1">(2)</ref>, and p(y)/p(x 1 ) becomes N/C in <ref type="bibr">(3)</ref>. In practice, we use a symmetric variant of this loss (that we empirically noticed to be better):</p><formula xml:id="formula_4">L = 1 2 ?(x 1 , x 2 ) + ?(x 2 , x 1 ) .<label>(5)</label></formula><p>Note that the naive cross entropy in (1) is in fact mathematically equivalent to our proposed loss function in <ref type="bibr">(4)</ref>, under the assumption that p(y) and p(x) are uniform. Finally, despite being very simple (only few lines of PyTorch-like pseudocode in Algorithm 1) our method sets a new state of the art in selfsupervised classification (Sec. 6.1).</p><p>Algorithm 1 Self-Classifier PyTorch-like Pseudocode # N : number of samples in batch # C : number of classes # t_r / t_c : row / column softmax temperatures # aug (): random augmentations # softmaxX (): softmax over dimension X # normX (): L1 normalization over dimension X for x in loader : s1 , s2 = model ( aug ( x )) , model ( aug ( x )) log_y_x1 = log ( N / C * norm0 ( softmax1 ( s1 / t_r ))) log_y_x2 = log ( N / C * norm0 ( softmax1 ( s2 / t_r ))) y_x1 = norm1 ( softmax0 ( s1 / t_c )) y_x2 = norm1 ( softmax0 ( s2 / t_c )) l1 = -sum ( y_x2 * log_y_x1 ) / N l2 = -sum ( y_x1 * log_y_x2 ) / N L = ( l1 + l2 ) / 2 L . backward () optimizer . step ()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we show mathematically how Self-Classifier avoids trivial solutions by design, i.e., a collapsing solution is not in the set of optimal solutions of our proposed loss function <ref type="bibr">(4)</ref>. Proofs are provided in Supplementary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture</head><p>In all our experiments, we used ResNet-50 [19] backbone (as customary for all compared SSL works) initialized randomly. Following previous work, for our projection heads we used an MLP with 2 layers (of sizes 4096 and 128) with BN, leaky-ReLU activations, and ? 2 normalization after the last layer. On top of the projection head MLP we had 4 classification heads into 1K, 2K, 4K and 8K classes respectively. Each classification head was a simple linear layer without additive bias term. Row-softmax temperature ? row was set to 0.1, while columnsoftmax temperature ? col -to 0.05. Unless mentioned otherwise, evaluation for unsupervised classification (Sec. 6.1) was done strictly using the 1K-classes classification head. For linear evaluation (Sec. 6.2) the MLP was dropped and replaced with a single linear layer of 1K classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Augmentations</head><p>We followed the data augmentations of BYOL <ref type="bibr" target="#b14">[15]</ref> (color jittering, Gaussian blur and solarization), multi-crop <ref type="bibr">[2]</ref> (two global views of 224?224 and six local views of 96 ? 96) and nearest neighbor augmentation <ref type="bibr" target="#b10">[11]</ref> (queue for nearest neighbor augmentation was set to 256K). We refer to Tab. 8 in Sec. 7 for performance results without multi-crop and nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimization</head><p>Unsupervised pre-training/classification. Most of our training hyper-parameters are directly taken from SwAV <ref type="bibr">[2]</ref>. We used a LARS optimizer [33] with a learning rate of 4.8 and weight decay of 10 ?6 . The learning rate was linearly ramped up (starting from 0.3) over the first 10 epochs, and then decreased using a cosine scheduler for 790 epochs with a final value of 0.0048 (for a total of 800 epochs). We used a batch size of 4096 distributed across 64 NVIDIA V100 GPUs.</p><p>Linear evaluation. Similarly to <ref type="bibr" target="#b7">[8]</ref> we used a LARS optimizer [33] with a learning rate of 0.8 and no weight decay. The learning rate was decreased using a cosine scheduler for 100 epochs. We used a batch size of 4096 distributed across 16 NVIDIA V100 GPUs. We have also tried the SGD optimizer in [18] with a batch size of 256, which gives similar results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Unsupervised Image Classification</head><p>We evaluate our approach on the task of unsupervised image classification using the large-scale ImageNet dataset (Tabs. 1 to 3). We report the standard clustering metrics: Normalized Mutual Information (NMI), Adjusted Normalized Mutual Information (AMI), Adjusted Rand-Index (ARI), and Clustering Accuracy (ACC). Our approach sets a new state-of-the-art performance for unsupervised image classification using ImageNet, on all four metrics (NMI, AMI, ARI and ACC), even when trained for a substantial lower number of epochs (Tab. 1). We compare our approach to the latest large-scale deep clustering methods <ref type="bibr">[32,</ref><ref type="bibr">28]</ref> that have been explicitly evaluated on ImageNet. Additionally, we also compare our approach to the latest self-supervised representation learning methods (using <ref type="table">Table 1</ref>: ImageNet unsupervised image classification using ResNet-50. NMI: Normalized Mutual Information, AMI: Adjusted Normalized Mutual Information, ARI: Adjusted Rand-Index, ACC: Clustering accuracy. ?: produced by fitting a k-means classifier on the learned representations of the training set (models from official repositories were used), and then running inference on the validation set (results for SimCLRv2 and InfoMin are taken from <ref type="bibr">[36]</ref> ImageNet-pretrained models provided in their respective official repositories) after fitting a k-means classifier to the learned representations computed on the training set. For all methods we run inference on the validation set (unseen during training).</p><p>The current state-of-the-art approach, SCAN [28], is a multi-stage algorithm that involves: 1) pre-training (800 epochs); 2) offline k-nearest neighbor mining; 3) clustering (100 epochs); and 4) self-labeling and fine-tuning (25 epochs). In contrast, Self-Classifier is a single-stage simple-to-implement model (Algorithm 1) that is trained only with minibatch SGD. At only 200 epochs Self-Classifier already outperforms SCAN with 925 epochs.</p><p>SCAN provided an interesting qualitative analysis of alignment of its unsupervised class predictions to a certain (single) level of the default (WordNet) ImageNet semantic hierarchy. In contrast, here we propose a more diverse set of quantitative metrics to evaluate the performance of self-supervised classification methods on various levels of the default ImageNet hierarchy, as well as on sev- ). We believe that this new set of hierarchical alignment metrics expanding on the leaf-only metric used so far, will allow deeper investigation of how self-supervised classification approaches perceive the internal taxonomy of classes of unlabeled data they are applied to, exposing their strength and weaknesses in a new and interesting light. We use these new metrics to compare our proposed approach to previous unsupervised clustering work <ref type="bibr">[28,</ref><ref type="bibr">32]</ref>, as well as state-of-the-art representation learning work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">27,</ref><ref type="bibr">34]</ref>. In Tab. 2 we report results for different numbers of ImageNet superclasses <ref type="bibr">(10, 29, 128, 466 and 591)</ref> resulting from cutting the default (WordNet) ImageNet hierarchy on different levels. See Supplementary for details of each superclass. The results in this table, that are significantly higher then the result for leaf (1000) classes for any hierarchy level, indicate that examples misclassified on the leaf level tend to be assigned to other clusters from within the same superclass. Furthermore, we see that Self-Classifier consistently outperforms previous work on all hierarchy levels.</p><p>In Tab. 3 we report the results on four ImageNet subpopulation datasets of BREEDS <ref type="bibr">[26]</ref>. These datasets are accompanied by class hierarchies re-calibrated by <ref type="bibr">[26]</ref> such that classes on same hierarchy level are of the same visual granularity. Each dataset contains a specific subpopulation of ImageNet, such as 'Entities', 'Living' things and 'Non-living' things, allowing for a more fine-grained <ref type="table">Table 3</ref>: ImageNet-subsets (BREEDS) unsupervised image classification using ResNet-50. The four BREEDS datasets are: Entity13, Entity30, Living17 and Nonliving26. NMI: Normalized Mutual Information, AMI: Adjusted Normalized Mutual Information, ARI: Adjusted Rand-Index, ACC: Clustering accuracy. ?: produced by fitting a k-means classifier on the learned representations of the training set (models from official repositories were used), and then running inference on the validation set. Results for SCAN and SeLa were produced using ImageNet-pretrained models provided in their respective official repositories evaluation of hierarchical alignment of self-supervised classification predictions. Again, we see consistent improvement of Self-Classifier over previous work and self-supervised representation baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Image Classification with Linear Models</head><p>We evaluate the quality of our unsupervised features using the standard linear classification protocol. Following the self-supervised pre-training stage, we freeze the features and train on top of it a supervised linear classifier (a single fullyconnected layer). This classifier operates on the global average pooling features of a ResNet. Tab. 4 summarizes the results and comparison to the state-of-the-art methods for various number of training budgets (100 to 800 epochs). In addition to good results for unsupervised classification (Sec. 6.1), Self-Classifier additionally achieves results comparable to state of the art for linear classification evaluation using ImageNet. Specifically, as detailed in Tab. 4, it is one of the top-3 result for 3 out of 4 of the training budgets reported, and top-1 in the 100 epochs category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Transfer Learning</head><p>We further evaluate the quality of our unsupervised features by transferring them to other tasks -object detection and instance segmentation. Tab. 5 reports results for VOC07+12 <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr">COCO [22]</ref> datasets. We fine-tune our pre-trained model end-to-end in the target datasets using the public codebase from MoCo <ref type="bibr">[18]</ref>. We obtain significant (? 2%) improvements in the more challenging COCO det/seg over all the self-supervised baselines.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative Results</head><p>In Supplementary, we visualize and analyse a subset of high/low accuracy classes predicted by Self-Classifier on unseen data (ImageNet validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablation Study</head><p>In this section, we evaluate the impact of the design choices of Self-Classifier. Namely, the loss function, the number of classes (C), number of classification heads, fixed vs learnable classifier, MLP architecture, Softmax temperatures (row and column), batch-size, some of the augmentations choices, and NN queue length. We evaluate the different models after 100 self-supervised epochs and report results on ImageNet validation set. We report both the K-NN (K=20) classifier accuracy (evaluating the learned representations) and the unsupervised clustering accuracy (evaluating unsupervised classification performance). Loss function. For both illustrating the generality of our proposed loss function and making more direct comparison with the unsupervised classification state-of-the-art <ref type="figure">(SCAN [28]</ref>), in Tab. 6 we report the results of running SCAN official code, while replacing their loss function (in the clustering step) with ours (Eq. <ref type="formula" target="#formula_4">(5)</ref>) and keeping everything else (e.g. classification heads and augmentations) same as in SCAN. As we can see, our proposed loss generalizes well and improves SCAN result (e.g. by 1.5% ARI and 0.5% ACC). Further results improvements are obtained using our full method (as shown in Tab. 1).</p><p>Number of classes and classification heads. Tab. 7a reports the results for various number of classes and classification heads. Very interestingly, and somewhat contrary to the intuition of previous unsupervised classification works <ref type="bibr">[28,</ref><ref type="bibr">32]</ref> who used the same number of classes for all heads, we found that using a different number of classes for each head while still keeping the total number of parameters constant (e.g. 15x1k vs. 1k+2k+4k+8k) improves results on both metrics. We believe that such a learning objective forces the model to learn a representation that is more invariant to the number of classes, thus improving its generalization performance.</p><p>Fixed/Learnable classifier. As expected, we found that a learnable classifier performs better than a fixed one (Tab. 7d).</p><p>MLP architecture. Tab. 7c reports the results for various sizes of hidden/output layers. Surprisingly, we found that decreasing the number of hidden layers and their size improves both metrics. As a result, our best model (4096/128 MLP) has 30% less parameters than the model used in SCAN <ref type="bibr">[28]</ref> (that used 2048 sized input to its cls. heads). In addition, we verified there is no peak performance difference between ReLU and leaky-ReLU activation in the MLP.</p><p>Softmax Temperature. <ref type="table" target="#tab_6">Table 7b</ref> reports the results for a range of Row/-Column softmax temperatures. We found that the ratio between the two temperatures is important for performance (specifically clustering accuracy). The model is robust to ratios (row over column) in the range of 2.0 -3.5.</p><p>Batch Size. <ref type="table" target="#tab_6">Table 7f</ref> reports the results for a range of batch size values (256 to 4096). Similarly to previous self-supervised work (and specifically clusteringbased), performance improves as we increase batch size.</p><p>Multi-crop and nearest neighbor augmentations. Tab. 8 reports the impact of removing multi-crop <ref type="bibr">[2]</ref> and nearest neighbor augmentations <ref type="bibr" target="#b10">[11]</ref> on linear classification accuracy and compares to other state-of-the-art methods.</p><p>Nearest neighbor queue length. The model is somewhat robust to a queue length in the range of 128K -512K (Tab. 7e), while increasing it further decreases performance. Most likely due to stale embeddings (as noted by <ref type="bibr" target="#b10">[11]</ref> as well).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Comparative Analysis</head><p>A common and critical element of all self-supervised learning methods is collapse prevention. In this section, we discuss the various approaches of state-of-theart models for preventing collapse. The approaches can be categorized into two categories: 1) negative samples; and 2) stop-grad operation. Where in practice, stop-grad operation includes two more sub-categories: 2.a) external clustering; and 2.b) momentum encoder. In this paper, we propose a third and completely new approach for collapse prevention -a non-collapsing loss function, i.e., a loss function without degenerate optimal solutions. Negative samples. SimCLR <ref type="bibr" target="#b4">[5]</ref> and Moco [18] prevent collapse by utilizing negative pairs to explicitly force dissimilarity.</p><p>External clustering. SwAV <ref type="bibr">[2]</ref>, SeLa [32] and SCAN [28] prevent collapse by utilizing external clustering algorithm such as K-Means (SCAN) or Sinkhorn-Knopp (SwAV/SeLa) for generating pseudo-labels. <ref type="table">Table 8</ref>: Performance without multi-crop and without nearest neighbor augmentations. ImageNet Top-1 linear classification accuracy after 100 epochs. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b6">7]</ref> are taken from <ref type="bibr" target="#b7">[8]</ref> SimCLR <ref type="bibr" target="#b4">[5]</ref> BYOL <ref type="bibr" target="#b14">[15]</ref> SwAV <ref type="bibr">[2]</ref>   <ref type="bibr" target="#b14">[15]</ref> and DINO <ref type="bibr">[3]</ref> prevent collapse by utilizing the momentum encoder proposed by MoCo. The momentum encoder generates a different yet fixed pseudo target in every iteration.</p><p>Stop-grad operation. SimSiam <ref type="bibr" target="#b7">[8]</ref> prevent collapse by applying a stop-grad operation on one of the views, which acts as a fixed pseudo label. In fact, except for SimCLR, all of the above methods can be simply differentiated by where exactly a stop-grad operation is used. SwAV/SeLa/SCAN apply a stop-grad operation on the clustering phase, while MoCo/BYOL/DINO apply a stop-grad operation on a second network that is used for generating assignments.</p><p>Non-collapsing loss function. In contrast, we show mathematically (Sec. 4) and empirically (Sec. 6) that Self-Classifier prevents collapse with a novel loss function <ref type="bibr">(4)</ref> and without the use of external clustering, pseudo-labels, momentum encoder, stop-grad nor negative pairs. More specifically, a collapsing solution is simply not in the set of optimal solutions of our proposed loss, which makes it possible to train Self-Classifier using just a single network and a simple SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions and Limitations</head><p>We introduced Self-Classifier, a new approach for unsupervised end-to-end classification and self-supervised representation learning. Our approach is mathematically justified and simple to implement. It sets a new state-of-the-art performance for unsupervised classification on ImageNet and achieves comparable to state of the art results for unsupervised representation learning. We provide a thorough investigation of our method in a series of ablation studies. Furthermore, we propose a new hierarchical alignment quantitative metric for self-supervised classification establishing baseline performance for a wide range of methods and showing advantages of our proposed approach in this new task. Limitations of this paper include: (i) our method relies on knowledge of the number of classes, but in some cases it might not be optimal as the true number of classes should really be dictated by the data itself. In this paper we relax this potential weakness by introducing the notion of multiple classification heads, but we believe further investigation would be an interesting future work direction; (ii) one of the most common sources of error we observed is merging of nearby classes (e.g. different breeds of cat), introducing additional regularization for reducing this artifact is also an interesting direction of future work. 10 Visualization of High/Low Accuracy Classes Predicted by Self-Classifier <ref type="figure">Fig. 2</ref>: High accuracy classes predicted by Self-Classifier on ImageNet validation set (unseen during training). Classes are sorted by accuracy, and images are sampled randomly from each predicted class. Note that the predicted classes capture a large variety of different backgrounds and viewpoints. This provides further evidence that Self-Classifier learns semantically meaningful classes without any labels. <ref type="figure" target="#fig_1">Fig. 3</ref>: Low accuracy classes predicted by Self-Classifier on ImageNet validation set (unseen during training). Images are sampled randomly from each predicted class. We see that even for low accuracy classes, unsupervised semantic grouping is reasonably good. A lot of the misclassified samples (with regards to ground truth) is due to subclasses from the same superclass that are clustered together (please see Tab. 2 for empirical results). For example, different types of birds, monkeys, fruits, spyders, sea animals, helmets, screens (TV vs computer screen), etc. In practice, such clustering is good, yet is not aligned with ImageNet ground truth and thus the accuracy of such clusters is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Theoretical Analysis</head><p>In this section, we show mathematically how Self-Classifier avoids trivial solutions by design, i.e., a collapsing solution is simply not in the set of optimal solutions of our proposed loss function <ref type="bibr">(4)</ref>. Furthermore, and more importantly, we show that by setting p(y) manually, the required optimal solution can be defined by the user. Proof. Consider the non-negative denominators y?[C] p(x 2 |y) and x1?[B] p(y|x 1 ) of the first and the second term of (4), respectively. To avoid unbounded loss, both denominators must be strictly positive. Thus, for every x 2 ? [N ] there is y ? [C] such as p(x 2 |y) &gt; 0, and for every y ? [C] there is x 1 ? [N ] such that p(y|x 1 ) &gt; 0. Using the symmetrized objective loss function <ref type="formula" target="#formula_4">(5)</ref>  Proof. The first term p(x2|y) ? p(x2|?) summed over y in <ref type="formula" target="#formula_3">(4)</ref> is normalized such that the probabilities assigned to each x 2 ? [N ] sum to one. Additionally, setting p(y) = 1 C and p(x) = 1 N renders the argument N C p(y|x1)</p><p>x 1 p(y|x1) of the logarithm in the second term in <ref type="bibr">(4)</ref> normalized such that the probabilities assigned to each y ? [C] sum to N C . Thus, an optimal zero loss solution is one in which ?y ? [C], exactly N C samples are assigned a probability of one, and ?x ? [N ], exactly 1 class is assigned probability of one, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Unsupervised Classification Accuracy &amp; Training Loss</head><p>Vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Epochs</head><p>In <ref type="figure" target="#fig_3">Fig. 4</ref> we plot the unsupervised classification accuracy and training loss vs. the number of training epochs of Self-Classifier. This figure further illustrates the fact that the training objective of Self-Classifier is aligned with semantically meaningful classification of unseen data (ImageNet validation set) despite being trained strictly with unlabeled data. Training Loss Top-1 Accuracy  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Training Epochs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Classification Accuracy &amp; Training Loss Vs. Number of Epochs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">Superclasses Datasets</head><p>In Tab. 2 we report results for different numbers of ImageNet superclasses <ref type="bibr">(10, 29, 128, 466 and 591)</ref> resulting from cutting the default (WordNet) ImageNet hierarchy on different levels -levels 2 to 6, respectively. More specifically: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 (</head><label>1</label><figDesc>Non-Zero Posterior Probability). Let B be a batch of N samples with two views per sample, (x 1 , x 2 ) ? B. Let p(y) and p(x) be the class and sample distributions, respectively, where y ? [C]. Let (5) be the loss function. Then, each class y ? [C] will have at least one sample y ? [C] with non-zero posterior probability p(x|y) &gt; 0 assigned into it, and each sample x ? [N ] will have at least one class y ? [C] with p(x|y) &gt; 0. Theorem 2 (Optimal Solution With Uniform Prior). Let B be a batch of N samples with two views per sample, (x 1 , x 2 ) ? B. Let p(y) and p(x) be the class and sample distribution, respectively, where y ? [C]. Then, the uniform probabilities p(y) = 1 C , p(x) = 1 N constitute a global minimizer of the loss (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 3 (</head><label>3</label><figDesc>Non-Zero Posterior Probability). Let B be a batch of N samples with two views per sample, (x 1 , x 2 ) ? B. Let p(y) and p(x) be the class and sample distributions, respectively, where y ? [C]. Let (5) be the loss function. Then, each class y ? [C] will have at least one sample y ? [C] with non-zero posterior probability p(x|y) &gt; 0 assigned into it, and each sample x ? [N ] will have at least one class y ? [C] with p(x|y) &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>allows dropping the subscripts 1 and 2 of x. Theorem 4 (Optimal Solution With Uniform Prior). Let B be a batch of N samples with two views per sample, (x 1 , x 2 ) ? B. Let p(y) and p(x) be the class and sample distribution, respectively, where y ? [C]. Then, the uniform probabilities p(y) = 1 C and p(x) = 1 N constitute a global minimizer of the loss (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Unsupervised classification accuracy &amp; training loss vs. number of training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ImageNet-superclasses unsupervised image classification accuracy using ResNet-50. We define new datasets that contain broad classes which each subsume several of the original ImageNet classes. See Supplementary for details of each superclass. ?: produced by fitting a k-means classifier on the learned representations of the training set (models from official repositories were used), and then running inference on the validation set. Results for SCAN and SeLa were produced using ImageNet-pretrained models provided in their respective official repositories</figDesc><table><row><cell></cell><cell cols="3">Number of ImageNet Superclasses</cell></row><row><cell>Method</cell><cell>10</cell><cell>29</cell><cell>128 466 591 1000</cell></row><row><cell cols="4">representation learning methods</cell></row><row><cell>SwAV  ? [2]</cell><cell cols="3">79.1 69.4 58.0 46.3 34.5 28.1</cell></row><row><cell>MoCoV2  ? [7]</cell><cell cols="3">80.0 72.8 63.8 51.4 36.8 30.6</cell></row><row><cell>DINO  ? [3]</cell><cell cols="3">79.7 71.3 60.7 49.2 37.8 30.7</cell></row><row><cell>OBoW  ? [13]</cell><cell cols="3">83.9 76.5 67.4 53.5 35.7 31.1</cell></row><row><cell>BarlowT  ? [34]</cell><cell cols="3">80.2 72.1 62.7 52.7 40.9 34.2</cell></row><row><cell cols="2">clustering based methods</cell><cell></cell><cell></cell></row><row><cell>SeLa [32]</cell><cell cols="3">55.2 44.9 40.6 36.6 37.8 30.5</cell></row><row><cell>SCAN [28]</cell><cell cols="3">85.3 79.3 71.2 59.6 44.7 39.9</cell></row><row><cell cols="4">Self-Classifier 85.7 79.7 71.8 60.0 46.7 41.1</cell></row><row><cell cols="4">eral hierarchies of carefully curated ImageNet subpopulations (BREEDS [26]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>ImageNet linear classification using ResNet-50. Top-1 accuracy vs. number of training epochs. Top-3 best methods per-category are underlined</figDesc><table><row><cell></cell><cell cols="4">Number of Training Epochs</cell></row><row><cell>Method</cell><cell>100</cell><cell>200</cell><cell>400</cell><cell>800</cell></row><row><cell>Supervised</cell><cell>76.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SimCLR [5]</cell><cell>66.5</cell><cell>68.3</cell><cell>69.8</cell><cell>70.4</cell></row><row><cell>MoCoV2 [7]</cell><cell>67.4</cell><cell>67.5</cell><cell>71.0</cell><cell>71.1</cell></row><row><cell>SimSiam [8]</cell><cell>68.1</cell><cell>70.0</cell><cell>70.8</cell><cell>71.3</cell></row><row><cell>SimCLRv2 [6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.7</cell></row><row><cell>InfoMin [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.0</cell></row><row><cell>BarlowT [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.2</cell></row><row><cell>OBoW [13]</cell><cell>-</cell><cell>73.8</cell><cell>-</cell><cell>-</cell></row><row><cell>BYOL [15]</cell><cell>66.5</cell><cell>70.6</cell><cell>73.2</cell><cell>74.3</cell></row><row><cell>NNCLR [11]</cell><cell>69.4</cell><cell>70.7</cell><cell>74.2</cell><cell>74.9</cell></row><row><cell>DINO [3]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.3</cell></row><row><cell>SwAV [2]</cell><cell>72.1</cell><cell>73.9</cell><cell>74.6</cell><cell>75.3</cell></row><row><cell>Self-Classifier</cell><cell>72.4</cell><cell>73.5</cell><cell>74.2</cell><cell>74.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Transfer learning: object detection and instance segmentation. Results for other methods are taken from [34]</figDesc><table><row><cell>Method</cell><cell>VOC07+12 det</cell><cell>COCO det</cell><cell>COCO seg</cell></row><row><cell></cell><cell cols="3">AP AP 50 AP 75 AP AP 50 AP 75 AP AP 50 AP 75</cell></row><row><cell>Supervised</cell><cell cols="3">53.5 81.3 58.8 38.2 58.2 41.2 33.3 54.7 35.2</cell></row><row><cell>MoCo-v2[7]</cell><cell cols="3">57.4 82.5 64.0 39.3 58.9 42.5 34.4 55.8 36.5</cell></row><row><cell>SwAV[2]</cell><cell cols="3">56.1 82.6 62.7 38.4 58.6 41.3 33.8 55.2 35.9</cell></row><row><cell>SimSiam[8]</cell><cell cols="3">57.0 82.4 63.7 39.2 59.3 42.1 34.4 56.0 36.7</cell></row><row><cell>BarlowT[34]</cell><cell cols="3">56.8 82.6 63.4 39.2 59.0 42.5 34.3 56.0 36.5</cell></row><row><cell cols="4">Self-Classifier 56.6 82.4 62.6 41.5 61.3 45.0 36.1 58.1 38.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation: loss function generality. For column definitions see Tab. 1. SCAN + Eq.(5)is SCAN with clustering step loss replaced with ours.</figDesc><table><row><cell>Method</cell><cell>NMI AMI ARI ACC</cell></row><row><cell>SCAN [28]</cell><cell>72.0 51.2 27.5 39.9</cell></row><row><cell cols="2">SCAN + our loss (Eq. (5)) 72.7 52.2 29.0 40.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study. After 100 epochs, reporting performance for ImageNet as accuracy of &lt;'k-NN' | 'unsupervised clustering'&gt; in each experiment.</figDesc><table><row><cell cols="8">(a) Classification heads. (2k) (4k) (8k) : 2k, 4k and 8k over-clustering accuracy.</cell></row><row><cell>1?1k</cell><cell cols="2">5?1k</cell><cell>10?1k</cell><cell cols="2">15?1k</cell><cell>1?2k</cell><cell>1?4k</cell><cell>1k+2k+4k+8k</cell></row><row><cell cols="8">Acc. (%) 59.6|34.1 58.7|34.0 58.6|33.5 58.8|33.9 59.3|38.8 (2k) 57.0|42.9 (4k) 61.7|37.3,40.6 (2k) ,44.2 (4k) ,48.0 (8k)</cell></row><row><cell cols="4">(b) Softmax Temperature</cell><cell></cell><cell></cell><cell></cell><cell>(c) MLP architecture</cell></row><row><cell cols="2">? column 0.07</cell><cell>? row</cell><cell>0.1</cell><cell></cell><cell></cell><cell cols="2">MLP hidden layer(s)</cell><cell>MLP output layer 128 256</cell></row><row><cell cols="5">0.03 59.9|36.9 59.2|36.9 0.05 58.9|29.2 61.7|37.3</cell><cell></cell><cell></cell><cell>1x4096 2x4096 2x8192</cell><cell>61.7|37.3 61.3|33.5 60.9|36.4 60.4|33.6 60.0|36.9 59.6|36.7</cell></row><row><cell cols="5">(d) Fixed / Learnable classifier</cell><cell></cell><cell cols="2">(e) Nearest neighbor queue length</cell></row><row><cell cols="5">Fixed Learnable</cell><cell cols="3">Queue len. 128K</cell><cell>256K</cell><cell>512K</cell><cell>1M</cell></row><row><cell cols="5">Acc. (%) 57.6|32.2 61.7|37.3</cell><cell cols="3">Acc. (%) 59.2|36.8 61.7|37.3 60.3|36.9 56.8|35.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(f) Batch size</cell></row><row><cell cols="4">Batch Size 256</cell><cell></cell><cell></cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell></row><row><cell cols="8">Acc. (%) 49.0|20.9 52.2|23.1 54.5|26.8 57.0|35.1 61.7|37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>16. Gutmann, M., Hyv?rinen, A.: Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. pp. 297-304 (2010) 17. Haeusser, P., Plapp, J., Golkov, V., Aljalbout, E., Cremers, D.Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: Meila, M., Zhang, T. (eds.</figDesc><table><row><cell>34. ) Proceedings</cell></row><row><cell>of the 38th International Conference on Machine Learning, ICML 2021, 18-24</cell></row><row><cell>: Associative deep July 2021, Virtual Event. Proceedings of Machine Learning Research, vol. 139,</cell></row><row><cell>clustering: Training a classification network with no labels. In: German Conference pp. 12310-12320. PMLR (2021)</cell></row><row><cell>on Pattern Recognition. pp. 18-32. Springer (2018) 35. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: European confer-</cell></row><row><cell>18. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised ence on computer vision. pp. 649-666. Springer (2016)</cell></row><row><cell>visual representation learning. In: Proceedings of the IEEE/CVF Conference on 36. Zheltonozhskii, E., Baskin, C., Bronstein, A.M., Mendelson, A.: Self-</cell></row><row><cell>Computer Vision and Pattern Recognition. pp. 9729-9738 (2020) supervised learning for large-scale unsupervised image clustering. arXiv preprint</cell></row><row><cell>19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: arXiv:2008.10312 (2020)</cell></row><row><cell>Proceedings of the IEEE conference on computer vision and pattern recognition.</cell></row><row><cell>pp. 770-778 (2016)</cell></row><row><cell>20. Ji, X., Henriques, J.F., Vedaldi, A.: Invariant information clustering for unsuper-</cell></row><row><cell>vised image classification and segmentation. In: Proceedings of the IEEE/CVF</cell></row><row><cell>International Conference on Computer Vision. pp. 9865-9874 (2019)</cell></row><row><cell>21. Li, J., Zhou, P., Xiong, C., Hoi, S.: Prototypical contrastive learning of unsuper-</cell></row><row><cell>vised representations. In: International Conference on Learning Representations</cell></row><row><cell>(2021)</cell></row><row><cell>22. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll?r, P.,</cell></row><row><cell>Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference</cell></row><row><cell>on computer vision. pp. 740-755. Springer (2014)</cell></row><row><cell>23. Misra, I., Maaten, L.v.d.: Self-supervised learning of pretext-invariant represen-</cell></row><row><cell>tations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and</cell></row><row><cell>Pattern Recognition. pp. 6707-6717 (2020)</cell></row><row><cell>24. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving</cell></row><row><cell>jigsaw puzzles. In: European Conference on Computer Vision. pp. 69-84. Springer</cell></row><row><cell>(2016)</cell></row><row><cell>25. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-</cell></row><row><cell>coders: Feature learning by inpainting. In: Proceedings of the IEEE conference on</cell></row><row><cell>computer vision and pattern recognition. pp. 2536-2544 (2016)</cell></row><row><cell>26. Santurkar, S., Tsipras, D., Madry, A.: Breeds: Benchmarks for subpopulation shift.</cell></row><row><cell>arXiv preprint arXiv:2008.04859 (2020)</cell></row><row><cell>27. Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., Isola, P.: What makes for</cell></row><row><cell>good views for contrastive learning? In: Advances in Neural Information Processing</cell></row><row><cell>Systems (2020)</cell></row><row><cell>28. Van Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M., Van Gool,</cell></row><row><cell>L.: Scan: Learning to classify images without labels. In: European Conference on</cell></row><row><cell>Computer Vision. pp. 268-285. Springer (2020)</cell></row><row><cell>29. Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-</cell></row><row><cell>parametric instance discrimination. In: Proceedings of the IEEE Conference on</cell></row><row><cell>Computer Vision and Pattern Recognition. pp. 3733-3742 (2018)</cell></row><row><cell>30. Xie, J., Girshick, R., Farhadi, A.: Unsupervised deep embedding for clustering</cell></row><row><cell>analysis. In: International conference on machine learning. pp. 478-487. PMLR</cell></row><row><cell>(2016)</cell></row><row><cell>31. Yang, J., Parikh, D., Batra, D.: Joint unsupervised learning of deep representations</cell></row><row><cell>and image clusters. In: Proceedings of the IEEE conference on computer vision and</cell></row><row><cell>pattern recognition. pp. 5147-5156 (2016)</cell></row><row><cell>32. YM., A., C., R., A., V.: Self-labelling via simultaneous clustering and representa-</cell></row><row><cell>tion learning. In: International Conference on Learning Representations (2020)</cell></row><row><cell>33. You, Y., Gitman, I., Ginsburg, B.: Large batch training of convolutional networks.</cell></row><row><cell>arXiv preprint arXiv:1708.03888 (2017)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Felis concolor; mantis, mantid; reel; fire screen, fireguard; iron, smoothing iron; jewelry, jewellery; bee; nail; heron; gar, garfish, garpike, billfish,  Lepisosteus osseus; pelican; maze, labyrinth; projector; cracker; torch; phalanger, opossum, possum; sandwich; boat; fig; cauliflower; vulture; quilt, comforter, comfort, puff; cushion; factory, mill, manufacturing plant, manufactory; ear, spike, capitulum; CD player; pool  table, billiard table, snooker table; barometer; ape; butterfly fish; modem; skirt; fountain; soup; sliding door; hippopotamus, hippo, river horse, Hippopotamus amphibius; indigo bunting, indigo finch, indigo bird, Passerina cyanea; groom, bridegroom; antelope; pepper; microwave, microwave oven; bowl; cabbage, chou; shirt; eagle, bird of Jove; wombat; paintbrush; Old World buffalo, buffalo; goose; stethoscope; microphone, mike; lacewing, lacewing fly; burrito; guinea pig, Cavia cobaya; elephant; washer, automatic washer, washing machine; scorpion; airship, dirigible; hair spray; handkerchief, hankie, hanky, hankey; spider; scuba diver; sofa, couch, lounge; dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk; breakwater, groin, groyne, mole, bulwark, seawall, jetty; jinrikisha, ricksha, rick-shaw; locomotive, engine, locomotive engine, railway locomotive; toilet seat; roof; knee pad; dishrag, dishcloth; keyboard instrument; hat, chapeau, lid; apron; streetcar, tram, tramcar, trolley, trolley car; tricycle, trike, velocipede; shark; table lamp; banana; jay; clip; ski; baby bed, baby's bed; ball; barracouta, snoek; weight, free weight, exercising weight; fence, fencing; nematode, nematode worm, roundworm; lampshade, lamp shade; unicycle, monocycle; hermit crab; camel; frog, toad, toad frog, anuran, batrachian, salientian; sauce; promontory, headland, head, foreland; bridge, span; motorcycle, bike; ballplayer, baseball player; artichoke, globe artichoke; nightwear, sleepwear, nightclothes; orange; airplane, aeroplane, plane; place of worship, house of prayer, house of God, house of worship; polecat, fitch, foulmart, foumart, Mustela putorius; potpie; sea cucumber, holothurian; oscilloscope, scope, cathode-ray oscilloscope, CRO; camera, photographic camera; bear; house finch, linnet, Carpodacus mexicanus; starfish, sea star; dining table, board; wild sheep; badger; mushroom; pen; carpenter's kit, tool kit; scorpaenid, scorpaenid fish; digital computer; zebra; pineapple, ananas; drilling platform, offshore rig; truck, motortruck; signboard, sign; cock; bison; vacuum, vacuum cleaner; nightingale, Luscinia megarhynchos; hornbill; military uniform; cabinet; perfume, essence; chambered nautilus, pearly nautilus, nautilus; lakeside, lakeshore; cardoon; snow leopard, ounce, Panthera uncia; meat loaf, meatloaf; dress, frock; wolf; dishwasher, dish washer, dishwashing machine; brace; tray; tower; eraser; ostrich, Struthio camelus; theater, theatre, house; glass, drinking glass; warplane, military plane; sea anemone, anemone; hog, pig, grunter, squealer, Sus scrofa; ant, emmet, pismire; bicycle, bike, wheel, cycle; crocodile; lemon; leopard, Panthera pardus; meter; dinosaur; suit, suit of clothes; hammer; goat, caprine animal; trouser, pant; cockroach, roach; half track; loaf of bread, loaf; conch; gallinule, marsh hen, water hen, swamphen; dog, domestic dog, Canis familiaris; turtle; gate; corn; flamingo; face mask; hawk; ladle; pan, cooking pan; cocktail shaker; umbrella; rail; space shuttle; printer; stringed instrument; syringe; swimsuit, swimwear, bathing suit, swimming costume, bathing costume; measuring stick, measure, measuring rod; handcart, pushcart, cart, go-cart; ship; plover; joystick; walking stick, walkingstick, stick insect; tench, Tinca tinca; telescope, scope. where, dummy45 is library. dummy46 is planetarium. dummy49 is restaurant, eating house, eating place, eatery. dummy47 is prison, prison house. dummy20 is hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa; agaric; bolete. 5. Level 6 contains 591 superclasses -German shepherd, German shepherd dog, German police dog, alsatian; hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa; Maltese dog, Maltese terrier, Maltese; pirate, pirate ship; cellular telephone, cellular phone, cellphone, cell, mobile phone; red fox, Vulpes vulpes; Dandie Dinmont, Dandie Dinmont terrier; sweet pepper; suspension bridge; spotted salamander, Ambystoma maculatum; kuvasz; coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch; violin, fiddle; gown; lory; schooner; organ, pipe organ; Saluki, gazelle hound; hot pot, hotpot; Border collie; briard; Windsor tie;</figDesc><table><row><cell>roundworm. dummy50 is handkerchief, hankie, hanky, hankey. dummy12 is stork; helmet; bustard; daisy; wardrobe, closet, press; desk; puffer, puffer-</cell></row><row><cell>sea cucumber, holothurian. dummy35 is chain mail, ring mail, mail, chain fish, blowfish, globefish; cooker; tights, leotards; hen; mouse, computer mouse;</cell></row><row><cell>armor, chain armour, ring armor, ring armour; breastplate, aegis, egis; cuirass; dwelling, home, domicile, abode, habitation, dwelling house; reservoir; binoc-</cell></row><row><cell>bulletproof vest. dummy7 is tree frog, tree-frog; bullfrog, Rana catesbeiana; ulars, field glasses, opera glasses; echidna, spiny anteater, anteater; jacamar;</cell></row><row><cell>tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui. dummy72 is bag; entertainment center; fly; bow; skunk, polecat, wood pussy; domestic</cell></row><row><cell>traffic light, traffic signal, stoplight; scoreboard; street sign. dummy57 is um-cat, house cat, Felis domesticus, Felis catus; dogsled, dog sled, dog sleigh;</cell></row><row><cell>brella. dummy74 is maze, labyrinth. stew; cattle, cows, kine, oxen, Bos taurus; lighter, light, igniter, ignitor;</cell></row><row><cell>4. Level 5 contains 466 superclasses -jellyfish; owl, bird of Minerva, bird of dock, dockage, docking facility; tiger, Panthera tigris; porcupine, hedgehog;</cell></row><row><cell>night, hooter; coral fungus; screwdriver; rotisserie; sharpener; dummy45; cream, ointment, emollient; chain saw, chainsaw; crab; lynx, catamount;</cell></row><row><cell>flatworm, platyhelminth; car, railcar, railway car, railroad car; timepiece, knife; necktie, tie; monkey; broccoli; bib; stove; missile; sewing machine;</cell></row><row><cell>timekeeper, horologe; bottle; chest of drawers, chest, bureau, dresser; eel; car, auto, automobile, machine, motorcar; screw; grasshopper, hopper; chair;</cell></row><row><cell>towel; fan; sandbar, sand bar; tripod; orchid, orchidaceous plant; squirrel; hare; display, video display; crane; lobster; audio system, sound system; bus,</cell></row><row><cell>sturgeon; plate rack; hip, rose hip, rosehip; cicada, cicala; balloon; otter; autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, om-</cell></row><row><cell>plate; seashore, coast, seacoast, sea-coast; phasianid; rabbit, coney, cony; nibus, passenger vehicle; outbuilding; jackfruit, jak, jack; lens cap, lens cover;</cell></row><row><cell>warthog; opener; plow, plough; bun, roll; dip; compass; lemur; rug, carpet, apple; projectile, missile; coral reef; chest; meerkat, mierkat; spoon; crutch;</cell></row><row><cell>carpeting; waffle iron; alp; gymnastic apparatus, exerciser; wild dog; pud-mushroom; squash; coat; sheep; cliff, drop, drop-off; pot; television, television</cell></row><row><cell>ding, pud; junco, snowbird; harvestman, daddy longlegs, Phalangium opilio; system; jaguar, panther, Panthera onca, Felis onca; scale, weighing machine;</cell></row><row><cell>column, pillar; horse cart, horse-cart; earthstar; mug; bookcase; leafhopper; llama; acarine; sea slug, nudibranch; valley, vale; oxcart; bee eater; body ar-</cell></row><row><cell>lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens; window mor, body armour, suit of armor, suit of armour, coat of mail, cataphract;</cell></row><row><cell>shade; punch; mercantile establishment, retail store, sales outlet, outlet; ban-rapeseed; window screen; lawn mower, mower; cucumber, cuke; fox; butter-</cell></row><row><cell>nister, banister, balustrade, balusters, handrail; espresso; cheetah, chetah, fly; racket, racquet; cassette player; grouse; hand blower, blow dryer, blow</cell></row><row><cell>Acinonyx jubatus; clog, geta, patten, sabot; acorn; platypus, duckbill, duck-drier, hair dryer, hair drier; giant panda, panda, panda bear, coon bear, Ail-</cell></row><row><cell>1. Level 2 contains 10 superclasses -furnishing; structure, place; fungus; con-billed platypus, duck-billed platypus, Ornithorhynchus anatinus; mink; re-uropoda melanoleuca; power drill; volcano; seal; pomegranate; refrigerator,</cell></row><row><cell>veyance, transport; plant, flora, plant life; apparel, toiletries; animal, animate mote control, remote; percussion instrument, percussive instrument; hyena, icebox; toucan; horse, Equus caballus; salamander; damselfish, demoiselle;</cell></row><row><cell>being, beast, brute, creature, fauna; food, nutrient; paraphernalia; person. hyaena; duck; dam, dike, dyke; parrot; wine, vino; alligator, gator; aba-sea urchin; stinkhorn, carrion fungus; sweater, jumper; sandpiper; stocking;</cell></row><row><cell>2. Level 3 contains 29 superclasses -echinoderm; bird; wheeled vehicle; furni-cus; spatula; spectacles, specs, eyeglasses, glasses; curtain, drape, drapery, dummy47; shoji; spoonbill; hummingbird; frozen dessert; stage; pizza, pizza</cell></row><row><cell>ture, piece of furniture, article of furniture; soft furnishings, accessories; pro-mantle, pall; memorial, monument; snipe; sock; buckeye, horse chestnut, pie; sea cow, sirenian mammal, sirenian; microscope; kangaroo; goldfinch,</cell></row><row><cell>duce, green goods, green groceries, garden truck; mollusk, mollusc, shellfish; conker; monitor; glove; sheath; mosquito net; brambling, Fringilla montif-Carduelis carduelis; cougar, puma, catamount, mountain lion, painter, pan-</cell></row><row><cell>vascular plant, tracheophyte; man-made structure, construction; instrument; ringilla; beaver; snorkel; armadillo; crayfish, crawfish, crawdad, crawdaddy; ther,</cell></row><row><cell>accessory, accoutrement, accouterment; geological formation, formation; ap-strawberry; space heater; cricket; custard apple; computer keyboard, keypad;</cell></row><row><cell>pliance; mammal, mammalian; amphibian; garment; fungus; craft; reptile, telephone, phone, telephone set; black-footed ferret, ferret, Mustela nigripes;</cell></row><row><cell>reptilian; arthropod; cooked food, prepared food; fish; sled, sledge, sleigh; bev-robin, American robin, Turdus migratorius; titmouse, tit; amphibian, am-</cell></row><row><cell>erage, drink, drinkable, potable; person; toiletry, toilet articles; equipment; phibious vehicle; tape player; bobsled, bobsleigh, bob; toaster; penguin; cuckoo;</cell></row><row><cell>worm; coelenterate, cnidarian. magpie; bed; hamster; coral; cannon; mat; snail; home theater, home theatre;</cell></row><row><cell>3. Level 4 contains 128 superclasses -dummy8; dummy6; seat; gastropod, uni-wind instrument, wind; goldfish, Carassius auratus; undergarment, unmen-</cell></row><row><cell>valve; building, edifice; spacecraft, ballistic capsule, space vehicle; medical tionable; patio, terrace; tank, army tank, armored combat vehicle, armoured</cell></row><row><cell>instrument; motor vehicle, automotive vehicle; alcohol, alcoholic drink, al-combat vehicle; snowmobile; pin; oystercatcher, oyster catcher; mongoose;</cell></row><row><cell>coholic beverage, intoxicant, inebriant; train, railroad train; rodent, gnawer; scarf; gyromitra; isopod; swan; adhesive bandage; firearm, piece, small-arm;</cell></row><row><cell>lagomorph, gnawing mammal; bag; dummy53; dummy79; dummy0; antho-water ouzel, dipper; bell cote, bell cot; boot; dummy46; snake, serpent, ophid-</cell></row><row><cell>zoan, actinozoan; piece of cloth, piece of material; dummy68; tool; screen; ian; albatross, mollymawk; damselfly; marmot; weasel; coffee maker; sub-</cell></row><row><cell>barrier; chiton, coat-of-mail shell, sea cradle, polyplacophore; baked goods; marine, pigboat, sub, U-boat; saltshaker, salt shaker; cap; beetle; slug; lock;</cell></row><row><cell>insect; neckwear; arachnid, arachnoid; person, individual, someone, some-whale; wild boar, boar, Sus scrofa; ray; makeup, make-up, war paint; limpkin,</cell></row><row><cell>body, mortal, soul; landing, landing place; bandage, patch; bony fish; area; Aramus pictus; geyser; measuring cup; lizard; jug; photocopier; radio, wire-</cell></row><row><cell>gallinaceous bird, gallinacean; dummy77; dummy69; handwear, hand wear; less; buckle; plunger, plumber's helper; hairpiece, false hair, postiche; potato,</cell></row><row><cell>dummy67; mountain, mount; coffee, java; bedclothes, bed clothing, bedding; white potato, Irish potato, murphy, spud, tater; lion, king of beasts, Pan-</cell></row><row><cell>dummy73; dummy14; facial accessories; dummy1; primate; sports equipment; thera leo; sloth, tree sloth; passenger train; shoe; dummy49; shovel; salmon;</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep adaptive image clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5879" to="5887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="9588" to="9597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Obow: Online bag-of-visual-words generation for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Triturus vulgaris; spotted salamander, Ambystoma maculatum; axolotl, mud puppy, Ambystoma mexicanum. dummy6 is African grey, African gray, Psittacus erithacus; sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita; lorikeet; macaw. dummy52 is plastic bag; purse; backpack, back pack, knapsack, packsack, rucksack, haversack; mailbag, postbag. dummy53 is buckle. dummy79 is chest. dummy0 is cock. dummy68 is cliff, drop, dropoff. dummy77 is brass, memorial tablet, plaque; triumphal arch; megalith, megalithic structure. dummy69 is promontory, headland, head, foreland. dummy67 is fountain. dummy73 is viaduct; suspension bridge; steel arch bridge. dummy14 is flatworm, platyhelminth. dummy34 is mask; sunglasses, dark glasses, shades; gasmask, respirator, gas helmet; ski mask. dummy1 is hen. dummy19 is gyromitra. dummy51 is crutch. dummy71 is valley, vale. dummy11 is starfish, sea star. dummy39 is scabbard; holster. dummy56 is shower curtain; theater curtain, theatre curtain. dummy78 is totem pole; pedestal, plinth, footstall; obelisk. dummy10 is jellyfish. dummy9 is ringneck snake, ring-necked snake, ring snake; king snake, kingsnake; thunder snake, worm snake, Carphophis amoenus; green mamba; boa constrictor, Constrictor constrictor; vine snake; garter snake, grass snake; horned viper, cerastes, sand viper, horned asp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (2020) aircraft; dryer, drier; proboscidean, proboscidian; kitchen utensil; centipede; wall unit; vegetable, veggie, veg; headdress, headgear; basidiomycete, basidiomycetous fungi; monotreme, egg-laying mammal; scientific instrument; aquatic mammal; crustacean; archosaur, archosaurian, archosaurian reptile; musical instrument, instrument; tracked vehicle; ascomycete; ungulate, hoofed mammal; chelonian, chelonian reptile; electronic equipment; dummy51; dummy71; dummy11; footwear, legwear; dummy39; dessert, sweet, afters; dummy56; cuculiform bird; fruit; door; carnivore; piciform bird; crocodilian reptile, crocodilian; dummy78; cosmetic; garment; aquatic bird; table; floor cover, floor covering; cartilaginous fish, chondrichthian; sled, sledge, sleigh; coraciiform bird; dummy10; serpentes; rig; vessel, watercraft; cycles; condiment; dummy13; flower; cephalopod, cephalopod mollusk; ratite, ratite bird, flightless bird; reef; spring, fountain, outflow, outpouring, natural spring; apodiform bird; dish; marsupial, pouched mammal; dummy15; tableware; bird of prey, raptor, raptorial bird; measuring instrument, measuring system, measuring device; lamp; passerine, passeriform bird; shore; photographic equipment; home appliance</title>
		<imprint/>
	</monogr>
	<note>Ibizan Podenco. street sign; ballpoint, ballpoint pen, ballpen, Biro; lumbermill, sawmill; king snake, kingsnake; kelpie; rifle; cup; picket fence, paling; golf ball; bakery, bakeshop, bakehouse; trolleybus, trolley coach, trackless trolley; Gordon setter; indri, indris, Indri indri. Indri brevicaudatus; tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui; lotion; fountain pen; black-and-tan coonhound; cobra; cowboy hat, ten-gallon hat; ringneck snake, ring-necked snake, ring snake; wreck; loudspeaker, speaker, speaker unit, loudspeaker system, speaker system; partridge; dial telephone, dial phone; electric fan, blower; bloodhound, sleuthhound; wallaby, brush kangaroo; iguanid, iguanid lizard; sailboat, sailing boat; dome</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Greater Swiss Mountain dog; airliner</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Melursus ursinus, Ursus ursinus; tabby, tabby cat; African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus; drum, membranophone, tympan; church, church building; pencil sharpener; long-horned beetle, longicorn, longicorn beetle; canoe; Persian cat; folding chair; green snake, grass snake; prison, prison house; rock crab, Cancer irroratus; ocarina</title>
	</analytic>
	<monogr>
		<title level="m">cab, hack, taxi, taxicab; macaque; sandal; sulphur butterfly, sulfur butterfly; spaghetti sauce, pasta sauce; bathing cap, swimming cap; gibbon, Hylobates lar; African elephant, Loxodonta africana; soup bowl; miniature pinscher; bittern; pullover, slipover; mobile home, manufactured home; purple gallinule; Band Aid; Italian greyhound; cockatoo; poncho; Doberman</title>
		<meeting><address><addrLine>Lhasa, Lhasa</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>apso; brass, memorial tablet, plaque; Weimaraner; wooden spoon. tiger cat; terrapin; fire engine, fire truck; library; fireboat; bolo tie, bolo, bola tie, bola; lady&apos;s slipper, lady-slipper, ladies&apos; slipper, slipper orchid; electric ray, crampfish, numbfish, torpedo; Norfolk terrier; vestment; pipe; coffee mug</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Boston terrier; swimming trunks, bathing trunks; barber chair; file, file cabinet, filing cabinet; grocery store, grocery, food market, market; limousine, limo; punching bag, punch bag, punching ball, punchball</title>
	</analytic>
	<monogr>
		<title level="m">racer, race car, racing car; scarabaeid beetle, scarabaeid, scarabaean; redbone; totem pole; miniskirt, mini; Boston bull</title>
		<imprint/>
	</monogr>
	<note>face powder; balance beam, beam; warship, war vessel, combat ship; scoreboard; guacamole; hand-held computer, handheld microcomputer; four-poster; neck brace; dowitcher; sports car. sport car; komondor; pay-phone, pay-station; wok; barrow, garden cart, lawn cart, wheelbarrow; fur coat. anguid lizard</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Urocyon cinereoargenteus; little blue heron, Egretta caerulea; rugby ball</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekingese</forename><surname>Pekinese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fox</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>anemone fish; eating apple. dessert apple; maillot; Labrador retriever</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">langur; odometer, hodometer, mileometer, milometer; croquet ball; running shoe; convertible, sofa bed; theater curtain, theatre curtain; pug, pug-dog; pop bottle</title>
		<imprint/>
	</monogr>
	<note>soda bottle; breastplate, aegis, egis; kimono</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Ciconia ciconia; ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle; palace; koala, koala bear</title>
		<imprint/>
	</monogr>
	<note>Phascolarctos cinereus; true frog, ranid; Irish setter. red setter; schipperke; chameleon, chamaeleon; triumphal arch; tiger beetle; garden spider. Aranea diademata</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">bow-tie, bowtie; steam locomotive; summer squash; water bottle; bicycle-built-for-two, tandem bicycle, tandem; Afghan hound, Afghan; ptarmigan; toy terrier; yurt; European fire salamander, Salamandra salamandra; water tower; red-backed sandpiper, dunlin, Erolia alpina; restaurant, eating house, eating place, eatery; stole; ping-pong ball; minivan; academic gown, academic robe, judge&apos;s robe; steel drum; gorilla, Gorilla gorilla; cello, violoncello; wood rabbit, cottontail, cottontail rabbit; rock beauty, Holocanthus tricolor; black grouse; toyshop; garter snake, grass snake; trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi; rubber eraser, rubber, pencil eraser; hamburger, beefburger, burger; black widow, Latrodectus mactans</title>
	</analytic>
	<monogr>
		<title level="m">Strix nebulosa; Crock Pot; golfcart, golf cart; maraca; soccer ball; colobus, colobus monkey; bath towel; bow tie</title>
		<imprint/>
	</monogr>
	<note>Rhodesian ridgeback; great grey owl. great gray owl. goblet; quill, quill pen; school bus; sax, saxophone; hair slide</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ursus Maritimus, Thalarctos maritimus; sea lion; rule, ruler; crash helmet; medicine chest, medicine cabinet</title>
	</analytic>
	<monogr>
		<title level="m">tup; steel arch bridge; magnetic compass; Walker hound, Walker foxhound; lycaenid, lycaenid butterfly; Irish wolfhound; agamid, agamid lizard; cliff dwelling; impala, Aepyceros melampus; EntleBucher; sarong; jersey, Tshirt, tee shirt; fiddler crab; Scotch terrier</title>
		<imprint/>
	</monogr>
	<note>Scottish terrier, Scottie; curlycoated retriever; ice bear, polar bear. shopping cart; chocolate sauce, chocolate syrup; gondola; barn; wolf spider, hunting spider; Yorkshire terrier; ruffed grouse, partridge, Bonasa umbellus</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">rocker; freight car; go-kart</title>
		<imprint/>
	</monogr>
	<note>Mexican hairless; bolete; obelisk; speedboat; horned viper, cerastes, sand viper, horned asp, Cerastes cornutus; barbell; Great Pyrenees; lab coat, laboratory coat; Model T. mashed potato</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">retriever; bullet train, bullet; black stork, Ciconia nigra; mitten</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cancer magister; Leonberg; Madagascar cat, ring-tailed lemur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loafer; Sunscreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lemur catta; tobacco shop</title>
		<imprint/>
	</monogr>
	<note>passenger car, coach, carriage; coot; Sussex spaniel; Dungeness crab. tobacconist shop, tobacconist; jean, blue jean, denim; parallel bars, bars; scabbard</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angora</surname></persName>
		</author>
		<title level="m">Angora rabbit; combination lock; dumbbell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Euarctos americanus; football helmet; pierid, pierid butterfly; Norwegian elkhound, elkhound; iPod; king penguin, Aptenodytes patagonica; chair of state; guenon, guenon monkey; water spaniel; shrine; traffic light, traffic signal, stoplight; Arctic fox, white fox, Alopex lagopus; Siamese cat, Siamese; notebook, notebook computer; toy spaniel; patas, hussar monkey, Erythrocebus patas; spider monkey</title>
	</analytic>
	<monogr>
		<title level="m">collie; watch, ticker; sundial; hartebeest; prayer rug, prayer mat; American black bear, black bear</title>
		<imprint/>
	</monogr>
	<note>Ateles geoffroyi; cocker spaniel. English cocker spaniel, cocker; barbershop; chickadee; water buffalo, water ox, Asiatic buffalo, Bubalus bubalis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">theater, movie theatre, movie house, picture palace; moving van; soft-coated wheaten terrier; forklift; Chihuahua; Pomeranian; corgi, Welsh corgi; ruddy turnstone, Arenaria interpres; bassinet; peacock; jeep, landrover; sea snake; lacertid lizard, lacertid; mountain bike, all-terrain bike, off-roader</title>
		<imprint>
			<publisher>cairn</publisher>
		</imprint>
	</monogr>
	<note>cairn terrier; necklace; barn spider</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">St Bernard; desktop computer; sandglass; can opener, tin opener; mountain sheep; flute, transverse flute; chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour; chow, chow chow; tile roof</title>
	</analytic>
	<monogr>
		<title level="m">Polaroid Land camera; macaw; bullterrier, bull terrier; Saint Bernard</title>
		<imprint/>
	</monogr>
	<note>Kerry blue terrier; Polaroid camera. bull mastiff; Indian elephant, Elephas maximus; American alligator, Alligator mississipiensis; paper towel; sorrel; howler monkey, howler; Japanese spaniel; cleaver, meat cleaver, chopper; minibus; Great Dane; pedestal, plinth, footstall; silky terrier. Sydney silky</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">police wagon, paddy wagon, patrol wagon, wagon, black Maria; merganser, fish duck, sawbill, sheldrake; marimba, xylophone; Airedale, Airedale terrier; abaya; springer spaniel, springer; piano, pianoforte, forte-piano; African grey, African gray, Psittacus erithacus; beer bottle; hand glass, simple microscope</title>
	</analytic>
	<monogr>
		<title level="m">Crocodylus niloticus; lionfish; gong, tam-tam; mortarboard; griffon, Brussels griffon</title>
		<imprint/>
	</monogr>
	<note>proboscis monkey, Nasalis larvatus; Siberian husky; convertible; quail; police van. magnifying glass; titi, titi monkey; African crocodile, Nile crocodile. Belgian griffon; Dutch oven; megalith, megalithic structure; newt, triton</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">dog; bookshop, bookstore, bookstall; clock; beagle; plastic bag; pillow; orangutan, orang, orangutang</title>
		<meeting><address><addrLine>Newfoundland, Newfoundland</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>Pongo pygmaeus</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Arctic wolf, Canis lupus tundrarum; danaid, danaid butterfly</title>
	</analytic>
	<monogr>
		<title level="m">Border terrier; coucal; sunglasses, dark glasses, shades; white wolf</title>
		<imprint/>
	</monogr>
	<note>passenger ship; feather boa, boa; chime, bell, gong; garbage truck, dustcart; consomme; rattlesnake, rattler; ringlet, ringlet butterfly; teapot; holster; baseball; guitar</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">killer, orca, grampus, sea wolf, Orcinus orca; hoopskirt, crinoline</title>
		<imprint/>
	</monogr>
	<note>welcome mat; bluetick; beacon, lighthouse, beacon light, pharos; trifle; bottle opener; gazelle; killer whale,. brain coral; borzoi, Russian wolfhound</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">station wagon, wagon, estate car, beach waggon, station waggon, waggon</title>
	</analytic>
	<monogr>
		<title level="m">affenpinscher, monkey pinscher, monkey dog; three-toed sloth, ai, Bradypus tridactylus; clumber, clumber spaniel; castle; planetarium; cowboy boot; mixing bowl; chimpanzee, chimp, Pan troglodytes; whippet; shed; gasmask, respirator, gas helmet; banjo; cradle; confectionery, confectionary, candy store; Old English sheepdog, bobtail; viaduct; yawl; English setter; German short-haired pointer; pretzel; Christmas stocking; radio telescope, radio reflector; sombrero; drake; cuirass; tiger shark, Galeocerdo cuvieri; leaf beetle, chrysomelid; ground beetle, carabid beetle; Belgian sheepdog</title>
		<imprint>
			<publisher>Bouvier des Flandres</publisher>
		</imprint>
	</monogr>
	<note>Bouviers des Flandres; chainlink fence; basset, basset hound; Appenzeller; butcher shop. meat market; electric locomotive; maillot, tank suit; otterhound, otter hound; mailbag, postbag; ambulance; brown bear, bruin, Ursus arctos</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">English bulldog; monitor, monitor lizard, varan; coffeepot; oboe, hautboy, hautbois; horizontal bar, high bar; black swan, Cygnus atratus; brassiere, bra, bandeau; schnauzer; great white shark, white shark, man-eater, maneating shark, Carcharodon carcharias</title>
	</analytic>
	<monogr>
		<title level="m">Chesapeake Bay retriever; keeshond; true lobster; mastiff; basketball; bulldog</title>
		<imprint/>
	</monogr>
	<note>wine bottle; sea turtle. marine turtle</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">brush-footed butterfly, four-footed butterfly; squirrel monkey</title>
		<imprint/>
	</monogr>
	<note>Saimiri sciureus; diaper, nappy, napkin; tennis ball; boa constrictor, Constrictor constrictor; ornithischian, ornithischian dinosaur</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canis</forename><surname>Dingo</surname></persName>
		</author>
		<title level="m">ship, cargo vessel; backpack, back pack, knapsack, packsack, rucksack, haversack; venomous lizard; bassoon; teiid lizard, teiid; stone wall; bench; mud turtle; hotdog, hot dog</title>
		<meeting><address><addrLine>Shetland</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>red hot; beer glass; turnstile; Shetland sheepdog, Shetland sheep dog</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">These datasets are accompanied by class hierarchies re-calibrated by [26] such that classes on same hierarchy level are of the same visual granularity. More specifically, 1. Entity13 contains 13 superclasses -garment</title>
	</analytic>
	<monogr>
		<title level="m">Tab. 3 we report the results on four ImageNet subpopulation datasets of BREEDS [26] (Entity13, Entity30, Living17, Nonliving26)</title>
		<imprint/>
	</monogr>
	<note>bird; reptile, reptilian; arthropod; mammal, mammalian; accessory, accoutrement, accouterment; craft; equipment; furniture, piece of furniture, article of furniture. instrument</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">man-made structure, construction</title>
		<imprint/>
	</monogr>
	<note>wheeled vehicle; produce, green goods, green groceries, garden truck</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">household appliance; kitchen utensil; measuring instrument, measuring system, measuring device; motor vehicle, automotive vehicle; musical instrument</title>
	</analytic>
	<monogr>
		<title level="m">Entity30 contains 30 superclasses -serpentes; passerine, passeriform bird; saurian; arachnid, arachnoid; aquatic bird; crustacean; carnivore; insect; ungulate</title>
		<imprint/>
	</monogr>
	<note>hoofed mammal; primate; bony fish; barrier; building, edifice; electronic equipment; footwear, legwear; garment; headdress, headgear; home appliance. instrument; neckwear; sports equipment; tableware; tool; vessel, watercraft; dish; vegetable, veggie, veg; fruit</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Living17 contains 17 superclasses -salamander; turtle; lizard; snake, serpent, ophidian; spider; grouse; parrot; crab; dog, domestic dog</title>
		<imprint/>
	</monogr>
	<note>Canis familiaris; wolf; fox; domestic cat, house cat, Felis domesticus, Felis catus; bear; beetle; butterfly; ape; monkey</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">body armour, suit of armor, suit of armour, coat of mail, cataphract; bottle; bus, autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, omnibus</title>
	</analytic>
	<monogr>
		<title level="m">Nonliving26 contains 26 superclasses -bag; ball; boat; body armor</title>
		<imprint/>
	</monogr>
	<note>passenger vehicle; car, auto, automobile, machine, motorcar; chair; coat; digital computer; dwelling, home, domicile, abode, habitation, dwelling house; fence, fencing; hat, chapeau, lid; keyboard instrument; mercantile establishment, retail store, sales outlet, outlet; outbuilding; percussion instrument, percussive instrument; pot; roof; ship; skirt; stringed instrument</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<title level="m">timepiece, timekeeper, horologe; truck, motortruck; wind instrument, wind; squash</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
