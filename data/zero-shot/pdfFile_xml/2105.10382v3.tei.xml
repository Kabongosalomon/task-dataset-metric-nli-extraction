<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning general and distinctive 3D local deep descriptors for point cloud registration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-12">12 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poiesi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
						</author>
						<title level="a" type="main">Learning general and distinctive 3D local deep descriptors for point cloud registration</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-12">12 May 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2022.3175371</idno>
					<note>Disclaimer: This work has been accepted for publication in the IEEE Transactions on Pattern Analysis and Machine Intelligence: Copyright: JOURNAL OF L A T E X CLASS FILES, VOL. NN, NO. N, AUGUST YYYY 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Point cloud registration</term>
					<term>deep learning-based descriptors</term>
					<term>local reference frame</term>
					<term>contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An effective 3D descriptor should be invariant to different geometric transformations, such as scale and rotation, robust to occlusions and clutter, and capable of generalising to different application domains. We present a simple yet effective method to learn general and distinctive 3D local descriptors that can be used to register point clouds that are captured in different domains. Point cloud patches are extracted, canonicalised with respect to their local reference frame, and encoded into scale and rotation-invariant compact descriptors by a deep neural network that is invariant to permutations of the input points. This design is what enables our descriptors to generalise across domains. We evaluate and compare our descriptors with alternative handcrafted and deep learning-based descriptors on several indoor and outdoor datasets that are reconstructed by using both RGBD sensors and laser scanners. Our descriptors outperform most recent descriptors by a large margin in terms of generalisation, and also become the state of the art in benchmarks where training and testing are performed in the same domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D point set registration is the problem of finding an optimal Euclidean transformation to align two partially overlapping 3D point sets such that they can be represented in a common reference frame. 3D point set registration is important for numerous applications including loop detection <ref type="bibr" target="#b0">[1]</ref> and object 6D pose estimation <ref type="bibr" target="#b1">[2]</ref>. 3D point set registration approaches can estimate this transformation from 3D points directly or through geometrically-informed descriptors <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>. In practice, there exists two main approaches. Approaches that iteratively minimise the distance between nearest-neighbouring points if their original reference frames are loosely aligned, e.g. Iterative Closest Point (ICP) <ref type="bibr" target="#b6">[7]</ref>, and approaches that find corresponding points through robust descriptor matching, e.g. RANSAC <ref type="bibr" target="#b7">[8]</ref>. This paper focuses on the latter, in particular on the design of general and distinctive compact descriptors for 3D point clouds that are captured in the real world <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. This problem is particularly challenging because point clouds can be captured with different sensors (e.g. RGBD <ref type="bibr" target="#b8">[9]</ref>, LiDAR <ref type="bibr" target="#b11">[12]</ref>) and in different contexts (e.g. indoors, outdoors), thus requiring descriptors to be effective across different domains.</p><p>Descriptors can be computed with or without a local reference frame (LRF) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>. LRF is a rigid transformation that enables descriptor rotation-invariance <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In turn, descriptors can be handcrafted or based on deep learning. Deep learning-based descriptors <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> largely outperform their handcrafted counterpart <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and can also be computationally efficient if sparse convolutions are used <ref type="bibr" target="#b9">[10]</ref>. These efficient methods can compactly encode local geometric information for each point (dense local descriptors) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, however they lack generalisation ability when trained and tested on different domains. Differently, patch-based descriptors encode local geometric information for a given patch at chosen keypoints <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref>. They have better generalisation ability than their dense counterpart, but reduced efficiency. In this paper we present a novel deep learning-based method to learn General and Distinctive (GeDi) patchbased descriptors for point cloud registration. Given a patch defined as a set of neighbouring points, we achieve rotationinvariance by computing a LRF that aims to rigidly transform the patch into its canonical representation <ref type="bibr" target="#b13">[14]</ref>. We process this canonicalised patch using a point permutation-invariant deep network to produce the unitary-length descriptor of the patch <ref type="bibr" target="#b20">[21]</ref>. Our deep network uses receptive fields with different radii to aggregate local geometric information at multiple scales. To mitigate the problem of a noisily estimated LRF, we learn a quaternion transformation network (QNet) that refines the canonicalisation operation before encoding the points into a descriptor. We train QNet and the encoding network concurrently through a Siamese approach by using contrastive learning <ref type="bibr" target="#b9">[10]</ref>. We perform an extensive evaluation to assess the generalisation and the distinctive abilities of GeDi on three large-scale datasets, i.e. ETH <ref type="bibr" target="#b21">[22]</ref>, KITTI <ref type="bibr" target="#b11">[12]</ref> and 3DMatch <ref type="bibr" target="#b8">[9]</ref>. We compare our method with 19 methods from the literature and show that GeDi outperforms them by a large margin in terms of generalisation ability, while also being as effective as other descriptors when trained and tested on the same domain. The source code is publicly available at https://github.com/fabiopoiesi/gedi. This paper extends our earlier work <ref type="bibr" target="#b4">[5]</ref> in several aspects. We add QNet and modify the deep network architecture used in <ref type="bibr" target="#b4">[5]</ref> in order to process patches at multiple scales. QNet ensures that the transformation applied to the canonicalised points lives in SO(3), property that is not guaranteed with the spatial transformer network (TNet) <ref type="bibr" target="#b4">[5]</ref>, unless orthogonality is enforced, e.g., via a regularisation loss. Without orthogonality, TNet may distort points, thus making multi-scale aggregation inconsistent across neighbourhoods with different sizes. QNet can be trained without needing this regularisation loss or a dedicated Chamfer loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>. We change our training strategy, moving from a farthestpoint sampling of patches to a fully randomised strategy. In <ref type="bibr" target="#b4">[5]</ref>, we processed the same points during training because we relied on data pre-processing as in <ref type="bibr" target="#b14">[15]</ref>. In this new work we perform random sampling at each iteration for the points transformed with the LRF and also for the canonicalised points encoded by the deep network. As a result, randomised sampling of the input point distribution acts as effective data augmentation. We also provide a comprehensive review of related works. Lastly, we significantly expand our experimental evaluation and analysis, considering additional recent methods, adding a new dataset (i.e. KITTI odometry <ref type="bibr" target="#b11">[12]</ref>), and including new qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Descriptors for point cloud registration can be designed by using handcrafted or deep learning-based algorithms, which in turn can be computed with or without local reference frame (LRF). Typically, handcrafted methods encode local geometric structures as histograms of, e.g., point coordinates, surface normals, and/or pairwise point/normal relationships, whereas deep learning-based methods learn dense or sparse representations of local geometric structures. Regardless of their formulation, these descriptors can be utilised with registration algorithms, such as RANSAC <ref type="bibr" target="#b7">[8]</ref>.</p><p>Handcrafted methods without LRF can encode local descriptors by using 3D point coordinates <ref type="bibr" target="#b22">[23]</ref> and surface normals <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>. The former includes the MCOV descriptor that is computed directly from a covariance matrix encoding the correlation between the geometric and the photometric information of a patch <ref type="bibr" target="#b22">[23]</ref>, such as relative angles and RGB values of points. The authors of MCOV show that local geometric representations promote robustness to rigid spatial transformations, clutter and variations of point densities. The latter includes descriptors such as SpinImage <ref type="bibr" target="#b18">[19]</ref>, Fast Point Feature Histograms (FPFH) <ref type="bibr" target="#b2">[3]</ref> and Point-Pair Features (PPF) <ref type="bibr" target="#b1">[2]</ref>. The SpinImage descriptor is based on rotationinvariant parametric representations of points within a patch <ref type="bibr" target="#b18">[19]</ref>. These representations are projected on a 2D image plane and mapped to a descriptor by computing a 2D histogram. FPFH and PPF use features that are computed as distances and relative angles between points and normals. FPFH is defined as the histogram of these features, while PPF is designed as point representations in the form of a hash table in order to make the task of point cloud registration efficient. While FPFH can be used to register large point clouds, PPF is more suitable for object 6D pose estimation.</p><p>Handcrafted LRF-based methods aim to compute a descriptor by firstly estimating a LRF, typically using the points of the patch, and secondly by building a compact representation that encodes the geometric properties of these points transformed by their LRF. This category includes the Intrinsic Shape Signatures (ISS) descriptor that is computed as a weighted covariance matrix that aggregates the geometric properties of points within the patch. The weights penalise distant points from the centroid. The LRF is obtained through the eigenvalue decomposition of this covariance matrix <ref type="bibr" target="#b19">[20]</ref>. ISS is then built using the occupational histogram of a spherical neighbourhood with a predefined radius around the centroid. The Signature of Histograms of Orientations (SHOT) descriptor defines the LRF as the eigenvalue decomposition of the covariance matrix to find two orthogonal axes <ref type="bibr" target="#b12">[13]</ref>. Unlike ISS, SHOT disambiguates the sign of these axes based on the geometric structure of the patch. The third axis is the cross product between these two axes. SHOT is then built as a combination of an occupational histogram and of the relative geometric properties within the patch. To improve robustness to clutter, the Triple Orthogonal Local Depth Images (TOLDI) descriptor extends SHOT's LRF by introducing a weighted aggregation of the points <ref type="bibr" target="#b13">[14]</ref>. Points are projected on the three 2D planes defined by its LRF, and the TOLDI descriptor is then built as the 2D histograms of the projections. Similary to ISS, SHOT and TOLDI, the Rotational Projection Statistics (RoPS) <ref type="bibr" target="#b23">[24]</ref> descriptor uses a covariance matrix to define its LRF, but based on the local connectivity of the points. Points are then projected on the 2D planes defined by its LRF, and the descriptor is built using low-order central moments and entropy of these points.</p><p>Deep learning-based methods without LFR model descriptor rotation invariance by either considering pairs of points, or by learning it via data augmentation, or by simply ignoring it. The 3DMatch approach transforms patches into volumetric voxel grids of Truncated Distance Function (TDF) values and processes them through a 3D convolutional network to output local descriptors <ref type="bibr" target="#b8">[9]</ref>. The PPFNet approach adds normals and PPF representations <ref type="bibr" target="#b1">[2]</ref> of the patch to the point coordinates, and processes this information with a PointNet-based deep network <ref type="bibr" target="#b24">[25]</ref>. PPFNet aggregates local and global features to build context-informed descriptors <ref type="bibr" target="#b3">[4]</ref>. PPF-FoldNet implements the idea of learning descriptors by means of an autoencoder <ref type="bibr" target="#b25">[26]</ref>, which is trained to reconstruct its input PPF representations. PPF-FoldNet discards point coordinate and normal information, and learns to produce descriptors directly from the PPF representations without using supervision <ref type="bibr" target="#b15">[16]</ref>. PPF-FoldNet is based on PointNet. The FCGF approach uses a fully-convolutional deep network to generate descriptors through 3D sparse convolutions <ref type="bibr" target="#b9">[10]</ref>. Because sparse convolutions are efficient to compute, FCGF processes the whole point cloud in one pass and outputs a descriptor per point. Unlike FCGF, D3Feat extracts descriptors by using dense deep features through a KPConv backbone <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The SpinNet approach produces rotation invariant representations by using a spatial point transformer that project the input points to a cylindrical space <ref type="bibr" target="#b5">[6]</ref>. A deep network that is based on 3D cylindrical convolutional layers is then utilised to output patch-based descriptors.</p><p>Deep learning-based methods with LRF aim to compute descriptors by training deep networks to process input points that are canonicalised with respect to their LRF. The 3DSmoothNet approach uses TOLDI's LRF to canonicalise points <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. As 3DMatch, 3DSmoothNet transforms points into volumetric voxel grids and processes them with a 3D convolutional network, however, instead of assigning TDF values to the voxels, 3DSmoothNet computes Gaussian smoothed representations based on the coordinates of the points within the patch. As 3DSmoothNet, the distinctive local 3D descriptors (DIP) use TOLDI to canonicalise points <ref type="bibr" target="#b4">[5]</ref> and PointNet to produce local descriptors. DIP uses a transformation network on the input points to improve eventual noisy canonicalisations. The local multi-view descriptors (LMVD) are computed through multi-viewpoint image rendering through a differentiable renderer <ref type="bibr" target="#b27">[28]</ref>. For a point of interest, LMVD defines the LRF of the viewpoint by using the point normal and a consistent upright orientation. LMVD produces descriptors by extracting local features maps from each viewpoint and by aggregating them via soft-view pooling. Authors in <ref type="bibr" target="#b28">[29]</ref> propose an unsupervised method to learn descriptors by using a Spherical CNN encoder that produces rotation-equivariant representations and a decoder that reconstructs the input points. Authors show how the LRF can be learnt in a end-to-end manner by exploiting the Spherical CNN output as it lives in SO(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head><p>Let X = {x} ? P be a local patch extracted from a point cloud P ? R 3 , where X is an unordered set of 3D points. We design an algorithm that calculates a compact descriptor of</p><formula xml:id="formula_0">X such that f = (? ? ? ?)(X ), where f ? R d is the d- dimensional descriptor of X ,</formula><p>? is the function that samples and canonicalises X through LRF, and ? ? is a deep network with learnable parameters ?. Without loss of generality, we use the 3D coordinates of X to compute f , i.e. x = (x, y, z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local sampling and canonicalisation</head><p>Sampling and canonicalisation involves (i) patch extraction, (ii) LRF estimation, and (iii) sampling of the points within the patch prior to processing them with the deep network.</p><p>Let X = {x :</p><p>x ?x 2 ? r} be the patch, wherex ? P is the patch centre, r is the patch radius and ? 2 is the Euclidean norm. Patches extracted from different regions of P may have different cardinalities due to occlusions or clutter. To learn a general representation of patches having the same geometric structures but with different cardinalities, we randomly sample m points within X . If patches have less than m points, we pad to m points by sampling with replacement from X . Patches with the same cardinality also enable us to efficiently train ? ? using minibatches. Let S m be the sampling function to produce the set of randomly sampled pointsX = S m (X ), whereX ? X and |X | = m.</p><p>The LRF estimation ofX involves the computation of three orthogonal axes: an axis corresponding to the normal of a plane estimated fromX , an axis corresponding to a repeatable vector lying on this plane, and an axis that is orthogonal to the previous two axes. Let Rx be the rotation matrix that we use to transformX from the reference frame of P to the LRF ofX . We use TOLDI to compute Rx <ref type="bibr" target="#b13">[14]</ref>.</p><p>Before transformingX with respect to its LRF, we sample n points fromX , where n &lt; m. Let S n be the sampling function to produce the set of randomly sampled pointsX = S n (X ), whereX ?X and |X | = n. To promote descriptor translation invariance, we represent the coordinates ofX relative to their patch centrex. To promote descriptor scale invariance, we normalise these coordinates relative tox by the radius of the patch r,</p><formula xml:id="formula_1">?(X ) = {y : y = Rx((x ?x)/r), x ?X },<label>(1)</label></formula><p>which becomes the input to our deep network. Note that n &lt; m to speed up the deep network computation, to handle large batches during training and because we experimentally observed that distinctiveness is upperbounded when a certain number of points is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep network design</head><p>Because ?(X ) is an unordered set of points, we use a deep network that is robust to permutations of the input points <ref type="bibr" target="#b20">[21]</ref>. Like DIP <ref type="bibr" target="#b4">[5]</ref>, we use a L2 normalisation layer to transform the network output into a d-dimensional descriptor f with unitary norm. Unlike DIP <ref type="bibr" target="#b4">[5]</ref>, which is computed using a single receptive field for the whole set of input points <ref type="bibr" target="#b24">[25]</ref>, our deep network uses a hierarchical structure of the receptive fields to build high-dimensional representations from local geometric structures at multiple scales <ref type="bibr" target="#b20">[21]</ref>. Our deep network learns to aggregate and to encode these representations using different kernel sizes along the hierarchy, making our descriptor more general and distinctive than DIP. Specifically, to aggregate and encode local geometric representations, we use a PointNet++ deep network <ref type="bibr" target="#b20">[21]</ref>. Because these multi-scale receptive fields cannot be used along with the spatial transformer network used in DIP, we use a different spatial transformer network that outputs a quaternion instead of a 3 ? 3 transformation. Quaternion network. DIP uses a dedicated deep network (TNet <ref type="bibr" target="#b24">[25]</ref>) to estimate a transformation that can mitigate eventual noisy canonicalisations of the input points. Because PointNet++'s grouping layers operate in the metric space through a ball query operation with fixed radius <ref type="bibr" target="#b20">[21]</ref>, TNet's transformation should live in SO(3). TNet's output can be constrained to be a SO(3) transformation via the regularisation loss term reg = I ? AA 2 F , where A ? R 3?3 is TNet's output <ref type="bibr" target="#b24">[25]</ref>. Alternatively, SVD orthogonalisation can be applied to TNet's output to produce a SO(3) transformation. We experimentally observed that these two solutions are suitable for PointNet++ and lead to similar results, however the former needs an extra loss term and weight hyperparameter, while the latter is less efficient due to the SVD orthogonalisation step. Therefore, inspired by <ref type="bibr" target="#b29">[30]</ref>, we introduce a PointNet-based deep network, namely QNet, which outputs a unit-norm quaternion, hence a SO(3) transformation by construction. QNet uses a L2 normalisation layer at the PoinNet's output to produce the unit-norm vector. QNet can be learnt concurrently with PointNet++, and neither requires an additional loss term with a weight hyperparameter to tune, nor adds computational overhead. Training procedure. Descriptors should be similar between corresponding patches and dissimilar between noncorresponding patches of different point clouds. We train our network through a Siamese approach that processes pairs of corresponding patches by using two branches with shared weights <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure">Fig. 1</ref> shows our training architecture.</p><p>Training begins by selecting point cloud pairs that overlap at least ? o <ref type="bibr" target="#b14">[15]</ref>. We randomly sample b pairs of corresponding points from the overlap region, which we treat as our patch centres. We perform local sampling and canonicalisation for each patch (Sec. 3.1), and input the result of this to our Siamese network with shared parameters. Each branch calculates a descriptor for a given patch. We learn the network  <ref type="figure">Fig. 1</ref>. GeDi's training pipeline. Two overlapping point clouds are aligned using a ground-truth transformation (T ). From the overlap region (grey) we randomly sample b patch centroids (black). We use a Siamese approach to train two deep neural networks with shared parameters concurrently.</p><p>Prior to the deep network processing, we perform the following operations: (i) for each centre (black) a patch (red) with radius r is extracted and the corresponding local reference frame is computed using a subset of the points of the patch; (ii) this patch is canonicalised using the local reference frame and n points are randomly sampled from the patch (green points); (iii) the coordinates of these n points are represented relative to the patch centre and normalised to obtain patches with unitary radius; (iv) these n points are given to the deep networks as input to learn the descriptor. The L2-normalisation layer is used to output descriptors with unitary norm. We train the deep network using the hardest-contrastive loss <ref type="bibr" target="#b9">[10]</ref>.</p><p>parameters using a hardest-contrastive loss <ref type="bibr" target="#b9">[10]</ref>, where the hardest negatives are sampled within each minibatch. There exist different strategies to mine negatives. DIP uses a FPSbased approach to avoid selecting negatives that are spatially close to the anchors <ref type="bibr" target="#b4">[5]</ref>. In GeDi, we mine negatives outside a spherical region of radius r C centred on each anchor <ref type="bibr" target="#b9">[10]</ref>. We experimentally found that this mining strategy combined with the random sampling of patches can be implemented efficiently, and enable us to build larger minibatches while promoting randomness during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We assess the generalisation ability of GeDi descriptors by using three large-scale point cloud datasets: 3DMatch <ref type="bibr" target="#b8">[9]</ref>, ETH <ref type="bibr" target="#b21">[22]</ref>, and KITTI <ref type="bibr" target="#b11">[12]</ref>. We follow the same train/test splits as in <ref type="bibr" target="#b5">[6]</ref>. 3DMatch is composed of 62 indoor scenes that are captured with RGBD sensors, containing dense point clouds of reconstructed rooms. The ground-truth transformations are from <ref type="bibr" target="#b14">[15]</ref>. As in <ref type="bibr" target="#b14">[15]</ref>, we also use an augmented version of 3DMatch, namely 3DMatchR, where each point cloud is randomly rotated by an angle within [0 ? , 360 ? ]. ETH includes four outdoor scenes that are captured with a laser scanner, containing partially overlapping point clouds <ref type="bibr" target="#b14">[15]</ref>. These sequences are used for testing only and the ground-truth transformations are from <ref type="bibr" target="#b14">[15]</ref>. KITTI contains 11 sequences of outdoor driving scenarios that are captured using a laser scanner. We follow the same evaluation setup as in <ref type="bibr" target="#b9">[10]</ref>. We compare GeDi with other descriptors using the same evaluation procedures as in <ref type="bibr" target="#b5">[6]</ref>. We compare GeDi against: SpinImage <ref type="bibr" target="#b18">[19]</ref>, FPFH <ref type="bibr" target="#b2">[3]</ref>, USC <ref type="bibr" target="#b30">[31]</ref>, SHOT <ref type="bibr" target="#b12">[13]</ref>, 3DMatch <ref type="bibr" target="#b8">[9]</ref>, CGF <ref type="bibr" target="#b31">[32]</ref>, FoldingNet <ref type="bibr" target="#b25">[26]</ref>, 3DFeat-Net <ref type="bibr" target="#b32">[33]</ref>, PPFNet <ref type="bibr" target="#b3">[4]</ref>, PPF-FoldNet <ref type="bibr" target="#b15">[16]</ref>, DirectReg <ref type="bibr" target="#b16">[17]</ref>, CapsuleNet <ref type="bibr" target="#b17">[18]</ref>, Equivari-ant3D <ref type="bibr" target="#b28">[29]</ref>, PerfectMatch <ref type="bibr" target="#b14">[15]</ref>, FCGF <ref type="bibr" target="#b9">[10]</ref>, D3Feat <ref type="bibr" target="#b10">[11]</ref> (we will use -rand and -pred as suffixes when descriptors are chosen randomly and through D3Feat's keypoint detector, respectively), LMVD <ref type="bibr" target="#b27">[28]</ref>, SpinNet <ref type="bibr" target="#b5">[6]</ref> and DIP <ref type="bibr" target="#b4">[5]</ref>. Our experiments are organised as follows: (i) we train GeDi on 3DMatch, and test it on ETH and KITTI; (ii) we train GeDi on KITTI and test it on 3DMatch; (ii) we train and test GeDi on data of the same dasasets according to their respective original train/test splits; (iv) we measure the inference time and perform an ablation study of GeDi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation measures</head><p>For 3DMatch and ETH, we use the feature-matching recall (FMR) <ref type="bibr" target="#b3">[4]</ref> to quantify the descriptor distinctiveness and to compare our results with other approaches. We use the same FMR's notation as in <ref type="bibr" target="#b4">[5]</ref>: ? indicates FMR, ? ? and ? ? indicate the mean and standard deviation of the average number of correctly matched patch pairs over the point clouds of the dataset, respectively. FMR indirectly measures the number of iterations required by a registration algorithm, e.g. RANSAC, to estimate the transformation between two point clouds <ref type="bibr" target="#b15">[16]</ref>. For KITTI, we randomly select a set of points from each point cloud pair, compute their descriptors, and use RANSAC <ref type="bibr" target="#b7">[8]</ref> to estimate the rigid transformation to register them. We set RANSAC as in <ref type="bibr" target="#b32">[33]</ref>. The estimated rigid transformation is compared to its ground truth by using the Relative Translational Error (RTE) and Relative Rotation Error (RRE) <ref type="bibr" target="#b33">[34]</ref>. We compute the success rate by considering a registration to be successful when both RTE and RRE are less then 2m and 5 ? , respectively. We report the average RTE and RRE values of the successful cases. Unlike RTE and RRE, FMR does not require RANSAC as it directly averages the number of correctly matched point clouds across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameters</head><p>Training. We randomly sample b=350 patch centres in the overlap region between each point cloud pair. In 3DMatch we sample m=4000 points for the LRF computation, while we use m=2000 in KITTI because its point clouds are sparser than those of 3DMatch. We experimentally observed that higher values of m have minor effects on the results. After the LRF computation we sample n=512 points per patch to train our deep network. The descriptor dimension is always set to d=32 as in <ref type="bibr" target="#b5">[6]</ref>. We set r=.5m for 3DMatch and r=2.5m for KITTI. For 3DMatch, we train for 10 epochs, performing 16K iterations per epoch. Each iteration processes a point cloud pair. For KITTI, we train for 122 epochs, performing 1.4K iterations per epoch. We use Stochastic Gradient Descent with an initial learning rate of 10 ?1 that decreases by a factor .1 every 3 epochs in 3DMatch and every 35 epochs in KITTI. We set weight decay to 5?10 ?5 and Dropout to .3 at the last MLP layer. In the hardest-contrastive loss, we set the same margins as in <ref type="bibr" target="#b9">[10]</ref> and the radius for negative mining equal to .2r. Because LRFs may slightly differ across different subsampled versions of the same patch, we learn descriptors that are robust to these variations by randomly rotating canonicalised patches during training. We rotate by independently sampling three angles in the range [?10 ? ,10 ? ]. Testing. We set n=1024 as we found it works well in practice. Patch radii are set to r=.6m in 3DMatch, r=1.5m in ETH and r=2.5m in KITTI. Sec. 4.4 reports our ablation study for n and r. As in <ref type="bibr" target="#b14">[15]</ref>, in 3DMatch and ETH, we compute the descriptors for c=5K randomly sampled points from each point cloud and compute their FMR scores. KITTI is typically tested by computing the descriptor of each point and by estimating the rigid transformation between two point clouds through RANSAC <ref type="bibr" target="#b5">[6]</ref>. KITTI point clouds are about 120K+ points, and the computation of a GeDi descriptor for each point is lengthy. Therefore, we randomly sample c=5K, 25K and 50K points from each point cloud, and compare our results against the other approaches. This setup is more challenging than the original one as it requires a greater discriminative ability of the descriptors as fewer are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative analysis</head><p>RGBD indoors ? ? ? laser scanner outdoors. Tab. 1 shows the results of GeDi trained on 3DMatch and tested on ETH. The results of FPFH, SHOT, 3DMatch, CGF and PerfectMatch are taken from <ref type="bibr" target="#b14">[15]</ref>. The results of FCGF and D3Feat are taken from <ref type="bibr" target="#b10">[11]</ref>. The results of the other methods are taken from the respective papers. GeDi outperforms the other methods by a large margin, improving FMR by 5.4% with respect to DIP <ref type="bibr" target="#b4">[5]</ref> and SpinNet <ref type="bibr" target="#b5">[6]</ref>. Tab. 2 shows the results of GeDi trained on 3DMatch and tested on KITTI. These results are reproduced by us, using authors' source codes and the parameters authors used in this setup. Where possible, we used subsets of points for the evaluation. GeDi consistently outperforms the other methods by a large margin, achieving a success rate of +5.41% and +7.93% compared to DIP and SpinNet, respectively, with c=50K. With c=5K, GeDi achieves +7.92% and +17.65% compared to DIP and SpinNet, respectively.</p><p>The experiments in Tabs. 1 and 2 show that deep learning patch-based descriptors (i.e. PerfectMatch, DIP, SpinNet, GeDi) are the best performing ones, whereas deep learning dense local descriptors, i.e. FCGF and D3Feat, show weaker generalisation ability. Unlike the other methods, D3Feat-pred uses a keypoint detector to predict the most likely points that can provide informative descriptors. Processing entire point clouds in a single pass to produce dense descriptors appears to affect invariance to rotations and to hinder the descriptor generalisation ability on different domains. PerfectMatch, LMVD and SpinNet compute handcrafted input data representations, and process these with 2D or 3D convolutional deep networks: PerfectMatch uses smoothed density value voxelization <ref type="bibr" target="#b14">[15]</ref>, LMVD uses multi-view projections <ref type="bibr" target="#b27">[28]</ref> and SpinNet uses spherical voxelisation <ref type="bibr" target="#b5">[6]</ref>. Although input data representations are designed either to retain most of the original 3D information or to enable invariance to geometric transformations, we can observe that these methods have an overall weaker generalisation ability than architectures that process 3D points directly, like DIP and GeDi. Unlike DIP, GeDi uses PointNet++ to make patch representations more informative through a cascade of receptive fields with different sizes, enabling us to capture and aggregate local geometric features at different scales. The results are taken from the following sources: SpinImage, SHOT, USC, FPFH and Equivariant3D are from <ref type="bibr" target="#b28">[29]</ref>; CGF, 3DMatch and PPFNet are from <ref type="bibr" target="#b3">[4]</ref>; FoldingNet and PPF-FoldNet are from <ref type="bibr" target="#b15">[16]</ref>; the other results are from the respective papers. GeDi outperforms all the other descriptors on both 3DMatch and 3DMatchR, thus showing its better distinctiveness ability also on data of the same domain. <ref type="figure" target="#fig_2">Fig. 2</ref> provides a detailed picture of the results on 3DMatch by .010 D3Feat-rand <ref type="bibr" target="#b10">[11]</ref> .607 .077 .172 .046 D3Feat-pred <ref type="bibr" target="#b10">[11]</ref> .627 .081 .178 .032 SpinNet <ref type="bibr" target="#b5">[6]</ref> .845 .059 .842 .058</p><p>GeDi .922 .059 .919 .059 varying FMR's inlier ratio and inlier distance thresholds <ref type="bibr" target="#b5">[6]</ref>. We can observe that GeDi can produce a larger number of distinctive descriptors than SpinNet, especially when the inlier threshold is .20 we have a +4.8% of FMR. Tab. 5 reports the results of GeDi on KITTI. The results are taken from the original papers. GeDi achieves a success rate of 99.82%; only one pair out of 555 failed the geometric registration. The success rate of GeDi is similar to that of D3Feat. D3Feat's RTE and RRE are .65cm and 9 ? lower than GeDi, respectively. Although D3Feat's translational and rotational errors are lower, we deem GeDi's result comparable to that of D3Feat as the transformation that is estimated with RANSAC can be improved, e.g., by refining it with ICP. Inference time. We compare the inference time of different descriptors that are described in Sec. 2. Experiments are executed using an Intel Xeon CPU E5-1620v3 @ 3.50GHz with NVIDIA GTX 1070 8GB. The chosen methods are FPFH <ref type="bibr" target="#b2">[3]</ref> (PCL implementation using OpenMP), SHOT <ref type="bibr" target="#b12">[13]</ref> (PCL implementation using OpenMP), FCGF <ref type="bibr" target="#b9">[10]</ref> and SpinNet <ref type="bibr" target="#b5">[6]</ref>.   for SHOT, 0.003ms for FCGF, 13.534ms for SpinNet and 1.454ms for GeDi. FCGF resulted to be the fastest method, which is due to its efficient implementation based on the Minkowski Engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Tab. 6 compares GeDi to other descriptors by reducing the number of sampled points from 5000 to 250. The results of the other methods are taken from <ref type="bibr" target="#b5">[6]</ref>. GeDi outperforms SpinNet by .9% on average, and by 3% when 250 points are sampled. According to the FMR formulation, we can observe that if we randomly pick 250 points, 97.3% of them on average are mutually-nearest neighbours in the descriptor space and close to each other in the metric space. Therefore, the likelihood that RANSAC would successfully register two point clouds of this test set is rather high. Next, we analyse a selection of GeDi configurations by using a subset of 3DMatch's training data. We use 7-sceneschess and 7-scenes-fire for training, and 7-scenes-office and 7-scenes-pumpkin for validation. There are 2492 point cloud pairs for training and 1640 for validation. In validation, we randomly sample 5K points per point cloud pair, for a total of 8.2M descriptors. We train with the same parameters that are described in Sec. 4.2. Tab. 7 reports the results on all our experimental configurations. The last row is the configuration we used throughout all our previous experiments.</p><p>The first part of Tab. 7 shows the usefulness of QNet and LRF canonicalisation. GeDi without QNet provides worse results than the version with it. To further investigate this behaviour, we compared the RRE between the rotations estimated by the LRFs and the composition of the latter with the rotations provided by QNet. We analysed ?1.4M matching patch pairs and Tab. 8 includes the results. We can observe that the RRE is smaller after the QNet estimated rotation is applied. LRF is key to achieve descriptor rotation invariance as FMR significantly worsens in 3DMatchR. The <ref type="bibr">TABLE 6</ref> Ablation study using the feature-matching recall (?) as a function of the number of sampled points on the 3DMatch dataset <ref type="bibr" target="#b8">[9]</ref>. second part of Tab. 7 shows the effect on FMR when d changes: we trained and evaluated GeDi using d=16, 64 and 128. FMR improves as d increases, suggesting that the larger the capacity to encode descriptor information, the better the performance. The third part of Tab. 7 shows GeDi's behaviour when the number of points sampled from each patch changes before passing them to the deep network. FMR improves as the number of points increases. Although the deep network is trained using 512 points as input, it can also handle larger numbers of input points, at the cost of a higher inference time. We used n=1024 throughout our experiments to have feedback in reasonable amounts of time. In the fourth part of Tab. 7 we can see that a larger amount of contextual information that is encoded within the descriptor affects distinctiveness. We analyse this behaviour by monitoring the distinctiveness throughout training, and by comparing cases when descriptors are learned using patch radii equal to .6 and 3. We computed the Euclidean distance between a chosen descriptor and all the other descriptors, and colourcoded the point cloud based on this distance. <ref type="figure">Fig. 3</ref> shows some examples of this when the radius is .6 (top) and 3 (bottom). The top row shows a rather localised region of low distances, i.e. only few descriptors in the neighbourhood of the chosen descriptor have a low distance. The bottom row shows that a larger amount of contextual information encoded in the descriptor affects distinctiveness. The patch  <ref type="figure">Fig. 3</ref>. Colour-coded distances (red=large, blue=small) between a descriptor and all the other descriptors. The queried descriptor is located on the chair's seat in the centre of the image. We used descriptor radii equal to .6 (top) and 3 (bottom). Each figure reports the training iteration (iter) and the average distance with respect to all the other descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>radius also affects training time: the descriptor with radius .6 provides better distinctiveness with fewer training iterations than with radius 3. In the last part of Tab. 7 we compare the results between QNet and TNet with enforced orthogonality. We enforce TNet's output orthogonality by using (i) the regularisation loss reg (Sec. 3.2) with weight .1 during training and (ii) the SVD orthogonalisation. We name the former tnet+reg and the latter tnet+svd. Results show that these three solutions are nearly equivalent in terms of FMR. However, as opposed to tnet+reg, we deem the solution with QNet more practical as it does not require neither an additional loss term nor a weighting hyperparameter to tune. We found the solution with QNet computationally more efficient than tnet+svd -we measured a 1.33-time increase when the SVD orthogonalisation step is used. <ref type="figure" target="#fig_3">Fig. 4</ref> shows qualitative registration results computed on ETH, KITTI and 3DMatch datasets using GeDi descriptors trained on 3DMatch. The registration is performed using RANSAC <ref type="bibr" target="#b34">[35]</ref>. Figs. 4a-c are correct registration results. Although GeDi is trained using data of 3DMatch indoor scenes (c), we can see how GeDi can generalise to the outdoor scenes of ETH (a) and KITTI (b). The result in (c) is a case with small overlap where we can notice that the registration is correct but not highly accurate. The point cloud pair in (d) is the incorrect registration result from KITTI discussed in Sec. 4.3. This pair is challenging because the overlap region between the two point clouds contains partial structures with little geometric information. In (e), we can observe that if there are flat surfaces and partially captured objects in the overlap region, the registration may fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We presented a novel approach to train general and distinctive 3D local descriptors through a deep network that is invariant to the permutations of the input points and that processes canonicalised patches. We embedded a quaternion transformation module in our deep network to facilitate patch canonicalisation, which is trained concurrently with the encoding module. Distinctiveness is achieved by training our deep network with a Siamese approach by using a hardestcontrastive loss. Compared to existing deep learning-based 3D descriptors, GeDi can better generalise across domains, while also being as effective as the other descriptors when it is trained and tested in the same domain. Future research directions include building end-to-end trainable LRF-based descriptors, designing a strategy to learn patch radii and improving computational efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Fabio Poiesi and Davide Boscaini are with the Technologies of Vision Lab in Fondazione Bruno Kessler, via Sommarive 18, Trento, 38123, Italy. E-mails: poiesi@fbk.eu, dboscaini@fbk.eu. ? This research was supported by the SHIELD project, funded by the European Union's Joint Programming Initiative -Cultural Heritage, Conservation, Protection and Use joint call, and partially by Provincia Autonoma di Trento (Italy) under L.P. 6/99 as part of the X-Loader4.0 project. Manuscript received mm dd, yyyy; revised mm dd, yyyy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Feature-matching recall as a function (left) of the inlier-ratio and (right) of the inlier-distance thresholds<ref type="bibr" target="#b4">[5]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative registration results on (a) ETH, (b,d) KITTI and (c,e) 3DMatch datasets using GeDi descriptors trained on 3DMatch. The point clouds are in their original reference frame before registration (left-hand side column) and in a common reference frame after registration (righthand side column). We show correct and incorrect registration results, and highlight regions of interest to facilitate the analysis of the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Feature</head><label>1</label><figDesc>Tab. 4 reports the results of GeDi on 3DMatch.</figDesc><table><row><cell></cell><cell cols="7">-matching recall (?): 3DMatch (training) ? ETH (testing).</cell></row><row><cell></cell><cell cols="6">Keys: H: Handcrafted. S: Supervised.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Gazebo</cell><cell cols="2">Wood</cell></row><row><cell cols="2">Method</cell><cell>Type</cell><cell cols="5">Summer Winter Autumn Summer</cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell cols="2">? ?</cell><cell>avg ?</cell></row><row><cell cols="2">FPFH [3]</cell><cell>H</cell><cell>.386</cell><cell>.142</cell><cell>.148</cell><cell cols="2">.208</cell><cell>.221</cell></row><row><cell cols="2">SHOT [13]</cell><cell>H</cell><cell>.739</cell><cell>.457</cell><cell>.609</cell><cell cols="2">.640</cell><cell>.611</cell></row><row><cell cols="2">3DMatch [9]</cell><cell>S</cell><cell>.228</cell><cell>.083</cell><cell>.139</cell><cell cols="2">.224</cell><cell>.169</cell></row><row><cell cols="2">CGF [32]</cell><cell>S</cell><cell>.375</cell><cell>.138</cell><cell>.104</cell><cell cols="2">.192</cell><cell>.202</cell></row><row><cell cols="3">PerfectMatch [15] S</cell><cell>.913</cell><cell>.841</cell><cell>.678</cell><cell cols="2">.728</cell><cell>.790</cell></row><row><cell cols="2">FCGF [10]</cell><cell>S</cell><cell>.228</cell><cell>.100</cell><cell>.148</cell><cell cols="2">.168</cell><cell>.161</cell></row><row><cell cols="2">D3Feat-rand [11]</cell><cell>S</cell><cell>.457</cell><cell>.239</cell><cell>.130</cell><cell cols="2">.224</cell><cell>.262</cell></row><row><cell cols="2">D3Feat-pred [11]</cell><cell>S</cell><cell>.859</cell><cell>.630</cell><cell>.496</cell><cell cols="2">.480</cell><cell>.616</cell></row><row><cell cols="2">LMVD [28]</cell><cell>S</cell><cell>.853</cell><cell>.720</cell><cell>.840</cell><cell cols="2">.783</cell><cell>.799</cell></row><row><cell cols="2">DIP [5]</cell><cell>S</cell><cell>.908</cell><cell>.886</cell><cell>.965</cell><cell cols="2">.952</cell><cell>.928</cell></row><row><cell cols="2">SpinNet [6]</cell><cell>S</cell><cell>.929</cell><cell>.917</cell><cell>.922</cell><cell cols="2">.944</cell><cell>.928</cell></row><row><cell>GeDi</cell><cell></cell><cell>S</cell><cell>.989</cell><cell>.965</cell><cell>.974</cell><cell cols="2">1.000</cell><cell>.982</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 2</cell><cell></cell><cell></cell></row><row><cell cols="8">Relative Translational Error (RTE) and Relative Rotation Error (RRE):</cell></row><row><cell></cell><cell cols="6">3DMatch (training) ? KITTI (testing).</cell></row><row><cell>c</cell><cell>Method</cell><cell></cell><cell cols="4">RTE[cm] avg ? std ? avg ? std ? RRE[ ? ]</cell><cell>Suc.[%]?</cell></row><row><cell></cell><cell>FCGF [10]</cell><cell></cell><cell>31.21</cell><cell>2.87</cell><cell>1.60</cell><cell>1.15</cell><cell>34.95</cell></row><row><cell></cell><cell cols="3">D3Feat-rand [11] 41.39</cell><cell>3.48</cell><cell>1.68</cell><cell>1.25</cell><cell>20.72</cell></row><row><cell>5K</cell><cell cols="3">D3Feat-pred [11] 33.66 SpinNet [6] 16.67</cell><cell>3.12 1.64</cell><cell>1.50 1.13</cell><cell>1.18 0.92</cell><cell>38.74 65.41</cell></row><row><cell></cell><cell>DIP [5]</cell><cell></cell><cell>13.58</cell><cell>1.25</cell><cell>0.93</cell><cell>0.92</cell><cell>75.14</cell></row><row><cell></cell><cell>GeDi</cell><cell></cell><cell>10.34</cell><cell>0.70</cell><cell>0.40</cell><cell>0.49</cell><cell>83.06</cell></row><row><cell></cell><cell>FCGF [10]</cell><cell></cell><cell>28.18</cell><cell>2.46</cell><cell>1.63</cell><cell>1.19</cell><cell>44.68</cell></row><row><cell></cell><cell cols="3">D3Feat-rand [11] 34.23</cell><cell>2.87</cell><cell>1.54</cell><cell>1.19</cell><cell>24.50</cell></row><row><cell>25K</cell><cell cols="3">D3Feat-pred [11] 35.67 SpinNet [6] 12.53</cell><cell>3.22 1.76</cell><cell>1.61 0.76</cell><cell>1.25 0.68</cell><cell>22.16 86.67</cell></row><row><cell></cell><cell>DIP [5]</cell><cell></cell><cell>9.84</cell><cell>0.85</cell><cell>0.56</cell><cell>0.61</cell><cell>91.17</cell></row><row><cell></cell><cell>GeDi</cell><cell></cell><cell>8.43</cell><cell>0.75</cell><cell>0.40</cell><cell>0.46</cell><cell>97.30</cell></row><row><cell>all</cell><cell>FCGF [10] D3Feat [11]</cell><cell></cell><cell>25.93 37.71</cell><cell>2.45 3.25</cell><cell>1.52 1.54</cell><cell>1.14 1.22</cell><cell>45.77 25.77</cell></row><row><cell>50K</cell><cell>SpinNet [6] DIP [5]</cell><cell></cell><cell>12.03 9.45</cell><cell>1.11 0.73</cell><cell>0.68 0.48</cell><cell>0.70 0.59</cell><cell>90.99 93.51</cell></row><row><cell></cell><cell>GeDi</cell><cell></cell><cell>8.21</cell><cell>0.70</cell><cell>0.40</cell><cell>0.51</cell><cell>98.92</cell></row><row><cell cols="8">Laser scanner outdoors ? ? ? RGBD indoors. Tab. 3 confirms</cell></row><row><cell cols="8">GeDi's generalisation ability across different sensors and</cell></row><row><cell cols="8">contexts, i.e. training on KITTI and testing on 3DMatch. The</cell></row><row><cell cols="8">results of FCGF, D3Feat and SpinNet are taken from [6].</cell></row><row><cell cols="8">FCGF and D3Feat show weaker generalisation ability than</cell></row><row><cell cols="8">SpinNet and GeDi, and a significant drop in FMR on</cell></row><row><cell cols="8">3DMatchR, suggesting that they cannot effectively encode</cell></row><row><cell cols="8">general rotation-invariant properties. GeDi outperforms Spin-</cell></row><row><cell cols="8">Net by 7.7% both on the original and on the augmented ver-</cell></row><row><cell cols="8">sion of 3DMatch, reinforcing that GeDi's design to produce</cell></row><row><cell cols="8">descriptors from raw 3D points promotes generalisation.</cell></row><row><cell cols="2">Same settings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3</head><label>3</label><figDesc>Feature-matching recall (?): KITTI (training) ? 3DMatch (testing).</figDesc><table><row><cell>Method</cell><cell cols="2">3DMatch ? ? std ?</cell><cell>3DMatchR ? ? std ?</cell></row><row><cell>FCGF [10]</cell><cell>.325</cell><cell>.074</cell><cell>.010</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">Feature-matching recall (?): 3DMatch (training) ? 3DMatch (testing).</cell></row><row><cell></cell><cell cols="4">Keys: U: Unsupervised. S: Supervised. H: Handcrafted.</cell></row><row><cell cols="2">c Method</cell><cell>Type</cell><cell cols="3">3DMatch ? ? std ? ? ? std ? dim. ? 3DMatchR Feat.</cell></row><row><cell></cell><cell>FoldingNet [26]</cell><cell>U</cell><cell cols="2">.613 .087 .023 .010</cell><cell>512</cell></row><row><cell>2048</cell><cell>PPFNet [4] DirectReg [17] CapsuleNet [18]</cell><cell>S S U</cell><cell cols="2">.623 .108 .003 .005 .746 .094 --.787 .084 .807 .062</cell><cell>64 512 512</cell></row><row><cell></cell><cell>GeDi</cell><cell>S</cell><cell cols="2">.979 .022 .977 .025</cell><cell>32</cell></row><row><cell></cell><cell>CGF [32]</cell><cell>S</cell><cell cols="2">.478 .094 .499 .092</cell><cell>32</cell></row><row><cell></cell><cell>3DMatch [9]</cell><cell>S</cell><cell cols="2">.596 .073 .011 .010</cell><cell>512</cell></row><row><cell></cell><cell>SpinImage [19]</cell><cell>H</cell><cell cols="2">.633 .106 .639 .098</cell><cell>153</cell></row><row><cell></cell><cell>PPF-FoldNet [16]</cell><cell>U</cell><cell cols="2">.718 .105 .731 .104</cell><cell>512</cell></row><row><cell></cell><cell>FPFH [3]</cell><cell>H</cell><cell cols="2">.754 .071 .767 .075</cell><cell>33</cell></row><row><cell></cell><cell>USC [31]</cell><cell>H</cell><cell cols="2">.868 .052 .877 .053</cell><cell>1980</cell></row><row><cell></cell><cell>SHOT [13]</cell><cell>H</cell><cell cols="2">.875 .034 .875 .036</cell><cell>352</cell></row><row><cell>5000</cell><cell>Equivariant3D [29] PerfectMatch [15] DIP [5]</cell><cell>U S S</cell><cell cols="2">.942 .040 .939 .048 .947 .027 .949 .025 .948 .046 .946 .046</cell><cell>512 32 32</cell></row><row><cell></cell><cell>FCGF [10]</cell><cell>S</cell><cell cols="2">.952 .029 .953 .033</cell><cell>32</cell></row><row><cell></cell><cell>D3Feat-rand [11]</cell><cell>S</cell><cell cols="2">.953 .027 .952 .032</cell><cell>32</cell></row><row><cell></cell><cell>D3Feat-pred [11]</cell><cell>S</cell><cell cols="2">.958 .029 .955 .035</cell><cell>32</cell></row><row><cell></cell><cell>LMVD [28]</cell><cell>S</cell><cell>.975 .028 .969</cell><cell>-</cell><cell>32</cell></row><row><cell></cell><cell>SpinNet [6]</cell><cell>S</cell><cell cols="2">.976 .019 .975 .019</cell><cell>32</cell></row><row><cell></cell><cell>GeDi</cell><cell>S</cell><cell cols="2">.979 .022 .976 .027</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We use a point cloud from 3DMatch composed of about 41K points and a patch radius of .6m for the local descriptors. FCGF requires point cloud voxelisation: we use a 2.5cm voxel size, which reduces the number of points processed by the deep network to about 16.5K. We measured the inference time per descriptor and obtained 1.598ms for FPFH, 5.544ms</figDesc><table><row><cell></cell><cell></cell><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>feature-match recall</cell><cell>.8 .9</cell><cell></cell><cell></cell><cell></cell><cell>feature-match recall</cell><cell>.8 .9</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>.7</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>.03</cell><cell>.05</cell><cell>.08 inlier ratio threshold .10 .13 .15</cell><cell>.18</cell><cell>.20</cell><cell>.03</cell><cell>.05</cell><cell cols="2">.08 inlier distance threshold [m] .10 .13 .15</cell><cell>.18</cell><cell>.20</cell></row><row><cell></cell><cell></cell><cell cols="2">FCGF</cell><cell>PerfectMatch</cell><cell></cell><cell>D3Feat</cell><cell></cell><cell cols="2">DIP</cell><cell>SpinNet</cell><cell>GeDi</cell></row><row><cell></cell><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>feature-match recall</cell><cell>.8 .9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">.03 .05 .08 .10 .13 .15 .18 .20 inlier distance threshold [m]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">Relative Translational Error (RTE) and Relative Rotation Error (RRE):</cell></row><row><cell cols="5">KITTI (training) ? KITTI (testing).</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">RTE[cm] avg ? std ? avg ? std ? RRE[ ? ]</cell><cell>Suc.[%]?</cell></row><row><cell>3DFeat-Net [33]</cell><cell>25.9</cell><cell>26.2</cell><cell>.57</cell><cell>.46</cell><cell>95.97</cell></row><row><cell>FCGF [10]</cell><cell>9.52</cell><cell>1.30</cell><cell>.30</cell><cell>.28</cell><cell>96.57</cell></row><row><cell>DIP (25K pts) [5]</cell><cell>8.69</cell><cell>.69</cell><cell>.44</cell><cell>.49</cell><cell>97.30</cell></row><row><cell>SpinNet [6]</cell><cell>9.88</cell><cell>.50</cell><cell>.47</cell><cell>.09</cell><cell>99.10</cell></row><row><cell>D3Feat [11]</cell><cell>6.90</cell><cell>.30</cell><cell>.24</cell><cell>.06</cell><cell>99.81</cell></row><row><cell>GeDi (25K pts)</cell><cell>7.22</cell><cell>.63</cell><cell>.32</cell><cell>.27</cell><cell>99.46</cell></row><row><cell>GeDi (50K pts)</cell><cell>7.55</cell><cell>.67</cell><cell>.33</cell><cell>.31</cell><cell>99.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? ? ? ? ?? ? ? ? ? ? ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3"># sampled points</cell></row><row><cell></cell><cell></cell><cell cols="3">5000 2500 1000</cell><cell>500</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>? ?</cell><cell>avg ?</cell></row><row><cell cols="2">PerfectMatch [15]</cell><cell>.947</cell><cell>.942</cell><cell>.926</cell><cell cols="2">.901 .829</cell><cell>.909</cell></row><row><cell>FCGF [10]</cell><cell></cell><cell>.952</cell><cell>.955</cell><cell>.946</cell><cell cols="2">.930 .899</cell><cell>.936</cell></row><row><cell cols="2">D3Feat-rand [11]</cell><cell>.953</cell><cell>.951</cell><cell>.942</cell><cell cols="2">.936 .908</cell><cell>.938</cell></row><row><cell cols="2">D3Feat-pred [11]</cell><cell>.958</cell><cell>.956</cell><cell>.946</cell><cell cols="2">.943 .933</cell><cell>.947</cell></row><row><cell cols="2">SpinNet [6]</cell><cell>.976</cell><cell>.975</cell><cell>.973</cell><cell cols="2">.963 .943</cell><cell>.966</cell></row><row><cell>GeDi</cell><cell></cell><cell>.979</cell><cell>.977</cell><cell>.976</cell><cell cols="2">.972 .973</cell><cell>.975</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE 7</cell><cell></cell><cell></cell></row><row><cell cols="7">Ablation study on different GeDi's implementation settings.</cell></row><row><cell cols="7">Keys: ST: Spatial Transformer network. LRF: Local Reference Frame.</cell></row><row><cell cols="7">The last row includes our setting in all the other experiments.</cell></row><row><cell cols="7">d ?? ? ? 32 1024 .6 n r[m] ST 3DMatch LRF .917 .254 .182 .919 .255 .182 3DMatchR</cell></row><row><cell cols="2">32 1024 .6</cell><cell>qnet</cell><cell cols="4">.787 .206 .191 .019 .007 .018</cell></row><row><cell cols="2">16 1024 .6</cell><cell>qnet</cell><cell cols="4">.902 .222 .164 .905 .224 .164</cell></row><row><cell cols="2">64 1024 .6</cell><cell>qnet</cell><cell cols="4">.937 .268 .186 .934 .269 .186</cell></row><row><cell cols="2">128 1024 .6</cell><cell>qnet</cell><cell cols="4">.937 .277 .189 .942 .277 .190</cell></row><row><cell>32 256</cell><cell>.6</cell><cell>qnet</cell><cell cols="4">.903 .206 .146 .896 .207 .146</cell></row><row><cell>32 512</cell><cell>.6</cell><cell>qnet</cell><cell cols="4">.928 .252 .176 .926 .253 .176</cell></row><row><cell cols="2">32 1536 .6</cell><cell>qnet</cell><cell cols="4">.934 .269 .187 .934 .268 .187</cell></row><row><cell cols="2">32 2048 .6</cell><cell>qnet</cell><cell cols="4">.937 .269 .187 .935 .269 .187</cell></row><row><cell cols="2">32 1024 .2</cell><cell>qnet</cell><cell cols="4">.741 .128 .112 .741 .128 .113</cell></row><row><cell cols="2">32 1024 1.0</cell><cell>qnet</cell><cell cols="4">.766 .178 .166 .763 .179 .167</cell></row><row><cell cols="2">32 1024 1.4</cell><cell>qnet</cell><cell cols="4">.510 .100 .121 .506 .101 .122</cell></row><row><cell cols="2">32 1024 1.8</cell><cell>qnet</cell><cell cols="4">.315 .059 .084 .306 .058 .085</cell></row><row><cell cols="7">32 1024 .6 tnet+reg .932 .265 .183 .930 .266 .184</cell></row><row><cell cols="7">32 1024 .6 tnet+svd .934 .268 .186 .939 .269 .187</cell></row><row><cell cols="2">32 1024 .6</cell><cell>qnet</cell><cell cols="4">.932 .267 .185 .931 .268 .186</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>RRE statistics before and after QNet computed on the ablation dataset. About 1.4M matching patch pairs are used in this analysis.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">median mean</cell><cell>std</cell></row><row><cell cols="2">before QNet</cell><cell>0.909</cell><cell>1.318</cell><cell>1.103</cell></row><row><cell cols="2">after QNet</cell><cell>0.497</cell><cell>1.087</cell><cell>1.137</cell></row><row><cell>iter: 5, avg dist: 0.034</cell><cell cols="4">iter: 1500, avg dist: 0.950 iter: 13450, avg dist: 1.031</cell></row><row><cell>iter: 5, avg dist: 0.025</cell><cell cols="4">iter: 1500, avg dist: 0.902 iter: 13450, avg dist: 0.959</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Loop closure detection using local 3D deep descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RAL</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6335" to="6342" />
			<date type="published" when="2022-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model Globally, Match Locally: Efficient and Robust 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast point feature histograms for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PPFNet: Global context aware local features for robust 3D point matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinctive 3D local deep descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Poiesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="341" to="406" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3DMatch: Learning the matching of local 3D geometry in range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully Convolutional Geometric Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SHOT: Unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TOLDI: An effective and robust approach for 3D local shape description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The perfect match: 3D point cloud matching with smoothed densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andreas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised learning of rotation invariant 3D local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D local features for direct pairwise registration</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3D scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intrinsic shape signatures: A shape descriptor for 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Challenging data sets for point cloud registration algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1705" to="1711" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">MCOV: a covariance descriptor for fusion of texture and shape features in 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cirujeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mateo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rotational projection statistics for 3D local surface description and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FoldingNet: Point cloud autoencoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">KPConv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning an Effective Equivariant 3D Descriptor Without Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spezialetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Iterative Transformer Network for 3D Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unique shape context for 3D data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning compact geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3DFeat-Net: Weakly supervised local 3D features for point cloud registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and accurate registration of structured point clouds with small overlaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
