<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Patch-VQ: &apos;Patching Up&apos; the Video Quality Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Ying</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maniratnam</forename><surname>Mandal</surname></persName>
							<email>mmandal@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
							<email>deeptigp@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook</orgName>
								<address>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Bovik</surname></persName>
							<email>bovik@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Patch-VQ: &apos;Patching Up&apos; the Video Quality Problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, "in-the-wild" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>User-generated content (UGC) and video streaming has exploded on social media platforms such as Facebook, Instagram, YouTube, and TikTok, each supporting millions and billions of users <ref type="bibr" target="#b0">[1]</ref>. It has been estimated that each day, about 4 billion video views occur on Facebook [2] and 1 billion hours are viewed on YouTube <ref type="bibr" target="#b2">[3]</ref>. Given the tremendous prevalence of Internet video, it would be of great value to measure and control the quality of UGC videos, both in capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.</p><p>Full-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on * ? Equal contribution ? The entity that conducted all of the data collection/experimentation. smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are unable to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR models are successfully deployed at the largest scales <ref type="bibr" target="#b3">[4]</ref>, NR video quality prediction on UGC content remains largely unsolved, for several reasons. First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, imperfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distortions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is wellknown that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality <ref type="bibr" target="#b4">[5]</ref>, because of neurophysiological processes that induce masking <ref type="bibr" target="#b5">[6]</ref>. Indeed, equal amounts of distortions may very differently affect the quality of two different videos <ref type="bibr" target="#b6">[7]</ref>.</p><p>Second, most existing video quality resources are too small and unrepresentative of the complex real-world distortions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. While three publicly avail-able databases of authentically distorted UGC videos are available <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>, they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distortions, subjectively rated by large numbers of human viewers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than standard object or action classification tasks.</p><p>Finally, although a few NR algorithms achieve reasonable performance on small databases <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>, most of them fail to account for the complex spacetime distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may significantly impact the overall perceived quality of a video <ref type="bibr" target="#b25">[25]</ref>. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.</p><p>We have made recent progress towards addressing these challenges, by learning to model the relationships that exist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches <ref type="figure" target="#fig_0">(Fig. 1</ref>), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them. This unique data collection allowed us to successfully learn to exploit interactions between local and global video quality perception and to create algorithms that accurately predict video quality and space-time quality maps. We summarize our contributions below:</p><p>? We built the largest video quality database in existence. We sampled hundreds of thousands of open source Internet UGC digital videos to match the feature distributions of social media UGC videos. Our final collection includes 39, 000 real-world videos of diverse sizes, contents, and distortions, 26 times larger than the most recent UGC dataset <ref type="bibr" target="#b17">[17]</ref>. We also extracted three types of v-patches from each video, yielding 117, 000 space-time video patches ("v-patches") in total (Sec. 3.1). ? We conducted the largest subjective video quality study to date. Our final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2). ? We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ <ref type="bibr" target="#b29">[29]</ref>, in parallel with 3D features using ResNet3D <ref type="bibr" target="#b30">[30]</ref>. The 2D and 3D features feed a time series regressor <ref type="bibr" target="#b31">[31]</ref> that learns to accurately predict both global video, as well as local spacetime v-patch quality, by exploiting the relations between them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller "in-the-wild" databases <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b14">15]</ref>, without finetuning (Secs. 4.1 and 5.3). ? We also create another unique prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Quality Datasets: Several public legacy video quality datasets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> have been developed in the past decade. Each of these datasets comprise a small number of unique source videos (typically 10-15), which are manually distorted by one of a few synthetic impairments (e.g., Gaussian blur, compression, and transmission artifacts). Hence, these datasets are quite limited in terms of content diversity and distortion complexity, and do not capture the complex characteristics of UGC videos. Early "inthe-wild" datasets <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref> included fewer than 100 unique contents, while more recent ones such as KoNViD-1k <ref type="bibr" target="#b14">[15]</ref>, LIVE-VQC <ref type="bibr" target="#b16">[16]</ref>, and YouTube-UGC <ref type="bibr" target="#b17">[17]</ref> contain relatively more videos (500-1500 per dataset), yet insufficient to train deep models. A more recent dataset, FlickrVid-150k <ref type="bibr" target="#b32">[32]</ref> claims to contain a large number of videos, yet, has the following notable drawbacks: (a) Only 5 quality ratings were collected on each video which, given the complexity of the task, are insufficient to compute reliable ground truth quality scores (at least 15-18 is recommended <ref type="bibr" target="#b33">[33]</ref>). (b) the database is not publicly available, hence limiting its use for any experiments or to validate its statistical integrity. (c) the videos are all drawn from Flickr, which is largely populated by professional and advanced amateur photographers, hence is not representative of social media UGC content. Shallow NR VQA models: Early NR VQA models were distortion specific <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40]</ref> and focused mostly on transmission and compression related artifacts. More recent and widely-used NR image quality prediction algorithms have been applied to frame difference statistics to create space-time video distortion models <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref>. In all these models, handcrafted statistical features are used to train shallow regression models to predict perceptual video quality, achieving high performance on legacy datasets. Recently proposed models <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> use dozens or hundreds of such perceptually relevant features and achieve state-of-the-art performance on the leading UGC datasets, yet their predictive capability remains far below human performance.</p><p>Deep NR VQA models: There is more progress in the development of top-performing deep models for NR image quality prediction <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref>, but relatively fewer deep NR-VQA models exist. The authors of <ref type="bibr" target="#b50">[50]</ref> proposed a general-purpose NR VQA framework based on weakly supervised learning and a resam- pling strategy. The NR VSFA <ref type="bibr" target="#b24">[24]</ref> model uses a CNN to extract frame-wise features followed by a gated recurrent unit to capture temporal features. These, and other attempts <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b24">24]</ref> mostly perform well on legacy datasets <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> and struggle on in-the-wild UGC datasets <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b14">15]</ref>. MLSP-VQA <ref type="bibr" target="#b32">[32]</ref> reports high performance on <ref type="bibr" target="#b14">[15]</ref>, but their code is not available, and we have been unable to reproduce their reported results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Large-Scale Dataset and Human Study</head><p>Next, we present details of the newly constructed video quality dataset and the subjective quality study we conducted on it. The new database includes 39, 075 videos and 117, 225 "v-patches" extracted from them, on which we collected about 5.5M quality scores in total from around 6, 300 unique subjects. This new resource is significantly larger and more diverse than any legacy (synthetic distortion) databases <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> or in-the-wild crowd-sourced datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref> (26 times larger than <ref type="bibr" target="#b17">[17]</ref>). We refer to the proposed dataset as the Large-Scale Social Video Quality (LSVQ) Database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Building the Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">UGC-Like Data Collection and Sampling</head><p>We selected two large public UGC video repositories to source our data: the Internet Archive (IA) <ref type="bibr" target="#b53">[53]</ref> and YFCC-100M <ref type="bibr" target="#b54">[54]</ref>, and collected a total of 400, 000 videos from them. Each video was randomly cropped to an average duration 7 seconds * using ffmpeg <ref type="bibr" target="#b55">[55]</ref>. Sampling "UGC-like" videos: Our dataset distinguishes itself from other in-the-wild video datasets in several ways. First, unlike KoNViD-1k <ref type="bibr" target="#b14">[15]</ref>, we did not restrict the collected videos to have fixed resolutions or aspect ratios, making the proposed dataset much more representative of realworld content. Second, we did not apply scaling or further processing which could affect the quality of the content. Finally, to obtain "UGC-like" videos, we used a mixed integer programming method <ref type="bibr" target="#b56">[56]</ref> to match a set of UGC feature histograms. Specifically, we computed the following 26 * Cropping to a fixed duration was not possible, since a video must begin with a key frame to be decoded properly. holistic spatial and temporal features on two video collections: (a) our aforementioned 400K video collection from IA and YFCC-100M and (b) 19K public, randomly selected videos from a social media website:</p><formula xml:id="formula_0">? Absolute Luminance L = R + G + B.</formula><p>? Colorfulness using <ref type="bibr" target="#b57">[57]</ref>.</p><p>? RMS Luminance Contrast <ref type="bibr" target="#b58">[58]</ref>. ? Number of detected faces using <ref type="bibr" target="#b59">[59]</ref>.</p><p>? Spatial Gaussian Derivative Filters (3 scales, 2 orientations) from Leung-Malik filter bank <ref type="bibr" target="#b60">[60]</ref>. ? Temporal Gaussian Derivatives (3 scales) first averaged along temporal dimension, followed by computing the mean and standard deviation along the spatial dimension.</p><p>The first five (spatial) features were computed on each frame, then the means and standard deviations of these features across all frames were obtained as the final features. As mentioned, we sampled and matched feature histograms and in the end, arrived at about 39,000 videos, with roughly equal amounts from IA and YFCC-100M. <ref type="figure" target="#fig_1">Fig.  2</ref> shows 16 randomly selected video frames from LSVQ, while <ref type="figure" target="#fig_2">Fig. 3</ref> plots the diverse sizes, aspect ratios and durations of the final set of videos. It is evident that we obtained a diverse UGC video dataset that is representative in content, resolution, aspect ratios, and distortions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Cropping Video-Patches</head><p>To closely study and model the relationship between global and local spatio-temporal qualities, we randomly cropped three different kinds of video patches or "v-patches" from each video: a spatial v-patch (sv-patch), a temporal v-patch (tv-patch), and a spatio-temporal v-patch (stvpatch). All three patches are videos obtained by cropping an original video in space, time, or both space and time, respectively ( <ref type="figure" target="#fig_3">Fig. 4</ref>). All v-patches have the same spatial aspect ratios as their source videos. Each sv-patch has the same temporal duration as their source videos, but cropped to 40% of spatial dimensions (16% of area). Each tvpatch has the same spatial size as its source, but clipped to 40% of temporal duration. Finally, each stv-patch was cropped to 40% along all three dimensions. Every v-patch is entirely contained within its source, but the volumetric overlap of each sv-patch and tv-patch with the same-source stv-patch did not exceed 25% (suppl. material). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Subjective Quality Study</head><p>Amazon Mechanical Turk (AMT) was used to collect human opinions on the videos and v-patches as in other studies <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b62">62]</ref>. We launched two separate AMT tasks -one for videos and the other for the three video patches. A total of 6, 284 subjects were allowed to participate on both tasks. On average, we collected 35 ratings on each video and v-patch. Subjects could participate in our study through desktops, laptops, or mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">AMT Study Design</head><p>The human intelligence task (HIT) pipeline is shown in <ref type="figure" target="#fig_4">Fig.  5</ref>. Each task began with general instructions, followed by a related quiz to check subjects' comprehension of the instructions, which they had to pass to proceed further. During training, each subject rated 5 videos to become famil- iar with the interface and the task. Then, they entered the testing phase, in which they rated 90 videos. Each video was played only once, following which the subject rated the video quality on a scale of 0-100 by sliding a cursor along the rating bar (suppl. material). Subjects could report a video as inappropriate (violent or pornographic), static or incorrectly oriented. We ensured that each video was downloaded before playback to avoid rebuffering and stalling. At the end, each subject answered several survey questions about the study conditions and their demographics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Subject Rejection</head><p>Next, we summarize the several checks we employed at various stages of the AMT task to identify and eliminate unreliable subjects <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b61">61]</ref> and participants with inadequate processing or network resources. During Instructions: If a participant's browser window resolution, version, zoom, and the time taken to load videos did not meet our requirements (suppl. material), they were not allowed to proceed. During Training: Although we ensured that each video was entirely downloaded prior to viewing, we also checked for any potential device-related video stalls. If the delay on any training video exceeded 2 seconds, or the total delay over the five training videos exceeded 5 seconds, the subject was not allowed to proceed (without prejudice). They were also stopped if a negative delay was detected (e.g., using plugins to speed up the video). During Task: At the middle of each subject's task, we checked for instability of the internet connection, and if more than 50% of the videos viewed until then had suffered from hardware stalls, the subject was disqualified. We also checked whether the subject had been giving similar quality scores to all videos, or was nudging the slider only slightly, both indicative of insincere ratings. Post task: In the test phase, of the 90 videos, 4, chosen at random, were repeated (seen twice at separate points), while another 4 were "golden" videos from KoNViD-1k <ref type="bibr" target="#b14">[15]</ref>, for which subjective ratings were available. After each task, we rejected a subject if their scores on the same repeated videos or on the gold standard videos were not similar enough.</p><p>Through all these careful checks, a total of 1,046 subjects were rejected over all sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Data Cleaning</head><p>Following subject rejection, we conducted extensive data cleaning: (1) We excluded all scores provided by the subjects who were blocked, or for whom &gt; 50% of the videos stalled during a session.</p><p>(2) We removed ratings given by people who did not wear their prescribed lenses during the study (1.13%), as uncorrected vision could affect perceived quality. (3) We applied ITU-R BT.500-14 <ref type="bibr" target="#b33">[33]</ref> (Annex 1, Sec 2.3) standard rejection to screen the remaining subjects. This resulted in 301 subjects being rejected (about 2.6%). (4) To detect (and reject) outliers, we first calculated the kurtosis coefficient <ref type="bibr" target="#b63">[63]</ref> of each score distribution, to determine normality. We then applied the Z-score method in <ref type="bibr" target="#b64">[64]</ref> if the distribution deemed Gaussian-like, and the Tukey IQR method <ref type="bibr" target="#b65">[65]</ref> otherwise (suppl. material). The total number of ratings collected after cleaning was around 5.6M (1.4M on videos and 4.1M on v-patches).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Data Analysis</head><p>Inter-subject consistency: On the cleaned data, we conducted an inter-subject consistency test <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b16">16]</ref>. Specifically, we randomly divided the subjects into two equal and disjoint sets and computed the Spearman Rank Correlation Coefficient (SRCC) <ref type="bibr" target="#b66">[66]</ref> between the two sets of MOS over 50 such random splits. We achieved an average SRCC of 0.86 on full videos, and 0.71, 0.71 and 0.67 for sv-patches, tv-patches, and stv-patches, respectively. This indicates a high degree of agreement between the human subjects, implying a successful screening process (suppl. material). Intra-subject consistency: We computed the Linear Correlation Coefficient (LCC) <ref type="bibr" target="#b67">[67]</ref> between subjective MOS against the original scores on the "golden" videos, obtaining a median PCC of 0.96 on full videos, and 0.946, 0.95, and 0.937 for sv-patches, tv-patches, and stv-patches, respectively. These high correlations further validate the efficacy of our data collection process. Relationship between patch and video quality: <ref type="figure" target="#fig_5">Fig 6</ref> shows scatter plots of the video MOS against each type of vpatch MOS. The calculated SRCC between the video MOS and the sv-patch, tv-patch and stv-patch MOS was 0.69, 0.77, and 0.67 respectively, indicating strong relationships between global and local quality, even though the v-patches are relatively small volumes of the original video data. MOS Distributions: <ref type="figure" target="#fig_6">Fig. 7</ref> plots the MOS distribution of the videos in the new dataset as compared to other popular "in-the-wild" video quality databases <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17]</ref>. The new dataset has a narrower distribution than the others, which again, matches actual social media data. Such a narrow distribution makes it more challenging to create predictive models that can parse finely differing levels of quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modeling a Blind Video Quality Predictor</head><p>Taking advantage of the unique potential of the new dataset (Sec. 3), we created a deep video quality prediction model, which we refer to as Patch-VQ (PVQ), and a spatio-temporal quality mapper called PVQ-Mapper, both of which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>Contrary to the way most deep image networks are trained, we did not crop, subsample, or otherwise process the input videos. Any such operation would introduce additional spatial and/or temporal artifacts, which can greatly affect video quality. Processing input videos of diverse aspect ratios, resolutions, and durations, however, makes training an end-to-end deep network impractical. To address this challenge, PVQ extracts spatial and temporal features on unprocessed original videos, and uses them to learn the local to global spatio-temporal quality relationships. As illustrated in <ref type="figure" target="#fig_7">Fig 8,</ref> PVQ involves three sequential steps: feature extraction, feature pooling, and quality regression. First, we extract features from both the 2D and 3D network streams, thereby capturing the spatial and temporal information from the whole video. Three kinds of v-patch features are also extracted from the output of both networks, using spatial and temporal pooling layers to capture local quality information. Finally, the pooled features from the video and the v-patches are processed by a time series network that effectively captures perceptual quality changes over time and predicts a single quality score per video. We provide more details of each step below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature Extraction</head><p>To capture the spatial aspects of both perceptual video quality and frame content, we extracted per frame (2D) spatial features using the PaQ-2-PiQ backbone pre-trained on the LIVE-FB Dataset <ref type="bibr" target="#b29">[29]</ref>. To capture temporal distortions, such as flicker, stutter, and focus changes, we extracted spatio-temporal (3D) features using a 3D ResNet-18 <ref type="bibr" target="#b30">[30]</ref> backbone, pre-trained on the Kinetics dataset <ref type="bibr" target="#b68">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature Pooling</head><p>Spatial and temporal pooling is applied in stages to extract features from the specified spatio-temporal regions of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Temporal Regression</head><p>The resulting space-time quality features are fed to In-ceptionTime <ref type="bibr" target="#b31">[31]</ref>, a state-of-the-art deep model for Time Series Classification (TSC). InceptionTime consists of a series of inception modules (with intermittent residual connections) followed by a global average pooling and a fully connected layer. The inception modules learn changes in the quality features over time, which is crucial to accurately predict the global video quality. Although RNNs have been used to model temporal video quality <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b72">72]</ref>, we have found that InceptionTime <ref type="bibr" target="#b31">[31]</ref> is much faster and easier to train compared to RNN, does not suffer from vanishing gradients, and gives better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Train and test splits: The entire dataset of videos, vpatches, and human annotations was divided into a training and two test sets. We first selected those videos having both of their spatial dimensions greater than 720, and reserved it for use as a secondary testing set (about 9% of the LSVQ  <ref type="bibr" target="#b22">[22]</ref> and VIDEVAL <ref type="bibr" target="#b23">[23]</ref>, that perform very well on existing UGC video databases. We also trained the VSFA <ref type="bibr" target="#b24">[24]</ref>, which extracts frame-level ResNet-50 <ref type="bibr" target="#b74">[74]</ref> features followed by a GRU layer to predict video quality. To study the efficacy of our local-to-global model, we trained two versions of our PVQ model, one with, and the other without the spatio-temporal v-patches. All models were trained and evaluated on the same train/test splits. Following the common practice in the field of video quality assessment, we report the performance using the correlation metrics SRCC and LCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Predicting global video quality</head><p>The quality prediction performance of the compared models on the new LSVQ dataset is summarized in <ref type="table" target="#tab_1">Table  2</ref>. As is evident, the shallow learner using traditional features (BRISQUE <ref type="bibr" target="#b73">[73]</ref>) did not perform well on our dataset. TLVQM <ref type="bibr" target="#b22">[22]</ref>, VSFA <ref type="bibr" target="#b24">[24]</ref>, and VIDEVAL <ref type="bibr" target="#b23">[23]</ref> performed better, indicating that they are capable of learning complex distortions. While both PVQ models (with and without patches) outperformed other models, including the v-patch data resulted in a performance boost on both test sets. Particularly on higher resolution test videos (Test-1080p), the proposed PVQ model (trained with v-patches) outperforms the strongest baseline by 3.6% on SRCC.  Performance on each v-patch: <ref type="table" target="#tab_2">Table 3</ref> sheds light into the capability of the compared models in predicting local quality. The two PVQ models delivered the best performance on all three types of v-patches, with the PVQ model trained on v-patches outperforming all baselines. From <ref type="table" target="#tab_1">Tables 2 and 3</ref>, we may conclude that PVQ effectively captures global and different forms of local spatio-temporal video quality.</p><p>Contribution of 2D and 3D streams: We also studied the contribution of the 2D and 3D features towards the performance of PVQ by training separate models on 2D (PVQ 2D ) and 3D (PVQ 3D ) features alone ( <ref type="table" target="#tab_3">Table 4</ref>). As can be observed, PVQ 3D achieves higher performance than PVQ 2D on both test sets. This further asserts that 3D features are more capable of capturing complex spatio-temporal distortions, and thus more favorable for VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of each v-patch:</head><p>To study the relative contributions of the three types of v-patches in PVQ, we trained three separate models utilizing each patch separately <ref type="table" target="#tab_3">(Table  4</ref>). Among the three, we observe that the highest performance is achieved when trained on stv-patches. Though stv-patches have relatively least volume ( <ref type="figure" target="#fig_3">Fig. 4)</ref>, they contain the most localized information on video quality distortions, which could explain its better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mobile-friendly version:</head><p>We also implemented an efficient version of PVQ for mobile and embedded vision applications (PVQ Mobile ), using the 2D and 3D versions of Mo-bileNetV2 <ref type="bibr" target="#b75">[75,</ref><ref type="bibr" target="#b76">76]</ref> for the two branches, and by reducing the RoIPool output size to 1 ? 1. Though there is a 6% decrease in performance as compared to PVQ (w/ v-patch), our mobile model requires only 1/5 as many parameters <ref type="table" target="#tab_3">(Table 4</ref>) compared to PVQ (w/ v-patch) and 1/2 as many parameters compared to VSFA <ref type="bibr" target="#b24">[24]</ref> (24M parameters).</p><p>Failure cases: The video in <ref type="figure" target="#fig_8">Fig 9 (a)</ref> was rated with a high score (MOS = 75.7) by human subjects, but was underrated by PVQ (predicted MOS = 47.4). We believe that an aes- thetic "bokeh" blur effect was interpreted as high quality content by subjects but such high levels of blur caused the model to predict low quality. The video in <ref type="figure" target="#fig_8">Fig 9 (b)</ref> was overrated by PVQ (predicted MOS = 54.7), considerably higher than the subject rating (MOS = 21). The video is of a computer generated game and does not appear very distorted. Yet, the subjects may have expected a higher resolution content for modern video games. These cases illustrate the challenges of creating models that closely align to human perception, while also highlighting the content diversity in the proposed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Predicting perceptual quality maps</head><p>We adapted the PVQ model (Sec. 4) to compute spatial and temporal quality maps on videos. Because of its flexible network architecture, PVQ is capable of predicting quality on any number (and sizes) of local spatio-temporal patches of an input video. We exploited this to create a temporal quality series and a first of its kind video quality map predictor, dubbed PVQ Mapper. Temporal quality series: A video is uniformly divided into 16 small temporal clips of 16 (continuous) frames each ? and a single quality score per clip is computed, thus capturing a temporal series of perceptual qualities across a video. Space-time quality maps: For space-time quality maps, we further divide each frame of each temporal clip defined above into a grid of 16 ? 16 non-overlapping spatial blocks of the same aspect ratio as the frame and compute a local space-time video clip quality. Bi-linear interpolation was applied to spatially re-scale the spatio-temporal quality predictions to match the input dimensions. <ref type="figure" target="#fig_0">Fig. 10</ref> depicts the temporal quality series and magma color space-time quality maps that were ?-blended (? = 0.8) with original frames picked from the center of each clip. The series shows the video quality evolving over time.</p><p>As may be observed, PVQ Mapper was able to accurately capture local quality loss, distinguishing blurred and underexposed areas from high-quality regions, and high-quality stationary backgrounds from fast-moving, streaky objects. Do v-patches matter for quality maps? <ref type="figure" target="#fig_0">Fig. 11</ref> shows spatial quality maps on two sample videos generated by PVQ Mapper, trained with and without using v-patches. In <ref type="figure" target="#fig_0">Fig. 11 (a)</ref>, the object in the foreground is focus blurred, whereas in <ref type="figure" target="#fig_0">Fig. 11 (b)</ref>, the dog is motion blurred and the desk is underexposed. These local quality distortions are not captured with PVQ Mapper (w/o v-patch) as indicated in the middle row, but are distinctly evident in the output of PVQ Mapper (w/ v-patch) as indicated in the bottom row. This indicates that PVQ Mapper that uses v-patches is able to better learn from both global and local video quality features and human judgments of them, and hence predict more accurate quality maps. <ref type="figure" target="#fig_0">Fig. 11</ref>: Improvement in quality maps when PVQ-Mapper is trained with patches illustrating that learning from both local space-time and global video quality yields more accurate predictions. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Cross-database comparisons</head><p>To emphasize the validity and generalizability of the PVQ model, we also tested it on the two popular, yet much smaller "in-the-wild" video databases: KoNViD-1k <ref type="bibr" target="#b14">[15]</ref> and LIVE-VQC <ref type="bibr" target="#b16">[16]</ref>  <ref type="table" target="#tab_0">(Table 1)</ref>. First, we compared the performance of PVQ against other popular models when each model was separately trained and tested on both datasets. As shown in <ref type="table" target="#tab_4">Table 5</ref>, PVQ competes very well with other models on KoNViD-1k, while improves the SRCC on LIVE-VQC by 2.8% compared to the strongest baseline.  <ref type="bibr" target="#b14">[15]</ref> and LIVE-VQC <ref type="bibr" target="#b16">[16]</ref>.</p><p>KoNViD-1k <ref type="bibr" target="#b14">[15]</ref> LIVE-VQC <ref type="bibr" target="#b16">[16]</ref> Model SRCC LCC SRCC LCC BRISQUE <ref type="bibr" target="#b73">[73]</ref> 0.657 0.658 0.592 0.638 V-BLIINDS <ref type="bibr" target="#b77">[77]</ref> 0.710 0.704 0.694 0.718 VSFA <ref type="bibr" target="#b24">[24]</ref> 0.773 0.775 0.773 0.795 TLVQM <ref type="bibr" target="#b22">[22]</ref> 0.773 0.769 0.799 0.803 VIDEVAL <ref type="bibr" target="#b23">[23]</ref> 0.783 0.780 0.752 0.751 PVQ (w/o v-patch) (Sec. <ref type="bibr" target="#b3">4)</ref> 0.791 0.786 0.827 0.837 To further study the generalizability of PVQ, we also compared the performance of all models when trained on the proposed dataset (LSVQ) but tested on the two aforementioned datasets. From <ref type="table" target="#tab_5">Table 6</ref>, it may be seen that PVQ transferred very well to both datasets. Specifically, our model outperforms the strongest baseline by 0.7% and 3.6% boost in SRCC on KoNViD-1k and LIVE-VQC respectively. This degree of database independence, both highlights the representativeness of the new LSVQ dataset and the general efficacy of the proposed PVQ model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>Predicting perceptual video quality is a long-standing problem in vision science, and more recently, deep learning. In recent years, it has dramatically increased in importance along with tremendous advances in video capture, sharing and streaming. Accurate and efficient video quality prediction demands the tools of large-scale data collection, visual psychometrics, and deep learning. To progress towards that goal, we built a new video quality database, which is substantially larger and diverse than previous ones. The database contains patch-level annotations that enable us (and others) to make global-to-local and local-to-global quality inferences, culminating in the accurate and generalizable PVQ model. We also created a space-time video quality mapping model, called PVQ Mapper, which utilizes learned patch quality attributes to accurately infer local space-time video quality, and is able to generate accurate spatio-temporal quality maps. We believe that the new LSVQ dataset, the PVQ model, and PVQ Mapper, can significantly advance progress on the UGC VQA problem, and enable quality-based monitoring, ingestion, and control of billions of videos streamed on social media platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material -Patch-VQ: 'Patching Up' the Video Quality Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cropping Patches</head><p>Deciding number of scales for cropping v-patches: In a psychometric study, specifically based on evaluating video quality, a subject needs roughly 15-20 seconds to rate each content. This limited the number of v-patches we could collect ratings on, and thus we decided to only include one scale for each type of v-patches. Scale here defines the dimensions of the v-patches, or the proportion of the video data contained in the patches. For simplicity, we use the same scale (40% of original dimensions) for extracting the three types of v-patches. Additional examples of extracted v-patch triplets have been shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Deciding size of v-patches: Empirically, sv-patches cropped at large scales are not local enough, and do not capture the local quality features satisfactorily. Alternately, smaller scales result in tv-patches too short in duration to collect reliable judgements. Similarly, the resulting stv-patches are too small and short to rate comprehensibly and reliably. We determined 40% to be the most suitable scale after examining v-patch samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Inter-subject consistency plots:</head><p>We have mentioned the average SRCC values, representative of the inter-subject consistencies, in Sec. 3.2.4. Along with that, we present the scatter plots of the two sets of subject MOS in <ref type="figure" target="#fig_1">Fig. 2</ref>. The narrow spread of the plots shows the high agreement, and hence higher consistency, among subject ratings. We also notice that the spread is highest (or, the correlation is lowest) in the case of stv-patches. This can be attributed to the fact that they account for only 6.4% of the video pixel volume, and sometimes, distortions prominent locally, might get masked or have little impact on perceived global video quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Consistency among subject demographics:</head><p>We utilized the subject data to study the effects of device parameters on MOS. The SRCC calculated between laptops and desktop computers (the most used devices in the study) was 0.7, whereas that between videos viewed on phones and other devices was 0.5. Although we collected relatively little data (3.7%) from phones, this reinforces the notion that perceptual video quality is impacted by viewing on a small device screen. We obtained the following correlations between the two major resolutions: 768 ? 1366 and 640 ? 360 (0.76); major viewing distances: less than 15 inches and 15-30 inches (0.76); major age groups: 20-30 and 30-40 (0.79); and genders (0.8), all of which are high, but low enough to be suggestive of further study. The consistency among the ratings from diverse subject demographics, when accumulated, result in the overall high consistency of the data (Sec. B.1), validating our data collection and cleaning methodologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Effect of playback delays on video quality:</head><p>Delays during playback could impact video quality <ref type="bibr" target="#b16">[16]</ref>. We found that &gt; 96% of the videos were viewed with delays &lt; 1s., while 86% of the videos played without delays. By comparing the scores of the delayed videos against the "golden" scores, we found that device delays had negligible impact on the mean scores, and that eliminating scores associated with delays did not impact data consistency. Hence, we did not impose device delays as a rejection criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Outlier rejection:</head><p>We removed the outliers in our data in two steps as described briefly in Sec. 3.2.3 -outlier subject rejection and outlier score rejection. The former rejection was video independent, whereas the latter was subject independent. Here, we elaborate the outlier score rejection, which was executed on all videos individually. We followed the standard outlier rejection techniques, but the technique applied was dependent on the score distribution. If, for a video, the scores were (approximately) Gaussian, the modified Z-scores method <ref type="bibr" target="#b64">[64]</ref> was applied, which is based on calculating the standard deviation of the distribution. Calculating the kurtosis helped determine the normality of the score distribution. Alternately, if the scores were deemed to be not normal, then we applied the Tukey IQR <ref type="bibr" target="#b65">[65]</ref> detection technique, which is based on calculating the interquartile range and is a more generalized method. Tuning the outlier rejection methods based on the nature of the score distribution yielded better consistency scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modeling Details</head><p>For training PVQ (Sec. 4), we used the Adam optimizer with ? 1 = .9 and ? 2 = .99, a weight decay of .01. The initial learning rate was set to be 0.001 and we followed the 1cycle policy <ref type="bibr" target="#b78">[78]</ref> to adjust the learning rate on the fly. We trained each model for 10 epochs and report the performance of the model on the two testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Amazon Mechanical Turk (AMT) Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Study Requirements:</head><p>Each video batch (and thus each video) was published on AMT in four phases. The first two phases targeted "reliable" workers (with AMT ratings &gt; 95%, and &gt; 10,000 HITs), who helped eliminate inappropriate (violent or pornographic) content and static videos. In the latter two phases, we reduced the numbers to 75% and 1000, respectively.</p><p>As each subject was viewing the instructions, we monitored several parameters to ensure that they could effectively participate. The following eligibility criteria were imposed -? Browser Window Resolution: At least 480p for mobile devices and 720p for others. ? Browser Zoom: Set to 100%. ? Browsers: Latest versions of Chrome, Firefox, Edge, Safari, and Chrome. ? Loading Time: Must be less than 20 secs for all the training videos.</p><p>In case they failed to meet any of the above criterion, subjects were prevented from progressing and informed accordingly. Apart from these, the subjects were also required to take a quiz reflecting their understanding of the instructions, and were allowed to proceed only if they answered at least five out of the six questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Interface:</head><p>The AMT interface comprised of a series of instruction pages, followed by the quiz, before they could start rating the videos. Workers were allowed to view the introductory page ( <ref type="figure" target="#fig_2">Fig. 3</ref>) before accepting to participate in the study. If accepted, they had to go through the instruction pages ( <ref type="figure" target="#fig_3">Fig. 4, 5, 6, 7)</ref>, which were timed. During the instructions, we checked whether they satisfied the study criteria as described in Sec. D.1. Following the instruction pages, they had to pass the quiz <ref type="figure" target="#fig_7">(Fig. 8</ref>) in order to proceed to the training and testing phases. The task included rating the played video ( <ref type="figure" target="#fig_8">Fig. 9</ref>) on a Likert scale <ref type="bibr" target="#b79">[79]</ref> marked with BAD, POOR, FAIR, GOOD, and EXCELLENT, as demonstrated in <ref type="figure" target="#fig_0">Fig. 10</ref>. A similar interface was used for the v-patch sessions as well.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Modeling local to global perceptual quality: From each video, we extract three spatio-temporal video patches (Sec. 3.1), which along with their subjective scores, are fed to the proposed video quality model. By integrating spatial (2D) and spatio-temporal (3D) quality-sensitive features, our model learns spatial and temporal distortions, and can robustly predict both global and local quality, a temporal quality series, as well as space-time quality maps (Sec. 5.2). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Sample video frames from the new database, each resized to fit. The actual videos are of highly diverse sizes and resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Left: Scatter plot of video width versus video height with marker size indicating the number of videos having a given dimension in the new LSVQ database. Right: Histogram of the durations (in seconds) of the videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Three kinds of video patches (v-patches) cropped from random space-time volumes from each video in the dataset. All v-patches are videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Study workflow for both video and v-patch sessions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Scatter plots of patch-video MOS correlations Video MOS vs svpatch (left), tv-patch (middle) and stv-patch (right) MOS cropped from the same video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Ground Truth MOS histograms of four "in-the-wild" databases. Starting from left, proposed LSVQ dataset, KoNViD-1k<ref type="bibr" target="#b14">[15]</ref>, LIVE-VQC<ref type="bibr" target="#b16">[16]</ref>, and YouTube-UGC<ref type="bibr" target="#b17">[17]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Illustrating the proposed PVQ model which involves 3 sequential steps: feature extraction, spatio-temporal pooling, and temporal regression (Sec. 4.1).interest (v-patches), allowing us to model local-to-global space-time quality relationships. Spatial Pooling: The extracted 2D and 3D features are independently passed through a spatial RoIPool (region-ofinterest pooling) layer<ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70]</ref>, with regions specified by the 3D v-patch coordinates. RoIPool helps compute a feature map with a fixed spatial extent of 2 ? 2. The RoIPool layer generates 4 feature vectors of size 2048 per frame and video clip, for all three v-patches and the full video. Temporal Pooling: The RoIPool layer is followed by an SoIPool (segment-of-interest pooling) layer<ref type="bibr" target="#b71">[71]</ref> that helps compute a feature map with a fixed temporal extent. Specifically, an SoIPool layer with a fixed temporal extent of 16 is applied on both 2D and 3D features of each v-patch and the full video. The SoIPool layer yields 4 feature vectors of size 16 ? 2048 per all three v-patches and the full video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Failure cases: Frames from video examples where predictions differed the most from the human quality judgements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Space-time quality maps: Space-time quality maps generated on a video using the PVQ Mapper (Sec. 5.2), and sampled in time for display. Four video frames are shown at top, with spatial quality maps (blended with the original frames using magma color) immediately under, while the bottom plots show the evolving quality of the video. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 1 :</head><label>1</label><figDesc>Examples of video patch (v-patch) triplets cropped from random space-time volumes from two exemplar videos in the dataset. All v-patches are videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 2 :</head><label>2</label><figDesc>Inter-subject consistency: Inter-subject scatter plot of MOS calculated between random 50% divisions of the human labels on all 39K videos (first from left) into disjoint subject sets. The same is plotted for the sv-patches (second), tv-patches (third) and stv-patches (fourth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 3 :</head><label>3</label><figDesc>Introductory Page</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 4 :Page 1 Fig. 5 :Fig. 6 :Page 3 Fig. 7 :Fig. 8 :Fig. 9 :Fig. 10 :</head><label>4156378910</label><figDesc>Instruction Instruction Page 2 Instruction Instruction Page 4 Quiz Video Playback Rating Slider</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of popular public-domain video quality datasets. Legacy datasets contain singular synthetic distortions, whereas "in-the-wild" databases contain videos impaired by complex mixtures of diverse, real distortions.</figDesc><table><row><cell>Database</cell><cell># Unique contents</cell><cell># Video Duration (sec)</cell><cell># Distor-tions</cell><cell># Video contents</cell><cell># V-Patch contents</cell><cell>Distortion type</cell><cell>Subjective study framework</cell><cell># Annotators</cell><cell># Annotations</cell></row><row><cell>MCL-JCV (2016) [11]</cell><cell>30</cell><cell>5</cell><cell>51</cell><cell>1,560</cell><cell>0</cell><cell>Compression</cell><cell>In-lab</cell><cell>150</cell><cell>78K</cell></row><row><cell>VideoSet (2017) [12]</cell><cell>220</cell><cell>5</cell><cell>51</cell><cell>45,760</cell><cell>0</cell><cell>Compression</cell><cell>In-lab</cell><cell>800</cell><cell>-</cell></row><row><cell>UGC-VIDEO (2019) [26]</cell><cell>50</cell><cell>&gt; 10</cell><cell>10</cell><cell>550</cell><cell>0</cell><cell>Compression</cell><cell>In-lab</cell><cell>30</cell><cell>16.5K</cell></row><row><cell>CVD-2014 (2014) [27]</cell><cell>5</cell><cell>10-25</cell><cell>-</cell><cell>234</cell><cell>0</cell><cell>In-capture</cell><cell>In-lab</cell><cell>210</cell><cell>-</cell></row><row><cell>LIVE-Qualcomm (2016) [28]</cell><cell>54</cell><cell>15</cell><cell>6</cell><cell>208</cell><cell>0</cell><cell>In-capture</cell><cell>In-lab</cell><cell>39</cell><cell>8.1K</cell></row><row><cell>KoNViD-1k (2017) [15]</cell><cell>1,200</cell><cell>8</cell><cell>-</cell><cell>1,200</cell><cell>0</cell><cell>In-the-wild</cell><cell>Crowdsourced</cell><cell>642</cell><cell>? 205K</cell></row><row><cell>LIVE-VQC (2018) [16]</cell><cell>585</cell><cell>10</cell><cell>-</cell><cell>585</cell><cell>0</cell><cell>In-the-wild</cell><cell>Crowdsourced</cell><cell>4,776</cell><cell>205K</cell></row><row><cell>YouTube-UGC (2019) [17]</cell><cell>1,500</cell><cell>20</cell><cell>-</cell><cell>1,500</cell><cell>4,500</cell><cell>In-the-wild</cell><cell>Crowdsourced</cell><cell>-</cell><cell>? 600K</cell></row><row><cell>Proposed database (LSVQ)</cell><cell>39, 075</cell><cell>5-12</cell><cell>-</cell><cell>39, 075</cell><cell>117, 225</cell><cell>In-the-wild</cell><cell>Crowdsourced</cell><cell>6, 284</cell><cell>5, 545, 594</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance on full-size videos in the LSVQ dataset. Higher values indicate better performance. Picture based model is italicized.</figDesc><table><row><cell></cell><cell>Test</cell><cell></cell><cell cols="2">Test-1080p</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>BRISQUE [73]</cell><cell>0.579</cell><cell>0.576</cell><cell>0.497</cell><cell>0.531</cell></row><row><cell>TLVQM [22]</cell><cell>0.772</cell><cell>0.774</cell><cell>0.589</cell><cell>0.616</cell></row><row><cell>VIDEVAL [23]</cell><cell>0.794</cell><cell>0.783</cell><cell>0.545</cell><cell>0.554</cell></row><row><cell>VSFA [24]</cell><cell>0.801</cell><cell>0.796</cell><cell>0.675</cell><cell>0.704</cell></row><row><cell>PVQ (w/o v-patch)</cell><cell>0.814</cell><cell>0.816</cell><cell>0.686</cell><cell>0.708</cell></row><row><cell>PVQ (w/ v-patch)</cell><cell>0.827</cell><cell>0.828</cell><cell>0.711</cell><cell>0.739</cell></row><row><cell cols="5">dataset: 3.5K videos and 10.5K v-patches). About 93.2%</cell></row><row><cell cols="5">of the videos in the reserved set have resolutions 1080p or</cell></row><row><cell cols="5">higher, hence we will refer to it as "Test-1080p". On the</cell></row><row><cell cols="5">remaining videos, we applied a typical 80-20 split, yielding</cell></row><row><cell cols="5">about 28.1K videos (and 84.3K v-patches) for training, and</cell></row><row><cell cols="4">7.4K videos (and 22.2K v-patches) for testing.</cell><cell></cell></row></table><note>Input processing and training: Each video was divided into 40 clips of 16 continuous frames. For feature extrac- tion, we used a batch size of 8 for 3D ResNet-18 and 128 for PaQ-2-PiQ. For spatial and temporal pooling, we provide sets of spatio-temporal coordinates (x 1 , x 2 , y 1 , y 2 , t 1 , t 2 ) of each v-patch. When training InceptionTime, we used a batch size of 128 and L1 loss to predict the output quality scores (details in suppl. material). Baselines and metrics: The model comparisons were done on both videos and v-patches. We compared with a popu- lar image model BRISQUE [73], by extracting frame-level features and training an SVR and two other shallow NR VQA models, TLVQM</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on the three v-patches in the LSVQ dataset. Picture based model is italicized.</figDesc><table><row><cell></cell><cell>sv-patch</cell><cell>tv-patch</cell><cell>stv-patch</cell></row><row><cell>Model</cell><cell>SRCC LCC</cell><cell>SRCC LCC</cell><cell>SRCC LCC</cell></row><row><cell>BRISQUE [73]</cell><cell>0.469 0.417</cell><cell>0.465 0.485</cell><cell>0.476 0.462</cell></row><row><cell>TLVQM [22]</cell><cell>0.575 0.543</cell><cell>0.523 0.536</cell><cell>0.561 0.563</cell></row><row><cell>VIDEVAL [23]</cell><cell>0.596 0.570</cell><cell>0.633 0.634</cell><cell>0.662 0.636</cell></row><row><cell>VSFA [24]</cell><cell>0.654 0.609</cell><cell>0.688 0.681</cell><cell>0.685 0.670</cell></row><row><cell>PVQ (w/o v-patch)</cell><cell>0.723 0.717</cell><cell>0.696 0.701</cell><cell>0.651 0.643</cell></row><row><cell>PVQ (w/ v-patch)</cell><cell>0.737 0.720</cell><cell>0.701 0.700</cell><cell>0.711 0.707</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies conducted on the Test split of the LSVQ dataset. Higher values indicate better performance.</figDesc><table><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell># parameters</cell></row><row><cell>PVQ 2D (w/ v-patch)</cell><cell>0.774</cell><cell>0.774</cell><cell>16.3 M</cell></row><row><cell>PVQ 3D (w/ v-patch)</cell><cell>0.805</cell><cell>0.805</cell><cell>38.3 M</cell></row><row><cell>PVQ (w/ sv-patch)</cell><cell>0.815</cell><cell>0.815</cell><cell>54.2 M</cell></row><row><cell>PVQ (w/ tv-patch)</cell><cell>0.817</cell><cell>0.818</cell><cell>54.2 M</cell></row><row><cell>PVQ (w/ stv-patch)</cell><cell>0.824</cell><cell>0.826</cell><cell>54.2 M</cell></row><row><cell>PVQ Mobile (w/ v-patch)</cell><cell>0.774</cell><cell>0.779</cell><cell>10.9 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Cross-database comparison 1: Performance when all models are separately trained and tested on KoNViD-1k</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Cross</figDesc><table><row><cell cols="5">-database comparison 2: Performance when all models are</cell></row><row><cell cols="5">separately trained on the new LSVQ database, then evaluated on KoNViD-</cell></row><row><cell cols="3">1k [15] and LIVE-VQC [16] without fine-tuning.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">KoNViD-1k [15]</cell><cell cols="2">LIVE-VQC [16]</cell></row><row><cell>Model</cell><cell>SRCC</cell><cell>LCC</cell><cell>SRCC</cell><cell>LCC</cell></row><row><cell>BRISQUE [73]</cell><cell>0.646</cell><cell>0.647</cell><cell>0.524</cell><cell>0.536</cell></row><row><cell>TLVQM [22]</cell><cell>0.732</cell><cell>0.724</cell><cell>0.670</cell><cell>0.691</cell></row><row><cell>VIDEVAL [23]</cell><cell>0.751</cell><cell>0.741</cell><cell>0.630</cell><cell>0.640</cell></row><row><cell>VSFA [24]</cell><cell>0.784</cell><cell>0.794</cell><cell>0.734</cell><cell>0.772</cell></row><row><cell>PVQ (w/o v-patch) (Sec. 4)</cell><cell>0.782</cell><cell>0.781</cell><cell>0.747</cell><cell>0.776</cell></row><row><cell>PVQ (w/ v-patch) (Sec. 4)</cell><cell>0.791</cell><cell>0.795</cell><cell>0.770</cell><cell>0.807</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? By changing the number of frames in each clip, the quality predictions can be made less or more dense.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TikTok by the Numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omnicore</surname></persName>
		</author>
		<ptr target="https://www.omnicoreagency.com/tiktok-statistics/.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Video Statistics</surname></persName>
		</author>
		<ptr target="https://99firms.com/blog/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">10 Youtube Statistics Every Marketer Should Know in 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Mohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oberlo</surname></persName>
		</author>
		<ptr target="https://www.oberlo.com/blog/youtube-statistics.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">VQpooling: Video quality pooling adaptive to perceptual distortion severity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="620" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image quality comparison between JPEG and JPEG2000. I. Psychophysical investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><forename type="middle">S</forename><surname>Triantaphillidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging Science and Technology</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="248" to="258" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A h.264/avc video database for the evaluation of quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naccari</forename><forename type="middle">S</forename><surname>Tubaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The TUM high definition video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Redl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dieopold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mcl-jcv: A JND-based h.264/avc video quality assessment dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">. J</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Videoset: A largescale compressed video quality dataset based on JND measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-O</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">VQEG HDTV phase I database</title>
		<ptr target="https://www.its.bldrdoc.gov/vqeg/projects/hdtv/hdtv.aspx.1" />
		<imprint/>
		<respStmt>
			<orgName>Video Quality Experts Group (VQEG</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vis3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The konstanz natural video database (konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<ptr target="http://database.mmsp-kn.de/konvid-1k-database.html.2,3,4" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<ptr target="http://live.ece.utexas.edu/research/LIVEVQC/index" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
	<note>html. 2, 3, 4, 5, 8</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Youtube UGC dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adsumilli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Design of no-reference video quality metrics with multiway partial least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klimpke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Habigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">No-reference video quality assessment based on artifact measurement and statistical analysis. Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asari</forename><forename type="middle">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="533" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">UGC-VQA: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Quality assessment of in-thewild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">UGC-VIDEO: perceptual quality assessment of usergenerated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CVD2014-a database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://live.ece.utexas.edu/research/incaptureDatabase/index.html" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. and Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Inceptiontime: Finding alexnet for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">No-reference video quality assessment using multi-level spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">500-14, methodologies for the subjective assessment of the quality of television images</title>
		<ptr target="https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.500-14-201910-I!!PDF-E.pdf.2" />
		<editor>International Telecommunication Union. ITU-R BT</editor>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Noreference video quality assessment for sd and hd h. 264/avc sequences based on continuous estimates of packet loss visibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-N</forename><forename type="middle">S</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>List</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Noreference pixel video quality monitoring of channel-induced distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="605" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A noreference bitstream-based perceptual model for video quality estimation of videos affected by coding artifacts and packet losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Kondi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pandremmenou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>L?vstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Vision and Electronic Imaging XX</title>
		<imprint>
			<biblScope unit="volume">9394</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">93941</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">No-reference video quality assessment using codec analysis. Transactions on Circuits and Systems for Video Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Forchhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1637" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Predictive no-reference assessment of video quality. Signal Processing: Image Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stavrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liotta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="20" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">No-reference quality metric for degraded and enhanced video. Digit. Video Image Qual. Perceptual Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Caviedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Oberti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="305" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">No-reference video quality evaluation for high-definition video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oelbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Diepold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spatio-temporal measures of naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1750" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Blind image quality assessment on real distorted images using deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information processing</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="946" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="130" to="141" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A deep neural network for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maniry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Int&apos;l Conf. Image Process. (ICIP)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3773" to="3777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fully deep blind image quality predictor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="220" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">NIMA: Neural image assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3998" to="4011" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RankIQA: Learning from rankings for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1040" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">End-to-end blind image quality assessment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Blind video quality assessment with weakly supervised learning and resampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="219" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia Conf. (MM)</title>
		<meeting>ACM Multimedia Conf. (MM)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Moving Image Archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Internet</forename><surname>Archive</surname></persName>
		</author>
		<ptr target="https://archive.org/details/movies.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://projects.dfki.uni-kl.de/yfcc100m/.3" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffmpeg</surname></persName>
		</author>
		<ptr target="https://ffmpeg.org/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A probabilistic approach to people-centric photo selection and sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arnfred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2609" to="2624" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Measuring colorfulness in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Suesstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Conf. on Human Vision and Electronic Imaging VIII</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Contrast in complex images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2032" to="2040" />
			<date type="published" when="1990-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Face detection using haar cascades. OpenCV-Python Tutorials</title>
		<idno>Online] Available: https:</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using threedimensional textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<ptr target="https://www.robots.ox.ac.uk/?vgg/research/texclass/filters.html.3" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Koniq-10K: Towards an ecologically valid and large-scale IQA database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08489</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurtosis</forename><surname>Measure Of</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="343" to="343" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Iglewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hoaglin</surname></persName>
		</author>
		<title level="m">How to Detect and Handle Outliers. The ASQC Basic References in Quality Control: Statistical Techniques</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Exploratory data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tukey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Addison-Wesley Pub. Co</publisher>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Rank correlation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><forename type="middle">George</forename><surname>Kendall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Thirteen ways to look at the correlation coefficient. The American Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Lee Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Nicewander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Comput. Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1040" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Info Process Syst (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Rethinking the faster rcnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Deep neural networks for noreference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vision and Pattern Recogn</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Resource efficient 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Gunduz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the dct domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">The 1cycle policy</title>
		<ptr target="https://fastai1.fast.ai/callbacks.one_cycle.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A technique for the measurement of attitudes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Likert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Psychology</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
