<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Optimization Framework for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
							<email>tanaka@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
							<email>ikami@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
							<email>yamasaki@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
							<email>aizawa@hal.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Optimization Framework for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>DNNs trained on large-scale datasets have achieved impressive results on many classification problems. Generally, accurate labels are necessary to effectively train DNNs. However, many datasets are constructed by crawling images and labels from websites and often contain incorrect noisy labels (e.g., YFCC100M <ref type="bibr" target="#b16">[17]</ref>, Clothing1M <ref type="bibr" target="#b20">[21]</ref>). This study addresses the following question: how can we effectively train DNNs on noisy labeled datasets without manually cleaning the data?</p><p>The prominent issue in training DNNs on noisy labeled datasets is that DNNs can learn or memorize, any training dataset, and this implies that DNNs are subject to total overfitting on noisy data.</p><p>To address this problem, commonly used regularization techniques including dropout and early stopping are helpful. However, these methods do not guarantee optimization because they prevent the networks from reducing the training loss. Another method involves using prior knowledge, such as the confusion matrix between clean and noisy labels, which typically cannot be used in real settings. Consequently, we need a new framework of optimization. In this study, we propose an optimization framework for learning on a noisy labeled dataset. We propose optimizing the labels themselves as opposed to treating the noisy labels as fixed. The joint optimization of network parameters and the noisy labels corrects inaccurate labels and simultaneously improves the performance of the classifier. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the concept of our proposal. The main contributions are as follows.</p><p>1. We propose a joint optimization framework for learning on noisy labeled datasets. Our optimization problem has two optimization network parameters and class labels that are optimized by an alternating strategy.</p><p>2. We observe that a DNN trained on noisy labeled datasets does not memorize noisy labels and maintains high performance for clean data under a high learning rate. This reinforces the findings of Arpit et al. <ref type="bibr" target="#b0">[1]</ref> that suggest that DNNs first learns simple patterns and subsequently memorize noisy data.</p><p>3. We evaluate the performance on synthetic and real noisy datasets. We demonstrate state-of-theart performance on the noisy CIFAR-10 dataset and a comparable performance on the Clothing1M dataset <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generalization abilities of DNNs</head><p>Recently, generalization and memorization abilities of neural networks have attracted increasing attention. Specifically, we focus on the ability of learning labels. <ref type="bibr">Zhang et al.</ref> showed that DNNs can learn any training dataset even if the training labels are completely random <ref type="bibr" target="#b21">[22]</ref>. This leads to two problems. First, the performance of a DNN decreases when it is trained on a noisy dataset and completely learns noisy labels. Second, it is difficult to learn which label is noisy given the perfect learning ability. To the best of our knowledge, most studies on deep learning with respect to noisy labels do not focus on the aforementioned problems that are caused by the memorization ability of DNNs. This study involves addressing these two problems to improve the classification accuracy by preventing completely learning for noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning on noisy labeled datasets</head><p>We briefly review existing studies on learning on noisy labeled datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization:</head><p>Regularization is an efficient method to deal with the issue of DNNs easily fitting noisy labels, as described in Section 2.1. Arpit et al. showed the performances of DNNs trained on noisy labeled datasets with several regularizations <ref type="bibr" target="#b0">[1]</ref> including weight decay, dropout, and adversarial training <ref type="bibr" target="#b5">[6]</ref>. Zhang et al. used a mixup <ref type="bibr" target="#b22">[23]</ref> involving the utilization of a linear combination of images and labels for training.</p><p>These techniques improve performance on clean labels. However, these methods do not explicitly deal with noisy labels, and therefore long-time training leads to performance degradation as follows: the performance of the last epoch is generally worse than that of the best epoch <ref type="bibr" target="#b22">[23]</ref>. Furthermore, it is not possible to use the training loss on the noisy labeled dataset as a measure of performance on clean labels. Therefore, training-loss based early stopping does not work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise transition matrix:</head><p>Let l and l GT be the noisy and true labels. We define the noise transition matrix T by t ij = p(l = j|l GT = i). Then, we can use T to modify the cross entropy loss as follows:</p><formula xml:id="formula_0">L(?, Y, X) = ? 1 n n i=1 log y T i T s(?, x i ) .<label>(1)</label></formula><p>This formulation was used in many studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. In deep learning, some studies presuppose the groundtruth noise transition matrix T <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> and achieve the state-of-the-art performance in the noisy CIFAR-10 dataset. Other studies estimate T from noisy data. Specifically, T is modeled by a fully connected layer and is trained in an end-to-end manner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11]</ref>. However, these methods do not carefully focus on the memorization ability of DNNs. Patrini et al. proposed an estimation method for T <ref type="bibr" target="#b13">[14]</ref>; however, its performance is slightly worse than that obtained with the true T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust loss function:</head><p>A few studies achieve noiserobust classification by using a noise-tolerant loss functions, such as ramp loss <ref type="bibr" target="#b2">[3]</ref> and unhinged loss <ref type="bibr" target="#b19">[20]</ref>. For further details please refer to <ref type="bibr" target="#b4">[5]</ref>. In deep learning, Ghosh et al. used mean square error and mean absolute error <ref type="bibr" target="#b3">[4]</ref> for noise-tolerant loss functions. It should be noted that they do not consider the problem that DNNs can learn arbitrary labels.</p><p>Other approaches using deep learning: Reed et al. used a bootstrapping scheme to handle noisy labels <ref type="bibr" target="#b14">[15]</ref>. Our method is similar to this study. Xiao et al. constructed a noise model with multiple noise types and trained two networks: an image classifier and a noise type classifier <ref type="bibr" target="#b20">[21]</ref>. It should be noted that this method requires a low amount of accurately labeled datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-training and pseudo-labeling</head><p>Pseudo-labeling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> is a type of self-training that is generally used in semi-supervised learning with few labeled data and many unlabeled data. In this technique, pseudo-labels are initially assigned to unlabeled data by predictions of a model trained on a clean dataset. Subsequently, the algorithm repeats retraining the model on both labeled and unlabeled data and updating pseudo-labels.</p><p>In semi-supervised learning, we know which data is labeled or not and only need to assign pseudo-labels to only unlabeled data. However, with respect to learning on noisy labeled data, it is necessary to treat all data equally because we do not know which data is noisy. Reed et al. proposed a self-training scheme <ref type="bibr" target="#b14">[15]</ref> for training a DNN on noisy labeled data. Their approach is similar to that proposed in this study. However, they use original noisy labels for learning until the end of training, and thus the performance is degraded by the remaining effects of noisy labels for a high noise rate <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref>. Conversely, we completely replace all labels by pseudo-labels and use the same for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Notation and Problem Statements</head><p>In this study, column vectors and matrices are denoted in bold (e.g. x) and capitals (e.g. X), respec-tively. Specifically, 1 is a vector of all-ones. We define hard-label spaces H = {y : y ? {0, 1} c , 1 y = 1} and soft-label spaces S = {y : y ? [0, 1] c , 1 y = 1}.</p><p>In supervised c-class classification problem setting, we have a set of n training images X = [x 1 , . . . , x n ] with ground-truth labels Y GT = [y GT 1 , . . . , y GT n ] = R c?n , where y GT i ? H is a one-hot vector representation of the true class label. The objective function is an empirical risk, such as the cross entropy, as follows:</p><formula xml:id="formula_1">L = ? 1 n n i=1 c j=1 y GT ij log s j (?, x i ),<label>(2)</label></formula><p>where ? denotes the set of network parameters and s denotes the output of the final layer, namely c-class softmax layer, of the network. If a clean training dataset is present, then the network parameters ? are learned by optimizing Eq. (2) by using a gradient descent method. However, in this study, we consider the classification problem with noisy labels as follows: Let y i be the noisy label, and only the noisy training label set Y = [y 1 , ? ? ? , y n ] is given. The task involves training CNNs to predict true labels. In the next section, we describe the proposed method for training on noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Classification with Label Optimization</head><p>In this section, we present our proposed training method with noisy labels. Generally, with respect to supervised learning on clean labels, the optimization problem is formulated as follows:</p><formula xml:id="formula_2">min ? L(?|X, Y ),<label>(3)</label></formula><p>where L denotes a loss function such as the cross entropy loss Eq. (2). Eq. (3) works well on clean labels. However, if we train the network by Eq. (3) on noisy labels, its performance decreases. As we will describe in Section 5.3, we experimentally found that a high learning rate suppresses the memorization ability of a DNN and prevents it from completely fitting to labels. Thus, we assume that a network trained with a high learning rate will have more difficulty fitting to noisy labels. In other words, the loss Eq. (3) is high for noisy labels and low for clean labels. Given this assumption, we obtain clean labels by updating labels in the direction to decrease Eq. (3). Therefore, we formulate the problem as the joint optimization of network parameters and labels as follows:</p><formula xml:id="formula_3">min ?,Y L(?, Y |X).<label>(4)</label></formula><p>The concept of our proposal is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><formula xml:id="formula_4">Algorithm 1 Alternating Optimization for t ? 1 to num_epochs do update ? (t+1) by SGD on L(? (t) , Y (t) |X) update Y (t+1) by Eq. (8) (hard-label) or Eq. (9) (soft-label) end for</formula><p>Our proposed loss function L(?, Y |X) is constructed by three terms as follows: <ref type="bibr" target="#b4">(5)</ref> where L c (?, Y |X), L p (?|X), L e (?|X) denote the classification loss and two regularization losses, respectively, and ? and ? denote hyper parameters. In this study, we use the Kullback-Leibler (KL)-divergence for L c (?, Y |X) as follows:</p><formula xml:id="formula_5">L(?, Y |X) = L c (?, Y |X)+?L p (?|X)+?L e (?|X),</formula><formula xml:id="formula_6">L c (?, Y |X) = 1 n n i=1 D KL (y i ||s(?, x i )),<label>(6)</label></formula><formula xml:id="formula_7">D KL (y i ||s(?, x i )) = c j=1 y ij log y ij s j (?, x i ) . (7)</formula><p>In the following subsections, we first describe an alternating optimization method to solve this problem, and we then describe the definition of L p , L e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Alternating Optimization</head><p>In our proposed learning framework, network parameters ? and class labels Y are alternatively updated as shown in Algorithm 1. We will describe the update rules of ? and Y .</p><p>Updating ? with fixed Y : All terms in the optimization problem Eq. (5) are sub-differentiable with respect to ?. Therefore, we update ? by the stochastic gradient descent (SGD) on the loss function Eq. (5).</p><p>Updating Y with fixed ?: In contrast to other methods, we update and optimize the labels that we perform the training on. With respect to updating Y , it is only necessary to consider the classification loss</p><formula xml:id="formula_8">L c (?, Y |X) from Eq. (5) with fixed ?. The optimization problem Eq. (6) on Y is separated for each y i .</formula><p>As a method of optimizing labels, two methods can be considered: the hard-label method and the soft-label method. In the case of the hard-label method, Y ? H is updated as follows:</p><formula xml:id="formula_9">y ij = ? ? ? 1 if j = argmax j s j (?, x i ) 0 otherwise .<label>(8)</label></formula><p>In the case of the soft-label method, the KL-divergence from s(?, x i ) to y i is minimized when y i = s, and thus the update rule for Y ? S is as follows:</p><formula xml:id="formula_10">y i = s(?, x i ).<label>(9)</label></formula><p>As we will describe in Section 5.4, we experimentally determined that the performance of the soft-label method exceeded that of the hard-label method. Thus, we applied soft-labels to all experiments if not otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Regularization Terms</head><p>We describe definitions and roles of two regularization losses of L p (?|X) and L e (?|X).</p><p>Regularization loss L p : The regularization loss L p (?|X) is required to prevent the assignment of all labels to a single class: In the case of minimizing only Eq. <ref type="formula" target="#formula_6">(6)</ref>, we obtain a trivial global optimal solution with a network that always predicts constant one-hot? ? H and each label y i =? for any image x i . To overcome this problem, we introduce a prior probability distribution p, which is a distribution of classes among all training data. If the prior distribution of classes is known, then the updated labels should follow the same. Therefore, we introduce the KL-divergence froms(?, X) to p as a cost function as follows:</p><formula xml:id="formula_11">L p = c j=1 p j log p j s j (?, X)<label>(10)</label></formula><p>This approach follows <ref type="bibr" target="#b9">[10]</ref>. The mean probabilit? s(?, X) in the training data is approximated by performing a calculation for each mini-batch B as Eq. <ref type="formula" target="#formula_0">(11)</ref>.</p><formula xml:id="formula_12">s(?, X) = 1 n n i=1 s(?, x i ) ? 1 |B| x?B s(?, x)<label>(11)</label></formula><p>This approximation cannot treat a large number of classes and extreme imbalanced classes, however it works well in the experiments on the noisy CIFAR-10 dataset and the Clothing1M dataset.</p><p>Regularization loss L e : The term L e is required for the training loss when we use the soft-label. We consider the case of Eq. (5) with ? = ? = 0. In this case, when Y is updated by Eq. (9), both ? and Y are stuck in local optima and the learning process does not proceed. To overcome this problem, we introduce an entropy term to concentrate the probability distribution of each soft-label to a single class as follows:</p><formula xml:id="formula_13">L e = ? 1 n n i=1 c j=1 s j (?, x i ) log s j (?, x i ).<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Additional Details</head><p>Our method has two steps for training on noisy labels. In the first step, we obtain clean labels by updating labels as described in Section 4.1. In the second step, we initialize the network parameters and train the network by usual supervised learning with the labels obtained in the first step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10:</head><p>We use the CIFAR-10 dataset <ref type="bibr" target="#b11">[12]</ref> and retain 10% of the training data for validation. Subsequently, we define three types of the training data, namely Symmetric Noise CIFAR-10 (SN-CIFAR), Asymmetric Noise CIFAR-10 (AN-CIFAR), and Pseudo Label CIFAR-10 (PL-CIFAR).</p><p>In SN-CIFAR, we inject the symmetric label noise. Symmetric label noise is as follows:</p><formula xml:id="formula_14">y i = y GT i</formula><p>with the probability of 1 ? r random one-hot vector with the probability of r .</p><p>(13) In AN-CIFAR, we inject the asymmetric label noise. The asymmetric label noise is discussed in <ref type="bibr" target="#b13">[14]</ref>. The rationale involves mimicking a part of the structure of real mistakes for similar classes: TRUCK ? AUTO-MOBILE, BIRD ? AIRPLANE, DEER ? HORSE, CAT ? DOG. Transitions are parameterized by r ? [0, 1] such that the probabilities of ground-truth and inaccurate class correspond to 1?r and r, respectively.</p><p>In PL-CIFAR, pseudo labels are assigned to unlabeled training data. Pseudo labels are generated by applying k-means++ <ref type="bibr" target="#b1">[2]</ref> to features that are outputs of pool5 layer of ResNet-50 <ref type="bibr" target="#b7">[8]</ref> pre-trained on ImageNet. This setting is motivated by transfer learning. The overall accuracy of the pseudo labels is 62.50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clothing1M:</head><p>We use the Clothing1M dataset <ref type="bibr" target="#b20">[21]</ref> to examine the performance of our method in a real setting. The Clothing1M dataset contains 1 million images of clothing obtained from several online shopping websites that are classified into the following 14 classes: T-shirt, Shirt, Knitwear, Chiffon, Sweater, Hoodie, Windbreaker, Jacket, Down Coat, Suit, Shawl, Dress, Vest, and Underwear. The labels are generated by using surrounding texts of the images that are provided by the sellers, and therefore contain many errors. In <ref type="bibr" target="#b20">[21]</ref>, it is reported that the overall accuracy of the noisy labels is 61.54%, and some pairs of classes are often confused with each other (e.g., Knitwear and Sweater). The Clothing1M dataset also contains 50k, 14k and 10k of clean data for training, validation, and testing, respectively although we do not use the 50k clean training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>We implemented all the models with the deep learning framework Chainer v2.1.0 <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10:</head><p>Training on SN-CIFAR, AN-CIFAR and PL-CIFAR, we used the network based on PreAct ResNet-32 <ref type="bibr" target="#b8">[9]</ref> as detailed in Appendix A. With respect to preprocessing, we performed mean subtraction and data augmentation by horizontal random flip and 32?32 random crops after padding with 4 pixels on each side. We used SGD with a momentum of 0.9, a weight decay of 10 ?4 , and batch size of 128.</p><p>In the first step of our method, we trained the network for 200 epochs and began updating labels from the 70th epoch. We determined the values of a learning rate and the hyper parameters (?, ? in Eq. (5)) for SN-CIFAR, AN-CIFAR, and PL-CIFAR respectively based on the validation accuracy. The details are described in each experimental section. As we will describe in Section 5.4, soft-labels performed better than hard-labels, and thus we applied soft-labels to all the experiments in Section 5.5, Section 5.6, and Section 5.7. In this case, the prior distribution p is uniform distribution because each class has the same number of images in the CIFAR-10 dataset. While updating the noisy label y i by the probability s, we used the average output probability of the network of the past 10 epochs as s. We experimentally determined that this averaging technique is useful in preventing inaccurate updates since it has a similar effect to ensemble.</p><p>In the second step of our method, we trained the network for 120 epochs on the labels obtained in the first step. We began training with a learning rate of 0.2 and divided it by 10 after 40 and 80 epochs. We used only L c for the training loss in this step.</p><p>Clothing1M: Training on the Clothing1M dataset, we used ResNet-50 pre-trained on ImageNet to align experimental condition with <ref type="bibr" target="#b13">[14]</ref>. For preprocessing, we resized the images to 256 ? 256, performed mean subtraction, and cropped the middle 224 ? 224. We used SGD with a momentum of 0.9, a weight decay of 10 ?3 , and batch size of 32.</p><p>In the first step of our method, we trained the network for 10 epochs and began updating labels from the 1st epoch. We used a learning rate of 8 ? 10 ?4 , and used 2.4 for ? and 0.8 for ?. While updating the noisy label y i by the probability s, we used the average output probability of the network of all the past epochs as s. We applied soft-labels to the experiment in Section 5.8.   At the end of training with a low learning rate, the value of the training loss is close to 0 even if the error rate is 0.9. In contrast, in the early phase of training with a high learning rate, an increase in the noise rate increases the training loss.</p><p>In the second step of our method, we trained the network for 10 epochs on the labels obtained in the first step. We began training with a learning rate of 5 ? 10 ?4 and divided it by 10 after 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Generalization and Memorization</head><p>To examine the effect of the learning rate (lr) and the noise rate (r) on the training loss and the test accuracy, we trained the network on SN-CIFAR with only the cross entropy loss. <ref type="figure">Fig. 2</ref> shows the test accuracy curve with different learning rates. We trained the network for 120 epochs with a learning rates of 0.2 or 0.02. In the case of the low learning rate (lr=0.02), the test accuracy was high at the early phase of training and then gradually  decreased because the network fitted the noisy labels. This is the same result reported in <ref type="bibr" target="#b0">[1]</ref>. Conversely, in the case of the high learning rate (lr=0.2), the network exhibited high test accuracies during training. This means that a high learning rate prevents the network from memorizing and fitting the noisy labels. <ref type="figure" target="#fig_2">Fig. 3</ref> shows how the manner in which training loss declines during training. We trained the network for 600 epochs. We commenced training with a learning rate of 0.2 and divided it by 10 after 200 and 400 epochs. At the end of training, our model fit the noisy labels even if the noise rate was high (for e.g., r = 0.9). However, with respect to training with a high learning rate, the training loss clearly increases when the noise rate increases. This indicates that it is possible to optimize the labels towards lowering the training loss when the learning rate is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Hard-Label vs. Soft-Label</head><p>To prove the effectiveness of the soft-label, we trained the network on SN-CIFAR (noise rate r = 0.7) for 1500 epochs by using the first step of our method. We compare the hard-label methods and the soft-label method. For the hard-labels methods, we update top 50, 500, 5000, or all labels whose current labels are most different from the predicted classes to the predicted hard-labels every epoch. For the soft-label method, we update all labels to the predicted softlabels every epoch. In <ref type="figure" target="#fig_4">Fig. 4</ref>, we show the recovery accuracy, which is defined as the accuracy of the reassigned labels, in the first step of our method. The soft-label method achieves faster convergence and better recovery accuracy than any hard-label methods.</p><p>Subsequently, by using the second step of our method, we performed training on the labels obtained in the first step. In the hard-label methods, updating 500 labels every epoch is optimal and the test accuracy is 85.7%. Conversely, the test accuracy of the soft-label method is 86.0%. It shows that though the recovery accuracy of the soft-label method obtained in the first step is 86.0%, which is approximately equal to 85.9% of the hard-label method (updating 500 labels every epoch), the test accuracy is improved by 0.3%. The reason why the soft-label method performed better is considered as that soft-labels contain the probabilities of each class in themselves. Soft-labels reflect confidences of the trained network unlike hard-labels, which are assigned by ignoring confidences. Our results indicate that confidences are important in the case of training on noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experiment on SN-CIFAR</head><p>To evaluate the performance of our method on synthesized noisy labels, we trained the network on SN-CIFAR (the noise rate r = 0.0, 0.1, 0.3, 0.5, 0.7, 0.9) by using our method. In the first step of our method, we used the optimal learning rate, ? and ? for each noise rate based on the validation accuracy as detailed in Appendix B. As a comparison, we also trained on initial noisy labels in the same manner as the second step of our method.</p><p>The results are reported in <ref type="table" target="#tab_1">Table 1</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, best denotes the scores of the epoch where the validation accuracy is optimal, and last denotes the scores at the end of training. The recovery accuracy for our method is defined as the accuracy of the reassigned labels. Conversely, other methods do not reassign the noisy labels, and thus the recovery accuracy is reported as the prediction accuracy on the ground-truth labels of the noisy training data.</p><p>Our method achieves overall better test accuracy and recovery accuracy on SN-CIFAR. When training was performed on initial noisy labels, the test accuracy decreases after approximately the 40th epoch (when we divided the learning rate by 10). This indicates that lowering the learning rate assists the network in fitting the noisy labels as described in Section 5.3. Conversely, when we trained on the labels optimized by our method, the test accuracy was high until the end of training. This is the important effects of our joint optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Experiment on AN-CIFAR</head><p>To evaluate the performance of our method in the settings in <ref type="bibr" target="#b13">[14]</ref>, we trained the network on AN-CIFAR (the noise rate r = 0.1, 0.2, 0.3, 0.4, 0.5) by using our method. In the first step of our method, we used a learning rate of 0.03 and used 0.8 for ? and 0.4 for ?, respectively for all the noise rates. As a comparison,  <ref type="figure" target="#fig_0">.1 95.1 91.8 86.4 58.</ref>3 <ref type="table">Table 2</ref>. Test and recovery accuracy of different baselines on the CIFAR-10 dataset with asymmetric noise. We report the average score of 5 trials. #2, #3 are the results by our implementation. we also performed training on initial noisy labels in the same manner as the second step of our method with the cross entropy loss or the forward corrected loss <ref type="bibr" target="#b13">[14]</ref>.</p><p>The results of our experiments are shown in Table 2. The forward corrected loss <ref type="bibr" target="#b13">[14]</ref> and the CNN-CRF model <ref type="bibr" target="#b18">[19]</ref> require the ground-truth noise transition matrix. Conversely, we need only the prior distribution p, and thus our condition is more general than that of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>Our method achieves significantly better test accuracy and recovery accuracy on AN-CIFAR. However, only when the noise rate is 50%, there is no significant improvement in accuracy when compared with other noise rates. Since we generated label noise to exchange CAT and DOG classes, it is impossible to accurately determine the class for CAT and DOG when the noise rate is 50%.</p><p>In a manner similar to Section 5.5, when training is performed on initial noisy labels, the test accuracy decreases due to the network fitting noisy labels with a low learning rate. This trend is also observed if we use the forward corrected loss <ref type="bibr" target="#b13">[14]</ref>, while the test accuracy does not decrease and remains high in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Experiment on PL-CIFAR</head><p>To evaluate the performance of our method in the settings of transfer learning, we trained the network on PL-CIFAR by using our method. In the first step of our method, we used a learning rate of 0.04 and used 1.2 for ? and 0.8 for ?. As a comparison, we also trained on initial pseudo-labels in the same manner as the second step of our method. <ref type="figure">Fig. 5</ref> shows the test accuracy curve with different labels, and <ref type="figure">Fig. 6</ref> shows the decline in the training loss during training. In both figures, we show the results of training on SN-CIFAR (the noise rate r = 0.3, 0.5) because the noise rate of the pseudo labels is between 0.3 and 0.5. Additionally, we show the results of training on the ground-truth labels because the training loss curve of training on optimized labels is near the curve for the same. Although the number of inaccurate labels in the pseudo labels exceeds that of the symmetric noise labels (r = 0.3), the value of the training loss of the pseudo labels is lower than that of the symmetric noise labels. This fact seems to conflict with extant knowledge that states that "the training loss increases when the noise rate increases", as described in Section 5.3. However, we can explain the reason of this conflict as follows: the difference in the training loss depends on the noise rate as well as the type of the noise. The pseudo labels are generated from the outputs of ResNet-50 pre-trained on ImageNet, and thus they are already considered as "the optimized labels" by the network. Thus, the pseudo labels were not updated adequately. The test accuracy of training on the labels recovered from the noisy labels is worse than that of training on the ground-truth labels, and this indicates that the optimized labels do not necessarily denote optimal labels. This is a limitation of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Experiment on the Clothing1M dataset</head><p>Finally, we trained the network on the Clothing1M dataset <ref type="bibr" target="#b20">[21]</ref> by using our method to evaluate the performance of our method in a real setting. As a compar- initial pseudo updated pseudo ground-truth <ref type="figure">Figure 5</ref>. The test accuracy curve with different labels. The test accuracy on the pseudo labels is lower than that on the symmetric noise labels even if the number of inaccurate labels is lower. This trend remains if the labels are updated.</p><p>ison, we also trained on initial noisy labels in the same manner as the second step of our method. The results of our experiments are shown in <ref type="table" target="#tab_3">Table 3</ref>. Additionally, we also show the scores (#1, #2) reported in <ref type="bibr" target="#b13">[14]</ref>. In #2, Patrini et al. exploited the curated labels of 50k clean data and their noisy versions in 1M noisy data to obtain the ground-truth noise transition matrix, which is not often used in real-world settings. Conversely, we only used the distribution of the noisy labels, which can be always used, for the prior distribution p, and therefore our condition is more general than #2. Nevertheless, our method achieves better test accuracy than #2 on the Clothing1M dataset.</p><p>In <ref type="figure" target="#fig_7">Fig. 7, Fig. 8, we</ref> show the examples of the images whose labels were reassigned to classes different from the original ones by our method. Additionally, we show the probability of the class that the label is reassigned to. When the probability is high, the label seems to be updated correctly. Conversely, when the probability is low, the label seems to be updated incorrectly. As opposed to the hard-labels, the soft-labels contain the probabilities of each class in themselves, and thus the network can consider the incorrectly updated labels as not important. Specifically, this effect contributes to improving the test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a joint optimization framework for learning on noisy labeled datasets, which alternatively updates network parameters and class labels. The performance of the framework is guaranteed by our finding that training under a high learning rate prevents the network from memorizing noisy labels. We showed that our framework performed remarkably well on the noisy CIFAR-10 dataset and the Clothing1M dataset,  <ref type="figure">Figure 6</ref>. The training loss curve with different labels. The training loss on the pseudo labels is lower than that on the symmetric noise labels even if the number of inaccurate labels is higher. Each of the training losses on the labels updated from different noisy labels follows the training loss on the ground-truth labels. This implies that the updated labels are completely optimized for the network.   outperforming the state-of-the-art methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="table" target="#tab_4">Table 4</ref> details the network architecture used in the experiments on the CIFAR-10 dataset. It is based on PreAct ResNet-32 <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dependency on Hyper Parameters</head><p>We show the hyper parameters used in the experiments on SN-CIFAR in <ref type="table" target="#tab_5">Table 5</ref>. If the noise rate is high, the optimal learning rate also tends to be high. The prediction accuracy is not so sensitive to the hyper parameters and our method demonstrated good performance with a different set of the hyper parameters as shown in <ref type="table" target="#tab_6">Table 6</ref> <ref type="bibr">, 7, 8, 9.</ref> In addition, Table 10, 11 show the validation accuracy with different t 1 and t 2 , where t 1 is the value at which to start label-updating, and t 2 is the value at which to stop label-updating. When we train the network with a high learning rate, the prediction accuracy retains high value, and thus we can start label-updating when the validation accuracy once reach high value. Labelupdating should be stopped after the training loss converge.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of Soft-Labeling</head><p>We show the analysis of the effect of soft-labeling on the noisy CIFAR-10 dataset in <ref type="table" target="#tab_1">Table 12</ref>, 13. The softlabels with high probability are almost correct. Conversely, when the probability is low, the label seems to be updated incorrectly. As opposed to the hard-labels, the soft-labels contain the probabilities of each class in themselves, and thus the network can consider the incorrectly updated labels as not important.  <ref type="table" target="#tab_1">Table 13</ref>. Recovery accuracies of the updated soft-labels whose maximum probabilities p are within each range (experimented on SN-CIFAR with noise rate = 0.7). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The concept of our joint optimization framework. Noisy labels are reassigned to the probability output by CNNs. Network parameters and labels are alternatively updated for each epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The training loss curve with different noise rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between the soft-label and the hardlabel methods, showing the recovery accuracy. The softlabel method achieves faster convergence than the hardlabel methods, and performs the best recovery accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>The images with the top-2 and the bottom-2 probabilities of T-shirt whose labels are reassigned from Hoodie to T-shirt.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>The images with the top-2 and the bottom-2 probabilities of Hoodie whose labels are reassigned from T-shirt to Hoodie.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>p 1 ?</head><label>1</label><figDesc>0.99 0.99 ? 0.95 0.95 ? 0.9 0.9 ? 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Test and recovery accuracy of different baselines on the CIFAR-10 dataset with symmetric noise. We report the average score of 5 trials. Cross Entropy Loss best 93.<ref type="bibr" target="#b4">5</ref> 91.0 88.4 85.0 78.4 41.1 100.0 96.4 92.7 88.2 80.1 41.4 last 93.4 87.0 72.2 55.3 36.6 20.4 100.0 91.1 74.6 57.6 39.6 21.7</figDesc><table><row><cell>#</cell><cell>method</cell><cell></cell><cell></cell><cell cols="2">Test Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Recovery Accuracy (%)</cell></row><row><cell></cell><cell>noise rate (%)</cell><cell>0</cell><cell>10</cell><cell>30</cell><cell>50</cell><cell>70</cell><cell>90</cell><cell>0</cell><cell>10</cell><cell>30</cell><cell>50</cell><cell>70</cell><cell>90</cell></row><row><cell>1 2</cell><cell>Our Method</cell><cell cols="12">best 93.4 92.7 91.4 89.6 85.9 58.0 100.0 97.9 95.1 91.7 86.3 58.2</cell></row><row><cell></cell><cell></cell><cell cols="6">last 93.6 92.9 91.5 89.8 86.0 58.3</cell><cell>99.9</cell><cell>98</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Test accuracy of different baselines on the Cloth-ing1M dataset. #1 and #2 are quoted from<ref type="bibr" target="#b13">[14]</ref>, and #3 is the reproduced result by our reimplementation.</figDesc><table><row><cell>#</cell><cell>method</cell><cell></cell><cell>accuracy</cell></row><row><cell cols="2">1 Cross Entropy Loss</cell><cell></cell><cell>68.94</cell></row><row><cell>2</cell><cell>Forward [14]</cell><cell></cell><cell>69.84</cell></row><row><cell cols="3">3 Cross Entropy Loss best</cell><cell>69.15</cell></row><row><cell></cell><cell>(reproduced)</cell><cell>last</cell><cell>66.76</cell></row><row><cell>4</cell><cell>Our Method</cell><cell>best</cell><cell>72.16</cell></row><row><cell></cell><cell></cell><cell>last</cell><cell>72.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The network architecture used in the experiments on CIFAR-10.</figDesc><table><row><cell cols="2">NAME DESCRIPTION</cell></row><row><cell>input</cell><cell>32?32 RGB imgae</cell></row><row><cell>conv</cell><cell>32 filters, 3?3, pad=1, stride=1</cell></row><row><cell>unit1</cell><cell>(pre-activation Residual Unit 32?32)?5</cell></row><row><cell>unit2a</cell><cell>pre-activation Residual Unit 32?64</cell></row><row><cell cols="2">unit2b (pre-activation Residual Unit 64?64)?4</cell></row><row><cell>unit3a</cell><cell>pre-activation Residual Unit 64?128</cell></row><row><cell cols="2">unit3b (pre-activation Residual Unit 128?128)?4</cell></row><row><cell>pool</cell><cell>Batch Normarization, ReLU,</cell></row><row><cell></cell><cell>Global average pool (8?8?1?1 pixels)</cell></row><row><cell>dense</cell><cell>Fully connected 128?10</cell></row><row><cell cols="2">output Softmax</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The hyper parameters used in the experiments on SN-CIFAR.</figDesc><table><row><cell>noise rate (%)</cell><cell>0</cell><cell>10</cell><cell>30</cell><cell>50</cell><cell>70</cell><cell>90</cell></row><row><cell>?</cell><cell>1.2</cell><cell>1.2</cell><cell>1.2</cell><cell>1.2</cell><cell>1.2</cell><cell>0.8</cell></row><row><cell>?</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell><cell>0.4</cell></row><row><cell>learning rate</cell><cell cols="6">0.01 0.02 0.03 0.04 0.08 0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Validation accuracy with different hyper parameters in the triple test (experimented on AN-CIFAR with noise rate = 0.4).</figDesc><table><row><cell></cell><cell cols="5">? = 0.4, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell>1.0</cell><cell>2.0</cell><cell>5.0</cell></row><row><cell>val (%)</cell><cell cols="7">91.9 92.0 91.7 92.0 92.1 92.1 88.8</cell></row><row><cell></cell><cell cols="5">? = 0.8, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell></row><row><cell>val (%)</cell><cell cols="7">90.8 91.7 91.8 92.0 91.6 89.5 91.1</cell></row><row><cell></cell><cell></cell><cell cols="3">? = 0.8, ? = 0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">learning rate 0.005 0.01 0.02 0.03 0.05 0.1</cell><cell>0.2</cell></row><row><cell>val (%)</cell><cell cols="7">90.6 90.9 91.3 92.0 92.1 91.3 88.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Validation accuracy with different hyper parameters in the triple test (experimented on AN-CIFAR with noise rate = 0.2).</figDesc><table><row><cell></cell><cell cols="5">? = 0.4, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell>1.0</cell><cell>2.0</cell><cell>5.0</cell></row><row><cell>val (%)</cell><cell cols="7">92.9 92.9 93.0 93.2 93.1 93.2 89.7</cell></row><row><cell></cell><cell cols="5">? = 0.8, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.5</cell><cell>1.0</cell><cell>2.0</cell></row><row><cell>val (%)</cell><cell cols="7">92.6 93.0 93.2 93.2 93.1 92.8 92.8</cell></row><row><cell></cell><cell></cell><cell cols="3">? = 0.8, ? = 0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">learning rate 0.005 0.01 0.02 0.03 0.05 0.1</cell><cell>0.2</cell></row><row><cell>val (%)</cell><cell cols="7">92.5 92.7 92.7 93.2 92.7 91.8 89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Validation accuracy with different hyper parameters in the triple test (experimented on SN-CIFAR with noise rate = 0.7).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Validation accuracy with different hyper parameters in the triple test (experimented on SN-CIFAR with noise rate = 0.3).</figDesc><table><row><cell></cell><cell cols="5">? = 0.8, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell><cell>1.0</cell><cell>1.2</cell><cell>2.0</cell><cell>5.0</cell></row><row><cell>val (%)</cell><cell cols="7">91.6 91.7 91.5 91.8 91.8 91.8 89.9</cell></row><row><cell></cell><cell cols="5">? = 1.2, learning rate = 0.03</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.5</cell><cell>0.8</cell><cell>1.0</cell><cell>2.0</cell></row><row><cell>val (%)</cell><cell cols="7">90.0 90.4 91.2 91.8 91.8 91.9 91.0</cell></row><row><cell></cell><cell></cell><cell cols="3">? = 1.2, ? = 0.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">learning rate 0.005 0.01 0.02 0.03 0.05 0.1</cell><cell>0.2</cell></row><row><cell>val (%)</cell><cell cols="7">90.1 90.7 91.0 91.8 92.1 91.1 89.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Validation accuracy with different t1 (start epoch) and t2 (stop epoch) in the triple test (experimented on AN-CIFAR with noise rate = 0.4, ? = 0.8, ? = 0.4, learning rate = 0.1).Table 11. Validation accuracy with different t1 (start epoch) and t2 (stop epoch) in the triple test (experimented on SN-CIFAR with noise rate = 0.7, ? = 1.2, ? = 0.8, learning rate = 0.08).</figDesc><table><row><cell>start epoch</cell><cell>0</cell><cell>50</cell><cell>70</cell><cell>100 150</cell></row><row><cell>val (%)</cell><cell cols="4">58.4 90.3 91.3 91.4 91.6</cell></row><row><cell cols="5">stop epoch 100 150 200 250 300</cell></row><row><cell>val (%)</cell><cell cols="4">91.8 91.5 91.3 90.8 90.7</cell></row><row><cell>start epoch</cell><cell>0</cell><cell>50</cell><cell>70</cell><cell>100 150</cell></row><row><cell>val (%)</cell><cell cols="4">38.0 84.7 85.5 86.1 85.6</cell></row><row><cell cols="5">stop epoch 100 150 200 250 300</cell></row><row><cell>val (%)</cell><cell cols="4">85.0 85.6 85.5 85.9 85.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Recovery accuracies of the updated soft-labels whose maximum probabilities p are within each range (experimented on AN-CIFAR with noise rate = 0.4).</figDesc><table><row><cell>p</cell><cell cols="5">1 ? 0.99 0.99 ? 0.95 0.95 ? 0.9 0.9 ? 0 1 ? 0</cell></row><row><cell>acc (%)</cell><cell>99.8</cell><cell>96.9</cell><cell>91.3</cell><cell>73.1</cell><cell>95.1</cell></row><row><cell>number</cell><cell>27046</cell><cell>8647</cell><cell>3484</cell><cell>5823</cell><cell>45000</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is partially supported by CREST (JPMJCR1686).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines with the ramp loss and the hard margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making risk minimization tolerant to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of semisupervised learning with the yarowsky algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning discrete representations via information maximizing self augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning deep networks from noisy labels with dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nokleby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Tronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making neural networks robust to label noise: a loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<title level="m">The new data and new challenges in multimedia research. Communications of the ACM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Computer Science, University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
