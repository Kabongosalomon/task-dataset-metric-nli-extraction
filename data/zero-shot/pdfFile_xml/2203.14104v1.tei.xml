<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilan</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Media Technology Institute</orgName>
								<orgName type="institution">Huawei Technologies Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjiang</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at: https: //github.com/ttlmh/Bridge-Prompt.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action recognition models have shown a promising capability to classify human actions in short video clips. In a real scenario, multiple correlated human actions commonly occur in particular orders, forming semantically meaningful human activities. Conventional action recognition approaches focus on analyzing single actions. However, they fail to fully reason about the contextual relations between adjacent actions, which provide potential temporal logic for understanding long videos. In this paper, we propose a prompt-based framework, Bridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so that it simultaneously exploits both out-of-context and contextual information from a series of ordinal actions in instructional videos. More specifically, we reformulate the individual action labels as integrated text prompts for supervision, which bridge the gap between individual action semantics. The generated text prompts are paired with corresponding video clips, and together co-train the text encoder and the video encoder via a contrastive approach. The learned vision encoder has a stronger capability for ordinal-action-related downstream tasks, e.g. action segmentation and human activity recognition. We evaluate the performances of our approach on several video datasets: Georgia Tech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset. Br-Prompt achieves state-of-the-art on multiple benchmarks. Code is available at: https: //github.com/ttlmh/Bridge-Prompt.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have witnessed the flourish of video analysis. Understanding human actions is the key to analyzing massive amounts of video data, which is conducive to a wide range of applications including video retrieval <ref type="bibr" target="#b7">[8]</ref>, video captioning <ref type="bibr" target="#b27">[28]</ref> and video summarization <ref type="bibr" target="#b1">[2]</ref>. Among the many sub-topics in action analysis, action recognition is ? Corresponding author. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Text prompts</head><p>This video contains four actions in total.</p><p>Firstly, the person is taking bread.</p><p>After that, he/she is putting cheese on bread.</p><p>Then, the step is adding mayonnaise and mustard.</p><p>Finally, put the bread on cheese and bread. a basic and core issue, which has made remarkable progress under various well-designed models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Meanwhile, the current research trend of video analysis is experiencing a transition from understanding singlesemantics short video clips to longer and more complex videos <ref type="bibr" target="#b37">[38]</ref>. The increased attention on instructional video analysis has shown the significance of understanding semantically rich video contents <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>. From the perspective of action analysis, conventional action recognition approaches focus on classifying the single action being performed in a short video clip <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>. In contrast, instructional video analysis methods need to study a series of actions being performed in longer time duration. In order to analyze instructional videos, we do not only need to understand the semantics of individual actions, but are also required to learn the semantic relations between contextual actions. Recently, some works have studied the mutual information between correlated actions in instructional videos using graph-based models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref>. The common approach is to regard each kind of action as a single node on a graph, where the edges between the nodes represent the contextual relations between adjacent actions.</p><p>However, the graph-based approaches are transductive, which are limited by the prior knowledge of input nodes and/or edges. Therefore, graph-based approaches are unable to address unknown types of nodes and thus are hard to extend and transfer. Moreover, under the existing framework of action recognition, the current way of depicting human actions is to allocate individual annotations to every single action, where different actions are treated as separate class IDs. This is practicable for recognizing separate actions, yet it is unable to depict the contextual relations between ordinal actions since individual class IDs cannot provide contextual information. The example of (a) and (b) in <ref type="figure" target="#fig_1">Figure 1</ref> further illustrates the limitations of conventional class-ID-based approaches.</p><p>In this paper, we discover that human language is a powerful tool to depict the ordinal semantics between correlated actions. Human language is able to describe multiple sequentially occurred events based on ordinal numerals and specific sentence patterns. For example, ordinal relations between taking bottle and pouring water can be described in:"the person firstly takes (the) bottle, and then pours water (into it)". The language naturally bridges the semantics between ordinal actions. In certain circumstances, even the textual descriptions of actions themselves can provide contextual information. For example, the ordinal relationship between actions of taking bread, putting cheese on bread and putting bread on cheese and bread is easy to be deduced literally. Moreover, language can intuitively extrapolate to unknown types of action. Given a new expression putting bread on bread, its semantics can be inferred from the expressions of known types of action. <ref type="figure" target="#fig_1">Figure 1</ref>(c) illustrates the effectiveness of language representations.</p><p>To this end, we propose a text-based learning method, Bridge-Prompt, for instructional video analysis. Motivated by the recent advances of prompt-based learning approach in Natural Language Processing (NLP) <ref type="bibr" target="#b24">[25]</ref> and visual recognition <ref type="bibr" target="#b30">[31]</ref>, we introduce a three-plus-one-level design of text prompts to analyze the video clips containing a series of ordinal actions. <ref type="figure" target="#fig_1">Figure 1</ref> shows the comparisons between conventional and Bridge-Prompt representations of ordinal actions. More specifically, we develop a prompt-based learning framework to jointly co-train the video and text encoders based on a specially designed video-text fusion module, so that we simultaneously exploit out-of-context and contextual action information towards a more comprehensive understanding of instructional videos. Our work digs deeper into the further potential of prompt-based learning approaches towards ordinal action understanding and instructional video analysis. Extensive experimental results on three benchmark datasets illustrate that the Bridge-Prompt-based approaches have achieved promising performances, and reach state-of-the-art on several benchmarks with the help of the prompt-based learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action analysis on instructional videos. Instructional video analysis is an increasingly popular trend in the field of video understanding. A wide variety of instructional video datasets have been proposed in recent years <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. Instructional videos include profuse semantic information of human activities. The conventional approaches on action recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> mainly focus on the datasets of trimmed video clips containing a single action in each video clip <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>. Based on the existing studies of action recognition, several works have extended the action analysis methods to instructional videos by paying attention to the relations between ordinal actions. GTRM <ref type="bibr" target="#b14">[15]</ref> utilizes a graph-based structure to depict the ordinal actions, and analysis is based on Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b19">[20]</ref>. GHRM <ref type="bibr" target="#b42">[43]</ref> also represents ordinal actions as a graph, while focusing on the long-term action recognition task. Besides, Shao et al. <ref type="bibr" target="#b32">[33]</ref> proposed the TransParser method for intra-and inter-action understanding via temporal action parsing. Different from the previous solutions, we make use of human language as a powerful semantic tool for analyzing ordinal actions in instructional videos. Prompt-based learning on computer vision. Promptbased learning approaches have been extensively studied in NLP <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. The pioneer language model as GPT-3 <ref type="bibr" target="#b3">[4]</ref> has shown its great few-show or zero-shot potential across various tasks. The core of prompt-based learning is to modify the input sample as a prompted version and embed the expected output information as an unfilled slot inside the prompt. CLIP <ref type="bibr" target="#b30">[31]</ref> introduces the prompt-based learning approach into the image recognition task by embedding the textual labels of the to-be-recognized objects into descriptive texts, and the classification procedure can be transformed into a video-text matching problem. Following the prompt-based design, ALIGN <ref type="bibr" target="#b18">[19]</ref> scales up the vision-language model by training on over one billion noisy image-text pairs and achieves better prompt-based prediction performances than CLIP. CoOp <ref type="bibr" target="#b43">[44]</ref> utilizes learnable tokens as textual prompts and gains a promotion on fewshot image classification. CLIP-Adapter <ref type="bibr" target="#b11">[12]</ref> combines the adapted features generated by the designed feature adapter with the CLIP feature to fit the few-shot classification. The prompt-based learning approach has not been widely developed on video understanding. ActionCLIP <ref type="bibr" target="#b39">[40]</ref> proposes a specially designed prompt-based paradigm for action recog- nition, but it mainly focuses on recognizing single actions in short video clips. Our proposed Bridge-Prompt aims at analyzing instructional videos, which is more challenging but more conducive to understanding human behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the overall pipeline design of Bridge-Prompt. The pipeline of our approach is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prompt Engineering</head><p>Prompt engineering refers to the design of an input text template that embeds the expected output strings as fillin-the-blank formats <ref type="bibr" target="#b3">[4]</ref> (e.g., cloze test). The objective of our prompt engineering procedure is to design specific forms of text prompts to describe groups of ordinal actions in instructional videos. Suppose a series of single actions (A = {a 1 , a 2 , ..., a K }) composes a specific kind of human activity. An easier way to design the prompts is to pose a blank-filling problem for every single action. For example, the prompt format as "the person is {vp i } right now" (vp i refers to the verb-phrase description for action a i ) can be used to abstract the semantics for each separate action of the character. However, since each action is still treated as an independent prompt instance, this strategy is unable to depict the contextual semantics between adjacent ordinal actions. For example, within the human activity of scrambling egg, the stir-frying egg action can only happen after cracking egg. A better form of text prompts towards ordinal action analysis should not only capture the out-of-context semantics of each separate action, but also bridge the gap between contextually related actions, and depict the overall semantics of the series of actions.</p><p>To better represent the series of actions in the Bridge-Prompt framework, we propose a three-plus-one-level design of prompt engineering for instructional videos: statistical prompt, ordinal prompt, semantic prompt, and integrated prompt. Considering the input video cut with K consecutive actions: 1) Statistical prompt captures the total count information for the series of actions. We use the format as "this video clip contains {num(K)} actions in total". The statistical prompt is denoted as y stat .</p><p>2) Ordinal prompt captures the positional information for each action. We use the format as "this is the {ord i } action in the video". The ordinal prompt is denoted as y i ord . The ordinal prompt set for x is denoted as:</p><formula xml:id="formula_0">Y ord = [y 1 ord , ..., y K ord ]<label>(1)</label></formula><p>3) Semantic prompt is the core of prompt design, which captures the semantic information of the actions. To integrate both out-of-context and contextual action information, we merge the ordinal information into the semantic prompts to create a multi-prompt format. We use the format as "{ord i }, the person is performing the action step of {vp i }" for action a i . The semantic prompt set for x can be denoted as:</p><formula xml:id="formula_1">Y sem = [y 1 sem , ..., y K sem ]<label>(2)</label></formula><p>3+1) Integrated prompt captures the overall information for video x. The integrated prompt is formed by the integra-Transformer Encoder</p><formula xml:id="formula_2">3 4 5 6 7 8 O R D 1 S E P 2 C N T 0 ? 3 4 5 6 7 8 0 ? Mean Pooling</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This is the first action.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This is the second action.</head><p>This is the third action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion Module Fusion Module Fusion Module</head><p>Firstly, the person is { label 1 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Secondly, he/she is { label 2 }.</head><p>Thirdly, the action is { label 3 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Firstly, the person is { label 1 }, secondly, he/she is { label 2 }, thirdly, the action is { label 3 }. This video has { num } actions.</head><p>Ordinal prompts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic prompts</head><p>Integrated prompt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical prompt</head><p>Frame-level features <ref type="figure">Figure 3</ref>. Detailed illustration of prompt formats and fusion encoder. The fusion encoder takes the encoded frame-wise features and the ordinal prompt embeddings as inputs. It employs a learnable count token to analyze the statistical information. We adopt an ordinal-attention manner, meaning that the module only focuses on a single action with respect to a particular ordinal each time.</p><p>The integrated semantics is extracted by mean-pooling operation.</p><p>tion of all semantic prompts Y sem . The integrated prompt y integ can be denoted by:</p><formula xml:id="formula_3">y integ = y 1 sem ? y 2 sem ? ... ? y K sem<label>(3)</label></formula><p>where ? refers to the string concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bridge-Prompt: Framework</head><p>Sampling for raw videos. The raw instructional video sample x 0 ? R L0?3?H?W contains L 0 RGB frames of size H ? W . Usually, L 0 is different for each raw video. Moreover, suppose K 0 actions are contained in x 0 , and K 0 is also unequal for different activities. Within each video, the duration of each action is unevenly distributed. We propose a sampling strategy by generating random video cut x ? R Lc?3?H?W from raw videos of a fixed length L c to extract useful information while improving training efficiency. Each cut x may contain a single action or several successive actions, where K denotes the action count for x.</p><p>The prompt engineering is conducted on those video cuts to generate the corresponding prompted text pair y. The sampling operation actually limits the temporal reception field of the model to a more localized range. The advantage of such a sampling strategy is to force the Bridge-Prompt model to focus more on the logical connections both within and between locally related actions.</p><p>Pre-training pipeline.</p><p>The sampled video cut x with L c frames [f 1 , ..., f Lc ] firstly passes through a frame-wise image encoder F I to generate the frame-level features</p><formula xml:id="formula_4">[F I (f 1 ), ..., F I (f Lc )].</formula><p>Meanwhile, according to the prompt rules, a set of textual prompts {y stat , Y ord , Y sem , y integ } can be generated for x.</p><p>A text encoder F T is introduced to extract the textual prompt embeddings {z stat , Z ord , Z sem , z integ } respectively. The frame-level features are then passed through a fusion encoder F F together with ordinal prompt embeddings to extract the clip-</p><formula xml:id="formula_5">level feature z i c = F F (F I (f 1 ), ..., F I (f Lc ), z i ord )</formula><p>for the i-th action of x. The design for the fusion module is the key to understanding both intra-action and inter-action information in x. We propose a Transformer-based structure for fusion. The information of ordinal prompt y i ord is fused into the fusion encoder to provide instructive information. We also embed a count token inside F F to collect the quantitative information to be matched with statistical prompt y stat . The details of the fusion approach for Bridge-Prompt pre-training will be discussed in the following sub-section. The clip-level feature is jointly learnt with both semantic prompts Y sem and integrated prompt y integ under a contrastive vision-text learning pattern. Fusion module.</p><p>The fusion encoder extracts the core information from the consecutive frame-level features. In other words, it tries to abstract the series of actions that occur in the input video clip. We utilize an ordinalattention manner for the fusion module, i.e., each time the fusion module only focuses on the action of a specific location. The ordinal-attention mechanism is implemented by adding the i-th ordinal prompt embeddings z i ord to the fusing inputs, which is an early-fusion strategy. We utilize a Transformer-Encoder structure for the fusion module. The joint vision-text representation learning maximizes the similarity between the encoded vision features and text features. A video clip x and its text description y can be encoded respectively with a video encoder and a text encoder, generating the clip representation z x and the text representation z y . The similarity between z x and z y can be defined as their cosine distance:</p><formula xml:id="formula_6">s(z x , z y ) = z x ? z y |z x | |z y |<label>(4)</label></formula><p>For a batch of the clip features Z x and its corresponding batch of text features Z y , the batch similarity matrix S is:</p><formula xml:id="formula_7">S(Z x , Z y ) = ? ? ? s(z x1 , z y1 ) ? ? ? s(z x1 , z y B ) . . . . . . . . . s(z x B , z y1 ) ? ? ? s(z x B , z y B ) ? ? ? (5)</formula><p>where B is the batch size. A text-wise/clip-wise softmaxnormalization function can be applied respectively along rows/columns on S(Z x , Z y ), generating S T (Z x , Z y ) and S V (Z x , Z y ). A ground truth batch similarity matrix GT is defined where the similarity score of positive pair equals to 1, while negative pair equals 0. Our objective is to maximize the similarity between S and GT . We define the Kullback-Leibler (KL) divergence for matrices as the multimodal contrastive loss:</p><formula xml:id="formula_8">D KL (P ?Q) = 1 N 2 N i=1 N i=1 P ij log P ij Q ij<label>(6)</label></formula><p>where P and Q are N ? N matrices. The contrastive loss for video-text pair can be defined as:</p><formula xml:id="formula_9">L = 1 2 [D KL (S T ?GT ) + D KL (S V ?GT )]<label>(7)</label></formula><p>Under the Bridge-Prompt framework, there are three parts of video-text contrastive losses in total: i) z i c fused by the i-th ordinal token with z i sem of corresponding ordinal prompt, notated as L i sem ; ii) mean-pooled z c fused by all ordinal tokens with z integ , notated as L integ ; iii) mean-pooled z [CN T ] with z stat , notated as L stat ; The overall loss objective for the Bridge-Prompt pretraining framework is as follows:</p><formula xml:id="formula_10">L = K i=1 L i sem + ? 1 L integ + ? 2 L stat<label>(8)</label></formula><p>where ? 1 and ? 2 balance the three losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Prompt-Based Inference</head><p>The "pre-train, prompt, and predict" paradigm in NLP has suggested that prompt-based design has the superiority of combining the objectives of downstream tasks into the pre-training procedure. The Bridge-Prompt framework has the capability of recognizing a series of actions by solving prompt-based cloze tests as "this video clip contains actions in total" or " , the person is performing the action of ". In practice, we first generate the text features for all relevant ordinal prompts, statistical prompts, and semantic prompts by the pre-trained text encoder. For each test video, we extract the clip-wise features embedded by different ordinal prompts z i c and the average statistical representation z [CN T ] using the pre-trained image encoder and fusion encoder. At first, we find the most matched embedding of statistical prompts with z [CN T ] to determine the total count of actions. Then, we find the most matched embedding of semantic prompt with each ordinal prompt-embedded clipwise feature z i c to determine each ordinal action one by one. As for the prompt variants, we vote among all variant formats to get the most matched prompt during inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate our proposed model on three challenging datasets. 50Salads <ref type="bibr" target="#b36">[37]</ref> contains 50 top-view 30-fps instructional videos regarding salad preparation. Totally 19 kinds of actions are contained in all videos. The 5-fold cross-validation is performed for evaluation, and the average results are reported. Georgia Tech Egocentric Activities (GTEA) [10] contains 28 egocentric 15-fps instructional videos of daily kitchen activities. Totally 74 classes of actions are summarized from all videos. We use the 4fold cross-validation to evaluate the performances, and the average results are reported. Breakfast [21] contains 1,712 third-person 15-fps videos of breakfast preparation activities. 48 types of different actions are included in all 10 different kinds of breakfast activities. For evaluation, we use the train-split setting as proposed in <ref type="bibr" target="#b15">[16]</ref>, with 1357 videos for training and 355 videos for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Sampling strategy.</p><p>The video cut sampling strategy is adjusted concerning frame rates and scales of different datasets. In general, we adopt a 16-frame window for each video cut. For GTEA dataset, we adopt multiple downsampling rates as 1, 2 and 4 respectively corresponding to the window striding rates of 2, 1 and 0.5. For 50Salads dataset, we use higher 24 and 32 downsampling rates with window striding rate of 1. For the Breakfast dataset, we a employ downsampling rate of 16 with a window striding rate of 2. Bridge-Prompt architectures. For the image and text encoders, we follow the setups as CLIP <ref type="bibr" target="#b30">[31]</ref> and Action-CLIP <ref type="bibr" target="#b39">[40]</ref>. We adopt ViT-B/16 <ref type="bibr" target="#b6">[7]</ref> as the image encoder F I , which is a 12-layer Transformer with input patch sizes of 16. The output representation for [CLS] token is regarded as the image feature. The text encoder F T is also a 12-layer Transformer with the width of 512 and 8 attention heads. The output representation for [EOS] token is regarded as the text feature. The output frame-wise feature of the image encoder is a 768-dimensional vector, which is mapped to a 512-dimensional latent vector to match the embedded text features. For the fusion module F F , we employ a Transformer-Encoder-based structure to fuse the information of both image and text features. The fusion module contains 6 layers. As for the details of the prompt engineering procedure, we utilize an invariant prompt format for ordinal prompts and statistical prompts. With respect to the semantical prompts (which also contribute to integrated prompts), we adopt 19 variant prompt formats (9 short variant versions for integrated prompts) to describe the action semantics. The average similarity of all variants are computed during the prompt-based inference stage. Training details. The image encoder and text encoder are together pre-trained on Kinetics-400 <ref type="bibr" target="#b4">[5]</ref> by <ref type="bibr" target="#b39">[40]</ref> before our training. We adopt AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer with the base learning rate of 5 ? 10 ?6 with a 0.2 weight decay. The first 10% of training epochs are set as a warm-up phase, and the learning rate gradually decays down to zero during the remaining epochs under a cosine schedule. The spatial resolution of the input video is 224 ? 224. For the loss function, we simply set ? 1 = ? 2 = 1. The model is trained for 50 epochs on GTEA and 50Salads, and 35 epochs on Breakfast. We use the batch size of 12 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Action Segmentation</head><p>The objective of action segmentation is to classify the action that occurs in each frame of a video <ref type="bibr" target="#b21">[22]</ref>. Different from action recognition, action segmentation processes videos with multiple action instances. In consequence, action segmentation approaches should not only understand the out-of-context semantics for each separate action, but also be aware of the logical relations between adjacent actions. Several works have been conducted, and have achieved promising segmentation results. Most of the current SOTA approaches on action segmentation utilize the frame-wise I3D <ref type="bibr" target="#b4">[5]</ref> features pre-trained on Kinetics extracted by <ref type="bibr" target="#b8">[9]</ref>, since the videos used for action segmentation are generally long videos that are hard to conduct direct analysis based on raw data. Bridge-Prompt utilizes a video cut-based approach to learn the contextual relations between adjacent actions locally, which is feasible on long videos. Since our approach is not specially designed for end-to-end action segmentation, we mainly adopt the Bridge-Prompt pre-trained image encoders to generate frame-wise features for raw videos. We test the action segmentation results based on current segmentation backbones. Evaluation metrics. To evaluate the action segmentation results, we adopt several metrics including frame-wise accuracy (Acc), segmental edit distance, and the segmental F1 score at overlapping thresholds {10%, 25%, 50%}, denote by F1@{10,25,50}. The frame-wise accuracy is the most direct and frequently used metric, whereas it is unable to penalize the over-segmentation errors in long-duration actions. The segmental edit distance and segmental F1 score <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> are proposed to handle over-segmentation errors and measure the segmentation quality. Comparisons with the state-of-the-art.</p><p>We compare the segmentation performances based on Bridge-Promptencoded frame-wise features with previous state-of-the-art methods. We use the ASFormer <ref type="bibr" target="#b41">[42]</ref> as the backbone model for proceeding action segmentation. Bridge-Prompt is used as the pre-training approach to train the frame-wise image encoder (ViT). The output 768-dimensional frame-wise representations are regarded as the training inputs for the action segmentation backbone. In comparison, the previous state-of-the-art approaches use 2048-dimensional I3D features as training inputs. We conduct action segmentation on the GTEA dataset and the 50Salads dataset. <ref type="table">Table 1</ref>, 2 compare the quantitative results of our approach. Specifically, we predict the 11 verbs of actions in GTEA for fair comparisons, and our method outperforms current state-of-the-art approaches under all five evaluation metrics. For comparison, we also evaluate the performances using raw features of ViT pre-trained by <ref type="bibr" target="#b39">[40]</ref>, which are inferior to the results using I3D-pre-trained features. However, after the ViT image encoder is further trained by Bridge-Prompt, the performances get obvious boosts. The performance of our approach also precedes previous stateof-the-art results on 50Salads. <ref type="figure" target="#fig_5">Figure 4</ref> shows the qualitative illustration of action segmentation on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Long-Term Activity Recognition</head><p>A series of ordinal actions in instructional videos generally form a high-level semantics of human activity. The objective of long-term activity recognition is to classify the types of activities in long videos. Recognizing a high-level activity requires understanding the basic relations and temporal evolution of its ordinal sub-actions. Since Bridge-Prompt aims to study the relations between ordinal actions,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc(%) Kinetics pre-trained I3D I3D <ref type="bibr" target="#b4">[5]</ref> 58.61 ActionVLAD <ref type="bibr" target="#b13">[14]</ref> 65.48 Timeception <ref type="bibr" target="#b15">[16]</ref> 67.07 VideoGraph <ref type="bibr" target="#b16">[17]</ref> 69.45 GHRM <ref type="bibr" target="#b42">[43]</ref> 75.49 Breakfast fine-tuned I3D (fine-tuned) <ref type="bibr" target="#b42">[43]</ref> 74.83 Br-Prompt (fine-tuned) 80.00</p><p>it is also capable of long-term activity recognition. To adapt our framework for long-term action recognition, we first pre-train the frame-level encoder based on the Bridge-Prompt framework, and extract the frame-wise features for each video. Then, we uniformly sample 64 segments in each video with 8 frames per segment as in <ref type="bibr" target="#b15">[16]</ref>. We use a simple Transformer-Encoder as a fusion module to respectively integrate segment-wise frames and different segments to generate video-wise representations. Then the human activities are predicted using prompt-based inferences.</p><p>Comparison with the state-of-the-art. The performances are evaluated on the Breakfast dataset as in <ref type="table" target="#tab_2">Table 3</ref>. The performance of Bridge-Prompt fine-tuned features precedes I3D fine-tuned features. Since Bridge-Prompt is not a specially designed architecture for activity recognition, our straightforward prompt-based recognition approach may be inferior to more complicated recognition backbones based on fine-tuned I3D (e.g. GHRM <ref type="bibr" target="#b42">[43]</ref>). The performance can be further improved by combining Bridge-Prompt representations with other high-level backbones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We perform several ablation studies on the GTEA dataset. Several adjustments have been conducted to evaluate the influence of different settings. Fusion approaches. We have studied more kinds of fusion strategies to integrate statistical or ordinal information into frame-wise features. They are listed as follows: (a) Vision-only fusion. Within the vision-only fusion, only the frame-wise features are regarded as inputs of the fusion Transformer. The output clip-wise features are contrastively learned together with statistical prompts, semantical prompts, and integrated prompts. Then it is added to the input frame-wise features as part of the positional embedding after a mapping operation. There are two ways of mapping: i. repeating the embedded vector along the width dimension; ii. computing the outer product between the embedded vector and ordinal prompt embedding. The output clip-wise features are contrastively learned together with all formats of text prompts as (a). (c) Ordinal prompt fused as weights of average. The ordinal prompt embedding can be linearly projected as a weight vector with its length equal to the clip length. Then it is served as the weights of pooling operation for the input frame-wise features. The output weights are punished by an L2 loss function to avoid acquiring impulse-shape weights. The output clip-wise features are contrastively learned together with all formats of text prompts as (a) and (b). (d) Early-fused ordinal prompts with a learnable count token. This is the fusion strategy adopted in our framework.</p><p>The action segmentation performances of different fusion strategies for Bridge-Prompt are evaluated on GTEA (split #1). <ref type="table" target="#tab_3">Table 4</ref> shows the quantitative results, which indicates that the fusion module is significant for improving the learning effectiveness of Bridge-Prompt. By merging ordinal information into the fusion module, the learned representations possess the focused information for each ordinal action. The fusion strategy (b) and (c) are more direct ways to integrate ordinal prompts, however, the ordinal prompt embeddings are not cross-attentioned with vision features. Specifically, the strategy (b) and (c) learn the information like "where may the first action be in any 16frame video clip?", while (d) focuses on "where is the first action among all the actions in this video?". The location for each ordinal action also depends on other adjacent actions, which makes the early-fusion way more convincible. Choice for loss functions. In our design, we consider three main components in the loss function: semantics, integrated semantics, and statistics. We perform ablation experiments to test the effectiveness of all three loss components. <ref type="table" target="#tab_4">Table 5</ref> shows the quantitative results, which indicates that all three losses make positive contributions to the final performance. It is reasonable since all the three text components are combined to depict both contextual and out-of-context semantics for a series of ordinal actions. Transferability studies. Text is a flexible and extensible form of supervision. Different from class IDs, knowledge in texts can be transferred to unseen forms of script based on the generalization ability of pre-trained language models. To verify the transferability of Bridge-Prompt, we conduct a test on the prompt-based ordinal action inferences. For humans, action knowledge can be transferred between similar activities. As an example, a person can possibly learn how to make tea if he/she knows how to make coffee, since the sub-actions of the two activities are highly similar. For a class ID-based model, it is unable to transfer the knowledge between similar activities without manual interventions. Under prompt-based inferences, it is as simple as replacing the filling-in words in prompts. To quantitatively explain the transfer effects, we conduct experiments by training the framework on one human activity and evaluating the prompt inference accuracy on another one. The results are displayed in <ref type="table" target="#tab_5">Table 6</ref>, which indicate that Bridge-Prompt has a promising zero-shot transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we have focused on the issue of ordinal action analysis in instructional videos. We proposed a promptbased learning framework, Bridge-Prompt, which models the semantic relations across ordinal actions. To capture both out-of-context and contextual information of ordinal actions, text prompts are designed to integrate statistical, ordinal, and semantic information. Further experiments are conducted on two downstream tasks including action segmentation and long-term action recognition. The results have demonstrated that Bridge-Prompt has strong capability in the analysis of ordinal actions. Limitations. Language can abstract the semantics from raw tedious videos. Although it is appealing to conduct largescale vision-language pre-training on massive instructional video datasets such as HowTo100M <ref type="bibr" target="#b28">[29]</ref>, we are limited by the computing resources. Fortunately, we find that the manual label is a more accurate and concise form of semantic abstraction. With the help of pre-trained language models, we are able to learn the semantics of ordinal actions in a more efficient and accurate way based on text supervision. Social impact. Despite the adaptiveness and convenience of the prompt-based approach to collaborate with vision models, it also means that fake labels are easier to create. To protect the vision-language model from potential attacks, label-filtering mechanisms and model selfinspections should be considered in practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparisons of conventional representations and Bridge-Prompt representations for ordinal actions. The human activity of making cheese sandwich contains four actions. Suppose the final action putting the bread on cheese and bread is unseen in training set. Conventional approaches in (a) and (b) are unable to depict the intra-semantics and inter-relations of all four actions, while our Bridge-Prompt representations in (c) is able to capture the full semantic information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of Bridge-Prompt pipeline. Bridge-Prompt takes the video cuts from minute-long raw inputs. After the special prompt engineering procedure, four types of text prompts are generated. Vision and text information are integrated both in the fusion module and during the video-text contrastive learning process. The proposed pipeline is able to capture the relations between ordinal actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The input tokens of the fusion encoder include a learnable count token [CNT], z i ord as a token [ORD], a split token [SEP], and L c visual tokens representing frame-level features. [ORD] indicates which number of actions the fusion encoder is focusing on. The encoded representations of L c frame-level features are mean-pooled to represent the cliplevel feature. Besides, we added a learnable count token to learn additional quantitative information of actions. The encoded representation z [CN T ] for [CNT] will pass through the same contrastive vision-text learning framework with statistical prompt embeddings z stat as a clip-level feature. Joint vision-text representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results for action segmentation task on (a) 50Salads, and (b) GTEA dataset. Part of the actions are annotated on the color bar. The Br-Prompt pre-trained representation has greater potential on action segmentation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) Ordinal prompt fused as positional embedding. The ordinal prompt embedding can be linearly projected as an embedded vector with its length equal to the clip length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Action segmentation results on GTEA dataset. Action segmentation results on 50Salads dataset.</figDesc><table><row><cell>GTEA</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>BCN [41]</cell><cell cols="2">88.5 87.1 77.3 84.4 79.8</cell></row><row><cell>MS-TCN++ [24]</cell><cell cols="2">88.8 85.7 76.0 83.5 80.1</cell></row><row><cell>ASRF [18]</cell><cell cols="2">89.4 87.8 79.8 83.7 77.3</cell></row><row><cell>G2L [13]</cell><cell cols="2">89.9 87.3 75.8 84.6 78.5</cell></row><row><cell>SSTDA [6]</cell><cell cols="2">90.0 89.1 78.0 86.2 79.8</cell></row><row><cell>SSTDA+HASR [1]</cell><cell cols="2">90.9 88.6 76.4 87.5 78.7</cell></row><row><cell>ASFormer (I3D) [42]</cell><cell cols="2">90.1 88.8 79.2 84.6 79.7</cell></row><row><cell>ASFormer (ViT)</cell><cell cols="2">88.5 86.2 77.6 87.1 75.6</cell></row><row><cell cols="3">Br-Prompt+ASFormer 94.1 92.0 83.0 91.6 81.2</cell></row><row><cell>50Salads</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>MS-TCN++ [24]</cell><cell cols="2">80.7 78.5 70.1 74.3 83.7</cell></row><row><cell>BCN [41]</cell><cell cols="2">82.3 81.3 74.0 74.3 84.4</cell></row><row><cell>SSTDA [6]</cell><cell cols="2">83.0 81.5 73.8 75.8 83.2</cell></row><row><cell>ASRF [18]</cell><cell cols="2">84.9 83.5 77.3 79.3 84.5</cell></row><row><cell>ASFormer (I3D) [42]</cell><cell cols="2">85.1 83.4 76.0 79.6 85.6</cell></row><row><cell cols="3">ASFormer+ASRF (I3D) 85.1 85.4 79.3 81.9 85.9</cell></row><row><cell>SSTDA+HASR [1]</cell><cell cols="2">86.6 85.7 78.5 81.0 83.9</cell></row><row><cell cols="3">Br-Prompt+ASFormer 89.2 87.8 81.3 83.8 88.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Human activity recognition results on Breakfast dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparisons of different fusion strategies for Bridge-Prompt by action segmentation results on GTEA dataset (split #1).</figDesc><table><row><cell>Fusion strategy</cell><cell>F1@{10,25,50}</cell><cell>Edit Acc</cell></row><row><cell>(a) Vision-only</cell><cell cols="2">90.3 87.4 76.5 86.2 81.0</cell></row><row><cell cols="3">(b) Pos-embedding i. 89.1 86.2 77.5 84.8 80.0</cell></row><row><cell cols="3">(b) Pos-embedding ii. 88.7 87.3 76.4 84.0 79.5</cell></row><row><cell>(c) Weights for avg.</cell><cell cols="2">91.8 88.1 79.1 86.5 83.7</cell></row><row><cell>(d) Early-fusion</cell><cell cols="2">91.0 89.6 82.1 88.7 81.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparisons of different loss choices for Bridge-Prompt by action segmentation results on GTEA dataset (split #1). Lsem+Linteg 88.6 83.6 77.1 83.3 81.2 Lsem+Linteg+Lstat 91.0 89.6 82.1 88.7 81.2</figDesc><table><row><cell>Loss components</cell><cell>F1@{10,25,50} Edit Acc</cell></row><row><cell>Lsem</cell><cell>87.4 82.5 70.6 81.9 79.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Prompt-based inference accuracies on GTEA. (coffee2tea refers to transferring the knowledge of making coffee to making tea, and so forth; AKL refers to training with all-knowing labels.)</figDesc><table><row><cell>Trans-type</cell><cell>coffee2 tea</cell><cell>cofhoney2 tea</cell><cell>hotdog2 pealate</cell><cell>peanut2 pealate</cell><cell>overall (AKL)</cell></row><row><cell cols="2">top-1 Acc(%) 38.8</cell><cell>41.7</cell><cell>15.5</cell><cell>24.6</cell><cell>54.5</cell></row><row><cell cols="2">top-5 Acc(%) 74.4</cell><cell>81.3</cell><cell>45.1</cell><cell>54.8</cell><cell>90.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Refining action segmentation with hierarchical video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyemin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongheui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16302" to="16310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alexandros I. Metsai, Vasileios Mezaris, and Ioannis Patras. Video summarization using deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Evlampios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adamantidou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="1838" to="1863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Action segmentation with joint selfsupervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno>ICLR, 2021. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04544</idno>
		<title level="m">Clip-adapter: Better vision-language models with feature adapters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16805" to="16814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="14024" to="14034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videograph: Recognizing minutes-long human activities in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seito</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2322" to="2331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Bilgin Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Activity graph transformer for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megha</forename><surname>Nawhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08540</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting clozequestions for few-shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Intra-and inter-action understanding via temporal action parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="730" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coin: A large-scale dataset for comprehensive instructional video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dajun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazheng</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.08472</idno>
		<title level="m">Actionclip: A new paradigm for video action recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Asformer: Transformer for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangqiu</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08568</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph-based high-order relation modeling for long-term action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7590" to="7598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cross-task weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3537" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Crosstask weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3537" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
