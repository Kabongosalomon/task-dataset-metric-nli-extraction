<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Regional Memory Network for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research and Tetras.AI 3 S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory 5 Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research and Tetras.AI 3 S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Regional Memory Network for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, several Space-Time Memory based networks have shown that the object cues (e.g. video frames as well as the segmented object masks) from the past frames are useful for segmenting objects in the current frame. However, these methods exploit the information from the memory by global-to-global matching between the current and past frames, which lead to mismatching to similar objects and high computational complexity. To address these problems, we propose a novel local-to-local matching solution for semi-supervised VOS, namely Regional Memory Network (RMNet). In RMNet, the precise regional memory is constructed by memorizing local regions where the target objects appear in the past frames. For the current query frame, the query regions are tracked and predicted based on the optical flow estimated from the previous frame. The proposed local-to-local matching effectively alleviates the ambiguity of similar objects in both memory and query frames, which allows the information to be passed from the regional memory to the query region efficiently and effectively. Experimental results indicate that the proposed RM-Net performs favorably against state-of-the-art methods on the DAVIS and YouTube-VOS datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) is a task of estimating the segmentation masks of class-agnostic objects in a video. Typically, it can be grouped into two categories: unsupervised VOS and semi-supervised VOS. The former does not resort to any manual annotation and interaction, while the latter needs the masks of the target objects in the first frame. In this paper, we focus on the latter. Even the object masks in the first frame are provided, semi-supervised VOS is still challenging due to object deformation, occlusion, appearance variation, and similar objects confusion.</p><p>With the recent advances in deep learning, there has been tremendous progress in semi-supervised VOS. Early methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> propagate object masks from previ- <ref type="figure">Figure 1</ref>. A representative example of video object segmentation on the DAVIS 2017 dataset. Compared to existing methods rely on optical flows (e.g. PReMVOS <ref type="bibr" target="#b17">[18]</ref>) and global feature matching (e.g. STM <ref type="bibr" target="#b21">[22]</ref>, EGMN <ref type="bibr" target="#b16">[17]</ref>, and CFBI <ref type="bibr" target="#b40">[41]</ref>), the proposed RMNet is more robust in segmenting similar objects. ous frames using optical flow and then refine the masks with a fully convolutional network. However, mask propagation usually causes error accumulation, especially when target objects are lost due to occlusions and drifting. Recently, matching-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b40">41]</ref> have attracted increasing attention as a promising solution to semi-supervised VOS. The basic idea of these methods is to perform global-to-global matching to find the correspondence of target objects between the current and past frames. Among them, the Space-Time Memory (STM) based approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> exploit the past frames saved in the memory to better handle object occlusion and drifting. However, these methods memorize and match features in the regions without target objects, which lead to mismatching to similar objects and high computational complexity. As shown in <ref type="figure">Figure 1</ref>, they are less effective to track and distinguish the target objects with similar appearance.</p><p>The mismatching in the global-to-global matching can be divided into two categories, as illustrated in <ref type="figure">Figure 2</ref>. (i) The target object in the current frame matches to the wrong object in the past frame (solid red line). (ii) The target ob- <ref type="bibr">Figure 2</ref>. Comparison between global-to-global matching and local-to-local matching. The green and red lines represent correct and incorrect matches, respectively. The proposed local-to-local matching performs feature matching between the local regions containing target objects in past and current frames (highlighted by the yellow bounding box), which alleviates the ambiguity of similar objects. ject in the past frame matches to the wrong object in the current frame (dotted red line). The two types of mismatching are caused by unnecessary matching between the regions without target objects in the past and current frames, respectively. Actually, the target objects appear only in small regions in each frame. Therefore, it is more reasonable to perform local-to-local matching in the regions containing target objects.</p><p>In this paper, we present the Regional Memory Network (RMNet) for semi-supervised VOS. To better leverage the object cues from the past frames, the proposed RMNet only memorizes the features in the regions containing target objects, which effectively alleviates the mismatching in (i). To track and predict the target object regions for the current frame, we estimate the optical flow from two adjacent frames and then warp the previous object masks to the current frame. The warped masks provide rough regions for the current frame, which reduces the mismatching in (ii). Based on the regions in the past and current frames, we present Regional Memory Reader, which performs feature matching between the regions containing target objects. The proposed Regional Memory Reader is time efficient and effectively alleviates the ambiguity of similar objects.</p><p>The main contributions are summarized as follows:</p><p>? We propose Regional Memory Network (RMNet) for semi-supervised VOS, which memorizes and tracks the regions containing target objects. RMNet effectively alleviates the ambiguity of similar objects. ? We present Regional Memory Reader that performs local-to-local matching between object regions in the past and current frames, which reduces the computational complexity. ? Experimental results on the DAVIS and YouTube-VOS datasets indicate that the proposed RMNet outperforms the state-of-the-art methods with much faster running speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Propagation-based Methods. Early methods treat video object segmentation as a temporal label propagation problem. ObjectFlow <ref type="bibr" target="#b30">[31]</ref>, SegFlow <ref type="bibr" target="#b5">[6]</ref> and DVSNet <ref type="bibr" target="#b38">[39]</ref> consider video segmentation and optical flow estimation simultaneously. To adapt to specific instances, MaskTrack <ref type="bibr" target="#b23">[24]</ref> fine-tunes the network on the first frame during testing. MaskRNN <ref type="bibr" target="#b9">[10]</ref> predicts the instance-level segmentation of multiple objects from the estimated optical flow and bounding boxes of objects. CINN <ref type="bibr" target="#b0">[1]</ref> introduces a Markov Random Field to establish spatio-temporal dependencies for pixels. DyeNet <ref type="bibr" target="#b15">[16]</ref> and PReMVOS <ref type="bibr" target="#b17">[18]</ref> combine temporal propagation and re-identification functionalities into a single framework. Apart from propagating masks with optical flows, object tracking is also widely used in semisupervised VOS. FAVOS <ref type="bibr" target="#b4">[5]</ref> predicts the mask of an object from several tracking boxes of the object parts. Lucid-Tracker <ref type="bibr" target="#b13">[14]</ref> synthesizes in-domain data to train a specialized pixel-level video object segmenter. SAT <ref type="bibr" target="#b2">[3]</ref> takes advantage of the inter-frame consistency and deals with each target object as a tracklet. Despite promising results, these methods are not robust to occlusion and drifting, which causes error accumulation during the propagation <ref type="bibr" target="#b20">[21]</ref>.</p><p>Matching-based Methods. To handle object occlusion and drifting, recent methods perform feature matching to find objects that are similar to the target objects in the rest of the video. OSVOS <ref type="bibr" target="#b1">[2]</ref> transfers the generic semantic information to the task of foreground segmentation by finetuning the network on the first frame. RGMP <ref type="bibr" target="#b19">[20]</ref> takes the mask of the previous frame as input, which provides a spatial prior for the current frame. PML <ref type="bibr" target="#b3">[4]</ref> learns a pixel-wise embedding using a triplet loss and assigns a label to each pixel by nearest neighbor matching in pixel space to the first frame. VideoMatch <ref type="bibr" target="#b10">[11]</ref> uses a soft matching layer that maps the pixels of the current frame to the first frame in the learned embedding space. Following PML and VideoMatch, FEELVOS <ref type="bibr" target="#b31">[32]</ref> extends the pixel-level matching mechanism by additionally matching between the current frame and the first frame. Based on FEELVOS, CFBI <ref type="bibr" target="#b40">[41]</ref> promotes the results by explicitly considering the feature matching of the target foreground object and the corresponding background. STM <ref type="bibr" target="#b21">[22]</ref> leverages a memory network to perform pixel-level matching from past frames, which outperforms all previous methods. Based on STM, KMN <ref type="bibr" target="#b27">[28]</ref> applies a Gaussian kernel to reduce the mismatched pixels. EGMN <ref type="bibr" target="#b16">[17]</ref> employs an episodic memory network to store frames as nodes and capture cross-frame correlations by edges. Other STMbased methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b41">42]</ref> use depth estimation and spatial constraint modules to improve the accuracy of STM. However, matching-based methods perform unnecessary matching in regions without target objects and are less effective to distinguish objects with similar appearance. <ref type="figure">Figure 3</ref>. Overview of RMNet. The proposed network considers the object motion for the current frame and the object cues from the past frames in memory. To alleviate the mismatching to similar objects, the regional memory and query embedding are extracted from the regions containing target objects. Regional Memory Reader efficiently performs local-to-local matching only in these regions. Note that "Reg. Att. Map" denotes "Regional Attention Map".</p><formula xml:id="formula_0">! ! " ! ! " " " !!"# " ! # !$ $% $&amp; ! # $ ! #$% ? ! #$% $ " ? " " &amp; ' ! &amp; ' ! ( " ( ? % ' ?' % ' ?(/2 ? % ' ?' % ' ?(/8 ? &amp; ?' &amp; ?(/8 ? &amp; ?' &amp; ?(/2 ? % ' ' % ' ?(/8 (/8??&amp;'&amp; ? % ' ' % ' ??&amp;'&amp; (/2?? % ' ' % ' (/2?? &amp; ' &amp; ?&amp;?'&amp;?(/2 ? &amp; ?' &amp; ?(/2 ? &amp; ?' &amp; ?( &amp; ) &amp; ' H?-?( ! ! , ? , ! "#$ {&amp; * }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regional Memory Network</head><p>The architecture of the proposed Regional Memory Network (RMNet) is shown in <ref type="figure">Figure 3</ref>. As in STM <ref type="bibr" target="#b21">[22]</ref>, the current frame is used as the query, and the past frames with the estimated masks are used as the memory. Different from STM that constructs the global memory and query embedding from all regions, RMNet only embeds the regions containing target objects in the memory and query frames. The regional memory and query embedding are generated by the dot product of the regional attention maps and feature embedding extracted from the memory and query encoders, respectively. Both of them consist of a regional key and a regional value.</p><p>In STM, the space-time memory reader is employed for global-to-global matching between all pixels in the memory and query frames. However, Regional Memory Reader in RMNet is proposed for local-to-local matching between the regional memory embedding and query embedding in the regions containing target objects, which alleviates the mismatching to similar objects and also accelerates the computation. Given the output of Regional Memory Reader, the decoder predicts the object masks for the query frame.</p><p>3.1. Regional Feature Embedding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Regional Memory Embedding</head><p>Recent Space-Time Memory based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> construct the global memory embedding for the past frames by using the features of the whole images. However, the features outside the regions where the target objects appear may lead to the mismatching to the similar objects in the query frame, as shown with the red solid line in <ref type="figure">Figure 2</ref>. To solve this issue, we present the regional memory embedding that only memorizes the features in the regions containing the target objects. Mask to Regional Attention Map. To generate the regional memory embedding, we apply a regional attention map to the global memory embedding. At the time step i, given the object mask M i ? N H?W at the feature scale, the regional attention map A j i ? R H?W for the j-th object is obtained as</p><formula xml:id="formula_1">A j i (x, y) = 1, x min ? x ? x max and y min ? y ? y max 0, otherwise<label>(1)</label></formula><p>where (x min , y min ) and (x max , y max ) are the top-left and bottom-right coordinates of the bounding box for the target object, which are determined by</p><formula xml:id="formula_2">x min = max((arg min x M i (x, y) = j) ? ?, 0) x max = min((arg max x M i (x, y) = j) + ?, W ) y min = max((arg min y M i (x, y) = j) ? ?, 0) y max = min((arg max y M i (x, y) = j) + ?, H) (2)</formula><p>where ? denotes the padding of the bounding box, which determines the error tolerance of the estimated masks in the past frames. Specially, we define A j i = 0 if the j-th object disappears in M i . Regional Memory Key/Value. Given the regional attention</p><formula xml:id="formula_3">map A j M = [A j 0 , . . . , A j t?1 ]</formula><p>of j-th object in the memory frames, the key k j M and value v j M in the regional memory embedding are obtained by the dot product of A j M and the global memory embedding from the memory encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Regional Query Embedding</head><p>As illustrated by the red dotted line in <ref type="figure">Figure 2</ref>, multiple similar objects in the query frame are easily mismatched in the global-to-global matching. Similar to the regional memory embedding, we present the regional query embedding that alleviates the mismatching to the similar objects in the query frame. Object Mask Tracking. To obtain the possible regions of target objects in the current frame, we track and predict a rough maskM j t . Specifically, we warp the mask M j t?1 of the previous frame with the optical flow F t estimated by the proposed TinyFlowNet. Mask to Regional Attention Map. As in regional memory embedding (Sec. 3.1.1), the estimated maskM j t is used to generate the regional attention map A j Q for the j-th object in the query frame. To better deal with occlusions, we define A j Q = 1 if the number of pixels is lower than a threshold ?, which triggers the global matching for the target object in the query frame. As illustrated in <ref type="figure" target="#fig_0">Figure 4</ref>, the matching region is expanded to the whole image when the target object disappears. Then, it shrinks to the region containing the target object when the object appears again. This mechanism benefits the optical flow based tracking, which allows the network to perceive the disappearance of objects and makes it more robust to object occlusion. Regional Query Key/Value. Similar to regional memory embedding, the key k j Q and value v j Q in the regional query embedding are obtained by the dot product of A j Q and the global query embedding from the query encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regional Memory Reader</head><p>In STM <ref type="bibr" target="#b21">[22]</ref>, the space-time memory reader is proposed to measure the similarities between the pixels in the query frame and memory frames. Given the embedded key of the memory k j M = {k j M (p)} ? R T ?H?W ?C/8 and the query k j Q = {k j Q (q)} ? R H?W ?C/8 of the j-th object, the similarity between p and q can be computed as</p><formula xml:id="formula_4">s j (p, q) = exp k j M (p)k j M (q) T<label>(3)</label></formula><p>where C and T denote the channels of the embedded key and the number of frames in memory, respectively. Let p = [p t , p x , p y ] and q = [q x , q y ] be the grid cell locations in k j M and k j Q , respectively. Then, the query at position q retrieves the corresponding value from the memory by</p><formula xml:id="formula_5">v j (q) = p s(p, q) p s(p, q) v j M (p)<label>(4)</label></formula><p>where v j M = {v j M (p)} ? R T ?H?W ?C/2 is the embedded value of the memory. The output of the space-time memory reader at position q is</p><formula xml:id="formula_6">y j (q) = v j Q (q), v j (q)<label>(5)</label></formula><p>where v j Q = {v j Q (q)} ? R H?W ?C/2 denotes the embedded value of the query and [?] represents the concatenation.</p><p>Based on the regional feature embedding, we propose Regional Memory Reader, which performs local-to-local matching in the regions containing the target objects, as shown in <ref type="figure">Figure 3</ref>. Compared to the global-to-global memory readers in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>, the proposed Regional Memory Reader alleviates the mismatching to the similar objects in both the memory and query frames.</p><p>Let R j M = {p} and R j Q = {q} be the feature matching regions of the j-th object in the memory and query frames, respectively. In the global-to-global memory readers, the similarities are derived by a large matrix product. That is, R j M and R j Q are all locations in the feature embedding of key and value, respectively. While in the proposed Regional Memory Reader, R j M and R j Q are defined as</p><formula xml:id="formula_7">R j M = p|A j M (p) = 0 R j Q = q|A j Q (q) = 0<label>(6)</label></formula><p>Therefore, for locations p / ? R j M or q / ? R j Q , the similarity between p and q is defined as</p><formula xml:id="formula_8">s j (p, q) = 0, p / ? R j M or q / ? R j Q<label>(7)</label></formula><p>Let h j Q and w j Q be the height and width of the region for the j-th object in the query frame, respectively. h j M and w j M denote the maximum height and width of the regions in the memory frames, respectively. Therefore, the time complexity of the space-time memory reader is O(T CH 2 W 2 ). Compared to the space-time memory reader, the proposed Regional Memory Reader is computationally efficient with the time complexity of O(T Ch j Q w j Q h j M w j M ). As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, h j Q , h j M H and w j Q , w j M W . Actually, the space-time memory reader is also a non-local network <ref type="bibr" target="#b34">[35]</ref>, which usually suffers from high computational complexity <ref type="bibr" target="#b42">[43]</ref> due to the global-to-global feature matching. The proposed local-to-local matching enables the time complexity of the memory reader to be significantly reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>TinyFlowNet. Compared to existing methods for optical flow estimation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>, TinyFlowNet does not use any time-consuming layers such as correlation layer <ref type="bibr" target="#b7">[8]</ref>, cost volume layer <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref>, and dilated convolutional layers <ref type="bibr" target="#b26">[27]</ref>. To reduce the number of parameters of TinyFlowNet, we use small numbers of input and output channels. Consequently, TinyFlowNet is 1/3 the size of FlowNetS <ref type="bibr" target="#b11">[12]</ref>. To further accelerate the computation, the input images are downsampled by 2 before fed into TinyFlowNet. Encoder. The memory encoder takes an RGB frame along with the object mask as input, in which the object mask is represented as a single channel probability map between 0 and 1. The input to the query encoder is only the RGB frame. Both the memory and query encoders use ResNet50 <ref type="bibr" target="#b8">[9]</ref> as the backbone network. To take a 4-channel tensor, the number of the input channels of the first convolutional layer in the memory encoder is changed to 4. The first convolutional layer in the query encoder remains unchanged as in ResNet50. The output key and value features are embedded by two parallel convolutional layers attached to the convolutional layer that outputs a 1/16 resolution feature with respect to the input image. Decoder. The decoder takes the output of Regional Memory Reader and predicts the object mask for the current frame. The decoder consists of a residual block and two stacks of refinement modules <ref type="bibr" target="#b21">[22]</ref> that gradually upscale the compressed feature map to the size of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>DAVIS. DAVIS 2016 <ref type="bibr" target="#b24">[25]</ref> is one of the most popular benchmarks for video object segmentation, whose validation set is composed of 20 videos annotated with high-quality masks for individual objects. DAVIS 2017 <ref type="bibr" target="#b25">[26]</ref> is a multi-object extension of DAVIS 2016. The training and validation sets contain 60 and 30 videos, respectively. YouTube-VOS. YouTube-VOS <ref type="bibr" target="#b37">[38]</ref> is the latest large-scale dataset for the video object segmentation, which contains 4,453 videos annotated with multiple objects. Specifically, YouTube-VOS contains 3,471 videos from 65 categories  <ref type="bibr" target="#b1">[2]</ref> 0.798 0.806 0.802 0.642 MaskRNN <ref type="bibr" target="#b9">[10]</ref> 0.807 0.809 0.808 -RGMP <ref type="bibr" target="#b19">[20]</ref> 0.815 0.820 0.818 0.104 FAVOS <ref type="bibr" target="#b4">[5]</ref> 0.824 0.795 0.810 0.816 CINN <ref type="bibr" target="#b0">[1]</ref> 0.834 0.850 0.842 -LSE <ref type="bibr" target="#b6">[7]</ref> 0.829 0.803 0.816 -VideoMatch <ref type="bibr" target="#b10">[11]</ref> 0.810 0.808 0.819 -PReMVOS <ref type="bibr" target="#b17">[18]</ref> 0.849 0.886 0.868 3.286 A-GAME ? <ref type="bibr" target="#b12">[13]</ref> 0.822 0.820 0.821 0.258 FEELVOS ? <ref type="bibr" target="#b31">[32]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Following the previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>, we take the region similarity and contour accuracy as evaluation metrics. Region Similarity J . We employ the region similarity J to measure the region-based segmentation similarity, which is defined as the intersection-over-union of the estimated segmentation and the ground truth segmentation. Given an estimated segmentation M and the corresponding ground truth mask G, the region similarity J is defined as</p><formula xml:id="formula_9">J = M ? G M ? G (8)</formula><p>Contour Accuracy F. Let c(M ) be the set of the closed contours that delimits the spatial extent of the mask M . The contour points of the estimated mask M and ground truth G are denoted as c(M ) and c(G), respectively. The precision P c and recall R c between c(M ) and c(G) can be computed by a bipartite graph matching <ref type="bibr" target="#b18">[19]</ref>. Therefore, the contour accuracy F between c(M ) and c(G) is defined as</p><formula xml:id="formula_10">F = 2P c R c P c + R c (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We implement the network using PyTorch <ref type="bibr" target="#b22">[23]</ref> and CUDA. All models are optimized using the Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with ? 1 = 0.9 and ? 2 = 0.999. Following <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>, the network is trained in two phases. First, it is pretrained on the synthetic dataset generated by applying random affine transforms to a static image with different pa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>J Mean F Mean Avg.</p><p>OnAVOS <ref type="bibr" target="#b32">[33]</ref> 0.645 0.713 0.679 OSMN <ref type="bibr" target="#b39">[40]</ref> 0.525 0.571 0.548 OSVOS <ref type="bibr" target="#b1">[2]</ref> 0.566 0.618 0.592 RGMP <ref type="bibr" target="#b19">[20]</ref> 0.648 0.686 0.632 FAVOS <ref type="bibr" target="#b4">[5]</ref> 0.546 0.618 0.582 CINN <ref type="bibr" target="#b0">[1]</ref> 0.672 0.742 0.707 VideoMatch <ref type="bibr" target="#b10">[11]</ref> 0.565 0.682 0.624 PReMVOS <ref type="bibr" target="#b17">[18]</ref> 0.739 0.817 0.778 A-GAME ? <ref type="bibr" target="#b12">[13]</ref> 0.672 0.727 0.700 FEELVOS ? <ref type="bibr" target="#b31">[32]</ref> 0.691 0.740 0.716 STM ? <ref type="bibr" target="#b21">[22]</ref> 0.792 0.843 0.818 KMN ? <ref type="bibr" target="#b27">[28]</ref> 0  Methods J Mean F Mean Avg.</p><p>OnAVOS <ref type="bibr" target="#b32">[33]</ref> 0.534 0.596 0.565 OSMN <ref type="bibr" target="#b39">[40]</ref> 0.377 0.449 0.413 RGMP <ref type="bibr" target="#b19">[20]</ref> 0.513 0.544 0.529 PReMVOS <ref type="bibr" target="#b17">[18]</ref> 0.675 0.757 0.716 FEELVOS <ref type="bibr" target="#b31">[32]</ref> 0.552 0.605 0.578 STM <ref type="bibr" target="#b21">[22]</ref> 0.680 0.740 0.710 CFBI <ref type="bibr" target="#b40">[41]</ref> 0.711 0.785 0.748</p><p>RMNet 0.719 0.781 0.750 rameters. Then, it is fine-tuned on DAVIS and YouTube-VOS. The parameters ? and ? are set to 4 and 10, respectively. For all experiments, the network is trained with a batch size of 4 on two NVIDIA Tesla V100 GPUs. All batch normalization layers are fixed during training and testing. The initial learning rate is set to 10 ?5 and the optimization is set to stop after 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Video Object Segmentation on DAVIS</head><p>Single object. We compare proposed RMNet with stateof-the-art methods on the validation set of the DAVIS 2016 dataset for single-object video segmentation. DAVIS contains only a small number of videos, which leads to overfitting and affects the generalization ability. Following the latest works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>, we also present the results trained with additional data from YouTube-VOS, which are denoted as ? in <ref type="table" target="#tab_0">Table 1</ref>. The experimental results indicate that the proposed RMNet is comparable to other competitive methods but with a faster inference speed.  <ref type="bibr" target="#b39">[40]</ref> 0.600 0.601 0.406 0.440 0.512 OSVOS <ref type="bibr" target="#b1">[2]</ref> 0.598 0.605 0.542 0.607 0.588 RGMP <ref type="bibr" target="#b19">[20]</ref> 0.595 -0.452 -0.538 BoLTVOS <ref type="bibr" target="#b33">[34]</ref> 0.716 -0.643 -0.711 PReMVOS <ref type="bibr" target="#b17">[18]</ref> 0.714 0.759 0.565 0.637 0.669 A-GAME <ref type="bibr" target="#b12">[13]</ref> 0.678 -0.608 -0.661 STM <ref type="bibr" target="#b21">[22]</ref> 0.797 0.842 0.728 0.809 0.794 KMN <ref type="bibr" target="#b27">[28]</ref> 0.814 0.856 0.753 0.833 0.814 EGMN <ref type="bibr" target="#b16">[17]</ref> 0.807 0.851 0.740 0.809 0.802 CFBI <ref type="bibr" target="#b40">[41]</ref> 0.811 0.858 0.753 0.834 0.814</p><p>RMNet 0.821 0.857 0.757 0.824 0.815</p><p>Multiple objects. To evaluate the performance of multiobject video segmentation, we test the proposed RMNet on the DAVIS 2017 benchmark. We report the performance of the val set of DAVIS 2017 in <ref type="table" target="#tab_2">Table 2</ref>, which shows that RM-Net outperforms all competitive methods. With additional YouTube-VOS data, RMNet archives a better accuracy and outperforms all state-of-the-art methods. We also evaluate RMNet on the test-dev set of DAVIS 2017, which is much more challenging than the val set. As shown in <ref type="table" target="#tab_4">Table 3</ref>, RMNet surpasses the state-of-the-art methods. The qualitative results on DAVIS 2017 are shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Video Object Segmentation on YouTube-VOS</head><p>Following the latest works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>, we compare the proposed RMNet to the state-of-the-art methods on the YouTube-VOS validation set (2018 version). As shown in <ref type="table" target="#tab_5">Table 4</ref>, RMNet achieves an average score of 0.815, which outperforms other methods. The qualitative results on the YouTube-VOS dataset are shown in <ref type="figure" target="#fig_1">Figure 5</ref>, which demonstrate that RMNet is more effective in distinguishing similar objects and performs better in segmenting small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>To demonstrate the effectiveness of each component in the proposed RMNet, we conduct the ablation study on the DAVIS 2017 val set. Regional Memory Reader. The proposed Regional Memory Reader performs regional feature matching in both the memory and query frames, which reduces the number of mismatched pixels and therefore saves computational time. To evaluate the effectiveness and efficiency of Regional Memory Reader, we replace the regional feature matching with global matching in the query frame and memory  <ref type="bibr" target="#b21">[22]</ref> and EGMN <ref type="bibr" target="#b16">[17]</ref> fail to segment the target objects before the 115-th frame.  frame. Compared to the memory readers that adopt global matching for memory or query frames, the proposed Regional Memory Reader achieves better results in terms of both accuracy and efficiency. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, the areas of the regions containing target objects are usually smaller than 20% of the whole image. <ref type="table">Table 5</ref> shows that the local-to-local matching in Regional Memory Reader is about 5 times faster (around 25 times smaller in FLOPS) than the global-to-global matching. <ref type="figure">Figure 7</ref> presents the visualization of the similarity scores of the target object in the memory readers, where the target object is highlighted in a red bounding box. Given the estimated mask of the target object in the query frame, we compute the similarity scores in the previous frame, where the label of the target object in the query frame is determined by the pixels with high similarities. Similarly, the pixels with similarity scores in the query frame are assigned with the label of the target object in the previous frame. As shown in <ref type="figure">Figure 7</ref>, the proposed Regional Memory Reader avoids the mismatching in the regions outside the target object in memory and query frames, which obtains better segmentation results.</p><p>Query Region Prediction. In RMNet, the regions for feature matching in the query frame are determined by <ref type="figure">Figure 7</ref>. The similarity scores of the target object in the previous and current frames. The target object is highlighted by a red bounding box. "M.R." and "Q.R." denote for "Memory Region" and "Query Region" in the regional memory and query embedding, respectively. <ref type="table">Table 5</ref>. The effectiveness of Regional Memory Reader. "M.R." and "Q.R." denote for "Memory Region" and "Query Region", where and ? represent the feature matching is regional or global for the frame, respectively.  the previous mask and the estimated optical flow. In FEELVOS <ref type="bibr" target="#b31">[32]</ref>, the regions for local feature matching are a local neighborhood of the locations where the target objects appear in the previous frame. KMN <ref type="bibr" target="#b27">[28]</ref> performs feature matching in the regions determined by a 2D Gaussian kernel whose center is the best-matched pixel with the highest similarity score. To evaluate the effectiveness of our "Flowbased Region", we compare its performance with different regions used for feature matching. In <ref type="table">Table 6</ref>, "Previous Region" and "Best-match Region" represent the regions determined by the methods used in FEELVOS and KMN, re-spectively. As shown in <ref type="table">Table 6</ref>, "Flow-based Region" outperforms "Previous Region" and "Best-match Region" in segmentation. "Previous Region" is based on the assumption that the motion between two frames is usually small, which is not robust to object occlusion and drifting. "Bestmatch Region" only considers the region determined by the best-matched pixels. However, the best-matched pixels are easily affected by lighting conditions and may be wrong for similar objects. TinyFlowNet. TinyFlowNet is designed to estimate optical flow between two adjacent frames. To evaluate the effectiveness of the proposed TinyFlowNet, we replace the TinyFlowNet with FlowNet2-CSS <ref type="bibr" target="#b11">[12]</ref> and RAFT <ref type="bibr" target="#b29">[30]</ref> that are pretrained on the FlyingChairs <ref type="bibr" target="#b7">[8]</ref>. As shown in <ref type="table">Table 7</ref>, the segmentation accuracies are almost the same when TinyFlowNet is replaced with FlowNet2-CSS and RAFT, which indicates that TinyFlowNet meets the need of region prediction in the query frame. Moreover, the proposed RMNet with TinyFlowNet is 6 and 16 times faster than with FlowNet2-CSS and RAFT, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Regional Memory Network (RMNet) for semi-supervised VOS. Compared to the STMbased methods, RMNet memorizes and tracks the regions containing target objects, which effectively alleviates the ambiguity of similar objects and also reduces the computational complexity for feature matching. Experimental results on DAVIS and YouTube-VOS indicate that the proposed method outperforms the state-of-the-art methods with much faster running speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>The changes of matching regions for the target object before and after occlusion, which is highlighted by red bounding boxes for each frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The challenging examples of multi-object video segmentation on the YouTube-VOS validation set (2018 version). All methods are tested at 720p without test-time augmentation. For the first video, both STM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>The long tail distribution of the area ratio of the bounding boxes of target objects on the training set of the DAVIS 2017 and YouTube-VOS datasets. The vertical dashed lines denote the mean value of the area ratio of bounding boxes for both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The quantitative evaluation on the DAVIS 2016 validation set. ? indicates using YouTube-VOS for training. The time is measured on an NVIDIA Tesla V100 GPU without I/O time. Methods J Mean F Mean Avg. Time (s)</figDesc><table><row><cell>OnAVOS [33]</cell><cell>0.861</cell><cell>0.849</cell><cell>0.855</cell><cell>0.823</cell></row><row><cell>OSVOS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The quantitative evaluation on the DAVIS 2017 validation set. ? indicates using YouTube-VOS for training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>The quantitative evaluation on the DAVIS 2017 test-dev set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The quantitative evaluation on the YouTube-VOS validation set (2018 version). The results of other methods are directly copied from<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>.</figDesc><table><row><cell>Methods</cell><cell>Seen</cell><cell></cell><cell cols="2">Unseen</cell><cell>Avg.</cell></row><row><cell></cell><cell>J</cell><cell>F</cell><cell>J</cell><cell>F</cell></row><row><cell>OnAVOS [33]</cell><cell cols="4">0.601 0.627 0.466 0.514 0.552</cell></row><row><cell>OSMN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>The effectiveness of the "Flow-based Region" compared to "Previous Region" and "Best-match Region" used in FEELVOS<ref type="bibr" target="#b31">[32]</ref> and KMN<ref type="bibr" target="#b27">[28]</ref>, respectively. The effectiveness of TinyFlowNet compared to FlowNet2-CSS<ref type="bibr" target="#b11">[12]</ref> and RAFT<ref type="bibr" target="#b29">[30]</ref> for optical flow estimation. The time for optical flow estimation is measured on an NVIDIA Tesla V100 GPU without I/O time.</figDesc><table><row><cell>Method</cell><cell cols="2">J Mean</cell><cell>F Mean</cell><cell>Avg.</cell></row><row><cell>Previous Region</cell><cell></cell><cell>0.762</cell><cell>0.822</cell><cell>0.792</cell></row><row><cell cols="2">Best-match Region</cell><cell>0.792</cell><cell>0.845</cell><cell>0.819</cell></row><row><cell cols="2">Flow-based Region</cell><cell>0.810</cell><cell>0.860</cell><cell>0.835</cell></row><row><cell>Method</cell><cell cols="4">J Mean F Mean Avg. Time (ms)</cell></row><row><cell>FlowNet2-CSS</cell><cell>0.814</cell><cell>0.860</cell><cell>0.837</cell><cell>59.93</cell></row><row><cell>RAFT</cell><cell>0.808</cell><cell>0.859</cell><cell>0.834</cell><cell>157.78</cell></row><row><cell>TinyFlowNet</cell><cell>0.810</cell><cell>0.860</cell><cell>0.835</cell><cell>10.05</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in a cnn-based higher-order spatio-temporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglian</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixelwise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SegFlow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video object segmentation by learning location-sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MaskRNN: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VideoMatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PRe-MVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast user-guided video object segmentation by interaction-and-propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith and Chintala.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>1704.00675</idno>
		<title level="m">The 2017 DAVIS challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SDC -stacked dilated convolution: A unified descriptor network for dense matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wasenm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongje</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Euntai</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">RAFT: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FEELVOS: fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Boltvos: Box-level tracking for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth-aware space-time memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunmu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequence-tosequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Syuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsuan-Kung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>Bang Zhang, and Pan Pan</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
