<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
						</author>
						<title level="a" type="main">Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph completion is an important task that aims to predict the missing relational link between entities. Knowledge graph embedding methods perform this task by representing entities and relations as embedding vectors and modeling their interactions to compute the matching score of each triple. Previous work has usually treated each embedding as a whole and has modeled the interactions between these whole embeddings, potentially making the model excessively expensive or requiring specially designed interaction mechanisms. In this work, we propose the multi-partition embedding interaction (MEI) model with block term format to systematically address this problem. MEI divides each embedding into a multi-partition vector to efficiently restrict the interactions. Each local interaction is modeled with the Tucker tensor format and the full interaction is modeled with the block term tensor format, enabling MEI to control the trade-off between expressiveness and computational cost, learn the interaction mechanisms from data automatically, and achieve state-of-the-art performance on the link prediction task. In addition, we theoretically study the parameter efficiency problem and derive a simple empirically verified criterion for optimal parameter trade-off. We also apply the framework of MEI to provide a new generalized explanation for several specially designed interaction mechanisms in previous models. The source code is released at https://github.com/tranhungnghiep/MEI-KGE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs are a popular data format for representing knowledge about entities and their relationships as a collection of triples, with each triple (h, t, r) denoting the fact that relation r exists between head entity h and tail entity t. Large real-world knowledge graphs, such as Freebase <ref type="bibr" target="#b2">[3]</ref> and Wikidata <ref type="bibr" target="#b30">[31]</ref> have found important applications in many artificial intelligence tasks, such as question answering, semantic search, and recommender systems, but they are usually incomplete. Knowledge graph completion, or link prediction, is a task that aims to predict new triples based on existing triples. Knowledge graph embedding methods perform this task by representing entities and relations as embeddings and modeling their interactions to compute a score that predicts the existence of each triple. These models also provide the embeddings as a useful representation of the whole knowledge graph that may enable new applications of knowledge graphs in artificial intelligence tasks <ref type="bibr" target="#b25">[26]</ref>.</p><p>In a knowledge graph embedding model, the matching score is computed based on the interaction between the entries of embeddings. The interaction mechanism is the function that computes the In Proceedings of the European Conference on Artificial Intelligence (ECAI), 2020. score from the embedding entries. The interaction pattern specifies which entries interact with each other and how; thus, it can define the interaction mechanism in a simple manner. For example, in DistMult <ref type="bibr" target="#b32">[33]</ref>, the interaction pattern is the diagonal matching matrix between head and tail embedding vectors, as detailed in Section 2.</p><p>Most previous works treat embedding as a whole and model the interaction between the whole embeddings. For example, the bilinear model RESCAL <ref type="bibr" target="#b19">[20]</ref> and the recent model TuckER <ref type="bibr" target="#b0">[1]</ref> can model very general interactions between every entry of the embeddings, but they cannot scale to large embedding size. One popular approach to this problem is to design special interaction mechanisms to restrict the interactions between only a few entries, for example, DistMult <ref type="bibr" target="#b32">[33]</ref> and recent state-of-the-art models HolE <ref type="bibr" target="#b18">[19]</ref>, ComplEx <ref type="bibr" target="#b28">[29]</ref>, and SimplE <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. However, these interaction mechanisms are specifically designed and fixed, which may pose questions about optimality or extensibility on a specific knowledge graph.</p><p>In this work, we approach the problem from a different angle. We explicitly model the internal structure of the embedding by dividing it into multiple partitions, enabling us to restrict the interactions in a triple to only entries in the corresponding embedding partitions of head, tail, and relation. The local interaction in each partition is modeled with the classic Tucker format <ref type="bibr" target="#b29">[30]</ref> to learn the most general linear interaction mechanisms, and the score of the full model is the sum score of all local interactions, which can be viewed as the block term format <ref type="bibr" target="#b5">[6]</ref> in tensor calculus. The result is a multi-partition embedding interaction (MEI) model with block term format that provides a systematic framework to control the trade-off between expressiveness and computational cost through the partition size, to learn the interaction mechanisms from data automatically through the local Tucker core tensors, and to achieve state-of-the-art performance on the link prediction task using popular benchmarks.</p><p>In general, our contributions include the following.</p><p>? We introduce a new approach to knowledge graph embedding, the multi-partition embedding interaction, which models the internal structure of the embeddings and systematically controls the tradeoff between expressiveness and computational cost. ? In this approach, we propose the standard multi-partition embedding interaction (MEI) model with block term format, which learns the interaction mechanism from data automatically through the Tucker core tensors. ? We theoretically analyze the framework of MEI and apply it to provide intuitive explanations for the specially designed interaction mechanisms in several previous models. In addition, we are the first to formally study the parameter efficiency problem and derive a simple optimal trade-off criterion for MEI. ? We empirically show that MEI is efficient and can achieve stateof-the-art results on link prediction using popular benchmarks.</p><p>In this section, we introduce the notations and review the related knowledge graph embedding models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>In general, we denote scalars by normal lower case such as a, vectors by bold lower case such as a, matrices by bold upper case serif such as A, and tensors by bold upper case sans serif such as A.</p><p>A knowledge graph is a collection of triples D, with each triple denoted as a tuple (h, t, r), such as (UserA, Movie1, Like), where h and t are head and tail entities in the entity set E and r belongs to the relation set R. A knowledge graph can be modeled as a labeled-directed multigraph, where the nodes are entities and each edge corresponds to a triple, with the relation being the edge label. A knowledge graph can also be represented by a third-order binary data tensor G ? {0, 1} |E|?|E|?|R| , where each entry g htr = 1 ? (h, t, r) exists in D.</p><p>Knowledge graph embedding models usually take a triple (h, t, r) as input and then represent it as embeddings and model their interactions to compute a matching score S(h, t, r) that predicts the existence of that triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Graph Embedding Methods</head><p>Knowledge graph embedding is an active research topic with many different methods. Based on the interaction mechanisms, they can be roughly divided into three main categories: (1) semantic matching models are based on similarity measures between the head and tail embedding vectors, (2) neural-network-based models are based on neural networks as universal approximators to compute the matching score, and (3) translation-based models are based on the geometric view of relation embeddings as translation vectors <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Semantic Matching Models RESCAL <ref type="bibr" target="#b19">[20]</ref> is a general model that uses a bilinear map to model the interactions between the whole head and tail entity embedding vectors, with the relation embedding being used as the matching matrix, such that</p><formula xml:id="formula_0">S(h, t, r) = h Mrt,<label>(1)</label></formula><p>where h, t ? R D are the embedding vectors of h and t, respectively, and Mr ? R D?D is the relation embedding matrix of r, with D being the embedding size. However, the matrix Mr grows quadratically with embedding size, making the model expensive and prone to overfitting. TuckER <ref type="bibr" target="#b0">[1]</ref> is a recent model extending RESCAL by using the Tucker format <ref type="bibr" target="#b29">[30]</ref>. However, it also models the interactions between the whole head, tail, and relation embedding vectors, making the core tensor in the Tucker format grow cubically with the embedding size, and also quickly becomes expensive. One approach to reducing computational cost is to design special interaction mechanisms that restrict the interactions between a few entries of the embeddings. For example, DistMult <ref type="bibr" target="#b32">[33]</ref> is a simplification of RESCAL in which the relation embedding is a diagonal matrix, equivalently a vector r ? R D , such that Mr = diag(r). Its score function can also be written as a trilinear product</p><formula xml:id="formula_1">S(h, t, r) = h, t, r = i hitiri,<label>(2)</label></formula><p>which is an extension of the dot product to three vectors.</p><p>DistMult is fast but restrictive and can only model symmetric relations. Most recent models focus on designing interaction mechanisms that aim to be richer than DistMult while achieving a low computational cost. For example, HolE <ref type="bibr" target="#b18">[19]</ref> uses a circular correlation between the head and tail embedding vectors; ComplEx <ref type="bibr" target="#b28">[29]</ref> uses complex-valued embedding vectors, h, t, r ? C D , and a special complex-valued vector trilinear product; and SimplE <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> represents each entity as two role-based embedding vectors and augments an inverse relation embedding vector. In our previous work <ref type="bibr" target="#b24">[25]</ref>, we analyzed knowledge graph embedding methods from the perspective of a weighted sum of trilinear products to propose a more advanced Quaternion-based interaction mechanism and showed its promising results, which were later confirmed in a concurrent work <ref type="bibr" target="#b33">[34]</ref>. However, these interaction mechanisms are specially designed and fixed, potentially causing them to be suboptimal or difficult to extend.</p><p>In this work, we propose a multi-partition embedding interaction framework to automatically learn the interaction mechanism and systematically control the trade-off between expressiveness and computational cost.</p><p>Semantic matching models are related to tensor decomposition methods where the embedding model can employ a standard tensor representation format in tensor calculus to represent the data tensor, such as the CP tensor rank format <ref type="bibr" target="#b9">[10]</ref>, Tucker format <ref type="bibr" target="#b29">[30]</ref>, and block term format <ref type="bibr" target="#b5">[6]</ref>. However, when applied to knowledge graph embedding, there are some differences, such as changing from continuous tensor to binary tensor, relaxation of constraints for data analysis, and different solvers <ref type="bibr" target="#b13">[14]</ref>. We analyze the connections to the related tensor decomposition methods in Section 3.2.</p><p>Neural-Network-based Models These models aim to learn a neural network, to automatically model the interaction. Recent models using convolutional neural networks such as ConvE <ref type="bibr" target="#b6">[7]</ref> can achieve good results by sharing the convolution weights. However, they are restricted by the input format to the neural network <ref type="bibr" target="#b6">[7]</ref>, and the operations are generally less expressive than direct interactions between the entries of the embedding vectors <ref type="bibr" target="#b18">[19]</ref>. We will empirically compare with them.</p><p>Translation-based Models The main advantages of these models are their simple and intuitive mechanism with the relation embeddings as the translation vectors <ref type="bibr" target="#b3">[4]</ref>. However, it has been shown that they have limitations in expressiveness <ref type="bibr" target="#b11">[12]</ref>. The recent model TorusE <ref type="bibr" target="#b7">[8]</ref> improves the translation-based models by embedding in the compact torus space instead of real-valued vector space and achieves good results. We will also empirically compare with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-Partition Embedding Interaction with Block Term Format</head><p>In this section, we motivate, formulate, and analyze the MEI model, illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We construct MEI with two main concepts:  general linear model that computes the weighted sum of all entry product combinations in the interacting partitions. The block term format <ref type="bibr" target="#b5">[6]</ref> emerges from the sum score of all local interactions.</p><p>Note that the concept of multi-partition embedding interaction is highly general and intuitive, as discussed in Section 3.2.2. In this paper, we specifically adopt the Tucker and block term tensor formats to realize a simple yet general standard MEI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Model</head><p>In each triple (h, t, r), the entities and relations embedding vectors h, t ? R De , and r ? R Dr are divided into multiple partitions conveniently denoted as the multi-partition embedding matrices H, T ? R K?Ce , and R ? R K?Cr , respectively. Note that the embedding sizes of entity and relation are not necessarily the same.</p><p>Formally, the score function of MEI is defined as the sum score of K local interactions, with each local interaction being modeled by the Tucker format,</p><formula xml:id="formula_2">S(h, t, r; ?) = K k=1 (W k?1 h k:?2 t k:?3 r k: ) ,<label>(3)</label></formula><p>where ? denotes all parameters in the model; W k ? R Ce?Ce?Cr is the global core tensor at partition k; h k: , t k: , and r k: are the corresponding partitions k 3 ; and? n denotes the n-mode tensor product with a vector <ref type="bibr" target="#b13">[14]</ref>, which contracts the modes of the resulting tensor to make the final result a scalar. The tensor product can be expanded as the following weighted sum</p><formula xml:id="formula_3">S(h, t, r; ?) = K k=1 Ce x=1 Ce y=1 Cr z=1 w xyz,k h kx t ky r kz ,<label>(4)</label></formula><p>where w xyz,k is a scalar element of the core tensor W k and h kx , t ky , and r kz denote the entries in the local partitions k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Theoretical Analysis</head><p>Let us discuss the theoretical foundations of MEI, draw connections to previous models, and study the optimal parameter efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Local Interaction Modeling</head><p>We first focus on analyzing the local interactions in MEI, called local MEI, which are the building blocks of the full MEI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tucker Format and Block Term Format</head><p>We choose to model the local interaction at each partition by the Tucker format <ref type="bibr" target="#b29">[30]</ref> of third-order tensor</p><formula xml:id="formula_4">S k (h, t, r; ?) = W k?1 h k:?2 t k:?3 r k:<label>(5)</label></formula><p>because the Tucker format provides the most general linear interaction mechanism between the embedding vectors, and its core tensor totally defines the interaction mechanism. With local interactions in Tucker format, the full MEI model computed by summing the scores of all local MEI models is in block term format <ref type="bibr" target="#b5">[6]</ref>. Both Tucker format and block term format are standard representation formats in tensor calculus. When applied in knowledge graph embedding, there are some important modifications, such as the data tensor contains binary instead of continuous values, which change the data distribution assumptions, guarantees, constraints, and the solvers. In our work, we express the model as a neural network and use deep learning techniques to learn its parameters as detailed below.</p><p>Recently, the Tucker format was independently used in knowledge graph embedding for modeling the interactions on the embedding vector as a whole <ref type="bibr" target="#b0">[1]</ref>, while we only use the Tucker format for modeling the local interactions in our model. Thus, their model corresponds to a vanilla Tucker model, which is the special case of MEI when K = 1. Note that this vanilla Tucker model suffers from the scalability problem when the embedding size increases, whereas MEI essentially solves this problem. Moreover, MEI provides a general framework to reason about knowledge graph embedding methods, as discussed in Section 3.2.2.</p><p>Parameterized Bilinear Format To better understand how the core tensor defines the interaction mechanism in local MEI, we can view the local interaction in Eq. 5 as a parameterized bilinear model, by rewriting the tensor products as</p><formula xml:id="formula_5">S k (h, t, r; ?) = W k?1 h k:?2 t k:?3 r k: = (W k?3 r k: )? 1 h k:?2 t k:<label>(6)</label></formula><p>= h k: (W k?3 r k: )t k:</p><formula xml:id="formula_6">(7) = h k: M W,r,k t k: ,<label>(8)</label></formula><p>where M W,r,k ? R Ce?Ce denotes the matching matrix of the bilinear model. Note that M W,r,k defines the interaction patterns of the bilinear map between h k: and t k: , but itself is defined by W k?3 r k: . Specifically, each element m W,r,k xy of the matching matrix M W,r,k is a weighted sum of the entries in r k: , weighted by the mode-3 tube vector w xy:,k of W k . Therefore, the core tensor W k defines the interaction patterns or the interaction mechanisms at partition k.</p><p>Compared with the standard bilinear model RESCAL, local MEI is more flexible and efficient because its matching matrices are generated from the relation embedding vectors. Moreover, the global core tensors enable information sharing between all entities and relations, which is particularly useful when the data are sparse.</p><p>Dynamic Neural Network Format For parameter learning, we express the Tucker format as a neural network to employ standard deep learning techniques such as dropout <ref type="bibr" target="#b21">[22]</ref> and batch normalization <ref type="bibr" target="#b10">[11]</ref> to reduce overfitting and improve the convergence rate. Specifically, Eq. 8 can be seen as a linear neural network, where h k: is the input of the network, M W,r,k is the weight of the hidden layer, h k: M W,r,k is the output of the hidden layer, t k: is the weight of the output neuron, and S k is the output of the network. Note that the weight of the hidden layer, M W,r,k , can be seen as the output of another neural network, where r k: is the input and the core tensor W k is the weight. Under this format, there are four layers to apply dropout and batch normalization: r k: , M W,r,k , h k: , and h k: M W,r,k , which are tuned as hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-Partition Embedding Interaction</head><p>There are several reasons why Multi-Partition Interaction is superior and preferable to Local-Partition Interaction. Here, we present some interpretations of the full MEI model to explain its properties.</p><p>Sparse Modeling The full MEI model can be seen as a special form of sparse parameterized bilinear models. The matching matrix of the full MEI model is constructed by the direct sum of the matching matrices of all local MEI models, and the result is a sparse parameterized block-diagonal matrix</p><formula xml:id="formula_7">M (s) W,r = ? ? M W,r,1 0 ? ? ? 0 0 M W,r,2 ? ? ? 0 . . . . . . . . . . . . 0 0 ? ? ? M W,r,K ? ? .<label>(9)</label></formula><p>The score function of the full MEI model can then be written as a bilinear model</p><formula xml:id="formula_8">S(h, t, r; ?) = h M (s) W,r t,<label>(10)</label></formula><p>where h, t, and r are the original embedding vectors before dividing into K partitions. Similarly, we can view MEI in the form of a special sparse Tucker model, where the sparse core tensor W (s) of MEI is constructed by the direct sum of the K local core tensors W1, . . . WK and the score function is written as</p><formula xml:id="formula_9">S(h, t, r; ?) = W (s)? 1 h? 2 t? 3 r.<label>(11)</label></formula><p>This view provides a concrete explanation for the interaction mechanism in the MEI model, as it can be seen as imposing a sparsity constraint on the core tensor, or equivalently the matching matrices, to make the model efficient.</p><p>Multiple Interactions and the Ensemble Boosting Effect An intuitive explanation of MEI is that it models multiple relatively independent interactions between the head and tail entities in a knowledge graph. These interactions correspond to the separate local partitions of the embedding vectors and together define the final matching score. Technically, MEI forms an ensemble of K local interactions by summing their scores, as seen in Eq. 3, similarly to ensemble averaging. However, we argue that MEI works as an ensemble boosting model in a similar manner to gradient boosting methods because the summing operation is done in training and all local MEI models are optimized together. This view intuitively explains the success of MEI when each local interaction is very simple, such as when the partition size is only 1 or 2. It also suggests the empirical benefit of the ensemble boosting effect in MEI with K &gt; 1 over the vanilla Tucker.</p><p>Vector-of-Vectors Embedding and the Meta-Dimensional Transforming-Matching Framework An important insight of MEI is that the embedding can be seen as a vector of vectors, which means a meta-vector where each meta-dimension corresponding to a local partition contains a vector entry instead of a scalar entry. Compared to scalar entry, a vector entry contains more information and allows more expressive yet simple transformation on each entry. By using this notion of vector-of-vectors embedding, we can view MEI as a transforming-matching framework, where the model simply transforms each meta-dimension entry of head embedding then matches it with the corresponding meta-dimension entry of tail embedding. This framework can serve as a novel general design pattern of knowledge graph embedding methods, as we show in Section 3.2.3 how it can explain the previous specially designed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Connections to Previous Specially Designed Interaction Mechanisms</head><p>There exist a few generalizations of previous embedding models that include DistMult, ComplEx, and SimplE; such as <ref type="bibr" target="#b11">[12]</ref> explaining them using a bilinear model, <ref type="bibr" target="#b0">[1]</ref> using a vanilla Tucker model, and <ref type="bibr" target="#b24">[25]</ref> using a weighted sum of trilinear products. However, these generalizations consider the embedding as a whole, here we present a new generalization that considers the embedding as a multi-partition vector to provide a more intuitive explanation of these models and their specially designed interaction mechanisms. We first construct the multi-partition embedding vector for these models. DistMult is trivial with C = 1 and D = K. For ComplEx and SimplE, C = 2 and D = 2K. In ComplEx, each partition k consists of the real and imaginary components of the entry k in a ComplEx embedding vector. In SimplE, each partition k consists of the two entries k in the two role-based embedding vectors. With this correspondence, these previous models can be written in the sparse bilinear model form of MEI in Eq. 9 and Eq. 10. For DistMult, each matching block M W,r,k is just a scalar entry of the relation embedding vector. More interestingly, for ComplEx, each matching block is a 2?2 matrix with the rotation pattern, parameterized by the relation embedding vector,</p><formula xml:id="formula_10">M W,r,k = Re(r k ) ?Im(r k ) Im(r k ) Re(r k ) .</formula><p>For SimplE, each matching block is a 2 ? 2 matrix with the reflection pattern, parameterized by the relation embedding vector,</p><formula xml:id="formula_11">M W,r,k = 0 r k r (a) k 0 ,</formula><p>where r (a) is the augmented inverse relation embedding vector. CP <ref type="bibr" target="#b9">[10]</ref> is similar to SimplE, but missing r (a) , making the matching matrix lose the geometrical interpretation, which is probably the reason why CP does not generalize well to new data, as reported in <ref type="bibr" target="#b24">[25]</ref>.</p><p>The interaction mechanisms of these models are totally characterized by the simple and fixed patterns in their matching blocks M W,r,k , which also specify the interaction restriction between the entries. In MEI, the interaction restriction can be varied by setting the partition size, and more importantly, the interaction patterns can be automatically learned from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Computational Analysis</head><p>Complexity For simplicity, we consider the same embedding size D = KC for both entity and relation. The parameters in a MEI model include the embedding vectors of all entities, all relations, and the core tensors. On a knowledge graph with |E| entities and |R| relations, the number of parameters in MEI is O(|E|D + |R|D + KC 3 ) = O(|E|D + |R|D + D 3 /K 2 ). In this paper's experiments, we restrict them to the simplified case of one single shared-core tensor for all K partitions, so the number of parameters in this case is</p><formula xml:id="formula_12">O(|E|D + |R|D + C 3 ) = O(|E|D + |R|D + D 3 /K 3 ).</formula><p>We note a few interesting observations. First, the core tensor size of the vanilla Tucker (when K = 1) is much larger than the sparse core of MEI, up to K 2 times in non-shared-core MEI and K 3 times in shared-core MEI. These factors can become crucial in practice; for example, with D = 1000 and K = 10, C = 100, the vanilla Tucker core has 1 billion parameters, making it infeasible on most GPUs, while shared-core MEI has only 1 million parameters in the core tensor. Second, the partition size C can be set independently from the embedding size D; thus, the core tensor sizes can be considered as growing linearly with K in the former case of non-shared-core MEI, and as constant in the latter case of shared-core MEI.</p><p>Parameter Efficiency By using Tucker format for local interactions, MEI with block term format is fully expressive. However, in practice, we usually do not care about the parameter upper bound for fully expressiveness of the model. The more interesting property of the model is its ability to efficiently capture complex patterns in the knowledge graph. In this regard, we define the criteria to measure the expressiveness and parameter efficiency of the model. To the best of our knowledge, we are the first to formally study the parameter efficiency in knowledge graph embedding.</p><p>From the interpretation of MEI as a transforming-matching framework in Section 3.2.2, where the model first transforms each head embedding partition then simply matches it with the corresponding tail embedding partition, we see that the ability to capture complex patterns depends totally on the transformation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1. (Expressiveness)</head><p>The expressiveness of the MEI model is measured by the degrees of freedom of the model provided by its transformation system. For example, a linear transformation in a 3-dimensional space has 9 degrees of freedom: 3 for translation, 3 for rotation, and 3 for scaling. For a MEI model with two partitions of size C = 3, the sum score of two local interactions has 9 + 9 = 18 degrees of freedom.</p><p>As mentioned earlier, the vanilla Tucker model can become excessively expensive when the embedding size is large, in which case, it is necessary to use a MEI model with a smaller partition size. To compare fairly across models, we define the parameter efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2. (Parameter efficiency)</head><p>The parameter efficiency of a model is measured by the ratio of its expressiveness and the number of parameters.</p><p>The size of a MEI model depends on the number of partitions and the partition size. Changing any of them affects the parameter count of the model, its expressiveness, and its parameter efficiency. The effect is rather complicated; when the partition size is small, the expressiveness and model size depend mainly on the number of entities and relations; however, when the partition size becomes large enough, the effects of the core tensor outweigh that of the embeddings. Interestingly, we show that the optimal partition size can be determined on any dataset with mild assumptions as stated in the following theorem. Theorem 1. (Optimal parameter efficiency) Given any MEI model that represents an arbitrary knowledge graph over |E| entities and |R| relations, it is optimal in terms of maximizing the parameter efficiency P if and only if the partition size</p><formula xml:id="formula_13">C = min( |E| + |R| P , D),</formula><p>where ? P denotes a special rounding function that selects the floor or ceiling values depending on where P evaluates to a larger value.</p><p>Proof. Consider an arbitrary knowledge graph over |E| entities and |R| relations, where |E|, |R| ? Z + fixed for this knowledge graph, and an arbitrary MEI model representing the given knowledge graph with partition size C, number of partitions K, and embedding size D = KC, where C, K, D ? Z + . The total parameter count is</p><formula xml:id="formula_14">T = |E|D + |R|D + KC 3 = |E|D + |R|D + DC 2 .</formula><p>There are |R| distinct matching matrices corresponding to the number of relations, each of which include K local interactions, so the total expressiveness of the model is</p><formula xml:id="formula_15">E = |R|KC 2 = |R|DC.</formula><p>The parameter efficiency of the model as defined in Definition 2 is P = E T . For simplicity, consider its inverse,</p><formula xml:id="formula_16">P ?1 = T E = |E|+|R| |R|C + C |R|</formula><p>and assume its continuous extension by interpolation <ref type="bibr" target="#b3">4</ref> . Noting that P ?1 only depends on C, we can take its first derivative w.r.t. C as</p><formula xml:id="formula_17">d dC [P ?1 ] = ? |E|+|R| |R|C 2 + 1 |R| ,</formula><p>which evaluates to 0 when C = |E| + |R|. The second derivative of P ?1 w.r.t. C is</p><formula xml:id="formula_18">d 2 dC 2 [P ?1 ] = 2 |E|+|R| |R|C 3 ,</formula><p>which is positive everywhere. (?) By the derivative tests, C = |E| + |R| is the global maximum of the unimodal parameter efficiency function P ; thus, the optimal partition sizes must be its floor or ceiling values, which are selected depending on P evaluations, that is, C = |E| + |R| P . When the embedding size D &lt; |E| + |R| P , we use the largest possible partition size; thus, the optimal C = min( |E| + |R| P , D), as required. (?) By Fermat's theorem on stationary points, all local maxima occur at critical points. C = |E| + |R| is the only feasible critical point; thus, C = min( |E| + |R| P , D) must be the only possible optimal partition sizes, as required.</p><p>Theorem 1 predicts that on WN18 and WN18RR with ? 40, 000 entities and relations, the optimal partition size would be ? 200. On FB15K and FB15K-237 with ? 15, 000 entities and relations, the optimal partition size would be ? 122. When C increases, P increases and is maximized at the optimal partition sizes and then starts decreasing. Thus, when the computational budget is high enough for a large embedding size D = KC, it is more parameter efficient to keep the partition size C close to the optimal value and increase the number of partitions K. These predictions are empirically verified in Section 4.2. Note that this criterion only provides a general guideline for choosing model size, but there are other detailed factors that can affect the model performance in practice, such as data sparsity, data distribution, and the ensemble boosting effect. When the dataset is very large, sparse, and unevenly distributed, it may be preferable to restrict C and try to maximize the empirical benefit of the ensemble boosting effect with a large number K of small local MEI models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>The learning problem in knowledge graph embedding methods can be modeled as the binary classification of every triple as existence and nonexistence. Because the number of nonexistent triples w.r.t. a knowledge graph is usually very large, we only sample a subset of them by the negative sampling technique <ref type="bibr" target="#b16">[17]</ref>, which replaces the h or t entities in each existent triple (h, t, r) with other random entities to obtain the locally related nonexistent triples (h , t, r) and (h, t , r) <ref type="bibr" target="#b3">[4]</ref>. The set of existent triples is called the true data D, and the set of nonexistent triples is called the negative sampled data D .</p><p>To construct the loss function, we define a Bernoulli distribution over each entry of the binary data tensor G to model the existence probability of each triple asp htr = g htr . The predicted probability of the model is computed by using the standard logistic function on the matching score as p htr = ?(S(h, t, r; ?)). We can then learn both the embeddings and the core tensor from data by minimizing the cross-entropy loss:</p><formula xml:id="formula_19">L(D, D ; ?) = ? (h,t,r)?D?D p htr log p htr +(1 ?p htr ) log(1 ? p htr ) ,<label>(12)</label></formula><p>wherep = 1 in D and 0 in D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We use four popular benchmark datasets for link prediction, as shown in <ref type="table" target="#tab_1">Table 1</ref>. WN18 <ref type="bibr" target="#b3">[4]</ref> and WN18RR <ref type="bibr" target="#b6">[7]</ref> are subsets of WordNet <ref type="bibr" target="#b17">[18]</ref>, which contains lexical relationships between words. FB15K <ref type="bibr" target="#b3">[4]</ref> and FB15K-237 <ref type="bibr" target="#b23">[24]</ref> are subsets of Freebase <ref type="bibr" target="#b2">[3]</ref>, which contains general facts. WN18 and FB15K are more popular, whereas WN18RR and FB15K-237 are recently built and more competitive. Evaluations We evaluate and analyze MEI on the link prediction task <ref type="bibr" target="#b3">[4]</ref>. In this task, for each true triple (h, t, r) in the test set, we replace h and t by every other entity to generate corrupted triples (h , t, r) and (h, t , r), respectively. The goal of the model is to rank the true triple (h, t, r) before the corrupted triples based on the score S. We compute popular evaluation metrics including M RR (mean reciprocal rank, which is robust to outlier rankings) and H@k for k ? {1, 3, 10} (Hits at k, which is how many true triples are correctly ranked in the top k) <ref type="bibr" target="#b28">[29]</ref>. The higher M RR and H@k are, the better the model performs. To avoid false-negative error, i.e., some corrupted triples are actually existent, we follow the protocols used in other works for filtered metrics <ref type="bibr" target="#b3">[4]</ref>. In this protocol, all existent triples in the training, validation, and test sets are removed from the corrupted triples set before computing the rank of the true triple.</p><p>Baselines To evaluate the prediction on the optimal parameter efficiency, we compare MEI1?200 (vanilla Tucker model) and MEI3?100. The aim is to show that the model with optimal parameter efficiency can achieve better results with even fewer parameters. We also evaluate MEI against several strong baselines including classic models such as TransE, RESCAL, DistMult, and recent state-of-theart models such as ComplEx, SimplE, and ConvE. We also compare MEI with TorusE that uses larger embedding size; ComplEx at K = 400 that was retuned with N3 weight decay, reciprocal relation, and full softmax loss; and RotatE without the adversarial sampling technique as this technique is not subjected to a specific model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementations</head><p>We trained MEI using mini-batch stochastic gradient descent with Adam optimizer <ref type="bibr" target="#b12">[13]</ref>. We followed the 1-N scoring procedure in <ref type="bibr" target="#b6">[7]</ref> for negative sampling of (h, t, r), where negative samples are reused multiple times for computation efficiency and the number of negative samples is different for each triple. The results of MEI1?200 are reproduced from the vanilla Tucker model in <ref type="bibr" target="#b0">[1]</ref>; note that the relation embedding size Dr = 30 on WN18 and WN18RR only. All hyperparameters of MEI3?100 are tuned by random search <ref type="bibr" target="#b1">[2]</ref>, including batch size, learning rate, decay rate, batch normalization, and dropout rates, which we will publish together with the code. Note that in these experiments, we restrict them to the simplified case of one single shared-core tensor for all K partitions, as an analogy to single interaction patterns in previous specially designed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Link Prediction Performance <ref type="table" target="#tab_2">Tables 2 and 3</ref> show the main results. In general, MEI strongly outperforms the baselines. MEI and ConvE both aim to learn the interaction between the embedding vectors, and interestingly, the multi-partition embedding interaction used in MEI can achieve better results than the convolutional neural networks used in ConvE. MEI also outperforms the general bilinear model RESCAL and other recent state-of-the-art bilinear models DistMult, ComplEx, and SimplE, which is explained by the fact that they are special cases of MEI with specific interaction patterns, as shown in Section 3.2. Compared with TorusE, the results show that an expressive interaction mechanism can help a smaller model outperform a much larger model. There are some recent techniques that help to improve the performance of old models, but we show that MEI can still outperform retuned ComplEx and RotatE reported with comparable settings. Moreover, note that MEI is highly general and potentially preferable for sophisticated datasets.</p><p>Optimal Parameter Efficiency Empirical results agree very well with the predictions of Theorem 1 about the optimal parameter efficiency. On WN18 and WN18RR, MEI1?200 consistently outperforms MEI3?100 using fewer parameters. On FB15K and FB15K-237, the model sizes are reversed due to different numbers of entities and relations, with MEI1?200 having two times more parameters than MEI3?100. On FB15K, as predicted, MEI3?100 consistently outperforms MEI1?200. On FB15K-237, MEI3?100 outperforms MEI1?200 most of the time, although not by a large margin, but uses only half the number of parameters. These results are particularly interesting because they suggest that when the embedding size <ref type="table">Table 2</ref>. Link prediction results on WN18 and FB15K. ? are reported in <ref type="bibr" target="#b18">[19]</ref>, ? are reported in <ref type="bibr" target="#b28">[29]</ref>, other results are reported in their papers. Best results are in bold, second-best results are underlined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analyses</head><p>Parameter Scale Comparison <ref type="table" target="#tab_3">Table 4</ref> compares the performance of MEI with that of ConvE <ref type="bibr" target="#b6">[7]</ref>, which aims to learn interaction mechanisms by a neural network, at different parameter scales. The results show that MEI achieves better results than ConvE at the same parameter count. Moreover, the small MEI model at 0.95M parameters remarkably outperforms the other model at 1.89M parameters. These results suggest that MEI is an effective framework to utilize the parameters of the model and to learn the interaction mechanisms automatically for knowledge graph embedding. Parameter Trade-off Analysis There are two kinds of parameters in the MEI model, the embeddings and the core tensors. Theorem 1 provides a guideline to trade-offs between them. For example, on FB15K-237, the parameter efficiency increases when the partition size increases up to C ? 122. However, there are other factors affecting this trade-off, such as the ensemble boosting effect that favors larger K and smaller C. We argue that due to this effect, MEI with K &gt; 1 has an empirical advantage compared with MEI with K = 1.</p><p>To evaluate this claim, we analyze the performance of MEI models with approximately the same parameter counts but different coretensor sizes on FB15K-237. To disambiguate the effects of larger core tensor, we made sure that the models with larger core tensors would have smaller parameter counts. <ref type="table" target="#tab_4">Table 5</ref> shows that the models with larger core tensor consistently achieve better results with even fewer total parameters, agreeing very well with Theorem 1. Interestingly, MEI with K = 3 achieves competitive results compared with MEI with K = 1, which suggest that the ensemble boosting effect benefits MEI with K &gt; 1, as we argued. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we proposed MEI, the multi-partition embedding interaction model with block term format, to systematically control the trade-off between expressiveness and computational cost, to learn the interaction mechanisms from data automatically, and to achieve state-of-the-art performance on the link prediction task. In addition, we theoretically studied the parameter efficiency problem and derived a simple criterion for optimal parameter trade-off. We discussed several interpretations and insights of MEI as a novel general design pattern for knowledge graph embedding, and we applied the framework of MEI to present a new generalized explanation for several specially designed interaction mechanisms in previous models.</p><p>In future work, we plan to conduct more experiments with MEI, especially regarding the ensemble boosting effect and the metadimensional transforming-matching framework. Other interesting directions include more in-depth studies of the embedding internal structure and the nature of multi-partition embedding interaction, especially with applications in other domains such as natural language processing, computer vision, and recommender systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extra Experiments</head><p>In this section, we present extra results obtained with well-tuned hyperparameters and recent training techniques in our new source code https://github.com/tranhungnghiep/MEI-KGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Results of Well-tuned Small Models</head><p>When data is large, the embedding size needs to increase to fit the data. However, real-world knowledge graphs are very large with billions of entities, so even the largest practical embedding sizes are relatively small compared to the data sizes. To simulate and study such scenarios, we examine the performance of small models with embedding size D = K ? C = 100 on four benchmark datasets WN18, FB15K, WN18RR, FB15K-237.</p><p>We compare three small models, the previous state-of-the-art small model ComplEx50?2 <ref type="bibr" target="#b14">[15]</ref>, the small model MEI10?10 <ref type="bibr" target="#b26">[27]</ref>, and the improved model MEIM10?10 <ref type="bibr" target="#b27">[28]</ref>. These models were well-tuned with recent training techniques including softmax crossentropy loss <ref type="bibr" target="#b6">[7]</ref> [15] <ref type="bibr" target="#b20">[21]</ref>. We also report RotatE results as a reference of previous state-of-the-art large model <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_5">Table 6</ref> shows that MEI10?10 strongly outperforms the previous state-of-the-art small model ComplEx50?2 on all datasets. Moreover, the improved MEIM10?10 even outperforms the much larger RotatE500?2 and RotatE1000?2 models. These results supports our theoretical analysis on the advantage of multi-partition embedding interaction, and agrees with recent group-theoretic analyses <ref type="bibr" target="#b4">[5]</ref> on the limitations of RotatE due to partition size C = 2, which our models systematically address. In summary, the results demonstrate our models' strong point of being both efficient and expressive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results of Well-tuned Base Models</head><p>The previous results of MEI was obtained using old training techniques such as binary cross-entropy loss and under-tuned hyperparameters. To see true performance of MEI in these extra experiments, we use recent training techniques including softmax cross-entropy loss, larger batch sizes, and well-tuned hyperparameters as presented in our published source code. Detailed analysis on the settings and hyperparameters' effects will be published in the future. <ref type="table" target="#tab_6">Table 7</ref> shows the results on three standard benchmark datasets WN18RR, FB15K-237, and YAGO3-10. We see can MEI achieves very good results with well-tuned settings and hyperparameters. This demonstrates the high quality and performance of MEI. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>MEI architecture: multi-partition embedding vectors that interact only between the corresponding partitions. This figure illustrates a MEI model with block term format in three different views for the local-partition interaction: Tucker format, parameterized bilinear format, and neural network format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell>Dataset</cell><cell>|E|</cell><cell>|R|</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell></row><row><cell>WN18</cell><cell>40,943</cell><cell cols="2">18 141,442</cell><cell>5,000</cell><cell>5,000</cell></row><row><cell>FB15K</cell><cell cols="5">14,951 1,345 483,142 50,000 59,071</cell></row><row><cell>WN18RR</cell><cell>40,943</cell><cell>11</cell><cell>86,835</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell cols="2">FB15K-237 14,541</cell><cell cols="4">237 272,115 17,535 20,466</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Link prediction results on WN18RR and FB15K-237. ? are reported in<ref type="bibr" target="#b8">[9]</ref>, ? are reported in<ref type="bibr" target="#b6">[7]</ref>, other results are reported in their papers. Best results are in bold, second-best results are underlined.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">WN18</cell><cell></cell><cell></cell><cell cols="2">FB15K</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row><row><cell>TransE [4]  ?</cell><cell>0.495</cell><cell>0.113</cell><cell>0.888</cell><cell>0.943</cell><cell>0.463</cell><cell>0.297</cell><cell>0.578</cell><cell>0.749</cell></row><row><cell>ConvE [7]</cell><cell>0.943</cell><cell>0.935</cell><cell>0.946</cell><cell>0.956</cell><cell>0.657</cell><cell>0.558</cell><cell>0.723</cell><cell>0.831</cell></row><row><cell>RESCAL [20]  ?</cell><cell>0.890</cell><cell>0.842</cell><cell>0.904</cell><cell>0.928</cell><cell>0.354</cell><cell>0.235</cell><cell>0.409</cell><cell>0.587</cell></row><row><cell>DistMult [33]  ?</cell><cell>0.822</cell><cell>0.728</cell><cell>0.914</cell><cell>0.936</cell><cell>0.654</cell><cell>0.546</cell><cell>0.733</cell><cell>0.824</cell></row><row><cell>ComplEx [29]</cell><cell>0.941</cell><cell>0.936</cell><cell>0.945</cell><cell>0.947</cell><cell>0.692</cell><cell>0.599</cell><cell>0.759</cell><cell>0.840</cell></row><row><cell>SimplE [12]</cell><cell>0.942</cell><cell>0.939</cell><cell>0.944</cell><cell>0.947</cell><cell>0.727</cell><cell>0.660</cell><cell>0.773</cell><cell>0.838</cell></row><row><cell>TorusE [8]</cell><cell>0.947</cell><cell>0.943</cell><cell>0.950</cell><cell>0.954</cell><cell>0.733</cell><cell>0.674</cell><cell>0.771</cell><cell>0.832</cell></row><row><cell>ComplEx new tuning [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.790</cell><cell>-</cell><cell>-</cell><cell>0.872</cell></row><row><cell>MEI1?200</cell><cell>0.953</cell><cell>0.949</cell><cell>0.955</cell><cell>0.958</cell><cell>0.795</cell><cell>0.741</cell><cell>0.833</cell><cell>0.892</cell></row><row><cell>MEI3?100</cell><cell>0.950</cell><cell>0.946</cell><cell>0.952</cell><cell>0.957</cell><cell>0.806</cell><cell>0.754</cell><cell>0.843</cell><cell>0.893</cell></row><row><cell></cell><cell></cell><cell cols="2">WN18RR</cell><cell></cell><cell></cell><cell cols="2">FB15K-237</cell><cell></cell></row><row><cell></cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row><row><cell>TransE [4]  ?</cell><cell>0.182</cell><cell>0.027</cell><cell>0.295</cell><cell>0.444</cell><cell>0.257</cell><cell>0.174</cell><cell>0.284</cell><cell>0.420</cell></row><row><cell>ConvE [7]</cell><cell>0.43</cell><cell>0.40</cell><cell>0.44</cell><cell>0.52</cell><cell>0.325</cell><cell>0.237</cell><cell>0.356</cell><cell>0.501</cell></row><row><cell>DistMult [33]  ?</cell><cell>0.43</cell><cell>0.39</cell><cell>0.44</cell><cell>0.49</cell><cell>0.241</cell><cell>0.155</cell><cell>0.263</cell><cell>0.419</cell></row><row><cell>ComplEx [29]  ?</cell><cell>0.44</cell><cell>0.41</cell><cell>0.46</cell><cell>0.51</cell><cell>0.247</cell><cell>0.158</cell><cell>0.275</cell><cell>0.428</cell></row><row><cell>TorusE [9]</cell><cell>0.452</cell><cell>0.422</cell><cell>0.464</cell><cell>0.512</cell><cell>0.305</cell><cell>0.217</cell><cell>0.335</cell><cell>0.484</cell></row><row><cell>RotatE w/o adv [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.297</cell><cell>0.205</cell><cell>0.328</cell><cell>0.480</cell></row><row><cell>MEI1?200</cell><cell>0.470</cell><cell>0.443</cell><cell>0.482</cell><cell>0.526</cell><cell>0.358</cell><cell>0.266</cell><cell>0.394</cell><cell>0.544</cell></row><row><cell>MEI3?100</cell><cell>0.458</cell><cell>0.426</cell><cell>0.470</cell><cell>0.521</cell><cell>0.359</cell><cell>0.266</cell><cell>0.395</cell><cell>0.544</cell></row></table><note>D is large enough, MEI with K &gt; 1 can both scale to larger embed- ding sizes and have better results than MEI with K = 1 partition.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Parameter scaling on FB15K-237.</figDesc><table><row><cell></cell><cell>Param.</cell><cell>Emb.</cell><cell></cell><cell></cell><cell>H@</cell><cell></cell></row><row><cell>Model</cell><cell>count</cell><cell>size</cell><cell>MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>ConvE</cell><cell>1.89M</cell><cell>96</cell><cell>.32</cell><cell cols="3">.23 .35 .49</cell></row><row><cell>ConvE</cell><cell>0.95M</cell><cell>54</cell><cell>.30</cell><cell cols="3">.22 .33 .46</cell></row><row><cell>MEI</cell><cell>1.89M</cell><cell>3?40</cell><cell>.34</cell><cell cols="3">.25 .38 .53</cell></row><row><cell>MEI</cell><cell>0.95M</cell><cell>3?20</cell><cell>.33</cell><cell cols="3">.24 .36 .51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Parameter trade-off analysis on FB15K-237.</figDesc><table><row><cell>Emb.</cell><cell>Param.</cell><cell>W</cell><cell></cell><cell></cell><cell>H@</cell><cell></cell></row><row><cell>size</cell><cell>count</cell><cell>size</cell><cell>MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>12?11</cell><cell>1.95M</cell><cell>1K</cell><cell cols="4">0.335 0.247 0.367 0.514</cell></row><row><cell>6?21</cell><cell>1.87M</cell><cell>9K</cell><cell cols="4">0.339 0.249 0.371 0.518</cell></row><row><cell>3?40</cell><cell>1.84M</cell><cell>64K</cell><cell>0.344</cell><cell>0.253</cell><cell cols="2">0.378 0.527</cell></row><row><cell>1?82</cell><cell>1.76M</cell><cell>551K</cell><cell cols="3">0.344 0.255 0.378</cell><cell>0.522</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Results of small MEI 10?10<ref type="bibr" target="#b26">[27]</ref> and MEIM 10?10<ref type="bibr" target="#b27">[28]</ref> models well-tuned with recent training techniques. ComplEx represents a previous state-of-the-art small model, tuned by<ref type="bibr" target="#b14">[15]</ref> and reported on their github page. RotatE represents a previous state-of-the-art large model<ref type="bibr" target="#b22">[23]</ref>, as a reference. Best results of small models are in bold and second best results are underlined. Best results of large models are in bold and italicized.</figDesc><table><row><cell></cell><cell></cell><cell>Param.</cell><cell></cell><cell>H@</cell></row><row><cell></cell><cell></cell><cell>count MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell></cell><cell>RotatE 500?2</cell><cell cols="3">40.961M 0.949 0.944 0.952 0.959</cell></row><row><cell>WN18</cell><cell>ComplEx 50?2</cell><cell cols="3">4.098M 0.950 0.940 0.950 0.950</cell></row><row><cell></cell><cell>MEI 10?10</cell><cell cols="3">4.099M 0.950 0.945 0.953 0.957</cell></row><row><cell></cell><cell>MEIM 10?10</cell><cell cols="3">4.108M 0.951 0.946 0.953 0.960</cell></row><row><cell></cell><cell cols="4">RotatE 1000?2 32.592M 0.797 0.746 0.830 0.884</cell></row><row><cell>FB15K</cell><cell>ComplEx 50?2</cell><cell cols="3">1.630M 0.780 0.730 0.810 0.860</cell></row><row><cell></cell><cell>MEI 10?10</cell><cell cols="3">1.631M 0.790 0.746 0.817 0.870</cell></row><row><cell></cell><cell>MEIM 10?10</cell><cell cols="3">1.640M 0.800 0.757 0.823 0.878</cell></row><row><cell></cell><cell>RotatE 500?2</cell><cell cols="3">40.954M 0.476 0.428 0.492 0.571</cell></row><row><cell>WN18RR</cell><cell>ComplEx 50?2</cell><cell cols="3">4.097M 0.460 0.430 0.470 0.520</cell></row><row><cell></cell><cell>MEI 10?10</cell><cell cols="3">4.098M 0.468 0.434 0.482 0.531</cell></row><row><cell></cell><cell>MEIM 10?10</cell><cell cols="3">4.107M 0.481 0.446 0.494 0.550</cell></row><row><cell></cell><cell cols="4">RotatE 1000?2 29.556M 0.338 0.241 0.375 0.533</cell></row><row><cell cols="2">FB15K-237 ComplEx 50?2</cell><cell cols="3">1.502M 0.340 0.250 0.370 0.520</cell></row><row><cell></cell><cell>MEI 10?10</cell><cell cols="3">1.503M 0.347 0.256 0.380 0.531</cell></row><row><cell></cell><cell>MEIM 10?10</cell><cell cols="3">1.512M 0.350 0.258 0.385 0.533</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Results of MEI models well-tuned with recent training techniques. 5?100 62.6M 0.578 0.505 0.622 0.710</figDesc><table><row><cell></cell><cell></cell><cell>Param.</cell><cell></cell><cell>H@</cell></row><row><cell></cell><cell></cell><cell>count MRR</cell><cell>1</cell><cell>3</cell><cell>10</cell></row><row><cell>WN18RR</cell><cell cols="4">MEI 3?100 13.3M 0.483 0.447 0.497 0.553</cell></row><row><cell cols="2">FB15K-237 MEI 3?100</cell><cell cols="3">5.4M 0.364 0.270 0.398 0.550</cell></row><row><cell cols="2">YAGO3-10 MEI</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The Graduate University for Advanced Studies, SOKENDAI, Japan. 2 National Institute of Informatics, Japan. {nghiepth, takasu}@nii.ac.jp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here and below, partitions are column vectors, transpose notation is omitted for simplicity. Illustration as row is just for easy visualization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Not to be confused with analytic continuation of analytic functions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the Cross-ministerial Strategic Innovation Promotion Program (SIP) Second Phase, "Big-data and AIenabled Cyberspace Technologies" by the New Energy and Industrial Technology Development Organization (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random Search for Hyper-Parameter Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Group Representation Theory for Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05100</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decompositions of a Higher-Order Tensor in Block Terms-Part II: Definitions and Uniqueness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lieven De Lathauwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1033" to="1066" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TorusE: Knowledge Graph Embedding on a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1819" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalized Translation-based Embedding of Knowledge Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Ebisu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="951" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Expression of a Tensor or a Polyadic as a Sum of Products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SimplE Embedding for Link Prediction in Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Seyed Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor Decompositions and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Canonical Tensor Decomposition for Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
		<meeting>the 2nd SysML Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proceedings of the 2013 International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Holographic Embeddings of Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Three-Way Model for Collective Learning on Multi-Relational Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ro-tatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and Their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and Their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Science for Industry 4.0 Workshop at EDBT/ICDT</title>
		<meeting>the Data Science for Industry 4.0 Workshop at EDBT/ICDT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Theory and Practice of Digital Libraries</title>
		<meeting>the 23rd International Conference on Theory and Practice of Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Intelligence</title>
		<meeting>the European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Nghiep</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuhiro</forename><surname>Takasu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2262" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Some Mathematical Notes on Three-Mode Factor Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ledyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="311" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wikidata: A Free Collaborative Knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embedding: A Survey of Approaches and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quaternion Knowledge Graph Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2735" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
