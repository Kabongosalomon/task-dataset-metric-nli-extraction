<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Temporal Action Localization with Query Adaptive Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sauradip</forename><surname>Nag</surname></persName>
							<email>s.nag@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<email>xiatian.zhu@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFlyTek-Surrey Joint Research Centre on Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Temporal Action Localization with Query Adaptive Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>NAG, ZHU, XIANG: FEW-SHOT TAL WITH QUERY ADAPTIVE TRANSFORMER 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing temporal action localization (TAL) works rely on a large number of training videos with exhaustive segment-level annotation, preventing them from scaling to new classes. As a solution to this problem, few-shot TAL (FS-TAL) aims to adapt a model to a new class represented by as few as a single video. Exiting FS-TAL methods assume trimmed training videos for new classes. However, this setting is not only unnaturalactions are typically captured in untrimmed videos, but also ignores background video segments containing vital contextual cues for foreground action segmentation. In this work, we first propose a new FS-TAL setting by proposing to use untrimmed training videos. Further, a novel FS-TAL model is proposed which maximizes the knowledge transfer from training classes whilst enabling the model to be dynamically adapted to both the new class and each video of that class simultaneously. This is achieved by introducing a query adaptive Transformer in the model. Extensive experiments on two action localization benchmarks demonstrate that our method can outperform all the stateof-the-art alternatives significantly in both single-domain and cross-domain scenarios. The source code can be found in https://github.com/sauradip/fewshotQAT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Temporal action localization (TAL) aims to identify the temporal duration (i.e., the start and end points) and class label of action instances in naturally untrimmed videos <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>. Existing TAL methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref> use training datasets composed of a large number of videos (e.g., hundreds) per class with exhaustive segment-level annotation. The annotation is tedious and costly. Further, for some rare classes collecting sufficient video instances may not even be feasible. These have severely limited the scalability and general usability of existing TAL methods. Inspired by the success of few-shot image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>, few-shot learning (FSL) has been recently introduced to TAL <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. A fewshot learning model is designed to eliminate the annotation of large training data. This is achieved by meta-learning which enables a model to adapt to any new class with as few as a single video. One of the key challenges in FS-TAL is how to capture the intra-class variation using only a handful (e.g., <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> training instances of a new class. One of the key objectives of meta-learning is to thus transfer such intra-class variation information from a large set of seen training classes to the new class to compensate for lack of training data.</p><p>Nonetheless, existing few-shot TAL (FS-TAL) methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> all adopt a setting under which trimmed videos are used to represent the new classes for model adaptation. This setting seems to problematic: <ref type="bibr" target="#b0">(1)</ref> As mentioned earlier, the TAL problem exists because most action instances are first captured in untrimmed videos sandwiched by background segments. An analogy is that objects always co-exist with background (e.g., tree/road/wall) in an image. So to obtain the trimmed new class video, one needs to first manually annotate (trim) the untrimmed videos. This begs the question: why not use the untrimmed video together with the annotation for model adaptation? <ref type="bibr" target="#b1">(2)</ref> Each new action class occurs in its own specific context (background), which carries important cues on how to segment it. Using trimmed videos means that a FS-TAL model is unable to exploit the contextual information for both knowledge transfer from seen classes and new unseen class adaptation.</p><p>In this work, we first introduce a new and more practical few-shot TAL (FS-TAL) problem setting. During both the training (meta-learning) and inference (model adaptation) stages, each class is represented by a support set comprising untrimmed videos with temporal annotation. A segmentation model is then built using the support set and applied to a query set containing untrimmed videos of the same class to locate the foreground action instances. This change of setting means that instead of meta-learning a model to temporally align the support set instances with the foreground segments of the query video as in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, we aim to meta-learn a foreground/background classifier that can be quickly adapted to new classes. To this end, we propose a novel FS-TAL model which meta-learns a query adaptive Transformer (QAT) for fast adaption of foreground/background classifier to a new class. In particular, this leverages the attention mechanism across the query video and few-shot classifier in order to better capture the intra-class invariance. As shown in <ref type="figure">Figure  1</ref>, our model has two key components, a snippet classifier that labels each video snippet into foreground or background, and a query adaptation module designed for query video adaptation. The former is a simple binary classifier constructed using the annotated untrimmed support set videos. The latter is formulated as a Transformer that takes both the classifier weight vector and query video features as input and outputs an updated classifier adapted to each query video. This QAT module is meta-learned and fixed during inference; therefore the whole model is inductive. Importantly, our model is flexible in that it can work in both the new setting proposed in this paper and the existing setting with trimmed support set. We make the following contributions: (1) We introduce a new and more practical FS-TAL problem setting. (2) We propose a novel FS-TAL model with a query adaptive Transformer for model adaptation to both a given new class and each query video. (3) Extensive experiments show that the proposed method yields new state-of-the-art performance on two TAL datasets (ActivityNet-v1.3 and Thumos <ref type="bibr">'14)</ref>. Under a more challenging and more realistic cross-domain setting, the advantage of our method remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Temporal Action Localization An intuitive approach to temporal action localization (TAL) is based on sliding window -first generating multi-scale segments and then classifying them <ref type="bibr" target="#b5">[6]</ref>. A key limitation with this pipeline is that a large number (thousands) of possible segments are necessary for achieving reasonable accuracy, which is computationally expensive. To overcome this issue, foreground/background modeling is introduced to generate action proposals <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43]</ref>. When proposal generation is poor, incorporating sliding windows could be helpful <ref type="bibr" target="#b10">[11]</ref>. For improving local segment-level feature representation, <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39]</ref> exploit graph convolutional networks to capture long-range contextual information. Nonetheless, assuming a pre-collected dataset of all action class during training, all these methods have poor scalability to large number of classes, due to the high annotation cost. Few-shot Learning For fast adaptation of a model to any given new class with few training samples, few-shot learning (FSL) provides a solution <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. It is often realized by meta-learning which simulates the behaviour of new tasks with novel classes represented by only a handful of labeled samples. This eliminates the requirement of labeling a large dataset for a new class. Representative approaches include hallucination (data augmentation) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">34]</ref>, initialization optimization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, metric learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. Beyond image classification, FSL has also been introduced to object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> and semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. In contrast to these image analysis problems, here we focus on the more challenging TAL problem. Note that the model in <ref type="bibr" target="#b12">[13]</ref>, though developed for object detection in images, can also work in the FS-TAL setting with trimmed support set. More specifically, unlike our query adaptive Transformer for classifier adaptation at the sample level, it leverages self-attention to contrast the regional features exhaustively across the query and support samples. We will compare with <ref type="bibr" target="#b12">[13]</ref> in our experiments ( <ref type="table" target="#tab_1">Table 1)</ref>. Few-shot Temporal Action Localization FSL has been introduced to temporal action location recently <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. Yang et al. <ref type="bibr" target="#b36">[37]</ref> propose the first FS-TAL setting with trimmed support set. It incorporates the sliding window idea in the matching network <ref type="bibr" target="#b30">[31]</ref> to localize action instances in untrimmed query videos. Later on, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> consider weak video-level annotation of untrimmed training videos. Similar to our proposed setting, the latest work <ref type="bibr" target="#b36">[37]</ref> also focuses on a singe new class at one time. However, a common limitation with these existing FS-TAL problem settings stems from the assumption of trimmed support set. As explained earlier, trimmed videos do not exist naturally and need to be obtained with the same amount of manual annotation as our setting. Importantly, the ignorance of background content in the original untrimmed video leads to the failure to exploit useful context information. We will compare the two FS-TAL settings in our experiments ( <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>Problem Formulation Given only a few videos from any unseen action class, we aim to learn a TAL model for that class. For FS-TAL, we assume a base category set C base for training, and a novel category set C novel for testing. For testing cross-class generalization, we ensure that the two class sets are disjoint: C base C novel = ?. Accordingly, the base and novel datasets are denoted as</p><formula xml:id="formula_0">D base = {(V i ,Y i ) ,Y i ? C base } and D novel = {(V i ,Y i ) ,Y i ? C novel } respectively. Under the proposed new setting, each training video V i is associated with segment-level annotation Y i = {(s t , e t , c),t ? {1, .</formula><p>., M}, c ? C} including M segment labels each with the start and end time locations and action class. In evaluation, for each task, we randomly sample a class L ? C novel from which K and one labeled videos are randomly sampled to construct the support set S and the query set Q respectively. The labels of S are accessible for model few-shot learning whilst that of Q are used for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Architecture</head><p>Our FS-TAL model is illustrated in <ref type="figure">Figure 1</ref>. It consists of a task-generic video embedding module (Sec. 3.2), and a task-specific snippet classification module (Sec. 3.3). We aim to  <ref type="figure">Figure 1</ref>: Overview of the proposed FS-TAL deep learning architecture. There are two main modules: (1) Video embedding for feature representation: It is pre-trained on the whole training set, and shared by all different tasks for more effective knowledge transfer from training classes to test classes. (2) Snippet classification for foreground prediction: It is learned specifically for every individual task in two steps. Initialized with the average of foreground snippet features, the first step learns the classifier on the support videos in a supervised manner. The second step further adapts the classifier weights to every query video with a query adaptive Transformer. The Transformer is meta-trained. The final localization result is obtained by thresholding snippet-level classification scores and temporal non-maximum suppression.</p><p>achieve optimal video embedding and classification for any new task with only a few (1 or 5) labeled support videos. To that end, we share video embedding component across all tasks, and exploit the classification component for tackling the task specificity. With the output of task adapted classification on every snippet of a test video, we apply a non-parametric localization process to obtain the segment predictions (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-Generic Video Embedding</head><p>To capture action location information of a video, we construct a video embedding module with two components including feature backbone and snippet embedding. Feature backbone In general, any video action models can be used such as C3D <ref type="bibr" target="#b28">[29]</ref>, I3D <ref type="bibr" target="#b3">[4]</ref> and TSM <ref type="bibr" target="#b17">[18]</ref>. For fair comparison with <ref type="bibr" target="#b37">[38]</ref>, we adopt the same backbone C3D as our default choice. It is characterized by conducting 3D convolution and pooling operations in 2D spatial and 1D temporal dimensions simultaneously, capturing both appearance and motion information. Given an input video V , we extract RGB X r ? R T ?d 1 and optical flow X o ? R T ?d 1 features at the snippet level, where T denotes the number of snippets and d 1 denotes the feature dimension. Each snippet is a short sequence of (e.g., 16 in our case) consecutive frames. We denote the concatenated features as X = [X r ; X o ] ? R T ?2d 1 . As in most TAL methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, the feature backbone is pre-trained on a large video classification dataset (e.g., Kinetics <ref type="bibr" target="#b15">[16]</ref>) and then frozen to serve as a feature extractor. Snippet embedding Whilst C3D features have already encoded local motion information due to using 3D convolution and optical flow, long-term structural information is lacking but critical for action localization. To address this issue, we adopt an off-the-shelf tempo-ral proposal model called GTAD <ref type="bibr" target="#b35">[36]</ref>. Other proposal models can be similarly integrated <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>. In particular, GTAD exploits a temporal graph and a semantic graph for modeling long-term temporal and contextual information concurrently. In our context, we utilize GTAD as a means for refining the C3D snippet features in a way that they become more sensitive to foreground (action content) and background. We use the output of some intermediate layer of GTAD as the snippet embedding. The layer selection will be evaluated in our experiments (Sec. 4).</p><p>Formally, taking C3D features X ? R T ?2d 1 of a video as input, GTAD can output the snippet embedding as X se ? R T ?C where C is the embedding dimension. Support and query videos share the same GTAD model. We denote X s se and X q se as the embedding of the support and query videos. Consider that snippet embedding would be largely shareable among different tasks, we train the GTAD model on the base dataset in a standard supervised learning way. The objective function includes a classification loss and a localization loss with respect to the ground-truth foreground mask <ref type="bibr" target="#b35">[36]</ref>. The trained GTAD and C3D form the video embedding module which provides generic video representations for the subsequent few-shot learning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Task-Specific Snippet Classification</head><p>In our architecture, few-shot learning is focused on the snippet classification component for capturing each task's specificity. We aim to build a binary classifier h ? (with ? the parameters) that can distinguish foreground action from background content in a video. Formally, the classifier model for predicting the foreground likelihood is a simple linear classifier as</p><formula xml:id="formula_1">p(t) = h ? (X se (t)) = ? (? ? [cos(X se (t), ? )]),<label>(1)</label></formula><p>where ? specifies the sigmoid function, ? is a temperature hyper-parameter, and cos is the cosine similarity. The snippet is indexed by t ? {1, ? ? ? , T }.</p><p>To make a classifier discriminative for each specific task, we introduce a two-step learningand-adapting strategy. In the first step, we learn the classifier weights on the support set in a supervised way. In the second step, we further adapt the support-set trained classifier weight to every query video with a query adaptive Transformer model. This aims to solve the intraclass variation problem. New class adaptation As the support set is composed of untrimmed videos with segmentlevel annotation, we can adapt the classifier to a new class with standard supervised learning. Given the ground-truth annotation, we label each snippet with foreground or background. To train the classifier, we use the cross entropy loss as the objective function:</p><formula xml:id="formula_2">L ce = ? 1 2K K ? k=1 [L f g (X s k ) + L bg (X s k )],<label>(2)</label></formula><formula xml:id="formula_3">L f g (X s k ) = l f g + l bg ? + l f g ? t?{1,??? ,T }? s k (t) log[p s k (t)],<label>(3)</label></formula><formula xml:id="formula_4">L bg (X s k ) = l f g + l bg ? + l bg ? t?{1,??? ,T } (1 ?? s k (t)) log[1 ? p s k (t)],<label>(4)</label></formula><p>where p s k (t) is the prediction of the t-th snippet X s k (t) from the k-th support video. ? is used to tackle extreme cases such as zero background/foreground. To balance the effect of foreground and background snippets in training, we introduce a balancing policy based on their sizes (l f g and l bg ). The idea is intuitive -less is more important.</p><p>The classifier can be trained by a small number of (e.g., 50?100) iterations. We denote ? * as the support-set trained classifier's weights. Given only a handful of labeled samples, how to initialize the classifier weights becomes more critical. Instead of random initialization, we found that the mean of foreground snippet's embedding serves as a stronger choice.</p><p>Query video adaptation Under the few-shot setting, a key challenge to overcome is the insufficient training samples in the support set for capturing the intra-class invariance of the new class. As a result, training the classifier only on the support videos often fails to capture the discriminative informative generalizable to individual query videos. To address this limitation, we propose a query adaptive Transformer model (with the parameters ?) which is based on self-attention <ref type="bibr" target="#b29">[30]</ref>.</p><p>Taking an input in a triplet of {(query, key, value)}, our Transformer outputs an undated query with attentive association with respect to the value. As our objective is to associate the classifier weights ? * with the query video X q se , we set (query, key, value) = (? * , X q se , X q se ). The attentive learning is then formulated as</p><formula xml:id="formula_5">A i (? * ) = ? * + so f tmax( ? * W Q (X q se W K ) T ? d )(X q se W V ),<label>(5)</label></formula><p>where W Q /W K /W V are learnable parameters (each is realized by a fully-connected layer) that projects the respective input to a d-dimension latent space. In a multi-head attention (MA) design, we combine a set of independent A i to form a richer learning process:</p><formula xml:id="formula_6">? * * = [A 1 (? * )..A m (? * )] MA + MLP(? * ) ? R L?256 .<label>(6)</label></formula><p>The MLP block has one fully-connected layer with residual skip connection. Layer norm is applied before both the MA and MLP block. Learning objective After the classifier has been learned on both support and query videos, it can be applied to the query video. We classify each snippet with Eq. (1) with the foreground probability as:</p><formula xml:id="formula_7">p (t) := h ? * * (X q se (t)).<label>(7)</label></formula><p>For training our Transformer (?), this prediction is then used to compute a cross-entropy loss (Eq. (2)) as objective. In meta-training, we conduct loss gradient back-propagation only once for each episode. We denote ? * as the optimized Transformer's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Inference</head><p>During testing, each time we are given a new task with one random unseen action class sampled from the novel dataset D novel . With the frozen video embedding module, we need to obtain a task-specific snippet classifier in two steps: supervised learning with K shots of support videos (Eq. (2)) and classifier weight adaptation on a query video by applying the meta-trained Transformer (note that our Transformer itself is frozen here). The classifier is then applied to predict the foreground probability of every snippet of a query/test video.</p><p>Action instance generation After we obtain the snippet-level classification results, we threshold on their foreground probabilities and take those consecutive snippets as action instance candidates. To indicate the prediction confidence of each candidate, we use the highest snippet foreground probability. We then adjust the confidence scores using temporal soft Non-Maximal Suppression (NMS) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>. Finally, we select top N candidates as the localization result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets We evaluate on two large-scale temporal action localization datasets. ActivityNet-v1.3 <ref type="bibr" target="#b2">[3]</ref> is a popular TAL benchmark. It contains 19,994 temporally annotated untrimmed videos in 200 action categories. THUMOS'14 <ref type="bibr" target="#b13">[14]</ref> is another widely used benchmark for action recognition and localization. There are 413 untrimmed videos from 20 different categories. The 20 classes are a subset of the 101 classes in UCF101 <ref type="bibr" target="#b25">[26]</ref>.</p><p>Few-shot learning setting To facilitate performance comparison, we use the same class split as introduced in <ref type="bibr" target="#b37">[38]</ref>. For both datasets, we split the videos into single instance and multiinstance according to the number of action instances per video. For the single instance case, we divide the videos with multiple action instances into independent single-instance videos. Every newly generated video is no longer than 768 frames. For each of the two cases, we divide all the classes into three non-overlapping subsets for training (80%), validation (10%) and testing (10%), respectively. The validation set is used for model parameter tuning and best model selection. We consider 1-shot and 5-shot. In our setting we adopt untrimmed support set, as opposed to <ref type="bibr" target="#b37">[38]</ref> using trimmed videos. For each test, we use 5000 random tasks and report their average result. Implementation Details For each untrimmed video, we extract its RGB frames at 16 FPS and at the resolution of 256 ? 256. We averagely divide each video into 100 (256 for THU-MOS) non-overlapping snippets and sample 8 frames for each snippet (i.e., T = 100). As <ref type="bibr" target="#b37">[38]</ref>, we filter out videos having less than 768 frames. We consider single-instance and multi-instance test videos, separately. The dimension of C3D feature is 500 (i.e., d 1 = 500). We take the penultimate layer's output (Layer-5) of GTAD's localization module as video embedding (256-D). The latent feature dimension d (Eq. (5)) of our query adaptive Transformer is 256. Dropout is used in our Transformer to alleviate model overfitting. We set the NMS threshold of 0.7/0.6 for ActivityNet/THUMOS. As the final TAL result, we take top 100/200 for ActivityNet/THUMOS. We adopt the Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with learning rate 0.004. We train the model for 50 epochs each with 200 episodes.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single instance videos Multi-instance videos</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with state-of-the-art</head><p>Competitors For comparative evaluation, we consider a few-shot object detection model <ref type="bibr" target="#b12">[13]</ref>, a one-shot video re-localization model <ref type="bibr" target="#b7">[8]</ref>, and the latest FS-TAL model <ref type="bibr" target="#b37">[38]</ref>. Because <ref type="bibr" target="#b7">[8]</ref> cannot tackle multiple support videos, we compare with a modified version of temporal action proposal model SST <ref type="bibr" target="#b1">[2]</ref> for 5-shot case. As in <ref type="bibr" target="#b37">[38]</ref>, a fusion layer is added on top of SST's GRU layer to incorporate the support video features, and the proposal with the largest confidence score is taken as the prediction. All the methods use the same C3D video feature backbone. For all the competitors, we use trimmed support set to keep their original designs. We evaluate the proposed model under both the previous setting (trimmed support set) and our new setting (untrimmed support set). This allows for absolute fair model comparison as well as setting comparison. When feeding trimmed support videos into our model, the background loss term L bg in Eq.</p><p>(2) will become zero; without any other formulation change, our model can be applied to the old setting. The difference is that now the Transformer is used to adapt a foreground template/prototype to each query video, instead of a foreground/background classifier. Note that none of the existing methods can be easily extended to operate under our new setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are compared in <ref type="table" target="#tab_1">Table 1</ref>. It is evident that our method achieves the best performance in all test settings when using the same trimmed support set. This suggests the superiority of our model over all alternative designs, verifying the proposed few-shot learning architecture. The margin is even larger in more strict metrics. Importantly, we see that the margin further increases in 5-shot case, indicating the superior capability of our method in leveraging multiple training videos. This is mainly due to the proposed query adaptive Transformer that can amplify the benefit of larger support-set via attentive query video adaptation, which is lacking in all existing methods. In the multi-instance setting on THUMOS'14, all the methods do not work well due to longer videos and short action instances. However, it is still encouraging that our model can double or triple the performance of alternatives at mAP@0.6-0.9 in such challenging test. We further examine the two FS-TAL problem settings with the proposed method. We make the following observations. In the single-instance setting, the model performance is marginally better in previous setting with trimmed videos in most cases. Our observation suggests that this is potentially due to lack of background diversity. However, when it comes to the more practical and challenging multi-instance setting, the opposite is true especially in the 5-shot case. This indicates that background helps model learning with useful context cues co-existing with action instances. Given these observations, we consider that the proposed setting is not only more practical but also provides more information for better modeling, as compared to the previous settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of Query Video Adaptation</head><p>In Section 3.3 we introduce a query adaptive Transformer for fast adapting the support-set trained classifier's weights to each query video. This aims to solve intra-class variation with FS-TAL as there is no sufficient training samples in support set to capture this variation. There may exist big appearance difference between the support and query video action instances (see <ref type="figure" target="#fig_2">Figure 2</ref> in Supplementary). Query video adaptation is thus critical. From <ref type="table" target="#tab_3">Table  2</ref> we can see that without the proposed query video the performance will drop significantly (3 ? 8%) in 1/5-shot settings of both datasets. This verifies the importance of learning the intra-class invariance problem and the ability of our Transfer model in adapting the classifier's weight to each query video, i.e., video-specific adaptation. In <ref type="figure" target="#fig_2">Figure 2</ref> we visually show that our query adaptive transformer is effective in adapting the classifier's weight to capture the specificity of the query video's foreground content.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-Domain Localization</head><p>Following the above single domain (dataset) FS-TAL evaluation, we further introduce a more challenging and more realistic cross-domain setting. As THUMOS'14 and ActivityNet-v1.3 present large differences in action instance length and background characteristics, they are suitable for cross-domain evaluation. We consider the single-instance setting. We compare our method with the state-of-the-art model <ref type="bibr" target="#b37">[38]</ref>. THUMOS ? ActivityNet In the first cross-domain experiment, we train a model on the base classes of THUMOS'14 (source domain) and test the model on the novel classes of ActivityNet-v1.3 (target domain). The results are reported in <ref type="table" target="#tab_5">Table 3</ref>. It is shown that the performance advantage of our method remains compared to the single-domain setting. For example, the mAP@0.5 margin of our model over <ref type="bibr" target="#b37">[38]</ref> is 4.8%/6.2% in the 1/5-shot cases.</p><p>Comparing the single-domain results in <ref type="table" target="#tab_1">Table 1</ref>, we can see that domain shift indeed negatively affects the performance of both models. ActivityNet ? THUMOS The second experiment considers the opposite transfer direction. At large we have similar observations with our model again outperforming <ref type="bibr" target="#b37">[38]</ref> in both 1/5shot setting. This suggests that our model can generalize to different transfer setups with consistent performance advantages.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Video Embedding Module</head><p>We evaluate the generality of our FS-TAL architecture in different video embedding designs.</p><p>In this test we select BMN <ref type="bibr" target="#b19">[20]</ref>. <ref type="table" target="#tab_7">Table 4</ref> shows that BMN is slightly inferior to GTAD for video embedding, which is consistent with the previous finding <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inference Efficiency</head><p>In inference, our model runs a small number of iterations for learning the linear classifier's weights on the support set, which increases slightly the computational overhead. We conduct a quantitative cost analysis in 5-shot multi-instance setting on ActivityNet-v1.3. We compared to the state-of-the-art model <ref type="bibr" target="#b37">[38]</ref>. For both methods, we track the speed of 100 FS-TAL tasks on a machine with one RTX2080Ti GPU. <ref type="table">Table 5</ref> shows that our method has very similar inference speed as <ref type="bibr" target="#b37">[38]</ref>, without efficiency disadvantage.   <ref type="table">Table 5</ref>: Inference efficiency test in the 5-shot multi-instance setting with a RTX2080 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>For visual analysis, we provide two qualitative examples in <ref type="figure" target="#fig_4">Figure 4</ref>. To visualize the intraclass variation challenge which our method in particular the proposed query adaptive Transformer aims to address, we show some common examples in <ref type="figure" target="#fig_3">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Choice of Video Embedding Layer</head><p>We examine which layer of GTAD <ref type="bibr" target="#b35">[36]</ref> is a good choice for video snippet embedding. In particular, we test five GTAD layers. The result curve in <ref type="figure">Figure 5</ref> shows that deeper layers are usually better than shallow ones, suggesting that snippet-level contextual information is useful for action localization. We select the layer-5 as our embedding layer as it has best cost-effectiveness. <ref type="figure">Figure 5</ref>: Ablation of GTAD video embedding layer in the single-instance setting on ActivityNet-v1.3. The number in round bracket is the embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a new and more practical few-shot temporal action localization (FS-TAL) problem. Unlike all existing settings, in our setting a new action class is represented by untrimmed support set with useful background segments to provide contextual information. We introduce a novel FS-TAL architecture that effectively transfers class-generic representation knowledge from training classes to any unseen test classes whilst adapting the model to any new class. To solve the large intra-class variation problem, we introduce a query adaptive Transformer that further dynamically adapts the support-set trained classifier's weights to each query video. Experiments on two popular TAL datasets verify the superiority of our method over existing alternatives in both the newly proposed setting with untrimmed labeled support set and previous settings with trimmed counterpart. Moreover, our method remains to be advantageous under a more realistic and challenging cross-domain setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>The effect of Query Video Adaptation with t-SNE visualization. It is shown that with the proposed query video adaptation, the classifier weight can be effectively pushed to be aligned with the foreground content of the query video sample. This improves learning the intra-class invariance of the new class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Intra-class variation example in the "Doing Fencing" class on ActivityNet-v1.3. As can be seen, the two videos present clear difference in viewpoint, scene setup, background, illumination, as well as instance length (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of (a) "BreakDancing" class on ActivityNet and (b) "Throw Discuss" class on THUMOS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>45.2 35.5 25.3 13.2 32.5 49.2 36.9 24.3 16.5 10.1 27.2 44.1 37.8 29.5 21.4 11.5 25.8 7.3 4.2 3.1 2.0 1.5 3.7 Ours</figDesc><table><row><cell></cell><cell>1 Shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hu et al. [13] 41.0 33.0 27.1 15.9 6.8 24.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.6 23.2 12.7 7.4 3.1 15.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Feng et al. [8] 43.5 35.1 27.3 16.2 6.5 25.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>31.4 25.5 16.1 8.9 3.2 17.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Yang et al. [38] 53.1 40.9 29.8 18.2 8.4 29.5 48.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.1 36.0 18.5 11.1 7.0 22.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="14">55.1 9 9.1 6.8 4.9 3.5 2.3 5.3</cell></row><row><cell></cell><cell>5 Shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5 shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="15">Buch et al. [2] 39.7 33.6 27.0 14.0 4.6 23.3 35.7 29.4 20.8 11.7 3.4 20.2 30.4 25.1 19.6 12.9 6.6 18.9 2.7 1.9 1.4 0.9 0.4 1.5</cell></row><row><cell cols="15">Hu et al. [13] 45.4 35.0 29.9 17.6 5.2 27.0 42.2 32.6 20.3 13.7 5.2 22.8 38.9 27.2 18.3 12.7 7.3 20.9 6.8 3.1 2.2 1.8 1.3 3.</cell></row></table><note>? 55.6 44.6 35.7 24.6 12.7 31.8 51.2 38.1 22.7 14.8 9.2 27.0 44.9 38.0 29.2 21.4 11.2 25.1 Yang et al. [38] 56.5 47.0 37.4 21.5 11.9 34.9 51.9 42.7 24.4 17.7 10.1 29.3 43.9 37.4 20.2 13.4 7.7 24.5 8.6 5.6 3.8 2.5 1.7 4.4 Ours 63.0 54.5 44.2 30.9 15.8 38.4 54.3 43.6 35.8 24.5 12.2 31.6 48.2 39.1 29.7 22.5 12.8 28.2 10.4 7.1 5.7 4.8 2.9 5.4 Ours? 63.8 54.2 43.9 31.4 16.4 38.5 56.1 47.2 32.4 24.3 13.7 32.7 51.8 42.7 32.6 23.4 11.9 30.2 13.8 11.3 8.4 6.3 4.2 7.1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>FS-TAL results (%).? : Using untrimmed support set (i.e., the new setting).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Effect of query video adaptation (QVA) in the multi-instance setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Cross-domain FS-TAL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Effect of video embedding in the 5-shot multi-instance setting.</figDesc><table><row><cell>Dataset</cell><cell>ActivityNet-v1.3</cell></row><row><cell>Metrics</cell><cell>Speed (seconds / task)</cell></row><row><cell>Yang et al. [38]</cell><cell>0.81</cell></row><row><cell>Ours</cell><cell>0.83</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Soft-nmsimproving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sst: Single-stream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan Qiu</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-example object detection with model communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video re-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Silco: Show a few images, localize the common object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fewshot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">One-shot action localization by learning sequence matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Localizing the common action among a few videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Canet: Classagnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Metal: Minimum effort temporal activity localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
