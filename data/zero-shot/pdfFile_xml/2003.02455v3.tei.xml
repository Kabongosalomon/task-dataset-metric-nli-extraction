<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PAC-Bayes Meta-learning with Implicit Task-specific Posteriors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
						</author>
						<title level="a" type="main">PAC-Bayes Meta-learning with Implicit Task-specific Posteriors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-PAC Bayes</term>
					<term>meta-learning</term>
					<term>few-shot learning</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new and rigorously-formulated PAC-Bayes meta-learning algorithm that solves few-shot learning. Our proposed method extends the PAC-Bayes framework from a single task setting to the meta-learning multiple task setting to upper-bound the error evaluated on any, even unseen, tasks and samples. We also propose a generative-based approach to estimate the posterior of task-specific model parameters more expressively compared to the usual assumption based on a multivariate normal distribution with a diagonal covariance matrix. We show that the models trained with our proposed meta-learning algorithm are well calibrated and accurate, with state-of-the-art calibration and classification results on few-shot classification (mini-ImageNet and tiered-ImageNet) and regression (multi-modal task-distribution regression) benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O NE unique ability of humans is to quickly learn new tasks with only a few training examples. This is due to the fact that humans tend to exploit prior experience to facilitate the learning of new tasks. Such exploitation is markedly different from conventional machine learning approaches, where no prior knowledge (e.g. training from scratch with random initialisation) <ref type="bibr" target="#b0">[1]</ref>, or weak prior knowledge (e.g., fine tuning from pre-trained models) <ref type="bibr" target="#b1">[2]</ref> are employed to learn a new task. This motivates the development of novel learning algorithms that can effectively encode the knowledge learnt from training tasks, and exploit that knowledge to quickly adapt to future tasks <ref type="bibr" target="#b2">[3]</ref>.</p><p>Prior knowledge can be helpful for future learning only if all tasks are assumed to be distributed according to a latent task distribution. Learning this latent distribution is, therefore, useful for solving an unseen task, even if the task contains a limited number of training examples. Many approaches have been proposed and developed to achieve this goal, namely: multi-task learning <ref type="bibr" target="#b3">[4]</ref>, domain adaptation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and meta-learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Among these, meta-learning has flourished as one of the most effective methods due to its ability to leverage the knowledge learnt from many training tasks to quickly adapt to unseen tasks.</p><p>Recent advances in meta-learning have produced stateof-the-art results in many benchmarks of few-shot learning data sets <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Learning from a few training examples is often difficult and easily leads to over-fitting, especially when no model uncertainty is taken into account. This issue has been addressed by several recent probabilistic meta-learning approaches that incorporate model uncertainty into prediction, e.g., LLAMA (based on Laplace Work in progress. method) <ref type="bibr" target="#b15">[16]</ref>, or PLATIPUS <ref type="bibr" target="#b12">[13]</ref>, Amortised Bayesian Metalearner (ABML) <ref type="bibr" target="#b16">[17]</ref> and VERSA <ref type="bibr" target="#b17">[18]</ref> that use variational inference (VI). However, these studies have not thoroughly investigated the errors evaluated on arbitrary tasks (including seen and unseen) sampled from the same task distribution and arbitrary samples generated from the same task. This results in limited theoretical generalisation guarantees. Moreover, most of these studies are based on variational functions that may not represent well the richness of the underlying distributions. For instance, a common choice for the variational distribution relies on a multivariate normal distribution with a diagonal covariance matrix, which can potentially worsen the prediction accuracy given its limited representability.</p><p>In this paper, we address the two problems listed above with the following technical novelties: (i) derivation of a rigorous meta-learning objective that upper-bounds the errors evaluated on any tasks and any samples of fewshot learning setting based on the PAC-Bayes framework, and (ii) proposal of a novel implicit modelling approach to expressively represent the posterior of task-specific model parameter. Our evaluation shows that the models trained with our proposed meta-learning algorithm is at the same time well calibrated and accurate, with state-of-the-art results in few-shot classification (mini-ImageNet and tiered-ImageNet) and regression (multi-modal task-distribution regression) benchmarks in terms of accuracy, Expected Calibration Error (ECE) and Maximum Calibration Error (MCE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our paper is related to probabilistic few-shot meta-learning techniques that have been developed to incorporate uncertainty into model estimation. LLAMA <ref type="bibr" target="#b15">[16]</ref> employs the Laplace method to extend the deterministic estimation assumed in MAML <ref type="bibr" target="#b12">[13]</ref> to a multivariate normal distribution. However, the need to estimate and invert the Hessian matrix arXiv:2003.02455v3 <ref type="bibr">[cs.</ref>LG] 30 Oct 2021 of a loss function makes this approach computationally challenging for large-scale models, such as deep neural networks. Variational inference (VI) addresses such scalability issue -remarkable examples of VI-based methods are PLATIPUS <ref type="bibr" target="#b18">[19]</ref>, BMAML <ref type="bibr" target="#b19">[20]</ref>, ABML <ref type="bibr" target="#b16">[17]</ref> and VERSA <ref type="bibr" target="#b17">[18]</ref>. Although these VI-based approaches have demonstrated impressive results in regression, classification as well as reinforcement learning, they do not provide any theoretical guarantee on the error induced by arbitrary or even unseen tasks sampled from the same task distribution as well as any samples belonging to the same task. Moreover, the variational distributions used in most of these works are overly-simplified as multivariate normal distribution with diagonal covariance matrices. This assumption, however, limits the expressiveness of the variational approximation, resulting in a less accurate prediction.</p><p>Our work is also related to the PAC-Bayes framework used in meta-learning that upper-bounds errors with certain confidence levels <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. The main difference between these previous works and ours is at the modelling of metaparameter and its objective function. In the previous works, the meta-parameter is the prior of task-specific parameter which is analogous to a regularisation in task adaptation step, while in our proposed method, the meta-parameter is the model initialisation. In addition, the existing works rely on a train-train setting <ref type="bibr" target="#b22">[23]</ref> where the all data of a task is used for task adaptation such as REPTILE <ref type="bibr" target="#b23">[24]</ref>, while ours follows the train-validation split with the bi-level optimisation objective shown below in (3). Such differences lead to a discrepancy in the formulation of the corresponding PAC-Bayes bounds. Another work closely related to our proposed method is exponentially weighted aggregation for lifelong learning (EWA-LL) <ref type="bibr" target="#b24">[25]</ref>. In EWA-LL, each task-specific model is decomposed into a shared feature extractor and a task-specific classifier, while in our approach, each model is an adapted or a fine-tuned version of the meta-parameter. Moreover, the setting of EWA-LL follows the train-train meta-learning approach, making the algorithm analogous to multi-task learning, while our proposed method is a train-validation meta-learning approach with the bi-level optimisation objective.</p><p>Our work has a connection to the statistical analysis of meta-learning that proves generalisation upper-bound for meta-learning algorithms <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Some typical recent works include the learning of the common regularisation that is used when adapting or fine-tuning on a specific task <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> to improve the performance of meta-learning algorithms in heterogeneous task environments, or analyse and optimise the regret induced by meta-learning algorithms in an online setting <ref type="bibr" target="#b31">[32]</ref>. Our work differs from this line of works at how the meta-parameter is modelled. In our case, the metaparameter of interest is the model initialisation, and our goal is to learn a variational distribution for such parameter, while existing works consider different parameter, such as the shared L2 regularisation parameters, as meta-parameter, and often learn a point estimate for such meta-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data generation model of a task</head><p>A data point of a task indexed by i considered in this paper consists of an input x ij ? X ? R d and a corresponding label y ij ? Y with j ? N. Such data points are generated in 2 steps. The first step is to generate the input x ij by sampling from some probability distribution D i . The second step is to determine the label y ij = f (x ij ), where f i : X ? Y is the "correct" labelling function. Note that both the probability distribution D i and the labelling function f i are unknown. To simplify the notations, (x ij , y ij ) ? (D i , f i ) is then used to denote such data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task instance</head><p>Definition. <ref type="bibr" target="#b32">[33]</ref> A task or a task instance T i consists of an unknown associated data generation model (D i , f i ), and a loss function i , denoted as:</p><formula xml:id="formula_0">T i = {(D i , f i ), i }.</formula><p>Remark 1. The loss function i is defined abstractly, and not necessarily some common loss functions, such as mean squared error (MSE) or cross-entropy. For example, i could be referred to as negative log-likelihood if the objective is maximum likelihood estimation, or variational-free energy if the objective is based on variational inference.</p><p>To solve a task T i , one needs to obtain an optimal task-specific model h(.; w * i ) : X ? Y, parameterised by w * i ? W ? R n , which minimises a loss function on the data of that task:</p><formula xml:id="formula_1">w * i = arg min wi E (xij ,yij )?(Di,fi) [ i (x ij , y ij ; w i )] . (1)</formula><p>In practice, since both D i and f i are unknown, the data generation model is replaced by a dataset consisting of a finite number of data-points generated according to the data generation model (D i , f i ), denoted as S i = {x ij , y ij } mi j=1 . The objective to solve that task is often known as empirical risk minimisation (ERM):</p><formula xml:id="formula_2">w ERM i = arg min wi 1 m i mi j=1 [ i (x ij , y ij ; w i )] .<label>(2)</label></formula><p>For simplicity, this paper considers two families of tasks: regression and classification. As a result, the label is a scalar Y ? R for regression and Y = {0, 1, . . . , n ? 1} for classification. In addition, the loss function used will be the same for each task family, hence, the subscript on the loss function will be dropped, and the loss is denoted as throughout the paper. Due to the commonality of the loss function across all tasks, we will drop the notation of when referring to a task. In other words, a task can be simply represented by either its data generation model (D i , f i ) or the associated dataset S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Meta-learning</head><p>The setting of the meta-learning problem considered in this paper follows the task environment <ref type="bibr" target="#b33">[34]</ref> that describes the unknown distribution p(D, f ) over a family of tasks. Each task T i is sampled from this task environment and can be represented as D</p><formula xml:id="formula_3">(t) i , D (v) i , f , where D (t) i and D (v) i</formula><p>are the probability of training and validation input data, ? ? <ref type="figure">Fig. 1</ref>: Meta-learning is an extension of hyper-parameter optimisation, where the meta-parameter ? is shared across all tasks. The solid arrows denote forward pass, while the dashed arrows indicate parameter inference, and rectangles illustrate the plate notations. The training subset</p><formula xml:id="formula_4">y (t) x (t) y (v) x (v)</formula><formula xml:id="formula_5">{(x (t) ij , y (t) ij )} m (t) i</formula><p>j=1 of task T i and the meta-parameter ? are used to learn the task-specific parameter ? i , corresponding to the lower-level optimisation in <ref type="bibr" target="#b2">(3)</ref>. The obtained ? i is then used to evaluate the error on the validation</p><formula xml:id="formula_6">subset {(x (v) ij , y (v) ij )} m (v) i j=1</formula><p>to learn the meta-parameter ?, corresponding to the upper-level optimisation in (3).</p><p>respectively, and they are not necessarily identical. The aim of meta-learning is to obtain a model trained on available training tasks such that the model can be fine-tuned on some labelled data of a testing task drawn from the same task environment to predict the label of unlabelled data on the same task accurately.</p><p>Such meta-learning methods use meta-parameters to model the common latent structure of the task distribution p(D, f ). Examples of meta-parameters are: model initialisation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>, learning rate when fine-tuning the model for a task <ref type="bibr" target="#b35">[36]</ref>, feature extractor <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref>, and optimiser <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38]</ref>. In this point of view, meta-learning can be considered as an extension of hyper-parameter optimisation in single-task learning, where the hyper-parameter of interest or metaparameter is shared across many tasks. Mathematically, the objective of meta-learning can be written as a bi-level optimisation:</p><formula xml:id="formula_7">min ? E q(?;?) E p(D,f ) E q(wi;? * i (?)) E D (v) i ,fi x (v) ij , y (v) ij ; w i s.t.: ? * i (?) = arg min ?i(?) E D (t) i ,fi E q(wi;?i(?)) x (t) ij , y (t) ij ; w i , i ? N,<label>(3)</label></formula><p>where q(?; ?), parameterised by ?, is a distribution over the meta-parameter ?, q(w i ; ?(?)), parameterised by ? i (?), is a distribution of task-specific parameter w i , E D (t) i ,fi means the expectation of input x    </p><formula xml:id="formula_8">ij = f x (t) ij , and E D (v)</formula><p>i ,fi is defined similarly. In addition, to simplify the notations, we drop the dependence of ? from ? i (?).</p><p>Note that the difference of the objective in (3) from hyper-parameter optimisation in single-task learning is at the upper-level where (3) consists of the additional expectation over all training tasks, denoted as E p(D,f ) .</p><p>Depending on how the loss function and distributions q(?; ?) and q(w i ; ? i ) are defined, one can obtain different meta-learning algorithms. For example, if corresponds to the loss in maximum likelihood and the lower-level is optimised by gradient descent with ? as the initialisation of</p><formula xml:id="formula_9">? i : ? ? ? ? ? ? ? ? ? (x, y; w) = ? ln p(y|x; w) ? = initialisation of ? i q(w i ; ? i ) = ?(w i ? ? i ) q(?; ?) = ? (? ? ?) ,<label>(4)</label></formula><p>where ?(.) is the Dirac delta function, then the objective in (3) can be simplified to:</p><formula xml:id="formula_10">min ? E p(D,f ) E D (v) i ,fi ? ln p y (v) ij |x (v) ij ; w i s.t.: w * i = arg min wi E D (t) i ,fi ? ln p y (t) ij |x (t) ij ; w i , i ? N,<label>(5)</label></formula><p>which resembles the MAML algorithm <ref type="bibr" target="#b12">[13]</ref>.</p><p>Another example is the probabilistic meta-learning algorithm that replaces the loss function by a form of the variational-free energy and uses some similar assumptions in MAML:</p><formula xml:id="formula_11">? ? ? ? ? ? ? ? ? ? ? ? ? (x, y; w) = ? ln p(y|x; w) + ln q(w i ; ? i ) p(w i ) ? = initialisation of ? i q(w i ; ? i ) = N w i ; ? ?i , diag ? 2 ?i q(?; ?) = ? (? ? ?) ,<label>(6)</label></formula><p>where N (.) denotes multivariate normal distribution, diag(.) denotes a diagonal matrix, and p(w i ) is the prior of w i . This formulation resembles ABML <ref type="bibr" target="#b16">[17]</ref> and VAM-PIRE <ref type="bibr" target="#b34">[35]</ref> algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">PAC-Bayes upper-bound in single-task learning</head><p>In practice, the data probability distribution D i and the labelling function f i of a task T i are unknown, but only a dataset S i consisting of finite input data and labels is available. Since the aim is to minimise the loss averaged over all data generated from (D i , f i ), it is, therefore, important to analyse the difference between such "true" loss and the empirical loss evaluated on a given dataset. Such difference can be upper-bounded by the Kullback-Leibler (KL) divergence shown in Theorem 1 with a certain level of confidence.</p><formula xml:id="formula_12">Theorem 1. [39] If a dataset S i consists of m i inputs x ik i.i.d.</formula><p>sampled from a data probability distribution D i and being labelled by f i , H is a hypothesis class, ? ? (0, 1] and a loss function : H ? Y ? [0, 1], then for any "posterior" q(w i ; ? i ) over a hypothesis h(.; w i ) ? H, parameterised by w i , the following holds with a probability at least 1 ? ?:</p><formula xml:id="formula_13">E q(wi;?i) E (Di,fi) [ (x ij , y ij ; w i )] ? 1 m i mi k=1 E q(wi;?i) [ (x ik , y ik ; w i )] + KL [q(w i ; ? i )||p(w i )] + ln mi ? 2(m i ? 1) , where p(w i ) is the prior of w i .</formula><p>Instead of minimising the "true" loss of a task, denoted as the left-hand side term in Theorem 1, which is intractable, one should minimise both the empirical loss and the KL divergence on the right-hand side. Indeed, the upper-bound in Theorem 1 is often used as a tractable learning objective function for the model of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PAC-Bayes meta-learning</head><p>In practice, the training and validation data probability distribution, D </p><formula xml:id="formula_14">(t) i = x (t) ij , y (t) ij m (t) i j=1 and S (v) i = x (v) ik , y (v) ik m (v) i k=1</formula><p>associated with task T i , are provided as illustrated in <ref type="figure">Figure 1</ref>  are known in training, while being unknown in testing. In addition, there are only T tasks sampled from the task environment that are available for training. Hence, it is important to derive an upper-bound, and in particular PAC-Bayes upper-bound, for both the "generalisation" losses in upper-and lowerlevels of (3). Since the lower-bound corresponds to solving a single task, the upper-bound derived in Theorem 1 can be straight-forwardly applied as the learning objective. The remaining problem lies on the formulation of the PAC-Bayes upper-bound for the loss in the upper-level of (3). This novel bound is shown in Theorem 2 with its detailed proof in Appendix A. Theorem 2. Given T tasks sampled from the same task environment p(D, f ), where each task has an associated dataset S i with samples generated from the task-specific data generation model (D i , f i ), then for a bounded loss function : W ? Y ? [0, 1] and any distributions q(?; ?) of meta-parameter ? and q(w i ; ? i ) of task-specific parameter w i , the following holds with the probability at least 1 ? ?, ?? ? (0, 1]:</p><formula xml:id="formula_15">E q(?;?) E p(D,f ) E q(wi;?i) E (D (v) i ,fi) x (v) ij , y (v) ij ; w i ? 1 T T i=1 1 m (v) i m (v) i k=1 E q(?;?) E q(wi;?) x (v) ik , y (v) ik ; w i + E q(?;?) [KL [q(w i ; ? i )||p(w i )]] + T 2 (T ?1)? ln m (v) i 2 m (v) i ? 1 + KL [q(?; ?)||p(?)] + T ln T ? 2(T ? 1) ,</formula><p>where p(w i ), ?i ? {1, . . . , T } is the prior of task-specific parameter w i and p(?) is the prior of meta-parameter ?.</p><p>Proof sketch. The proof is carried out in three steps: (i) derive a PAC-Bayes upper-bound for unseen samples generated from task-specific data generation model (D</p><formula xml:id="formula_16">(v) i , f i ), ?i ? {1, .</formula><p>. . , T } by adapting the proof of Theorem 1 as shown in Appendix A.1, (ii) derive a PAC-Bayes upper-bound for unseen tasks by applying Theorem 1 as shown in Appendix A.2, and (iii) combine the two obtained results as shown in Appendix A.3 to complete the proof. Remark 2. One limitation of Theorems 1 and 2 is the assumption of bounded loss which restricts the loss function within [0, 1]. Although there are several works that extend PAC-Bayes bound for unbounded losses <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>, their formulations still needs to assume the boundedness of the moment generating function of some particular loss functions. This assumption is, however, impractical since some common loss functions in practice, such as mean squared error or cross-entropy, do not possess such property. Nevertheless, our main focus in this paper is to provide a theoretical generalisation guarantee for meta-learning using PAC-Bayes theory. Such extension is not necessary since in the implementation our loss is clipped to be within [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Practical meta-learning objective</head><p>Instead of following the objective in <ref type="formula" target="#formula_7">(3)</ref>, which is intractable due to the unknown data generation model of each task and the unknown task environment, we utilise the results in Theorems 1 and 2 to propose a new objective function for a practical meta-learning that theoretically guarantees the generalisation errors due to unseen samples of a task and unseen tasks sampled from the task environment. The new objective is formulated by replacing the optimisation in the lower-level by the minimisation of the corresponding upperbound in Theorem 1, and the optimisation in the upper-level by the minimisation of the upper-bound in Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Meta-learning with implicit task-specific posterior</head><p>Given the new objective function where both the lower-and upper-levels are replaced by the minimisation of the PAC-Bayes upper-bounds in Theorems 1 and 2, there are a total of four distributions of interest: the "posteriors" q(?; ?) and q(w i ; ? i ), i ? {1, . . . , T }, and the priors p(?) and p(w i ).</p><p>As priors represent the modelling assumption and are chosen before observing data, both p(?) and p(w i ) are, therefore, assumed to be multivariate normal distributions with diagonal covariance matrices:</p><formula xml:id="formula_17">p(?) = N ?; 0 0 0, ? 2 ? I (7) p(w i ) = N w i ; 0 0 0, ? 2 w I ,<label>(8)</label></formula><p>where 0 0 0 is a vector containing all zeros, I is the identity matrix, ? ? &gt; 0 and ? w &gt; 0 are hyper-parameters. For the posterior q(?; ?) of the meta-parameter ?, it is often assumed to be a Dirac delta function: q(?; ?) = ?(???) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35]</ref>. Such assumption would, however, make the KL divergence KL [q(?; ?)||p(?)] in Theorem 2 undefined. In this paper, q(?; ?) is assumed to be a multivariate normal distribution with a fixed diagonal covariance matrix, denoted as:</p><formula xml:id="formula_18">q(?; ?) = N (?; ? ? , ? 0 I) ,<label>(9)</label></formula><p>where ? 0 is a hyper-parameter. Such notation also means that ? = ? ? . The only distribution left is the task-specific "posterior" q(w i ; ? i ). In general, q(w i ; ? i ) can be modelled following one of the two general types: prescribed and implicit <ref type="bibr" target="#b43">[44]</ref>. For example, ABML <ref type="bibr" target="#b16">[17]</ref> and VAMPIRE <ref type="bibr" target="#b34">[35]</ref> are prescribed approaches where q(w i ; ? i ) is assumed to be a multivariate normal distribution with a diagonal covariance matrix. Such approximation is, however, inexpressive, resulting in a poor estimation. In contrast, implicit modelling only has access to the samples generated from the distribution of interest without assuming any analytical form of such distribution, e.g. the generator in generative adversarial networks <ref type="bibr" target="#b44">[45]</ref>. In this paper, we use the implicit modelling approach to expressively represent q(w i ; ? i ).</p><p>The distribution q(w i ; ? i ) is now defined at a more fundamental level whereby data is generated through a stochastic mechanism without specifying a parametric distribution. A parameterised model (i.e., a generator G represented by a deep neural network) is used to model the sample generation:</p><formula xml:id="formula_19">w i ? q(w i ; ? i ) ? w i = G(z; ? i ), z ? p(z),<label>(10)</label></formula><p>where z ? Z ? R z is the latent noise sampled from a latent noise distribution p(z), which is often selected as the uniform in [0, 1] z or the standard normal distribution. In our implementation, we observe that due to the unconstrained support space of the standard normal distribution, latent noise sampled from such distribution may vary drastically, resulting in a large variation of the generated task-specific model parameter w i and potentially, making the training difficulty, especially at the beginning of the training. We, therefore, use the uniform distribution on [0, 1] z as the latent noise distribution p(z) to bound the support space Z of the latent noise to make the training more stable.</p><p>Due to the nature of implicit models, the KL diver-</p><formula xml:id="formula_20">gence term KL [q(w i ; ? i )||p(w i )] in the lower-level and KL [q(w i ; ? * i )||p(w i )]</formula><p>in the upper-level of the bi-level optimisation objective cannot be evaluated either analytically or symbolically. We, therefore, propose to employ the lowerbound of the KL divergence shown in Lemma 1 to estimate the value of the KL divergence to train the meta-learning model. <ref type="bibr" target="#b45">[46]</ref>). For any measurable function ?(h) on a set of predictors under consideration H, and any distributions P and Q on H, the following holds:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 (Compression lemma</head><formula xml:id="formula_21">E Q [?(h)] ? ln E P e ?(h) ? KL [Q P ] . Further, sup ? E Q [?(h)] ? ln E P e ?(h) = KL [Q P ] .</formula><p>Proof. Please refer to the proof in Appendix B.</p><p>To model the function ? : W ? R in Lemma 1 to estimate the lower-bound of KL divergences, we use a neural network with parameter ? i . The objective to obtain ? is to maximise the left-hand side in Lemma 1, which can be expressed as:</p><formula xml:id="formula_22">? * i = arg max ?i E q(wi;?i) [?(w i ; ? i )] ? ln E p(wi) e ?(wi;?i) .<label>(11)</label></formula><p>The KL divergence between the task-specific posterior q(w i ; ? i ) and prior p(w i ) can then be estimated as:</p><formula xml:id="formula_23">KL [q(w i ; ? i )||p(w i )] = E q(wi;?i) [?(w i ; ? * i )] ? ln E p(wi) e ?(wi;? * i ) .<label>(12)</label></formula><p>One problem that arises when estimating the losses in both the lower-and upper-level of the meta-learning objective is to learn a different ? i for each different task while ? and ? 0 not converged do <ref type="bibr">5:</ref> sample: ? ? N (?; ? ? , ? 0 I) <ref type="bibr">6:</ref> sample a mini-batch of T tasks <ref type="bibr">7:</ref> for each task T i do <ref type="bibr">8:</ref> </p><formula xml:id="formula_24">? i , ? i ? OPTIMISE LOWER-LEVEL(?, ? 0 , T i ) 9:</formula><p>use ? i to calculate PAC-Bayes upper-bound in Theorem 2 <ref type="bibr">10:</ref> use ? i to calculate the KL lower-bound in <ref type="formula" target="#formula_2">(12)</ref> 11: end for <ref type="bibr">12:</ref> ? ? SGD (average of T losses obtained in step 9) <ref type="bibr">13:</ref> ? 0 ? SGA(average of T KL lower-bounds obtained in step 10) SGD/SGA = stochastic gradient descent/ascent <ref type="bibr">14:</ref> end while <ref type="bibr">15:</ref> return ? and ? 0 16: end procedure <ref type="bibr">17:</ref> procedure OPTIMISE LOWER-LEVEL(?, ? 0 , T i ) <ref type="bibr">18:</ref> initialise task-specific parameter: ? i ? ? <ref type="bibr">19:</ref> initialise task-specific ?-net: ? i ? ? 0 20: ? * i ? arg max ?i KL lower-bound in (11) <ref type="bibr">21:</ref> use ? * i to calculate KL divergence in Eq. <ref type="formula" target="#formula_2">(12)</ref> 22:</p><p>? * i ? arg min ?i PAC-Bayes upper-bound in Theorem 1 <ref type="bibr">23:</ref> return ? * i and ? * i 24: end procedure T i by training the neural network ? from scratch. The downside of such naive implementation is the significant increase in training time. We, therefore, propose to learn a good initialisation of ? i using MAML <ref type="bibr" target="#b12">[13]</ref> to reduce the time of the KL divergence estimation. With this assumption, we define ? 0 as the meta-parameters (or initialisation) of ? i . Within each task, we initialise ? i at ? 0 and optimise the loss in (11) w.r.t. ? i using gradient descent. ? 0 is then obtained by optimising the validation loss evaluated on ? * i w.r.t. ? 0 . The proposed meta-learning method, therefore, consists of two parameters of interest: the hyper-meta-parameter ? and the meta-parameter ? 0 of the ?-network. Each of the parameters is optimised following their corresponding bi-level optimisation objective functions similar to <ref type="bibr" target="#b2">(3)</ref>. Algorithm 1 shows the training procedure of the proposed approach. In addition, we name the proposed approach as statistical implicit PAC-Bayes meta-learning or SImPa for short, to simplify the text when comparing to other meta-learning methods.</p><p>One potential drawback of the implicit modelling approach is the curse of dimensionality, resulting in a computationally expensive training process. This is an active research question when dealing with generative models. This issue can be addressed by encoding the high-dimensional data, such as images, to a feature embedding space by supervised-learning on the same training data set. For example, in one of our experiments, we also show how to use image features extracted from a wide-residual network trained on tiered-ImageNet training set <ref type="bibr" target="#b14">[15]</ref> to mitigate this issue. This strategy reduces the dimension of the input space, leading to smaller task-specific model parameter w i , and eventually decreasing the size of the generator. The trade-off lies in the possibility of losing relevant information that can affect the performance on hold-out tasks.</p><p>It is also worth noting that our proposed method is easier to train than prior probabilistic meta-learning methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> because we no longer need to tune the weighting factor of KL [q(w i ; ? i )||p(w i )] in both levels of the bi-level optimisation objective. Although weighting such KL divergence terms can be justified by casting the optimisation in each level of the meta-learning objective to a constrained optimisation as shown in ?-VAE <ref type="bibr" target="#b46">[47]</ref>, the weighting factor in such case is the corresponding Lagrange multiplier of the constrained optimisation. Thus, simply setting that weighting factor as a "tunable" hyper-parameter may result in a sub-optimal solution. In contrast, our proposed approach does not need to re-weight the KL divergences. The tradeoff of our approach lies in the need to set the confidence parameter ?, but tuning ? is arguably more intuitive than tuning the correct weighting factor for the KL divergence terms done in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, SImPa is evaluated on few-shot regression and classification problems and compared to common metalearning baselines, such as MAML <ref type="bibr" target="#b12">[13]</ref>, ABML <ref type="bibr" target="#b16">[17]</ref>, PLATI-PUS <ref type="bibr" target="#b18">[19]</ref> and BMAML <ref type="bibr" target="#b19">[20]</ref>.</p><p>In the experiments, the loss function is assumed to be the negative log-likelihood (NLL): ? ln p(y ij |x ij ; w i ).</p><p>In particular, the NLL is the mean-squared error (MSE) for regression and cross-entropy for classification. Following the assumption of bounded losses made in Section 4, the data-related losses in both the lower-and upper-level, (x ij , y ij ; w i ) are clipped to [0, 1]. In addition, Monte Carlo (MC) sampling is used to evaluate the expectation over q(?; ?) and q(w i ; ? i ). In particular, one sample of metaparameter ? and one sample of task-specific parameter w i are used in training, while these numbers are one and thirty-two in testing, respectively. The asymmetric choice of those hyper-parameters is to optimise running time in training, while maximising the prediction performance in evaluation. For the hyper-parameters defined in <ref type="formula" target="#formula_70">(7)</ref>, <ref type="formula" target="#formula_17">(8)</ref> and <ref type="formula" target="#formula_18">(9)</ref>, ? ? = ? w = 1 and ? = 10 ?6 . In addition, the confidence parameters is selected as ? = ? i = 0.1, ?i ? {1, . . . , T }. The number of tasks per an update of the parameters of interest is T = 20.</p><p>In term of generating w i for each task T i , we use latent noise vectors z sampled from the uniform distribution in [0, 1] 128 . The generator in (10) is modelled by a fullyconnected neural network with 2 hidden layers. Each of these layers consists of 256 and 512 hidden units, respectively, and is activated by rectified linear unit without any batch normalisation. The output of the final layer is then activated by hyperbolic tangent function to constrain the parameter of the base network, avoiding the loss value from exploding. The ?-network has an "inverted" architecture of the generator, which consists of 3-hidden layers. These layers consist of 512, 256 and 128 hidden units, respectively, and are also activated by rectified linear unit without any batch normalisation. Adam optimiser <ref type="bibr" target="#b47">[48]</ref> is used to optimise both the hyper-meta-parameter ? and ? 0 with the same learning rate of 10 ?4 . To train the ?-network for each task, 512 MC samples of the base network parameters are sampled from both q(w i ; ? i ) and p(w i ) to evaluate the lower-bound of the KL divergence in <ref type="bibr" target="#b10">(11)</ref>. To optimise (11) w.r.t. ? i , gradient descent is used with a learning rate of 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Regression</head><p>The experiment in this subsection is a multi-modal task distribution where half of the data is generated from sinusoidal functions, while the other half is from linear functions <ref type="bibr" target="#b18">[19]</ref>. Similar to existing works in the literature <ref type="bibr" target="#b18">[19]</ref>, the base network to solve each regression task is a fully-connected network with 2 hidden layers, where each layer has 40 hidden units. Linear rectifier function is used as activation function, and no batch-normalisation is used. Gradient descent is used as the optimiser for the lower-level optimisation with learning rate fixed at 10 ?3 and iterated 5 times.</p><p>As shown in <ref type="figure">Figure 2</ref>, SImPa is able to vary the prediction variance, especially when there is more uncertainty in the training data, while MAML can only output a single value at each data point. For a quantitative comparison, we train many probabilistic meta-learning methods, including PLATIPUS <ref type="bibr" target="#b18">[19]</ref>, BMAML <ref type="bibr" target="#b19">[20]</ref> and ABML <ref type="bibr" target="#b16">[17]</ref>, in the same regression problem. Here, BMAML consists of 10 particles trained without Chaser Loss. As shown in <ref type="figure">Figure 3a</ref>, SImPa achieves much smaller MSE, comparing to MAML, PLATIPUS and ABML, and comparable NLL to the nonparametric BMAML when being evaluated on the same hold-out tasks.</p><p>To further evaluate the predictive uncertainty, we employ the reliability diagram based on the quantile calibration for regression <ref type="bibr" target="#b48">[49]</ref>. The reliability diagram shows a correlation between predicted and actual probability. A perfectly calibrated model will have its predicted probability equal to the actual probability, and hence, align well with the diagonal y = x. The results in <ref type="figure">Figure 3b</ref> show that the model trained with SImPa achieves the best calibration among all the methods considered. Due to the nature of a deterministic approach, MAML <ref type="bibr" target="#b12">[13]</ref> is represented as a horizontal line, resulting in a poorly calibrated model. The two probabilistic meta-learning methods, PLATIPUS and ABML, perform better than MAML; however, the averaged slopes of their performance curves are quite close to MAML, implying that their multivariate normal posteriors of task-specific model parameters have small covariance diagonal values. This may be caused by their exclusive reliance on less-expressive multivariate normal distributions with diagonal covariance matrices. The performance of BMAML is slightly better than PLATIPUS and ABML due to its non-parameteric modelling approach. In contrast, SImPa employs a much richer variational distribution q(w i ; ? i ) for task specific parameters, and therefore, produces a model with better calibration. For another quantitative comparison, we plot the expected calibration error (ECE) <ref type="bibr" target="#b49">[50]</ref>, which is the weighted average of the absolute errors measuring from the diagonal, and the maximum calibration error (MCE) <ref type="bibr" target="#b49">[50]</ref>, which returns the maximum of absolute errors in <ref type="figure">Figure 3c</ref>. Overall, SImPa outperforms all of the state-of-the-art methods in both ECE and MCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Few-shot classification</head><p>We evaluate SImPa on the N -way k-shot setting, where a meta learner is trained on many related tasks containing N classes with k examples per class (m (t) i = kN ). The evaluation is carried out by comparing the results of SImPa against the results of state-of-the-art methods on three popular fewshot learning benchmarking data sets: Omniglot <ref type="bibr" target="#b2">[3]</ref>, mini-ImageNet <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> and tiered-ImageNet <ref type="bibr" target="#b50">[51]</ref>.</p><p>Omniglot dataset consists of 50 different alphabets with a total of 1623 characters drawn online via Amazon's Mechanical Turk by 20 different people. Hence, Omniglot is often considered as a "transposed" MNIST since Omniglot has many classes, but each class has 20 images. We follow the original train-test split where 30 alphabets are used for training, while the other 20 alphabets are used for testing. To be consistent with previous evaluations, we pre-process by down-sampling all images to 28-by-28 pixels. No data augmentation, such as rotation, is used. Note that for the task formation, many existing meta-learning methods in the literature use non-standard train-test split where characters of all 50 alphabets are mixed, and randomly split. This splitting potentially forms easier tasks since knowing a character in an alphabet might help to classify other characters within that same alphabet. Moreover, the mixed and random split is different from evaluation to evaluation, making it challenging to fairly compare different meta-learning methods.</p><p>Mini-ImageNet <ref type="bibr" target="#b36">[37]</ref> is another dataset used to evaluate classification performance between different meta-learning methods. The dataset consists of 100 classes, where each class contains 600 colour images taken from ImageNet <ref type="bibr" target="#b51">[52]</ref>. We follow the standard train-test split which uses 64 classes for training, 16 classes for validation, and 20 classes for testing <ref type="bibr" target="#b9">[10]</ref>. The images in the dataset are pre-processed by down-sampling to 84-by-84 pixels before any training is carried out. No data augmentation, such as image flipping or rotation, is used.</p><p>Tiered-ImageNet is one of the largest subsets of Ima-geNet, which consists of total 608 classes grouped into 34 high-level categories <ref type="bibr" target="#b50">[51]</ref>. Tiered-ImageNet is often used as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD 1-SHOT 5-SHOT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Omniglot [3] -standard 4-block CNN</head><p>MAML <ref type="bibr" target="#b12">[13]</ref> 97.143 ? 0.005 Prototypical nets <ref type="bibr" target="#b11">[12]</ref> 96.359 ? 0.006 BMAML <ref type="bibr" target="#b19">[20]</ref> 94.104 ? 0.008 ABML <ref type="bibr" target="#b16">[17]</ref> 97.281 ? 0.004 SImPa 98.352 ? 0.005</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-ImageNet [10] -standard 4-block CNN</head><p>Matching nets <ref type="bibr" target="#b36">[37]</ref> 43.56 ? 0.84 55.31 ? 0.73 Meta-learner LSTM <ref type="bibr" target="#b9">[10]</ref> 43.44 ? 0.77 60.60 ? 0.71 MAML <ref type="bibr" target="#b12">[13]</ref> 48.70 ? 1.84 63.15 ? 0.91 Prototypical nets <ref type="bibr" target="#b11">[12]</ref>  <ref type="bibr" target="#b0">1</ref> 49.42 ? 0.78 68.20 ? 0.66 LLAMA <ref type="bibr" target="#b15">[16]</ref> 49.40 ? 1.83 PLATIPUS <ref type="bibr" target="#b18">[19]</ref> 50.13 ? 1.86 ABML <ref type="bibr" target="#b16">[17]</ref> 45.00 ? 0.60 SImPa 51.72 ? 0.48 63.49 ? 0.40</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-ImageNet [10] -non-standard network</head><p>Relation nets <ref type="bibr" target="#b52">[53]</ref> 50.44 ? 0.82 65.32 ? 0.70 VERSA <ref type="bibr" target="#b17">[18]</ref> 53.40 ? 1.82 67.37 ? 0.86 SNAIL <ref type="bibr" target="#b53">[54]</ref> 55.71 ? 0.99 68.88 ? 0.92 adaResNet <ref type="bibr" target="#b54">[55]</ref> 56.88 ? 0.62 71.94 ? 0.57 TADAM <ref type="bibr" target="#b55">[56]</ref> 58.50 ? 0.30 76.70 ? 0.30 LEO <ref type="bibr" target="#b14">[15]</ref> 61.76 ? 0.08 77.59 ? 0.12 LGM-Net <ref type="bibr" target="#b56">[57]</ref> 69. <ref type="bibr" target="#b12">13</ref>   a benchmark for large-scaled few-shot learning. We also follow the standard train-test split that consists of 20 categories for training, 6 categories for validation, and 8 categories for testing. In addition, our evaluation is carried out by employing the features extracted from a residual network trained on the data and classes from the training set <ref type="bibr" target="#b14">[15]</ref>.</p><p>The evaluation is carried out in 2 cases: one with raw image data and the other with 640-dimensional image features extracted from a wide-residual network trained solely on training data <ref type="bibr" target="#b14">[15]</ref>. In the "standard" case, the base network is the "standard" convolutional network consisting of 4 modules. Each module has a convolutional layer with 32 1. Trained on 30-way 1-shot setting 2. Use extracted features <ref type="bibr" target="#b14">[15]</ref> as input 3-by-3 filters, followed by a batch normalisation, ReLU and 2-by-2 max-pooling. The output of the final module is flatten and connected to a fully connected layer to predict the label of the input image. In the "non-standard" case, the base network is a fully-connected network with 2 hidden layers. Each layer consists of 128 and 32 hidden units activated by rectified linear unit without any batch-normalisation.</p><p>We report the classification accuracy of SImPa on these three data sets in <ref type="table" target="#tab_1">Table 1</ref>. For Omniglot, we use the published code to reproduce the results for some common meta-learning methods to fairly compare with SImPa. The accuracy averaged over more than 1 million testing tasks show that the proposed SImPa is better than competing meta-learning methods in the literature. For mini-ImageNet, SImPa achieves the best empirical results for the 1-shot setting when the base model is the "standard" CNN, and for the 5-shot setting when a different network architecture is used. SImPa shows the second best results for the 5shot setting with the 4-layer CNN and the 1-shot setting with the different network architecture. Note that for the 5-shot setting using standard CNN, Prototypical networks need to train with a much higher "way" which is harder to learn, and might help the trained model to perform better on easier tasks with lower "way". For tiered-ImageNet, SImPa outperforms the current state-of-the-art in 1-shot setting, while being comparable in 5-shot setting. To obtain a fairer comparison, we re-run MAML on the image data of mini-ImageNet using a ResNet10, which has about 5 million parameters (ours has about 8 millions parameters). However, MAML, with and without L2 regularisation, overfits the training data (our best result for MAML was 89% accuracy on train, while only 42% on test). This known issue of overfitting when using larger networks in MAML was mentioned in the MAML paper <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">Section 5.2]</ref>. We also try a similar model for ABML <ref type="bibr" target="#b16">[17]</ref>, but observed no improvement.</p><p>Similarly to the experiment for regression, we use reliability diagrams <ref type="bibr" target="#b49">[50]</ref> to evaluate the predictive uncertainty. For a fair comparison, we re-implement several probabilistic meta-learning approaches, including MAML <ref type="bibr" target="#b12">[13]</ref>, PLATI-PUS <ref type="bibr" target="#b18">[19]</ref>, BMAML <ref type="bibr" target="#b19">[20]</ref> and ABML <ref type="bibr" target="#b16">[17]</ref>, using the 4-block CNN as the base model, trained under the same setting, and plot their reliability chart. The performance curves in the reliability diagram show how well calibrated a model is when testing across many unseen tasks. A perfectly calibrated model will have its values overlapped with the identity function y = x, indicating that the probability associated with the label prediction is the same as the true probability. To ease the visualisation, we normalise the reliability chart by subtracting the predicted accuracy by its corresponding value on the diagonal y = x, as shown in <ref type="figure" target="#fig_10">Figure 4a</ref>. Hence, for the normalised reliability chart, the closer to y = 0, the better the calibration. Visually, the model trained with SImPa shows better calibration than the ones trained with other meta-learning methods. To further evaluate, we compute the expected calibration error (ECE) and maximum calibration error (MCE) <ref type="bibr" target="#b49">[50]</ref> of the models trained with these methods. The results plotted in <ref type="figure" target="#fig_10">Figure 4b</ref> show that the model trained with SImPa achieves the smallest ECE and MCE among all the methods considered in this comparison. The most competitive method to SImPa, regarding ECE and MCE, is ABML, but note that ABML has a worse classification accuracy than SImPa, as shown in <ref type="table" target="#tab_1">Table 1</ref> (Top) -see row "ABML [17]".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>As mentioned in Remark 2, the loss function is clipped to [0, 1] to satisfy the assumption made in PAC-Bayes framework. We observed that it affects the training of SImPa, mostly at the early stages. The reason might be the imbalance between the clipped loss and the regularisation terms related to KL divergence, making the learning overregularised with slow convergence. In the implementation, we first train SImPa without such regularisation terms for 1,000 tasks, and then add them back to the loss with such regularisation. This facilitates the training process, allowing the algorithm to converge faster. As specified in subsection 4.3, the usage of implicit distribution, and in particular the generator in (10) to generate the parameter for the neural network of interest, leads to an exponential increasing of the number of learnable parameters. For example, in the classification of mini-ImageNet using the 4-convolutional module neural network, the number of parameter of the base network is 32,645, requiring us to have a generator with 16.7 million parameters. Such a large generator limits the scalability of SImPa, making it inapplicable for larger base networks. One workaround solution might be to utilise the architecture design proposed in Hypernetworks <ref type="bibr" target="#b59">[60]</ref> that shares parameters to reduce the size of the generator. The trade-off is the slight decreasing of the generator expressiveness when generating the parameter for the base neural network.</p><p>Another limitation of SImPa is the need of GPU memory and training time comparing to other meta-learning approaches, such as MAML. In our implementation, the simplest baseline MAML needs 6 GPU-hours to train until convergence, while probabilistic baselines, such as ABML, BMAML and PLATIPUS, take about 30 GPU-hours to converge under the same setting. For SImPa, it requires more than 48 hours to converge. The reason of such long training time lies on the size of the meta-parameter, the need to train the ? network to estimate KL divergence, and the number of Monte Carlo samples used. Please refer to Appendix C for a detailed analysis of the running time complexity between SImPa and other meta-learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We introduce and formulate a new probabilistic algorithm for few-shot meta-learning. The proposed algorithm, SImPa, is based on PAC-Bayes framework which theoretically guarantees error induced on unseen tasks and unseen samples within each task. In addition, the proposed method employs a generative approach that implicitly models the posterior of task-specific model parameter q(w i ; ? i ), resulting in more expressive variational approximation compared to the usual variational methods using multivariate normal posteriors with diagonal covariance matrices, such as PLATIPUS <ref type="bibr" target="#b18">[19]</ref> or ABML <ref type="bibr" target="#b16">[17]</ref>. The uncertainty, in the form of the learnt implicit distributions, can introduce more variability into the decision made by the model, resulting in well-calibrated and highly-accurate prediction. The algorithm can be combined with different base models that are trainable with gradient-based optimisation, and is applicable in regression and classification. We demonstrate that the algorithm has state-of-the-art calibration and prediction results on unseen data in a multi-modal 5-shot learning regression problem, and achieve state-of-the-art calibration and classification results on few-shot 5-way tasks on mini-ImageNet and tiered-ImageNet data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by Australian Research Council grants CE140100016 and FT190100525. We also thank the Phoenix High Performance Computing service at the University of Adelaide to provide super-computing resources for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTAL MATERIAL A PROOF OF PAC-BAYES FEW-SHOT META-LEARNING BOUND</head><p>The derivation is divided into three steps. The first two steps are to derive the PAC-Bayes bound for the generalisation errors induced by the unseen tasks, and the unseen queried examples within each task. The novel bound is then constructed by combining the results obtained in the first two steps and presented in Theorem 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 PAC-Bayes upper-bound for unseen validation of a single task</head><p>This subsection presents a PAC-Bayes upper-bound for</p><formula xml:id="formula_25">E q(?;?) E D (v) i ,fi E q(wi;? * i ) x (v) ij , y (v)</formula><p>ij ; w i for task T i . Note that this loss is different from the left hand-side term of Theorem 1 (presented in subsection 3.4) at the expectation over the posterior q(?; ?).</p><formula xml:id="formula_26">Lemma 2. If S (v) i consists of m (v) i input x (v)</formula><p>ik sampled from a data probability distribution D i and labelled by f i , H is a hypothesis class with each hypothesis h parameterised by w i , : H ? Y ? [0, 1] is a loss function, q(w i ; ? * i ) is a "posterior" over the hypothesis parameter and p(w i ) is a prior, then the following holds:</p><formula xml:id="formula_27">Pr ? ? ?E q(?;?) E D (v) i ,fi E q(wi;? * i ) x (v) ij , y (v) ij ; w i ? 1 m (v) i m (v) i k=1 E q(?;?) E q(wi;? * i ) x (v) ik , y (v) ik ; w i + R i ? ? ? ? 1 ? ? i ,</formula><p>where: ? i ? (0, 1], and</p><formula xml:id="formula_28">R i = E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + ln m (v) i ?i 2(m (v) i ? 1<label>(13)</label></formula><p>Proof. To simplify the proof, let ?L is the difference between the true loss and empirical loss:</p><formula xml:id="formula_29">?L = E D (v) i ,fi x (v) ij , y (v) ij ; w i ? 1 m (v) i m (v) i k=1 x (v) ik , y (v) ik ; w i .</formula><p>Given the Fubini's theorem to interchange the expectations, the problem can be written as:</p><formula xml:id="formula_30">Pr ? ? ? ? E q(?;?) E q(wi;? * i ) [?L] ? E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + ln m (v) i ?i 2(m (v) i ? 1 ? ? ? ? ? 1 ? ? i .<label>(14)</label></formula><p>Applying Lemma 1 (presented in Appendix B) with 2(m</p><formula xml:id="formula_31">(v) i ? 1)?L 2 as ?(h) gives: 2 m (v) i ? 1 E q(wi;? * i ) L 2 ? KL [q(w i ; ? * i )||p(w i )] + ln E p(wi) e 2 m (v) i ?1 ?L 2 .<label>(15)</label></formula><p>To prove Lemma 2, the left-hand side term in <ref type="bibr" target="#b14">(15)</ref> is lower-bounded, while the last term of the right-hand side in (15) is upper-bounded.</p><p>To lower-bound the left-hand side term, Jensen's inequality is applied on the convex function x 2 to obtain:</p><formula xml:id="formula_32">2 m (v) i ? 1 E q(wi;? * i ) [?L] 2 ? 2 m (v) i ? 1 E q(wi;? * i ) L 2 .<label>(16)</label></formula><p>The results in <ref type="bibr" target="#b14">(15)</ref> and <ref type="bibr" target="#b15">(16)</ref> lead to:</p><formula xml:id="formula_33">2 m (v) i ? 1 E q(wi;? * i ) [?L] ? KL [q(w i ; ? * i )||p(w i )] + ln E p(wi) e 2 m (v) i ?1 ?L 2 .<label>(17)</label></formula><p>Taking expectation over q(?; ?) on both sides gives:</p><formula xml:id="formula_34">2 m (v) i ? 1 E q(?;?) E q(wi;? * i ) [?L] ? E q(?;?) KL [q(w i ; ? * i )||p(w i )] + ln E p(wi) e 2 m (v) i ?1 ?L 2 .<label>(18)</label></formula><p>Note that ? x is a concave function. Hence, one can apply Jensen's inequality on the right-hand side term to obtain an upper-bound. This results in:</p><formula xml:id="formula_35">2 m (v) i ? 1 E q(?;?) E q(wi;? * i ) [?L] ? E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + E q(?;?) ln E p(wi) e 2 m (v) i ?1 ?L 2 .<label>(19)</label></formula><p>To upper-bound the last term in <ref type="bibr" target="#b14">(15)</ref>, Lemma 3 (presented in Appendix B) is then used. To do that, ?L is required to satisfied the assumption of Lemma 3. This can be done by applying the Hoeffding's inequality.</p><p>Consider each loss value (x</p><formula xml:id="formula_36">(v) ik , y ik ; w i ) ? [0, 1] as an i.i.d. random variable with true mean E D (v) i ,fi x (v) ij , y (v) ij ; w i and empirical mean 1 m (v) i m (v) i k=1 x (v) ik , y (v)</formula><p>ik ; w i . Hence, applying Hoeffding's inequality gives:</p><formula xml:id="formula_37">Pr (|?L| ? ) ? e ?2m (v) i ,f ) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ln E (D m (v) i i ,f ) E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ln m (v) i .<label>(23)</label></formula><p>Taking the expectation over the distribution q(?; ?) on both sides and applying Fubini's theorem to interchange the two expectations on the left-hand side give:</p><formula xml:id="formula_38">E (D m (v) i i ,f ) E q(?;?) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ln m (v) i .<label>(24)</label></formula><p>The lower-bound (or the term at the left-hand side of the above inequality) can be lower-bounded further by applying Markov's inequality:</p><formula xml:id="formula_39">Pr E q(?;?) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ? E (D m (v) i i ,f ) E q(?;?) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 , ? &gt; 0.<label>(25)</label></formula><p>This implies that:</p><formula xml:id="formula_40">Pr E q(?;?) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ? ln m (v) i , ? &gt; 0.<label>(26)</label></formula><p>Equivalently, one can write the above inequality as:</p><formula xml:id="formula_41">Pr E q(?;?) ln E q(wi;? * i ) e 2 m (v) i ?1 ?L 2 ? ? 1 ? ln m (v) i , ? &gt; 0.<label>(27)</label></formula><p>Hence, adding an expectation of a KL divergence on both sides of the inequality inside the probability function and taking square root gives:</p><formula xml:id="formula_42">Pr E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + E q(?;?) ln E p(wi) e 2 m (v) i ?1 ?L 2 ? ? E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + ? 1 ? ln m (v) i , ? &gt; 0.<label>(28)</label></formula><p>The results in <ref type="bibr" target="#b18">(19)</ref> and <ref type="bibr" target="#b27">(28)</ref> lead to:</p><formula xml:id="formula_43">Pr 2 m (v) i ? 1 E q(?;?) E q(wi;? * i ) [?L] ? E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + ? 1 ? ln m (v) i , ? &gt; 0.<label>(29)</label></formula><p>Setting</p><formula xml:id="formula_44">? i = ln m (v) i</formula><p>and dividing both sides of the inequality inside the probability function by 2 m</p><formula xml:id="formula_45">(v) i ? 1 give: Pr ? ? ?E q(?;?) E q(wi;? * i ) [?L] ? E q(?;?) [KL [q(w i ; ? * i )||p(w i )]] + ln m (v) i ?i 2 m (v) i ? 1 ? ? ? ? 1 ? ? i .<label>(30)</label></formula><formula xml:id="formula_46">1 m (v) i m (v) i k=1 E q(?;?) E q(wi;? * i ) x (v) ik , y (v) ik ; w i + R i ? ? ? ? 1 ? T i=1 ? i .<label>(32)</label></formula><p>Given Corollary 1 (presented in Appendix A.2) and the result in <ref type="bibr" target="#b31">(32)</ref>, one can apply Corollary 2 (presented in Appendix B) to obtain the following:</p><formula xml:id="formula_47">Pr E q(?;?) E (D,f ) E D (v) i ,fi E q(wi;? * i ) x (v) ij , y (v) ij ; w i ? ? 1 T T i=1 1 m (v) i m (v) i k=1 E q(?;?) E q(wi;? * i ) x (v) ik , y (v) ik ; w i + R i + R 0 ? ? ? ? 1 ? T j=0 ? j .<label>(33)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTAL MATERIAL B AUXILIARY LEMMAS</head><p>Lemma 1 (Compression lemma <ref type="bibr" target="#b45">[46]</ref>). For any measurable function ?(h) on a set of predictors under consideration H, and any distributions P and Q on H, the following holds:</p><formula xml:id="formula_48">E Q [?(h)] ? ln E P e ?(h) ? KL [Q P ] . Further, sup ? E Q [?(h)] ? ln E P e ?(h) = KL [Q P ] .</formula><p>Proof. For a measurable function ?(h):</p><formula xml:id="formula_49">E Q [?(h)] = E Q ln e ?(h) Q(h) P (h) P (h) Q(h) = KL [Q||P ] + E Q ln e ?(h) P (h) Q(h) .<label>(34)</label></formula><p>Applying Jensen's inequality on the last term gives:</p><formula xml:id="formula_50">E Q [?(h)] ? KL [Q||P ] + ln E Q e ?(h) P (h) Q(h) = KL [Q||P ] + ln E P e ?(h) .<label>(35)</label></formula><p>Re-arrange the term proves the first part of the lemma, which is the lower-bound of the KL divergence.</p><p>To prove the second part of the lemma, we simply show that there exists a function ?(h) that makes the lower-bound achieve the maximal value which is the KL divergence. Let:</p><formula xml:id="formula_51">?(h) = ln Q(h) P (h) ,<label>(36)</label></formula><p>then the lower-bound can be expressed as:</p><formula xml:id="formula_52">E Q [?(h)] ? ln E P e ?(h) = KL [Q||P ] ? ln E P Q(h) P (h) = KL [Q||P ] .<label>(37)</label></formula><p>That completes the proof.</p><p>Lemma 3 (Exercise 31.1 in <ref type="bibr" target="#b60">[61]</ref>). Let X be a non-negative random variable that satisfies: Pr (X ? ) ? e ?2m 2 , ? ? 0. Prove that: E e 2(m?1)X 2 ? m.</p><p>Proof. We will present the expectation of interest in term of probability of X. For simplicity, let Y = e 2(m?1)X 2 . Since X 2 ? [0, +?), then Y ? [1, +?). According to the layer cake representation <ref type="bibr" target="#b61">[62,</ref><ref type="bibr">Page 26]</ref>:</p><formula xml:id="formula_53">Y = Y 0 dt = +? 1 1 (Y ? t) dt,</formula><p>where 1(A) is the indicator function of event A.</p><p>One important property of indicator function is that:</p><formula xml:id="formula_54">E [1 (Y ? t)] = Pr (Y ? t) .</formula><p>.</p><p>With the above representation, we can express the expectation of interest as:</p><formula xml:id="formula_55">E [Y ] = E +? 1 1 (Y ? t) dt = +? 1 E [1 (Y ? t)] dt (Fubini's theorem) = +? 1</formula><p>Pr (Y ? t) dt.</p><p>Or:</p><formula xml:id="formula_56">E e 2(m?1)X 2 = +? 1</formula><p>Pr e 2(m?1)X 2 ? x dx.</p><p>We will change the variable from x to to utilise the given inequality. Let:</p><p>x = e 2(m?1) 2 , and since ? 0, then:</p><formula xml:id="formula_57">= ln x 2(m ? 1)</formula><p>, and dx = 4(m ? 1) e 2(m?1) 2 d .</p><p>Hence, the expectation of interest can be written in term of the changed variable as:</p><formula xml:id="formula_58">E e 2(m?1)X 2 = +? 0 Pr e 2(m?1)X 2 ? e 2(m?1) 2 4(m ? 1) e 2(m?1) 2 d = +? 0 Pr (X ? ) ?e ?2m 2 4(m ? 1) e 2(m?1) 2 d ? +? 0 4(m ? 1) e ?2 2 d = m ? 1 &lt; m.</formula><p>Lemma 4. For i = 1 : n, if X i and Y i are random variables, then:</p><formula xml:id="formula_59">p n i=1 X i ? n i=1 Y i ? p n i=1 (X i ? Y i ) .</formula><p>Proof. The proof is quite direct:</p><formula xml:id="formula_60">X i ? Y i =? n i=1 X i ? n i=1 Y i .<label>(38)</label></formula><p>Hence, applying the probability for implication completes the proof.</p><p>Lemma 5. For n events A i with i = 1 : n, the following holds:</p><formula xml:id="formula_61">p n i=1 A i ? n i=1 p(A i ) ? (n ? 1), ?n ? 2.</formula><p>Proof. Proof can be done by induction. For n = 2:</p><formula xml:id="formula_62">p(A 1 ? A 2 ) = p(A 1 ) + p(A 2 ) ? p(A 1 ? A 2 ) ? p(A 1 ) + p(A 2 ) ? 1.</formula><p>Suppose that it is true for case n:</p><formula xml:id="formula_63">p n i=1 A i ? n i=1 p(A i ) ? (n ? 1).</formula><p>We prove that this is also true for case (n + 1):</p><formula xml:id="formula_64">p n+1 i=1 A i = p n i=1 A i + p(A n+1 ) ? p n i=1 A i A n+1 ? p n i=1 A i + p(A n+1 ) ? 1 ? n i=1 p(A i ) ? (n ? 1) + p(A n+1 ) ? 1 (assumption of induction for case n) ? n+1 i=1 p(A i ) ? ((n + 1) ? 1) .</formula><p>It is, therefore, true for (n + 1), and hence, the proof. Lemma 6. Let X i and Y i are random variables with i = 1 : n. If p(X i ? Y i ) ? 1 ? ? i with ? i ? (0, 1], then:</p><formula xml:id="formula_65">p n i=1 X i ? n i=1 Y i ? 1 ? n i=1 ? i .</formula><p>Proof. Applying Lemmas 4 and 5 for the left-hand side term of this lemma gives:</p><formula xml:id="formula_66">p n i=1 X i ? n i=1 Y i ? p n i=1 (X i ? Y i ) (Lemma 4) ? n i=1 p ((X i ? Y i )) ? (n ? 1) (Lemma 5) ? n i=1 (1 ? ? i ) ? (n ? 1) = 1 ? n i=1 ? i .<label>(39)</label></formula><formula xml:id="formula_67">Corollary 2. If p(a ? b) ? 1 ? ? 1 and p(b ? c) ? 1 ? ? 2 with ? 1 , ? 2 ? (0, 1], then: p(a ? c) ? 1 ? ? 1 ? ? 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTAL MATERIAL C COMPLEXITY ANALYSIS</head><p>In this section, we analyse the running time complexity of different meta-learning algorithms related to SImPa per one gradient update for the parameter of interest. These methods include:</p><p>? point estimate such as MAML <ref type="bibr" target="#b12">[13]</ref>, ? probabilistic modelling based on variational inference where the posterior is approximated by a multivariate normal distribution with a diagonal covariance matrix <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35]</ref>, ? SImPa. For simplicity, we assume that the number of samples within each training subset S (t) i is the same across all tasks: m</p><formula xml:id="formula_68">(t) 0 = m (t) i , ?i ? {1, .</formula><p>. . , T }, and so is the validation subset. In addition, as all the methods mentioned are implemented with their first-order versions, the analysis also relies on such assumption. Furthermore, given that such algorithms are implemented with automatic differentiation, the running time complexity of a back-propagation is linear w.r.t. the number of the model's parameters.</p><p>To ease the analysis, we re-state the definition of some notations and define some new ones as shown in <ref type="table" target="#tab_2">Table 2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Deterministic point estimate meta-learning (MAML)</head><p>C.1.1 Lower-level optimisation for each task</p><p>The back-propagation of a single gradient update is O(m (t) 0 n). This is then repeated N lower times. Hence, the total complexity to adapt to each task is O(N lower m (t) 0 n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Upper-level optimisation</head><p>Given the task-specific parameter w i obtained in the lower-level of (3), one can calculate the gradient of the validation loss on each task w.r.t. the meta-parameter similarly to back-propagation. The complexity is, therefore, O(m </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Probabilistic meta-learning with multivariate normal distributions</head><p>These methods have similar complexity as the deterministic point estimate method, except the association of Monte Carlo sampling to draw w i from q(w i ; ? i ) when evaluating the training and validation losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Lower-level optimisation for each task</head><p>The optimisation for the lower-level in (3) now has 2 steps:</p><p>? Sampling for task-specific parameter: w i ? q(w i ; ? i ), resulting in a complexity of O(n) ? Back-propagation with complexity of O(m (t) 0 n). These steps are repeated from m w samples, and iterated N lower , resulting in a total complexity of O(m w N lower (m (t) 0 + 1)n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Upper-level optimisation</head><p>Similarly, the upper-level is also affected by the Monte Carlo sampling of task-specific parameter w i from q(w i ; ? i ). This results in a complexity of O(m w T (m </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 SImPa</head><p>The forward pass of SImPa is more complicated than the deterministic point estimate and the probabilistic modelling mentioned above. The reason is that the parameter of interest is not the meta-parameter ?, but the hyper-parameter ? or ? ? , which is one level higher in the hierarchical structure.</p><p>In addition, we assume that the generator is much larger than the base model; n n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3.1 Lower-level optimisation</head><p>The optimisation for the lower-level is carried out in 2 main steps. The first step is to train the ? network which consists of:</p><formula xml:id="formula_69">? Sample m ? latent noise vectors {z ? } m ? ?=1</formula><p>: O(m ? z) with z being assumed to be much less than n ? Forward pass to generate w i from the generator represented by the implicit distribution q(w i ; ? i ): O(m ? n ) ? Sample w i from the prior p(w): O(m ? n) ? Back-propagate to train the ? network: O(2m ? n ) This process is repeated N ? times, resulting in a total complexity of O(3N ? m ? n ) to train ?-network.</p><p>Given the trained ?-network for a particular task, the second step is to adapt the meta-parameter to the task-specific parameter:</p><p>? Generate w i from the generator: O(n ) ? Back-propagation to train the task-specific generator: O(m To ease the analysis, we specify the running time complexity of the three methods in <ref type="table" target="#tab_3">Table 3</ref>.  To make it even easier to compare, we also add a "practical" setting into <ref type="table" target="#tab_3">Table 3</ref>. This setting is the one done in our experiments with the following hyper-parameter values: We note that despite the difference of the running time complexity, in the implementation using GPU, some operations, such as matrix multiplication, are implemented efficiently in parallel or vectorisation. Hence, the difference of running time in practice might not be the same as the one analysed in this appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Complexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General</head><formula xml:id="formula_70">? N lower = 5 ? N ? = 1 ? m<label>(</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>meta-learning, are unknown, and only two datasets with finite examples, S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>i</head><label></label><figDesc>are not necessarily identical, and the label y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>hyper-meta-parameter ? = ? ? 3:initialise ?-network meta-parameter ? 0 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The sinusoidal function used in this experiment is in the form y = A sin(x + ?) + , where A and ? are uniformly sampled from [0.1, 5] and [0, ?], respectively, while the linear function considered is in the form y = ax + b + , where a and b are randomly sampled from [?3, 3]. The noise is sampled from N (0, 0.3 2 ). The experiment is carried out under the 5-shot setting: m (t) i = 5, and the validation set S (v) i consists of m (v) i = 50 data points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>SImPa and MAML are compared in a regression problem when training is based on multi-modal data -half of the tasks are generated from sinusoidal functions, and the other half are from linear functions. The shaded area is the prediction made by SImPa ? 3? standard deviation. Quantitative comparison between various probabilistic meta-learning approaches averaged over 1000 unseen tasks shows that SImPa has a comparable MSE error and the smallest calibration error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 :</head><label>4</label><figDesc>Calibration of the "standard" 4-block CNN trained with different meta-learning methods on 5-way 1-shot classification tasks on mini-ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>base model parameters = |w i | n the number of the hyper-parameters = |? ? | = |?| N lower the number of gradient updates to minimise lower-level function in (3) N ? the number of gradient updates to learn the task-specific ?-net mw the number of Monte Carlo samples drawn from task-specific posterior q(w i ; ? i ) m ? the number of Monte Carlo samples drawn from hyper-meta posterior q(?; ?) m (t) 0 the number of samples in the training subset of each task m (v) 0the number of samples in the validation subset of each task m ? the number of samples generated to train the ? network in SImPa T the number of tasks within a mini-batch to update the meta-parameter of interest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>And since there are T tasks in total, the complexity of this step is O(m (v) 0 T n). The total complexity of the whole algorithm is: O((N lower m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>)n). The total running time complexity is, therefore, O(m w (N lower (m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>This is repeated m w times, resulting in a complexity of O(m w (m (t) 0 + 1)n . And again, this whole process of training both the ?-network and the generator is repeated N lower times. Thus, the totalcomplexity is O(N lower (m w (m (t) 0 + 1) + 3N ? m ? )n ).C.3.2 Upper-level optimisationGiven the task-specific generator obtained in the optimisation of the lower-level in(3), the gradient of the hyper-metaparameter in the upper-level can be calculated in 2 steps:? Sample ? from q(?; ?): O(n ) ? Backward to calculate the gradient: O(m (v) 0 n ) This is done for T tasks, resulting in a complexity of O(T (m (v) 0 + 1)n ) per a single ?. The optimisation in both the lower-and upper-levels is then repeated m ? times for m ? samples of ?. Thus, the total complexity of SImPa is: O(m ? (N lower (m w (m (t) 0 + 1) + 3N ? m ? ) + T (m (v) 0 + 1))n ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>? (N lower (mw(m (t) 0 + 1) + 3N ? m ? ) + T (m (v) 0 + 1))n ) O(125n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>? m ? = m w = 1 for SImPa and m w = 4 for probabilistic methods that are based on multivariate normal distributions with diagonal covariance matrices ? T = 5 ? N ? = 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>The few-shot 5-way classification accuracy results</cell></row><row><cell>(in percentage, with 95% confidence interval) of SImPa</cell></row><row><cell>averaged over 1 million tasks on Omniglot (top), and 600</cell></row><row><cell>tasks on mini-ImageNet (middle-top and middle-bottom)</cell></row><row><cell>and tiered-ImageNet (bottom) datasets. For each experi-</cell></row><row><cell>ment, we select the top two methods with highest mean</cell></row><row><cell>values, and apply t-test with 95 percent confidence. If they</cell></row><row><cell>are significantly different, we highlight the method with</cell></row><row><cell>largest mean in bold, otherwise, we highlight both of them.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Notations used in the running time complexity analysis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Running time complexity per one gradient update of different meta-learning methods.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. assume the sampling of N ? samples is parallel in GPU</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PAC-Bayes upper-bound for unseen tasks</head><p>The PAC-Bayes upper-bound on the generalisation loss for unseen tasks can be obtained as a corollary of Theorem 1 (presented in subsection <ref type="bibr">3.4)</ref>. In particular, if:</p><p>ij ; w i , ? data generation is the task environment p(D, f ), ? dataset consists of T &gt; 1 tasks i.i.d. sampled from the task environment, ? hypothesis is ?, ? posterior is q(?; ?), ? and prior is p(?), then one can apply Theorem 1 to obtain a PAC-Bayes upper-bound for unseen tasks as shown in Corollary 1.</p><p>where: ? 0 ? (0, 1], and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 PAC-Bayes upper-bound for meta-learning</head><p>Theorem 2. Given T tasks sampled from the same task environment p(D, f ), where each task has an associated dataset S i with samples generated from the task-specific data generation model (D i , f i ), then for a bounded loss function : W ? Y ? [0, 1] and any distributions q(?; ?) of meta-parameter ? and q(w i ; ? i ) of task-specific parameter w i , the following holds with the probability at least 1 ? ?, ?? ? (0, 1]:</p><p>where p(w i ), ?i ? {1, . . . , T } is the prior of task-specific parameter w i and p(?) is the prior of meta-parameter ?.</p><p>Proof. First, the upper-bound for the unseen examples of a single-task obtained from Lemma 2 (presented in Appendix A.1) is extended for T training tasks by using Lemma 6 (presented in Appendix B) with the following substitution:</p><p>ik ; w i + R i to obtain:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael T Rosenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 7, 8)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recnorm: Simultaneous normalisation and classification applied to speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evolutionary principles in selfreferential learning (On learning how to learn: the meta-meta-... hook)&quot;. Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technische Universit?t M?nchen</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 3, 7, 8)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 3, 8)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 3-6, 8, 19</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MetaGAN: An Adversarial Approach to Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2371" to="2380" />
		</imprint>
	</monogr>
	<note>cit. on p. 1)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 6, 8)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recasting Gradient-Based Meta-Learning as Hierarchical Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 8)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Amortized Bayesian Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Beatson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>cit. on pp. 1-4, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-Learning Probabilistic Inference for Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cit. on pp. 1, 2, 8)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probabilistic Model-Agnostic Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9537" to="9548" />
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 3, 6, 8, 9</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian Model-Agnostic Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7343" to="7353" />
		</imprint>
	</monogr>
	<note>cit. on pp. 2, 6, 8)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A PAC-Bayesian bound for lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Pentina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How Important is the Train-Validation Split in Meta-Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<idno>PMLR. 2021</idno>
	</analytic>
	<monogr>
		<title level="m">In: International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regret bounds for lifelong learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alquier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="269" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Algorithmic Stability and Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The benefit of multitask representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to learn around a common mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Denevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning-to-learn stochastic gradient descent with biased regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Denevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1566" to="1575" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online-within-online metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Denevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Advantage of Conditional Meta-Learning for Biased Regularization and Fine Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Denevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Ciliberto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adaptive Gradient-Based Meta-Learning Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5917" to="5928" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Meta-Learning in Neural Networks: A Survey</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A model of inductive bias learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="198" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Uncertainty in model-agnostic metalearning using variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision. 2020</title>
		<imprint>
			<biblScope unit="page" from="3090" to="3100" />
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 4, 19</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
	<note>cit. on pp. 3, 7, 8)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PAC-Bayesian model averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Statistical learning theory and stochastic optimization: Ecole d&apos;Et? de Probabilit?s de Saint-Flour XXXI-2001</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Catoni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PAC-Bayesian theory meets Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1884" to="1892" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the properties of variational approximations of Gibbs posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alquier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chopin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="8374" to="8414" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simpler PAC-Bayesian bounds for hostile data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alquier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Guedj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="887" to="902" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monte Carlo methods of inference for implicit statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard J</forename><surname>Diggle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gratton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="193" to="212" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
	<note>cit. on p. 4)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On Bayesian bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
	<note>cit. on pp. 5, 16</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distribution calibration for regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5897" to="5906" />
		</imprint>
	</monogr>
	<note>cit. on p. 6</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On Calibration of Modern Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cit. on pp. 7, 8)</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cit. on pp. 7, 8)</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cit</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to Compare: Relation Network for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A simple neural attentive metalearner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Rapid adaptation with conditionally shifted neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3661" to="3670" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boris N Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaiyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3825" to="3834" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
	<note>cit. on p. 8)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2001" />
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Setting ? 0 = ? T and ? i = (T ?1)? T 2 , ?i ? {1</title>
		<imprint/>
	</monogr>
	<note>T } completes the proof</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
