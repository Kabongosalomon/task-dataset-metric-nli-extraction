<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PYANNOTE.AUDIO: NEURAL BUILDING BLOCKS FOR SPEAKER DIARIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Manuel</forename><surname>Coria</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gelly</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Korshunov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Lavechin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Fustes</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadrien</forename><surname>Titeux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wassim</forename><surname>Bouaziz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Philippe</forename><surname>Gill</surname></persName>
						</author>
						<title level="a" type="main">PYANNOTE.AUDIO: NEURAL BUILDING BLOCKS FOR SPEAKER DIARIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/pyannote/pyannote-audio</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speaker diarization</term>
					<term>voice activity detec- tion</term>
					<term>speaker change detection</term>
					<term>overlapped speech detection</term>
					<term>speaker embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce pyannote.audio, an open-source toolkit written in Python for speaker diarization. Based on PyTorch machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines. pyannote.audio also comes with pre-trained models covering a wide range of domains for voice activity detection, speaker change detection, overlapped speech detection, and speaker embedding -reaching state-of-the-art performance for most of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speaker diarization is the task of partitioning an audio stream into homogeneous temporal segments according to the identity of the speaker. As depicted in <ref type="figure">Figure 1</ref>, this is usually addressed by putting together a collection of building blocks, each tackling a specific task (e.g. voice activity detection, clustering, or re-segmentation).</p><p>In this paper, we introduce pyannote.audio, an opensource toolkit written in Python and based on PyTorch machine learning framework, that provides end-to-end neural implementations for each of them. A few open-source toolkits already exist that also address speaker diarization. Each one of them have its own pros and cons and we encourage the reader to build their own opinion by trying them: S4D (SIDEKIT for diarization <ref type="bibr" target="#b4">[5]</ref>) "provides an educational and efficient toolkit for speaker diarization including the whole chain of treatment". Among all pyannote.audio alternatives, it is the most similar: written in Python, it provides most of the afore-This research was partly funded by the French National Research Agency (ANR) through the ODESSA (ANR-15-CE39-0010) and PLUM-COT (ANR-16-CE92-0025) projects. We would like to thank Claude Barras for providing the overlapped speech detection output corresponding to system L 1 in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b0">[1]</ref>, Neville Ryant for the speaker diarization output of the winning submission to DIHARD 2019 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, Marie Kune?ov? for the overlapped speech detection output corresponding to system "AMI test (all subsets) + dereverberation" in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b3">[4]</ref>, and Sylvain Meignier for the speaker diarization output of <ref type="bibr" target="#b4">[5]</ref> on ETAPE dataset. mentioned blocks, and goes all the way down to the actual evaluation of the system. However, it differs from pyannote.audio in its focus on traditional approaches (i.e. before the major shift towards deep learning) and the lack of joint optimization of the pipeline;</p><p>Kaldi provides a few speaker diarization recipes but is not written in Python and is mostly dedicated to building speech and speaker recognition systems <ref type="bibr" target="#b5">[6]</ref>;</p><p>ALIZ? and its LIA SpkSeg extension for speaker diarization are written in C++ and do not provide recent deep learning approaches for speaker diarization <ref type="bibr" target="#b6">[7]</ref>; pyAudioAnalysis is written in Python and addresses more general audio signal analysis, though it can be used for speaker diarization <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEATURE EXTRACTION WITH BUILT-IN DATA AUGMENTATION</head><p>While pyannote.audio supports training models from the waveform directly (e.g. using SincNet learnable features <ref type="bibr" target="#b8">[9]</ref>), the pyannote.audio.features module provides a collection of standard feature extraction techniques such as MFCCs or spectrograms using the implementation available in the librosa library <ref type="bibr" target="#b9">[10]</ref>. They all inherit from the same FeatureExtraction base class that supports on-the-fly data augmentation which is very convenient for training neural networks. For instance, it supports extracting features from random audio chunks while applying additive noise from databases such as MUSAN <ref type="bibr" target="#b11">[11]</ref>. Contrary to other tools that generate in advance a fixed number of augmented versions of each original audio file, pyannote.audio generates a virtually infinite number of versions as the augmentation is done on-the-fly every time an audio chunk is processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEQUENCE LABELING</head><p>pyannote.audio.labeling provides a unified framework to train (usually recurrent) neural networks for several speaker diarization sub-modules, including voice activity detection <ref type="bibr" target="#b12">[12]</ref>, speaker change detection <ref type="bibr" target="#b13">[13]</ref>, overlapped speech detection <ref type="bibr" target="#b14">[14]</ref>, and even re-segmentation <ref type="bibr" target="#b15">[15]</ref>.  <ref type="figure">Fig. 1</ref>. pyannote.audio provides a collection of modules that can be jointly optimized to build a speaker diarization pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Principle</head><p>Each of them can be addressed as a sequence labeling task where the input is the sequence of feature vectors X = {x 1 , x 2 , . . . , x T } and the expected output is the corresponding sequence of labels y = {y 1 , y 2 , . . . , y T } with y t ? 1; K where the number of classes K depends on the task. pyannote.audio provides generic code to train a neural network f : X ? y that matches a feature sequence X to the corresponding label sequence y. The choice of the actual neural network architecture is left to the user, though pyannote.audio does provide pre-trained PyTorch models sharing the same generic PyanNet base architecture summarized in <ref type="figure">Figure 2</ref>.</p><p>Because processing long audio files of variable lengths is neither practical nor efficient, pyannote.audio relies on shorter fixed-length sub-sequences. At training time, fixedlength sub-sequences are drawn randomly from the training set to form mini-batches, increasing training samples variability (data augmentation) and training time (shorter sequences). At test time, audio files are processed using overlapping sliding windows of the same length as used in training. For each time step t, this results in several overlapping sequences of K-dimensional prediction scores, which are averaged to obtain the final score of each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Voice activity detection</head><p>Voice activity detection is the task of detecting speech regions in a given audio stream or recording. It can be addressed in pyannote.audio using the above principle with K = 2: y t = 0 if there is no speech at time step t and y t = 1 if there is. At test time, time steps with prediction scores greater than a tunable threshold ? VAD are marked as speech. Overall, this essentially implements a simplified version of the voice activity detector originally described in <ref type="bibr" target="#b12">[12]</ref>. Pre-trained models are available, reaching state-of-the-art performance on a range of datasets, as reported in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Speaker change detection</head><p>Speaker change detection is the task of detecting speaker change points in a given audio stream or recording. It can be addressed in pyannote.audio using the same sequence labeling principle with K = 2: y t = 0 if there is no speaker change at time step t and y t = 1 if there is. To address the class imbalance problem and account for human annotation imprecision, time steps {t | |t ? t * | &lt; ?} in the close temporal neighborhood of a speaker change point t * are artificially labeled as positive for training. In practice, the order of magnitude of ? is 200ms. At test time, time steps corresponding to prediction scores local maxima and greater than a tunable threshold ? SCD are marked as speaker change points. Overall, this essentially implements a version of the speaker change detector originally described in <ref type="bibr" target="#b13">[13]</ref>. Pre-trained models are available, reaching state-of-the-art performance on a range of datasets, as reported in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overlapped speech detection</head><p>Overlapped speech detection is the task of detecting regions where at least two speakers are speaking at the same time. It is addressed in pyannote.audio using the same sequence labeling principle with K = 2: y t = 0 if there is zero or one speaker at time step t and y t = 1 if there are two speakers or more. To address the class imbalance problem, half of the training sub-sequences are artificially made of the weighted sum of two random sub-sequences, as depicted in <ref type="figure" target="#fig_1">Figure 3</ref>. At test time, time steps with prediction scores greater than a tunable threshold ? OSD are marked as overlapped speech. Pretrained models are available, reaching state-of-the-art performance on a range of datasets, as reported in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Re-segmentation</head><p>Re-segmentation is the task of refining speech turns boundaries and labels coming out of a diarization pipeline. Though it is an unsupervised task, it is addressed in pyannote.audio using the same sequence labeling principle with K = ? + 1 where ? is the number of speakers hypothesized by the diarization pipeline: y t = 0 if no speaker is active at time step t and y t = k if speaker k ? 1; ? is active. Because the re-segmentation step is unsupervised by design, one cannot pre-train re-segmentation models. For each audio file, a new re-segmentation model is trained from scratch using the (automatic, hence imperfect) output of a diarization pipeline as training labels. Once trained for a number of epochs (one epoch being one complete pass on the file), the model is applied on the very same file it was trained from -making this approach completely unsupervised. Each time step is assigned to the class (non-speech or one of the ? speakers)  with highest prediction scores. This essentially implements a version of the re-segmentation approach originally described in <ref type="bibr" target="#b15">[15]</ref> where it was found that = 20 is a reasonable number of epochs. This re-segmentation step may be extended to also assign the class with the second highest prediction score to overlapped speech regions <ref type="bibr" target="#b14">[14]</ref>. As reported in <ref type="table">Table 5</ref>, this may lead to significant performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SEQUENCE EMBEDDING AND CLUSTERING</head><p>Like voice activity detection, clustering is one of the most important part of any speaker diarization pipeline. It consists in grouping speech segments according to the actual identity of the speaker. As of October 2019, most best performing speaker diarization systems rely on x-vectors as input to the clustering step. They are usually extracted from a fixed-length sliding window, and the pairwise similarity matrix is obtained with probabilistic linear discriminant analysis (PLDA). There exist plenty of open-source implementations of x-vectors and PLDA already <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, and pyannote.audio does not provide yet another version of those approaches.</p><p>Instead, the clustering step is simplified by using metric learning approaches to train speaker embedding that are di-rectly optimized for a predefined (usually cosine) distance, reducing the need for techniques like PLDA 1 . As reported in <ref type="table" target="#tab_2">Table 4</ref>, a growing collection of metric learning approaches are implemented in pyannote.audio.embedding that provides a unified framework for this family of approaches.</p><p>Like other blocks, speaker embeddings can be trained either from handcrafted features, or from the waveform directly in an end-to-end manner. Though pre-trained end-to-end models do not reach state-of-the-art performance on Vox-Celeb speaker verification task, <ref type="table">Table 5</ref> shows that they do lead to state-of-the-art performance for some speaker diarization test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">TUNABLE PIPELINES</head><p>While each building block has to be trained separately, pyannote.audio.pipeline combines them into a speaker diarization pipeline whose hyper-parameters are optimized jointly to minimize the diarization error rate (or any other metric available in pyannote.metrics <ref type="bibr" target="#b17">[17]</ref>). As discussed in <ref type="bibr" target="#b15">[15]</ref>, this joint optimization process usually leads to better results than the late combination of multiple building blocks that were tuned independently from each other. <ref type="table">Table 5</ref> reports the results of the pipeline we introduced in <ref type="bibr" target="#b15">[15]</ref> for which a bunch of hyper-parameters where jointly optimized, including ? VAD for voice activity detection and ? SCD for speaker change detection. We also report the improvement brought by the integration of the overlapped speech detection in the re-segmentation step (introduced in Section 3.5 and further described in <ref type="bibr" target="#b14">[14]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REPRODUCIBLE RESULTS</head><p>pyannote.audio provides a set of command line tools for training, validation, and application of modules listed in <ref type="bibr" target="#b0">1</ref> x-vector aficionados would still suggest to use PLDA anyway...  <ref type="table">Table 3</ref>. Evaluation of pre-trained overlapped speech detection models, in terms of precision (%) and recall (%). Results on the development set are reported using small font size. We report two pyannote.audio variants: the first one is based on handcrafted features (MFCCs) and the other one is an end-to-end model processing the waveform directly. Baseline corresponds to the best result we could find in the literature as of October 2019.  <ref type="bibr" target="#b14">[14]</ref> 24.8 24.7 <ref type="table">Table 5</ref>. Evaluation of speaker diarization pipelines in terms of diarization error rate (%). Baseline corresponds to the best result we could find in the literature as of October 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMI</head><p>Since <ref type="bibr" target="#b18">[18]</ref> only reports confusion error rates (assuming oracle speech activity detection, omitting overlapped speech regions, and using a collar), those are marked as crossed-out $ $ $ $ numbers. <ref type="figure">Figure 1</ref>. Reproducible research is facilitated by the systematic use of pyannote.metrics <ref type="bibr" target="#b17">[17]</ref> and configuration files, while strict enforcement of train/dev/eval split with pyannote.database ensures machine learning good practices. It also comes with a collection of pre-trained models whose performance has already been reported throughout the paper in <ref type="table" target="#tab_0">Table 1</ref> for voice activity detection, <ref type="table" target="#tab_1">Table 2</ref> for speaker change detection, <ref type="table">Table 3</ref> for overlapped speech de-tection, and <ref type="table" target="#tab_2">Table 4</ref> for speaker embedding. While speaker embeddings were trained and tested on VoxCeleb <ref type="bibr" target="#b19">[19]</ref>, all other models (including the full diarization pipeline) were trained, tuned, and tested on three different datasets, covering a wide range of domains: meetings for AMI <ref type="bibr" target="#b20">[20]</ref>, broadcast news for ETAPE <ref type="bibr" target="#b21">[21]</ref>, and up to 11 different domains for DIHARD <ref type="bibr" target="#b2">[3]</ref>.</p><p>They all rely on the same generic PyanNet architecture which is depicted in <ref type="figure">Figure 2</ref>. Sequence labeling tasks were trained on 2s audio chunks, either with handcrafted MFCC features (19 coefficients extracted every 10ms on 25ms windows, with first-and second-order derivatives) or with trainable SincNet features (using the configuration of the original paper <ref type="bibr" target="#b8">[9]</ref>). The end-to-end variant consistently outperforms the one based on handcrafted features for all tasks but speaker embedding (for which we have yet to train the handcrafted features variant), defining a new state-of-the-art performance for most cases.</p><p>The rest of the network includes two stacked bi-directional LSTM recurrent layers (each with 128 units in both forward and backward directions), no temporal pooling, two feedforward layers (128 units, tanh activation) and a final classification layer (2 units, softmax activation). Speaker embedding uses a slightly wider (512 units instead of 128) and deeper (3 recurrent layers instead of 2) network, relies on x-vectorlike statistical temporal pooling <ref type="bibr" target="#b16">[16]</ref> and has been trained on shorter 500ms audio chunks. More details on the training process (number of epochs, optimizer, etc.) can be found directly in the associated configuration files, available from pyannote.audio repository.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>To increase the number of positive training samples for overlapped speech detection, pyannote.audio creates artificial samples by summing two random audio chunks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Fig. 2. Generic PyanNet end-to-end architecture used for sequence labeling (without pooling) and embedding (with pooling). Evaluation of pre-trained voice activity detection models, in terms of detection error (DetER %), false alarm (FA %), and missed detection (Miss %) rates. Results on the development set are reported using small font size. We report two pyannote.audio variants: the first one is based on handcrafted features (MFCCs) and the other one is an end-to-end model processing the waveform directly. Baseline corresponds to the best result we could find in the literature as of October 2019.</figDesc><table><row><cell></cell><cell></cell><cell cols="6">SincConv feature extraction convolutional layers</cell><cell>recurrent layers</cell><cell>temporal pooling</cell><cell>feed-forward layers</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AMI</cell><cell></cell><cell></cell><cell>DIHARD</cell><cell>ETAPE</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DetER</cell><cell></cell><cell cols="2">FA</cell><cell>Miss</cell><cell>DetER</cell><cell>FA</cell><cell>Miss DetER</cell><cell>FA</cell><cell>Miss</cell></row><row><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.2 [2, 3]</cell><cell>6.5</cell><cell>4.7 7.7 [5]</cell><cell>7.5</cell><cell>0.2</cell></row><row><cell cols="3">pyannote (MFCC) [12]</cell><cell cols="5">6.3 5.5 3.5 3.1 2.7 2.4</cell><cell>10.5 10.0 6.8 5.4 3.7 4.6</cell><cell>5.6 4.2 5.2 3.6 0.4 0.6</cell></row><row><cell cols="3">pyannote (waveform)</cell><cell cols="5">6.0 5.8 3.6 3.4 2.4 2.4</cell><cell>9.9 9.3 5.7 3.7 4.2 5.6</cell><cell>4.9 3.7 4.2 2.9 0.7 0.8</cell></row><row><cell>audio</cell><cell></cell><cell>+</cell><cell>=</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>speaker</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>A</cell><cell>C</cell><cell>B</cell><cell></cell></row><row><cell>y overlap</cell><cell cols="6">000000000 000000000 000110110</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of pre-trained speaker change detection models, in terms of speech turn coverage (%) and purity (%). Results on the development set are reported using small font size. We report two pyannote.audio variants: the first one is based on handcrafted features (MFCCs) and the other one is an end-to-end model processing the waveform directly. Baseline corresponds to the best result we could find in the literature as of October 2019.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">DIHARD</cell><cell>ETAPE</cell></row><row><cell></cell><cell cols="2">Purity Coverage</cell><cell cols="2">Purity Coverage</cell><cell>Purity Coverage</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>91.0 [13]</cell><cell>90.9 [13]</cell></row><row><cell>pyannote (MFCC)</cell><cell>89.4 90.0</cell><cell cols="2">78.7 75.2 92.4 90.0</cell><cell cols="2">74.5 76.6 90.1 90.0</cell><cell>95.9 95.7</cell></row><row><cell cols="2">pyannote (waveform) 90.4 90.0</cell><cell cols="2">84.2 83.5 86.8 84.5</cell><cell cols="2">93.7 93.4 89.3 90.0</cell><cell>97.2 98.2</cell></row><row><cell></cell><cell cols="2">AMI</cell><cell cols="2">DIHARD</cell><cell>ETAPE</cell></row><row><cell></cell><cell>Precision</cell><cell cols="2">Recall Precision</cell><cell cols="2">Recall Precision</cell><cell>Recall</cell></row><row><cell>Baseline</cell><cell cols="2">75.8 80.5 [4] 44.6 50.2 [4]</cell><cell></cell><cell></cell><cell>60.3 [1] 52.7 [1]</cell></row><row><cell>pyannote (MFCC)</cell><cell>91.9 90.0</cell><cell>48.4 52.5</cell><cell cols="2">58.0 73.8 17.6 14.0</cell><cell>67.1 55.0 57.3 55.3</cell></row><row><cell>pyannote (waveform)</cell><cell>86.8 90.0</cell><cell>65.8 63.8</cell><cell cols="2">64.5 75.3 26.7 24.4</cell><cell>69.6 60.0 61.7 63.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Evaluation of speaker embedding on VoxCeleb 1 speaker verification task, in terms of equal error rate (%). WIP indicates that training is still ongoing.</figDesc><table><row><cell>Baseline</cell><cell></cell><cell>3.1 [16]</cell></row><row><cell>pyannote triplet loss</cell><cell></cell><cell>7.0 WIP</cell></row><row><cell cols="3">pyannote additive angular margin loss 10.0 WIP</cell></row><row><cell>pyannote constrastive loss</cell><cell></cell><cell>17.4 WIP</cell></row><row><cell>pyannote center loss</cell><cell></cell><cell>16.6 WIP</cell></row><row><cell cols="2">pyannote congenerous cosine loss</cell><cell>18.4 WIP</cell></row><row><cell>AMI</cell><cell cols="2">DIHARD ETAPE</cell></row><row><cell>Baseline8.4 [18]</cell><cell></cell><cell>27.1 [2] 24.5 [5]</cell></row><row><cell>pyannote [15]4.6 29.6 29.7</cell><cell></cell><cell>34.4 31.5 24.0 18.3</cell></row><row><cell>... with overlap</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Impact of overlapping speech detection on speaker diarization for broadcast news and debates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Charlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Barras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sylvain</forename><surname>Li?nard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2013</title>
		<meeting>ICASSP 2013</meeting>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian HMM Based x-Vector Clustering for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk?</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Rohdin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ernock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Second DIHARD Diarization Challenge: Dataset, Task, and Baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="978" to="982" />
		</imprint>
	</monogr>
	<note>Alejandrina Cristia</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detection of Overlapping Speech for the Purposes of Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Kune?ov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Hr?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbyn?k</forename><surname>Zaj?c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlasta</forename><surname>Radov?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">S4D: Speaker Diarization Toolkit in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Alexandre</forename><surname>Broux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Desnous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Larcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Petitrenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carrive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Meignier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Catalog</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aliz?, a free Toolkit for Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Bonaster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wils</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Meignier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">pyaudioanalysis: An open-source python library for audio signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Giannakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with sincnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SLT 2018</title>
		<meeting>SLT 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Metsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Nieto ; Ryuichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunwoo</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pius</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian-Robert</forename><surname>Friesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stter</surname></persName>
		</author>
		<imprint>
			<pubPlace>Jack Mason, Frank Zalkow, Dan Ellis, Eric Battenberg, ,; Daro Here, Thassilo, Taewoon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vollrath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carr</surname></persName>
		</author>
		<idno>librosa/librosa: 0.7.0</idno>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MU-SAN: A Music, Speech, and Noise Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimization of RNN-Based Speech Activity Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2018-03" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="646" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Barras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Overlap-Aware Resegmentation for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Latan?</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leibny Paola</forename><surname>Garcia-Perera</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Submitted to ICASSP 2020</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Speech Turn Segmentation and Affinity Propagation for Speaker Diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Barras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1393" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">X-Vectors: Robust DNN Embeddings for Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2018</title>
		<meeting>ICASSP 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">metrics: a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>Bredin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterizing Performance of Speaker Diarization Systems on Far-Field Speech Using Standard Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP 2018</title>
		<meeting>ICASSP 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voxceleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. InterSpeech</title>
		<meeting>InterSpeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ETAPE Corpus for the Evaluation of Speech-based TV Content Processing in the French Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Carr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Giraudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Galibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC 2012</title>
		<meeting>LREC 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
