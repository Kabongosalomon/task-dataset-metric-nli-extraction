<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating the accurate depth from a single image is challenging since it is inherently ambiguous and ill-posed. While recent works design increasingly complicated and powerful networks to directly regress the depth map, we take the path of CRFs optimization. Due to the expensive computation, CRFs are usually performed between neighborhoods rather than the whole graph. To leverage the potential of fully-connected CRFs, we split the input into windows and perform the FC-CRFs optimization within each window, which reduces the computation complexity and makes FC-CRFs feasible. To better capture the relationships between nodes in the graph, we exploit the multi-head attention mechanism to compute a multi-head potential function, which is fed to the networks to output an optimized depth map. Then we build a bottom-up-top-down structure, where this neural window FC-CRFs module serves as the decoder, and a vision transformer serves as the encoder. The experiments demonstrate that our method significantly improves the performance across all metrics on both the KITTI and NYUv2 datasets, compared to previous methods. Furthermore, the proposed method can be directly applied to panorama images and outperforms all previous panorama methods on the MatterPort3D dataset. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth prediction is a classical task in computer vision and is essential for numerous applications such as 3D reconstruction, autonomous driving, and robotics <ref type="bibr">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>. Such a vision task aims to estimate the depth map from a single color image, which is an ill-posed and inherently ambiguous problem since infinitely many 3D scenes can be projected to the same 2D scene. Therefore, this task is challenging for traditional methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b29">30]</ref>, which are usually limited to low-dimension and sparse distances <ref type="bibr" target="#b21">[22]</ref>, or known and fixed objects <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recently, many works have employed the deep networks to directly regress the depth maps and achieved good performances <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Nevertheless, since there are <ref type="figure">Figure 1</ref>. The neural window fully-connected CRFs take image feature F and upper-level prediction X as input, and compute the fully-connected energy E in each window, which is then fed to the networks to output an optimized depth map. no geometric constraints of multi-view <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> to exploit, the focus of most works is designing more powerful and more complicated networks. This renders this task a difficult fitting problem without the help of other guidance.</p><p>In traditional monocular depth estimation, some methods build the energy function from Markov Random Fields (MRFs) or Conditional Random Fields (CRFs) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. They exploit the observation cues, such as the texture and position information, along with the last prediction to build the energy function, and then optimize this energy to obtain a depth prediction. This approach is demonstrated to be effective in guiding the estimation of the depth, and is also introduced in some deep methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. However, they are all limited in neighbor CRFs rather than fullyconnected CRFs (FC-CRFs) due to the expensive computation, while the fully-connected CRFs capture the relationship between any node in a graph and are much stronger.</p><p>To address the above challenge, in this work we segment the input to multiple windows, and build the fullyconnected CRFs energy within each window, in which way the computation complexity is reduced considerably and the fully-connected CRFs becomes feasible. To capture more relationships between the nodes in the graph, we exploit the multi-head attention mechanism <ref type="bibr" target="#b34">[35]</ref> to compute the pairwise potential of the CRFs, and build a new neural CRFs module, as is shown in <ref type="figure">Figure 1</ref>. By employing this neural window FC-CRFs module as decoder, and a vision transformer as encoder, we build a straightforward bottom-uptop-down network to estimate the depth. To make up for the isolation of each window, a window shift action <ref type="bibr" target="#b20">[21]</ref> is performed, and the lack of global information in these window FC-CRFs is addressed by aggregating the global features from global average pooling layers <ref type="bibr" target="#b44">[45]</ref>.</p><p>In the experiments, our method is demonstrated to outperform previous methods by a significant margin on both the outdoor dataset, KITTI <ref type="bibr">[8]</ref>, and the indoor dataset, NYUv2 <ref type="bibr" target="#b31">[32]</ref>. Although the state-of-the-art performance on KITTI and NYUv2 has been saturated for a while, our method further reduces the errors considerably on both datasets. Specifically, the Abs-Rel error and the RMS error of KITTI are decreased by 10.3% and 9.8%, and that of NYUv2 are decreased by 7.8% and 8.2%. Our method now ranks first among all submissions on the KITTI online benchmark. In addition, we evaluate our method on the panorama images. As is well-known, the networks designed for perspective images usually perform poorly on the panorama dataset <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. Remarkably, our method also sets a new state-of-the-art performance on the panorama dataset, MatterPort3D <ref type="bibr">[3]</ref>. This demonstrates that our method can handle the common scenarios in the monocular depth prediction task.</p><p>The main contributions of this work are then summarized as follows:</p><p>? We split the input image into sub-windows and perform fully-connected CRFs optimization within each window, which reduces the high computation complexity and makes the FC-CRFs feasible.</p><p>? We employ the multi-head attention to capture the pairwise relationships in the window FC-CRFs, and embed this neural CRFs module in a network to serve as the decoder.</p><p>? We build a new bottom-up-top-down network for monocular depth estimation and show a significant improvement of the monocular depth across all metrics on KITTI, NYUv2, and MatterPort3D datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional Monocular Depth Estimation</head><p>Prior to the emergence of deep learning, monocular depth estimation is a challenging task. Many published works limit themselves in either estimating 1-D distances of obstacles <ref type="bibr" target="#b21">[22]</ref> or limited in several known and fixed objects <ref type="bibr" target="#b22">[23]</ref>. Then Saxena et al. <ref type="bibr" target="#b29">[30]</ref> claim that local features alone are insufficient to predict the depth of a pixel, and the global context of the whole image needs to be considered to infer the depth. Therefore, they use a discriminativelytrained Markov Random Field (MRF) to incorporate multiscale local and global image features, and model both depths at individual pixels as well as the relation between depths at different pixels. In this way, they infer good depth maps from the monocular cues like colors, pixel positions, occlusion, known object sizes, haze, defocus, etc. Since then, MRFs <ref type="bibr" target="#b30">[31]</ref> and CRFs <ref type="bibr" target="#b36">[37]</ref> have been well used in monocular depth estimation in traditional methods. However, the traditional approaches still suffer from estimating accurate high-resolution dense depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Neural Networks Based Monocular Depth</head><p>In monocular depth estimation, neural network based methods have dominated most benchmarks. There are mainly two kinds of approaches for learning the mapping from images to depth maps. The first approach directly regresses the continuous depth map from the aggregation of the information in an image <ref type="bibr">[1,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. In this approach, coarse and fine networks are first introduced in <ref type="bibr">[6]</ref> and then improved by multi-stage local planar guidance layers in <ref type="bibr" target="#b16">[17]</ref>. A bidirectional attention module is proposed in <ref type="bibr">[1]</ref> to utilize the feed-forward feature maps and incorporate the global context to filter out ambiguity. Recently, more methods have begun to employ vision transformers to aggregate the information of images <ref type="bibr" target="#b27">[28]</ref>. The second approach tries to discretize the depth space and convert the depth prediction to a classification or ordinal regression problem <ref type="bibr">[2,</ref><ref type="bibr">7]</ref>. A spacing-increasing quantization strategy is used in <ref type="bibr">[7]</ref> to discretize the depth space more reasonably. Then, an adaptive bins division is computed by the neural networks for better depth quantization. Also, other approaches bring in auxiliary information to help the training of the depth network, such as sparse depth <ref type="bibr" target="#b9">[10]</ref> or segmentation information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. All these approaches try to directly regress the depth map from the image feature, which falls into a difficult fitting problem. The structures of their networks become increasingly complex. In contrast to these works, we build an energy with the fully-connected CRFs, and then optimize this energy to obtain a high-quality depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural CRFs for Monocular Depth</head><p>Since the graph models, like MRFs and CRFs, are effective in traditional depth estimation, some methods try to embed them into neural networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. These methods regard the patches of pixels as nodes and perform the graph optimization. One such approach first uses networks to regress a coarse depth map and then utilizes CRFs to refine it <ref type="bibr" target="#b18">[19]</ref>, where the post-processing function of CRFs is proven to be effective. However, the CRFs are separated from neural networks. To better combine CRFs and networks, other methods integrate CRFs into the layers of the neural networks and train the whole framework endto-end <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38]</ref>. But they are all limited to CRFs rather than fully-connected CRFs due to the high computation complexity. <ref type="figure">Figure 2</ref>. Graph model of fully-connected CRFs and window fully-connected CRFs. In a fully-connected CRFs graph (a), taking the orange node as an example, it is connected to all other nodes in the graph. In a window fully-connected CRFs, however, the orange node is only connected to all other nodes within one window.</p><p>In this work, different from previous methods, we split the whole graph into multiple sub-windows, such that the fully-connected CRFs become feasible. Also, inspired by recent works in vision transformer <ref type="bibr">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>, we use the multi-head attention mechanism to capture the pairwise relationship in FC-CRFs and propose a neural window fullyconnected CRFs module. This module is embedded into the network to play the role of the decoder, such that the whole framework can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Window Fully-connected CRFs</head><p>This section first introduces the window fully-connected CRFs, followed by its integration with neural networks. Afterward, the network structure is displayed, where the neural window FC-CRFs module is embedded into a top-downbottom-up network to serve as the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully-connected Conditional Random Fields</head><p>In traditional methods, Markov random fields (MRFs) or conditional random fields (CRFs) are leveraged to handle dense prediction tasks such as monocular depth estimation <ref type="bibr" target="#b29">[30]</ref> and semantic segmentation <ref type="bibr">[4]</ref>. They are shown to be effective in correcting the erroneous predictions based on the information of the current and adjacent nodes. Specifically, in a graph model, these methods favor similar label assignments to nodes that are proximal in space and color.</p><p>Thus, in this work we employ CRFs to help the depth prediction. Since the depth prediction of the current pixel is determined by long-range pixels in one image, to increase the receptive field, we use fully-connected CRFs <ref type="bibr" target="#b15">[16]</ref> to build the energy. In a graph model, the energy function of the fully-connected CRFs is usually defined as</p><formula xml:id="formula_0">E(x) = i ? u (x i ) + ij ? p (x i , x j ),<label>(1)</label></formula><p>where x i is the predicted value of node i, and j denotes all other nodes in the graph. The unary potential function ? u is computed for each node by the predictor according to the image features. The pairwise potential function ? p connects pairs of nodes as</p><formula xml:id="formula_1">? p = ?(x i , x j )f (x i , x j )g(I i , I j )h(p i , p j ),<label>(2)</label></formula><p>where ?(x i , x j ) = 1 if i = j and ?(x i , x j ) = 0 otherwise, I i is the color of node i, p i is the position of node i. The pairwise potential usually considers the color and position information to enforce some heuristic punishments, which make the predicted values x i , x j more reasonable and logical.</p><p>In regular CRFs, the pairwise potential only computes the edge connection between the current node and neighboring nodes. In fully-connected CRFs, however, the connections between the current node and any other nodes in a graph need to be computed, as shown in <ref type="figure">Figure 2</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Window Fully-connected CRFs</head><p>Although the fully-connected CRFs can bring globalrange connections, its disadvantage is also obvious. On the one hand, the number of edges connecting all pixels in an image is large, which makes the computation of this kind of pairwise potential quite resource-consuming. On the other hand, the depth of a pixel is usually not determined by distant pixels. Only pixels within some distance need to be considered.</p><p>Therefore, in this work we propose the window-based fully-connected CRFs. We segment an image into multiple patch-based windows. Each window includes N ?N image patches, of which each patch is composed of n ? n pixels. In our graph model, each patch rather than each pixel is regarded as one node. All patches within one window are fully-connected with edges, while the patches of different windows are not connected, as displayed in <ref type="figure">Figure 2</ref> (b). In this case, the computation of pairwise potential only considers the patches within one window, so that the computation complexity is reduced significantly.</p><p>Taking an image with h ? w patches as an example, the computation complexity of FC-CRFs and window FC-CRFs for one iteration are</p><formula xml:id="formula_2">?(FC-CRFs) = hw ? ?(? u ) + hw(hw ? 1) ? ?(? p ) ?(Window FC) = hw ? ?(? u ) + hw(N 2 ? 1) ? ?(? p ),<label>(3)</label></formula><p>where N is the window size, ?(? u ) and ?(? p ) are the computation complexity of one unary potential and one pairwise potential, respectively.</p><p>In the window fully-connected CRFs, all windows are non-overlapped, which means there is no information connection between any windows. The adjacent windows, however, are physically connected. To resolve the isolation of windows, we shift the windows by ( N 2 , N 2 ) patches in the image and calculate the energy function of shifted windows after computing that of the original windows, similar to swin-transformer <ref type="bibr" target="#b20">[21]</ref>. In this way, the isolated neighboring pixels are connected in the shifted windows. Hence, each time we calculate the energy function, we calculate two energy functions successively, one for the original windows and the other one for the shifted windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Neural Window FC-CRFs</head><p>In traditional CRFs, the unary potential is usually acted by a distribution over the predicted values, e.g.,</p><formula xml:id="formula_3">? u (x i ) = ? log P (x i |I),<label>(4)</label></formula><p>where I is the input color image and P is the probability distribution of the value prediction. The pairwise potential is usually computed according to the colors and positions of pixel pairs, e.g.,</p><formula xml:id="formula_4">? p (x i , x j ) = ?(x i , x j )||x i ? x j ||e ? ||I i ?I j || 2? 2 e ? ||p i ?p j || 2? 2 .</formula><p>(5) This potential encourages distinct-color and distant pixels to have various value predictions while punishing the value discrepancies in similar-color and adjacent pixels.</p><p>These potential functions are designed by hands and cannot be too complicated. Thus they are hard to represent high-dimensional information and describe complex connections. So in this work, we propose to use neural networks to perform the potential functions.</p><p>For the unary potential, it is computed from the image features such that it can be directly obtained by the network as</p><formula xml:id="formula_5">? u (x i ) = ? u (I, x i ),<label>(6)</label></formula><p>where ? is the parameters of a unary network.</p><p>For the pairwise potential, we realize that it is composed of values of the current node and other nodes, and a weight computed based on the color and position information of the node pairs. So we reformulate it as</p><formula xml:id="formula_6">? p (x i , x j ) = w(F i , F j , p i , p j )||x i ? x j ||,<label>(7)</label></formula><p>where F is the feature map and w is the weighting function. We calculate the pairwise potential node by node. For each node i, we sum all its pairwise potentials and obtain</p><formula xml:id="formula_7">? pi = ?(F i , F j , p i , p j )x i + j =i ?(F i , F j , p i , p j )x j ,<label>(8)</label></formula><p>where ?, ? are the weighting functions and will be computed by the networks. Inspired by recent works in transformer <ref type="bibr">[5,</ref><ref type="bibr" target="#b34">35]</ref>, we calculate a query vector q and a key vector k from the feature map of each patch in a window and combine vectors of all patches to matrices Q and K. Then we calculate the dot product of matrices Q and K to get the potential weight between any pair, after which the predicted values X are multiplied by the weights to get the final pairwise potential. To introduce the position information, we also add a relative position embedding P . Therefore, the equation 8 can be calculated as</p><formula xml:id="formula_8">? pi = SoftMax(q ? K T + P ) ? X i ? pi = SoftMax(Q ? K T + P ) ? X,<label>(9)</label></formula><p>where ? denotes dot production. Thus, the output of the SoftMax gets the weights ? and ? of Equation <ref type="bibr">8</ref>. Therefore, the dot product between Q and K calculates the scores between each node with any other node, which determines the  message passing weights with P , while the dot product between previous prediction X and the output of the SoftMax performs the message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Structure</head><p>Overview. To embed the neural window fully-connected CRFs into a depth prediction network, we build a bottomup-top-down structure, where four levels of CRFs optimizations are performed, as is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. We embed this neural window FC-CRFs module into the network to act as a decoder, which predicts the next-level depth according to the coarse depth and image features. For the encoder, we employ the swin-transformer <ref type="bibr" target="#b20">[21]</ref> to extract the features.</p><p>For an image with the size of H ? W , there are four levels of image patches for the feature extraction encoder and the CRFs optimization decoder, from 4 ? 4 pixels to 32 ? 32 pixels. At each level, N ? N patches make up a window. The window size N is fixed at all levels, so there will be H 4N ? W 4N windows at the bottom level and H 32N ? W</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>32N</head><p>windows at the top level.</p><p>Global Information Aggregation. At the top level, to make up for the lack of global information of the window FC-CRFs, we use the pyramid pooling module (PPM) <ref type="bibr" target="#b44">[45]</ref> to aggregate the information of the whole image. Similar to <ref type="bibr" target="#b44">[45]</ref>, we use global averaging pooling of scales 1, 2, 3, 6 to extract the global information, which is then concatenated with the input feature to map to the top-level prediction X by a convolutional layer.</p><p>Neural Window FC-CRFs Module. In each neural window FC-CRFs block, there are two successive CRFs optimizations, one for regular windows and the other one for shifted windows. To cooperate with the transformer encoder, the window size N is set to 7, which means each window contains 7 ? 7 patches. The unary potential is com-puted by a convolutional network and the pairwise potential is computed according to equation 9. In each CRFs optimization, multiple-head Q and K are calculated to obtain multi-head potentials, which can enhance the relationship capturing ability of the energy function. From the top level to the bottom level, a structure of 32, 16, 8, 4 heads is adopted. Then the energy function is fed into an optimization network composed of two fully-connected layers to output the optimized depth map X .</p><p>Upscale Module. After the neural window FC-CRFs decoders at the top three levels, a shuffle operation is performed to rearrange the pixels, by which the image is up-</p><formula xml:id="formula_9">scaled from h 2 ? w 2 ? d to h ? w ? d 4 .</formula><p>On the one hand, this operation increases the flow to the next level with a larger scale without losing the sharpness like upsampling. On the other hand, this reduces the feature dimension to lighten the subsequent networks.</p><p>Training Loss. Following previous works <ref type="bibr">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, we use a Scale-Invariant Logarithmic (SILog) loss proposed by <ref type="bibr">[6]</ref> to supervise the training. Given the ground-truth depth map, we first calculate the logarithm difference between the predicted depth map and the real depth:</p><formula xml:id="formula_10">?d i = logd i ? log d * i ,<label>(10)</label></formula><p>where d * i is the ground-truth depth value andd i is the predicted depth at pixel i.</p><p>Then for K pixels with valid depth values in an image, the scale-invariant loss is computed as</p><formula xml:id="formula_11">L = ? 1 K i ?d 2 i ? ? K 2 ( i ?d i ) 2 ,<label>(11)</label></formula><p>where ? is a variance minimizing factor, and ? is a scale constant. In our experiments, ? is set to 0.85 and ? is set to 10 following previous works <ref type="bibr" target="#b16">[17]</ref>.  <ref type="table">Table 2</ref>. Quantitative results on the official split of KITTI dataset. Eight widely used metrics are reported for the validation set while only four metrics are available from the online evaluation server for the test set. "SILog" error is the main ranking metric. Our method ranks 1st among all submissions on the KITTI depth prediction online benchmark at the submission time of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our work is implemented in Pytorch and experimented on Nvidia GTX 2080 Ti GPUs. The network is optimized end-to-end with the Adam optimizer (? 1 = 0.9, ? 1 = 0.999). The training runs for 20 epochs with the batch size of 8 and the learning rate decreasing from 1 ? 10 ?4 to 1 ? 10 ?5 . The output depth map of our network is of 1 4 ? 1 4 size of the original image, which is then resized to the full resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>KITTI dataset. KITTI dataset <ref type="bibr">[8]</ref> is the most used benchmark with outdoor scenes captured from a moving vehicle. There are two mainly used splits for monocular depth estimation. One is the training/testing split proposed by Eigen et al. <ref type="bibr">[6]</ref> with 23488 training image pairs and 697 testing images. The other one is the official split proposed by Geiger et al. <ref type="bibr">[8]</ref> with 42949 training image pairs, 1000 validation images, and 500 testing images. For the official split, the ground-truth depth maps for the testing images are withheld by the online evaluation benchmark.</p><p>NYUv2 dataset. NYUv2 <ref type="bibr" target="#b31">[32]</ref> is an indoor datasets with 120K RGB-D videos captured from 464 indoor scenes. We follow the official training/testing split to evaluate our method, where 249 scenes are used for training and 654 images from 215 scenes are used for testing.</p><p>MatterPort3D dataset. To verify the effectiveness of our method on more domains, we also evaluate our method on the panorama images. MatterPort3D <ref type="bibr">[3]</ref> is the biggest real-world dataset among all widely used datasets in panorama depth estimation. Following the official split, we use 7829 images from 61 houses to train our network and then evaluate the model on the merged set of 957 validation images and 2014 testing images. All images are resized to 1024 ? 512 in both training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluations</head><p>Evaluation on KITTI. For outdoor scenes, we evaluate our method on the KITTI dataset. We first perform the training and testing on the Eigen split, of which the testing images are available so that the network can be better tuned. The results are reported in <ref type="table" target="#tab_1">Table 1</ref>, where we can see that our method outperforms previous methods by a significant margin. Almost all errors are reduced by about  <ref type="table">Table 3</ref>. Quantitative results on NYUv2. "Abs Rel" and "RMSE" are the main ranking metrics. "*" means using additional data.</p><p>10%. Specifically, the "Abs-Rel", "Sq Rel", "RMSE" and "RMSE log " errors are decreased by 10.3%, 18.4%, 9.8%, and 10.2%, respectively. Although our method is trained without additional data, it can outperform previous methods trained with additional training data.</p><p>We then evaluate our method on the KITTI official split, where the testing images are hidden. The results on the validation set and the testing set are all presented in Table 2. The results of the testing set are cited from the online benchmark and the results of the validation set are cited from BANet <ref type="bibr">[1]</ref>. Here we can see that our method reduces the main ranking metric, the SILog error, markedly. Our method now ranks 1st among all submissions on the KITTI depth prediction online server. The colorful visualizations of the predicted depth maps and the error maps generated by the online server are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. Our method predicts cleaner and smoother depth while maintaining sharper edges of objects, e.g., the edges of the humans.</p><p>Evaluation on NYUv2. For indoor scenes, we evaluate our method on the NYUv2 dataset. Since the state-of-theart performance on NYUv2 dataset has been saturated for a while, some methods have begun to use additional data to pretrain the model and then finetune it on NYUv2 training set <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. Differently, without any additional data, our method can significantly improve the performance in all metrics, as is shown in <ref type="table">Table 3</ref>. Specifically, the "Abs Rel" error is reduced to within 0.1 and the "? &lt; 1.25 2 " accuracy reaches 99%. This emphasizes the contribution of our method in improving the results. The qualitative results in <ref type="figure">Figure 5</ref> illustrate that our method estimates better depth especially in difficult regions, such as repeated texture, messy environment, and bad light.</p><p>Evaluation on MatterPort3D. As is studied in previous works, directly applying a deep network for perspective images to the standard representation of spherical panoramas, i.e., the equirectangular projection, is suboptimal, as it becomes distorted towards the poles <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>. As such, methods in this task try all kinds of ways to con-  <ref type="table">Table 5</ref>. Ablation study on the Eigen split of KITTI dataset. The first six metrics of those used in <ref type="table" target="#tab_1">Table 1</ref> are reported here. "S" refers to window shift, "R" refers to rearrange upscale, and "P" refers to PPM head. The last three rows display the results of using different numbers of heads. vert the panorama images to distortion-free shape, e.g., the cubemap projection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b35">36]</ref>, the horizontal feature representation <ref type="bibr" target="#b32">[33]</ref>, and spherical convolutional filters <ref type="bibr" target="#b33">[34]</ref>. In comparison to the above-mentioned methods, we directly apply our network designed for perspective images to the panorama images, and outperforms all previous methods, as is presented in <ref type="table">Table 4</ref>. Specifically, the "Abs Rel" and "Abs" errors are decreased by 14.8% and 20.0%.</p><p>In addition, we realize that the number of the training set of MatterPort3D is small, so we collect more data in the real world. We use 50K images to pretrain the network and then finetune it on the MatterPort3D training set, which results in a better performance, as shown in <ref type="table">Table 4</ref>. The model pretrained with more data is denoted by "Ours*". This demonstrates the pretraining with more images can clearly boost the performance in panorama depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Abalation Study</head><p>To better inspect the effect of each module in our method, we evaluate each component by an ablation study and present the results in <ref type="table">Table 5</ref>.</p><p>Baseline vs. Neural CRFs. To verify the effectiveness of the proposed neural window fully-connected FC-CRFs, we build a baseline model. This model is a well-used UNet structure with the same encoder as ours. In other words, compared to our full method, the PPM head and the rearrange upscale are removed, and the decoder is replaced by the well-used convolutional decoder. Then based on this baseline, we only replace the decoder with our neural window FC-CRFs module, and obtain a noticeable performance improvement as shown in <ref type="table">Table 5</ref>. The "Abs Rel" error is reduced from 0.069 to 0.055, and then to 0.054 by adding the shift action. This demonstrates the effectiveness of the neural window FC-CRFs in estimating accurate depths.</p><p>Rearrange upscale.</p><p>On top of the basic neural FC-CRFs structure, we add the rearrange upscale module. The performance increment gained from this module is not large, but visually the output depth maps have sharper edges, and the parameters of the network are reduced.</p><p>PPM head. The PPM head aggregates the global information, which is lacking in window FC-CRFs. This module can help in some regions that are difficult for estimating with only local information, e.g., the complex texture and the white walls. From the results in <ref type="table">Table 5</ref>, we see this module contributes to the performance of our framework.</p><p>Multi-head energy. The CRFs energy is calculated in a multi-head manner. With more heads, the ability of capturing the pairwise relationship would be stronger but the weight of the network would be heavier. In previous experiments, the numbers of the heads in four levels are set to <ref type="bibr">32, 16, 8, 4.</ref> Here we use fewer heads to see how a lightweight structure performs. From the results in <ref type="table">Table 5</ref>, fewer heads lead to a small performance decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a neural window fully-connected CRFs module to address the monocular depth estimation problem. To solve the expensive computation of FC-CRFs, we split the input into sub-windows and calculate the pairwise potential within each window. To capture the relationships between nodes of the graph, we exploit the multi-head attention to compute a neural potential function. This neural window FC-CRFs module can be directly embedded into a bottom-up-top-down structure and serves as a decoder, which cooperates with a transformer encoder and predicts accurate depth maps. The experiments show that our method significantly outperforms previous methods and sets a new state-of-the-art performance on KITTI, NYUv2, and MatterPort3D datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Point Cloud Visualization</head><p>To better see the 3D shape of the estimated depth map, we project the 2D pixels of the color image back to the 3D world utilizing the estimated depth map. The generated point clouds on the test set of NYUv2 dataset are displayed in <ref type="figure" target="#fig_0">Figure 3</ref>, where the structures of the 3D world are recovered reasonably.</p><p>Furthermore, to recover the complete scenarios, we collect some new indoor panorama images in the real world, and apply our model to these unseen images. The estimated depth maps and the projected 3D point clouds are displayed in <ref type="figure" target="#fig_1">Figure 4</ref>, where the whole structures of the rooms are successfully reconstructed. The floors, ceilings, and the walls keep flat from the near to far. The straight lines are kept straight and the right angles are kept right. The decent performance on the unseen images shows the generalization ability of our model, which has the potential to be applied to real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Network structure of the proposed framework. The encoder first extracts the features in four levels. A PPM head aggregates the global and local information and makes the initial prediction X from the top image feature F. Then in each level, the neural window fully-connected CRFs module builds multi-head energy from X and F, and optimizes it to a better prediction X . Between each level a rearrange upscale is performed considering the sharpness and network weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on the KITTI online benchmark, which are generated by the online server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Method capAbs Rel ? Sq Rel ? RMSE ? RMSE log ? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ? Quantitative results on the Eigen split of KITTI dataset. Seven widely used metrics are reported. "Abs Rel" error is the main ranking metric. Note that the "Sq Rel" error is calculated in a different way here. "*" means using additional data for training. Abs Rel ? Sq Rel ? iRMSE ? RMSE ? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ?</figDesc><table><row><cell cols="2">Eigen et al. [6] 0-80m</cell><cell>0.190</cell><cell>1.515</cell><cell>7.156</cell><cell>0.270</cell><cell></cell><cell>0.692</cell><cell>0.899</cell><cell>0.967</cell></row><row><cell cols="2">Liu et al. [20] 0-80m Xu et al. [38] 0-80m DORN [7] 0-80m</cell><cell>0.217 0.122 0.072</cell><cell>? 0.897 0.307</cell><cell>7.046 4.677 2.727</cell><cell>? ? 0.120</cell><cell></cell><cell>0.656 0.818 0.932</cell><cell>0.881 0.954 0.984</cell><cell>0.958 0.985 0.995</cell></row><row><cell cols="2">Yin et al. [39] 0-80m BTS [17] 0-80m</cell><cell>0.072 0.059</cell><cell>? 0.241</cell><cell>3.258 2.756</cell><cell>0.117 0.096</cell><cell></cell><cell>0.938 0.956</cell><cell>0.990 0.993</cell><cell>0.998 0.998</cell></row><row><cell cols="2">PackNet-SAN [10] 0-80m Adabin [2] 0-80m</cell><cell>0.062 0.058</cell><cell>? 0.190</cell><cell>2.888 2.360</cell><cell>? 0.088</cell><cell></cell><cell>0.955 0.964</cell><cell>? 0.995</cell><cell>? 0.999</cell></row><row><cell cols="2">DPT* [28] 0-80m PWA [18] 0-80m</cell><cell>0.062 0.060</cell><cell>? 0.221</cell><cell>2.573 2.604</cell><cell>0.092 0.093</cell><cell></cell><cell>0.959 0.958</cell><cell>0.995 0.994</cell><cell>0.999 0.999</cell></row><row><cell></cell><cell>Ours 0-80m</cell><cell>0.052</cell><cell cols="2">0.155 2.129</cell><cell>0.079</cell><cell cols="2">0.974</cell><cell>0.997</cell><cell>0.999</cell></row><row><cell cols="3">Method SILog ? DORN [7] dataset val 12.22</cell><cell>11.78</cell><cell>3.03</cell><cell>11.68</cell><cell>3.80</cell><cell>0.913</cell><cell>0.985</cell><cell>0.995</cell></row><row><cell>BTS [17]</cell><cell>val</cell><cell>10.67</cell><cell>7.51</cell><cell>1.59</cell><cell>8.10</cell><cell>3.37</cell><cell>0.938</cell><cell>0.987</cell><cell>0.996</cell></row><row><cell>BA-Full [1]</cell><cell>val</cell><cell>10.64</cell><cell>8.25</cell><cell>1.81</cell><cell>8.47</cell><cell>3.30</cell><cell>0.938</cell><cell>0.988</cell><cell>0.997</cell></row><row><cell>Ours</cell><cell>val</cell><cell>8.31</cell><cell>5.54</cell><cell>0.89</cell><cell>6.34</cell><cell>2.55</cell><cell>0.968</cell><cell>0.995</cell><cell>0.998</cell></row><row><cell cols="3">DORN [7] online test BTS [17] online test BA-Full [1] online test PackNet-SAN [10] online test PWA [18] online test Ours online test 10.39 11.77 11.67 11.61 11.54 11.45</cell><cell>8.78 9.04 9.38 9.12 9.05 8.37</cell><cell>2.23 2.21 2.29 2.35 2.30 1.83</cell><cell>12.98 12.23 12.23 12.38 12.32 11.03</cell><cell>? ? ? ? ? ?</cell><cell>? ? ? ? ? ?</cell><cell>? ? ? ? ? ?</cell><cell>? ? ? ? ? ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method Abs Rel? Abs ? RMSE ? RMSE log ? ? &lt; 1.25 ? ? &lt; 1.25 2 ? ? &lt; 1.25 3 ?</figDesc><table><row><cell>OmniDepth [46] 0.2901</cell><cell>0.4838 0.7643</cell><cell>0.1450</cell><cell>0.6830</cell><cell>0.8794</cell><cell>0.9429</cell></row><row><cell>BiFuse [36] 0.2048</cell><cell>0.3470 0.6259</cell><cell>0.1134</cell><cell>0.8452</cell><cell>0.9319</cell><cell>0.9632</cell></row><row><cell>SliceNet [25] 0.1764</cell><cell>0.3296 0.6133</cell><cell>0.1045</cell><cell>0.8716</cell><cell>0.9483</cell><cell>0.9716</cell></row><row><cell>HoHoNet [33] 0.1488</cell><cell>0.2862 0.5138</cell><cell>0.0871</cell><cell>0.8786</cell><cell>0.9519</cell><cell>0.9771</cell></row><row><cell>UniFuse [14] 0.1063</cell><cell>0.2814 0.4941</cell><cell>0.0701</cell><cell>0.8897</cell><cell>0.9623</cell><cell>0.9831</cell></row><row><cell cols="2">Ours 0.0906 0.2252 0.4778</cell><cell>0.0638</cell><cell>0.9197</cell><cell>0.9761</cell><cell>0.9909</cell></row><row><cell cols="2">Ours* 0.0793 0.1970 0.4279</cell><cell>0.0575</cell><cell>0.9376</cell><cell>0.9812</cell><cell>0.9933</cell></row><row><cell cols="6">Table 4. Quantitative results on the Matterport3D dataset. "*" means using additional data for training.</cell></row><row><cell cols="2">Setting Abs Rel Sq Rel RMSE R log 1.25 1.25 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Baseline 0.069 0.256 2.610 0.103 0.947 0.993</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Neural CRFs 0.055 0.185 2.322 0.086 0.965 0.995</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ S 0.054 0.174 2.297 0.084 0.968 0.996</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ S + R 0.054 0.168 2.271 0.083 0.970 0.996</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ S + R + P 0.052 0.155 2.129 0.079 0.974 0.997</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8, 4, 2, 1 0.055 0.165 2.203 0.083 0.970 0.996</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">16, 8, 4, 2 0.054 0.162 2.172 0.081 0.972 0.997</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">32, 16, 8, 4 0.052 0.155 2.129 0.079 0.974 0.997</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Window size Image size Time Memory Abs Rel RMSE Feature extract 352 ? 1216 95ms 2.6G 2 352 ? 1216 121ms 2.7 G 0.061 2.524 4 352 ? 1216 122ms 2.7 G 0.058 2.358 7/8 352 ? 1216 124ms 2.7 G 0.055 2.234 16 352 ? 1216 136ms 2.9 G 0.055 2.188 24 352 ? 1216 149ms 3.5 G 0.055 2.201 32 352 ? 1216 192ms 5.1 G Efficiency experiments on the Eigen split of KITTI dataset. The first row is the time of other modules except the CRFs decoder, and then we add that of different window size. Models are retrained with a random crop of 352 ? 352 of original images due to the high memory consumption of large window FC-CRFs. "OOM" denotes out of memory. BTS, DPT, and our models are all tested on RTX 2080 Ti. The time of [4] is only the CRFs postprocessing.pecially in difficult regions, such as repeated texture, messy environment, and bad light.</figDesc><table><row><cell>0.054 2.232</cell></row><row><cell>48 352 ? 1216 345ms 7.3 G OOM in training 7.5s -[5] 86 ? 107 -[7] 55 ? 305 59.1s [2, 3] 480 ? 640 0.5s -</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Project page: https://weihaosky.github.io/newcrfs</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Weihao Yuan Xiaodong Gu Zuozhuo Dai Siyu Zhu Ping TanAlibaba Group</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Structure</head><p>More details of our network are shown in <ref type="figure">Figure 1</ref>. The top-level output of the encoder is fed into the PPM head <ref type="bibr">[8]</ref> for aggregating the local and global information. Then in each level, a graph with patches as nodes is built, which is split into k windows with size of N ? N . From the top level to the bottom level, a combination of 32, 16, 8, 4 is adopted for the head numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficiency Experiments</head><p>To inspect the efficiency of the neural window FC-CRFs, we perform the experiments of different window size and report the efficiency and accuracy in <ref type="table">Table 1</ref>. The window FC-CRFs of window size 2 costs 121 ? 95 = 26 ms and 2.7 ? 2.6 = 0.1 G without considering the feature extraction. With the increase of window size N , the improving of the accuracy gradually tapers and is almost saturated at 0.055 when N = 7, but the increase of the cost grows up exponentially, costing 250 ms and 5.3 G when N = 48. Our window FC-CRFs of window size N = 2 is equivalent to traditional CRFs, and that of window size 352 ? 1216 is equivalent to the FC-CRFs. FC-CRFs consumes much more memory and computation than our window FC-CRF, even of large N (e.g. 48). In traditional graph-based methods, the CRFs for depth estimation costs 59.1 s in <ref type="bibr">[7]</ref>, and the FC-CRFs for segmentation costs 0.5 s in <ref type="bibr">[2,</ref><ref type="bibr">3]</ref>, while our window FC-CRFs of 7 ? 7 costs only 29 ms (without considering the feature extraction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Qualitative Results</head><p>To make more comparisons to previous state-of-the-art methods, we display more qualitative results of BTS <ref type="bibr">[4]</ref>, Adabins <ref type="bibr">[1]</ref>, and our method on the test set of NYUv2 <ref type="bibr">[6]</ref> dataset, as is shown in <ref type="figure">Figure 2</ref>. From the results, our method estimates better depth and recover more details, es-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shift window fully connected CRF</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bidirectional attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhra</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean Marie Uwabeza</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11746" to="11752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
	<note>Kayhan Batmanghelich, and Dacheng Tao</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><forename type="middle">Tan</forename><surname>Dro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13201</idno>
		<title level="m">Deep recurrent optimizer for structure-from-motion</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse auxiliary networks for unified monocular depth prediction and completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth estimation with convolutional conditional random field network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="546" to="554" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual ACM symposium on User interface software and technology</title>
		<meeting>the 24th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifuse: Unidirectional fusion for 360 panorama depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualie</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1519" to="1526" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="582" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Patch-wise attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eojindl</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anton Van Den Hengel, and Mingyi He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High speed obstacle avoidance using monocular vision and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Michels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hmm-based surface reconstruction from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Ikehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Kurematsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sdnet: Semantically guided depth estimation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kretz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="288" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Agus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Gobbetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11536" to="11545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3997" to="4008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using multi-scale continuous crfs as sequential deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1426" to="1440" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hohonet: 360 indoor holistic understanding with latent horizontal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="707" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bifuse: Monocular 360 depth estimation via bi-projection fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A depth estimating method from a single image using foe crf. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhou</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="9491" to="9506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Structured attention guided convolutional neural fields for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3917" to="3925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mfusenet: Robust depth estimation with learned multiscopic fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3113" to="3120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end nonprehensile rearrangement with deep reinforcement learning and simulation-to-reality transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="119" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reinforcement learning in topology-based representation for human body movement with whole arm manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation</title>
		<meeting>the IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stereo matching by self-supervision of multiscopic vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5702" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Omnidepth: Dense depth estimation for indoors spherical panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Zioulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="448" to="465" />
		</imprint>
	</monogr>
	<note>Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Shariq Farooq Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il Hong</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A depth estimating method from a single image using foe crf. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhou</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="9491" to="9506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
