<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal-Relational CrossTransformers for Few-Shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Masullo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilo</forename><surname>Burghardt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Mirmehdi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bristol</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal-Relational CrossTransformers for Few-Shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared. <ref type="bibr" target="#b0">1</ref> Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Few-shot methods aim to learn new classes with only a handful of labelled examples. Success in few-shot approaches for image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8]</ref> and object recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref> has triggered recent progress in fewshot video action recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref>. This is of particular interest for fine-grained actions where collecting enough labelled examples proves challenging <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Recent approaches that achieve state-of-the-art performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref> acknowledge the additional challenges in few-shot video recognition, due to varying action lengths and temporal dependencies. However, these match the query video (i.e. the video to be recognised) to the single best video in the support set (i.e. the few labelled examples per class), e.g. <ref type="bibr" target="#b26">[27]</ref>, or to the average across all support set videos belonging to the same class <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Inspired by part-based few-shot image classification <ref type="bibr" target="#b7">[8]</ref>, we consider that, within a few-shot regime, it is advantageous to compare sub-sequences of the query video to sub-sequences of <ref type="bibr" target="#b0">1</ref> Code is available at https://github.com/tobyperrett/TRX <ref type="figure">Figure 1</ref>: For a 3-way 5-shot example, pairs of temporallyordered frames in the query (red, green, blue) are compared against all pairs in the support set (max attention with corresponding colour). Aggregated evidence is used to construct query-specific class prototypes. We show a correctlyrecognised query using our method from SSv2 class "Failing to put something into something because it does not fit". all support videos when constructing class prototypes. This better accumulates evidence, by matching sub-sequences at various temporal positions and shifts.</p><p>We propose a novel approach to few-shot action recognition, which we term Temporal-Relational CrossTransformers (TRX). A query-specific class prototype is constructed by using an attention mechanism to match each query sub-sequence against all sub-sequences in the support set, and aggregating this evidence. By performing the attention operation over temporally-ordered sub-sequences rather than individual frames (a concept similar to that in many-shot action-recognition works, e.g. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>), we are better able to match actions performed at different speeds and in different parts of videos, allowing distinction between fine-grained classes. <ref type="figure">Fig. 1</ref> shows an example of how a query video attends to multiple support set videos using temporally-ordered tuples.</p><p>Our key contributions can be summarised as follows:</p><p>? We introduce a novel method, called the Temporal-Relational CrossTransformer (TRX), for few-shot action recognition.</p><p>? We combine multiple TRXs, each operating over a different number of frames, to exploit higher-ordered temporal relations (pairs, triples and quadruples).</p><p>? We achieve state-of-the-art results on the few-shot benchmarks for Kinetics <ref type="bibr" target="#b4">[5]</ref>, Something-Something V2 (SSv2) <ref type="bibr" target="#b11">[12]</ref>, HMDB51 <ref type="bibr" target="#b15">[16]</ref> and UCF101 <ref type="bibr" target="#b20">[21]</ref>.</p><p>? We perform a detailed ablation, demonstrating how TRX utilises multiple videos from the support set, of different lengths and temporal shifts. Results show that using tuple representations improves over single-frames by 5.8% on SSv2 where temporal ordering proves critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Few-shot classification methods have traditionally fallen into one of three categories -generative <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref>, adaptationbased <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref> and metric-based <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20]</ref>. Generative methods use examples from the target task to generate additional task-specific training data with which to fine-tune a network. Adaptation-based methods (e.g. MAML <ref type="bibr" target="#b10">[11]</ref>) aim to find a network initialisation which can be fine-tuned with little data to an unseen target task. Metric-based methods (e.g. Prototypical <ref type="bibr" target="#b19">[20]</ref> or Matching <ref type="bibr" target="#b23">[24]</ref> Networks) aim to find a fixed feature representation in which target tasks can be embedded and classified.</p><p>Recent works which perform well on few-shot image classification have found that it is preferable to use a combination of metric-based feature extraction/classification combined with task-specific adaptation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>. Most relevant to this paper, the recently introduced CrossTransformer <ref type="bibr" target="#b7">[8]</ref> uses an attention mechanism to align the query and support set using image patch co-occurrences. This is used to create query-specific class prototypes before classification within a prototypical network <ref type="bibr" target="#b19">[20]</ref>. Whilst this is effective for few-shot image classification, one potential weakness is that relative spatial information is not encoded. For example, it would not differentiate between a bicycle and a unicycle. This distinction is typically not needed in <ref type="bibr" target="#b7">[8]</ref>'s tested datasets <ref type="bibr" target="#b21">[22]</ref>, where independent part-based matching is sufficient to distinguish between the classes.</p><p>Few-shot video action recognition methods have had success with a wide range of approaches, including mem-ory networks of key frame representations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and adversarial video-level feature generation <ref type="bibr" target="#b8">[9]</ref>. Recent works have attempted to make use of temporal information. Notably, <ref type="bibr" target="#b2">[3]</ref> aligns variable length query and support videos before calculating the similarity between the query and support set. <ref type="bibr" target="#b26">[27]</ref> combines a variety of techniques, including spatial and temporal attention to enrich representations, and jigsaws for self-supervision. <ref type="bibr" target="#b3">[4]</ref> achieves state-of-the-art performance by calculating query to support-set frame similarities. They then enforce temporal consistency between a pair of videos by monotonic temporal ordering. Their method can be thought of as a differentiable generalisation of dynamic time warping. Note that the above works either search for the single support video <ref type="bibr" target="#b26">[27]</ref> or average representation of a support class <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> that the query is closest to. A concurrent work to ours attempts to resolve this through query-centred learning <ref type="bibr" target="#b32">[33]</ref>. Importantly, all prior works perform attention operations on a frame level, as they tend to use single-frame representations.</p><p>Compared to all prior few-shot action recognition methods, our proposed method attends to all support set videos, using temporal-relational representations from ordered tuples of frames, sampled from the video. By attending to sub-sequences, our method matches actions at different speeds and temporal shifts. Importantly, we use a combination of different CrossTransformers to match tuples of different cardinalities, allowing for higher-order temporal representations. We next describe our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We propose a method for few-shot action recognition that considers the similarity between an ordered subsequence of frames (referred to as a tuple) to all subsequences in the support set, through multiple CrossTransformer attention modules. This allows the same query video to match to tuples from several support set videos. After stating the problem definition in Section 3.1, for ease of understanding our TRX method, we start from a simplified version, building in complexity and generality up to the full method, which is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>In Section 3.2, we consider a single ordered pair of frames, sampled from the query video. We propose a temporal CrossTransformer to compare this query pair to ordered pairs of frames from videos in the support set. This allows the construction of 'query pair'-specific class prototypes. We then expand to multiple ordered pairs of frames from the query video. Finally, in Section 3.3, motivated by the need to model more complex temporal relationships, we generalise from pairs to tuples. We model a separate temporal CrossTransformer for each tuple cardinality to construct query-cardinality-specific class prototypes. These are combined to classify the query video, based on the distances to all class prototypes.  representations of the query and support set videos are constructed. These are concatenated representations of pairs/triplets of frames (sampled from the video), temporally-ordered. Two temporal CrossTransformers, T 2 and T 3 , then construct separate class prototypes for each representation (pairs and triplets) using separate query ?, key ? and value ? linear maps for each transformer. This produces a query-specific class prototype, one per CrossTransformer. The query representation is also passed through the value linear maps ?, and distances are calculated to the prototypes. The distances are averaged, and the query is recognised as belonging to the closest class. Details are in Section 3.</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? T 3 T 2 ? ? ? ? ? ? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In few-shot video classification, inference aims to classify an unlabelled query video into one of several classes, each represented by a few labelled examples unseen in training, referred to as the 'support set'. In this paper, we focus on K-shot where K &gt; 1, i.e. the support set contains more than one video. Similar to prior works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref>, we follow episodic training, i.e. random sampling of few-shot tasks from the training set. For each episode, we consider a C-way K-shot classification problem. Let Q = {q 1 , ? ? ? , q F } be a query video with F uniformly sampled frames. The goal is to classify Q into one of the classes c ? C. For the class c, its support set S c contains K videos, where the k th video is denoted S c k = {s c k1 , ? ? ? , s c kF } 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal CrossTransformer</head><p>We consider the temporal relation of two frames sampled from a video to represent the action, as actions are typically changes in appearance and are poorly represented by a single frame. We thus sample a pair of ordered frames from the query video with indices p = (p 1 , p 2 ), where 1 ? p 1 &lt; p 2 ? F , and define the query representation as:</p><formula xml:id="formula_1">Q p = [?(q p1 ) + PE(p 1 ), ?(q p2 ) + PE(p 2 )] ? R 2?D , (1)</formula><p>where ? : R H?W ?3 ? R D is a convolutional network to obtain a D-dimensional embedding of an input frame, and PE(?) is a positional encoding given a frame index <ref type="bibr" target="#b22">[23]</ref>.</p><p>We compare the query representation Q p to all possible pair representations from the support set videos, allowing it to match actions at various speeds and locations within the support set. We define the set of all possible pairs as</p><formula xml:id="formula_2">? = {(n 1 , n 2 ) ? N 2 : 1 ? n 1 &lt; n 2 ? F )}.<label>(2)</label></formula><p>A single frame-pair representation of video k in the support set of class c with respect to the ordered pair of indices</p><formula xml:id="formula_3">m = (m 1 , m 2 ) ? ? is S c km = [?(s c km1 )+PE(m 1 ), ?(s c km2 )+PE(m 2 )] ? R 2?D .<label>(3)</label></formula><p>The set of all pair representations in the support set for class c is</p><formula xml:id="formula_4">S c = {S c km : (1 ? k ? K) ? (m ? ?)}.<label>(4)</label></formula><p>We propose a temporal CrossTransformer T , based on the spatial CrossTransformer <ref type="bibr" target="#b7">[8]</ref>, but adapted from image patches to frame pairs, to calculate query-specific class prototypes. The CrossTransformer includes query ?, key ? and value ? linear maps, which are shared across classes:</p><formula xml:id="formula_5">?, ? : R 2?D ? R d k and ? : R 2?D ? R dv . (5)</formula><p>The correspondence between the query pair and pair m of support video k in class c is calculated as</p><formula xml:id="formula_6">a c kmp = L(? ? S c km ) ? L(? ? Q p ),<label>(6)</label></formula><p>where L is a standard layer normalisation <ref type="bibr" target="#b0">[1]</ref>. We apply the Softmax operation to acquire the attention map</p><formula xml:id="formula_7">a c kmp = exp(a c kmp )/ ? d k l,n exp(a c lnp )/ ? d k .<label>(7)</label></formula><p>This is then combined with value embeddings of the support set v c km =? ? S c km , in order to compute the query-specific prototype with respect to the query Q p ,</p><formula xml:id="formula_8">t c p = km? c kmp v c km .<label>(8)</label></formula><p>Now that we have a query-specific class prototype, we calculate the embedding of the query Q p with the value linear map such that u p =? ? Q p . This ensures that the query and support representations undergo the same operations. The CrossTransformer T computes the distance between the query and support set S c by passing Q p such that</p><formula xml:id="formula_9">T (Q p , S c ) = t c p ? u p .<label>(9)</label></formula><p>Finding a single pair that best represents the action Q is a difficult problem. Instead, we consider multiple pairs of frames from the query video, such that the query representation is defined as Q = {Q p : p ? ?}. In Section 4.3.6, we compare exhaustive pairs to random pairs of frames.</p><p>To calculate the distance between Q and S c , we accumulate the distances from all query pairs, i.e.</p><formula xml:id="formula_10">T (Q, S c ) = 1 |?| p?? T (Q p , S c ).<label>(10)</label></formula><p>During training, negative query-class distances T are passed as logits to a cross-entropy loss. During inference, the query Q p is assigned the class of the closest query-specific prototype, i.e. arg min c T (Q p , S c ). Note that the Softmax operation in Eq. 7 is performed separately for each query pair Q p (i.e. matches are scaled separately for each p). Our Temporal CrossTransformer thus accumulates matches between pair representations of the query video (hence the term temporal) and all pairs of frames in the support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal-Relational CrossTransformers</head><p>A shortcoming of the above method is that an ordered pair of frames might not be the best representation of an action, particularly when fine-grained distinctions between the classes are required. Consider two classes: "picking an object up" vs "moving an object". To discern between these two actions, a method would require at least three framesi.e. whether the object is put down eventually or remains in hand. Similarly, consider full-body actions such as "jumping" vs "tumbling". This highlights the need to explore higher-order temporal representations.</p><p>We extend the Temporal CrossTransformer to a Temporal-Relational CrossTransformer (TRX) by considering a sub-sequence of ordered frames of any length. We use ? to indicate the length, or cardinality, of a tuple. For example, ?=2 for a pair, ?=3 for a triple. We generalise to possible tuples for any ?, such that</p><formula xml:id="formula_11">? ? = {(n 1 , ..., n ? ) ? N ? : ?i(1 ? n i &lt; n i+1 ? F )}<label>(11)</label></formula><p>The associated query representation with respect to the tuple with indices p = (p 1 , ..., p ? ) ? ? ? , generalising the pair representation in Eq. 1, is</p><formula xml:id="formula_12">Q ? p = [?(q p1 ) + PE(p 1 ), ..., ?(q p? ) + PE(p ? )] ? R ??D .<label>(12)</label></formula><p>This is done similarly for the support set representations.</p><p>We define the set of cardinalities as ?. For example, pairs, triples and quadruples of frames would be ?={2, 3, 4}. We use one TRX per cardinality, as parameters can only be defined for a known input dimensionality (e.g. Eq. 5). Each TRX T ? includes query, key and value linear maps corresponding to the dimensionality of ?:</p><formula xml:id="formula_13">? ? , ? ? : R ??D ? R d k and ? ? : R ??D ? R dv . (13)</formula><p>Each T ? outputs the distance between the query and support set with respect to tuples of cardinality ?. We then accumulate distances from the various TRXs, such that:</p><formula xml:id="formula_14">T ? (Q, S c ) = ??? T ? (Q ? , S c? ).<label>(14)</label></formula><p>Note that averaging the outputs from each TRX first (as in Eq. 10 for ?=2) balances the varying number of tuples for each ?. As with a single TRX, during training, the negative distance for each class is passed as the logit to a crossentropy loss. During inference, the query is assigned the class which is closest to the query with respect to T ? .</p><p>Summary: TRX in its complete form considers a set of cardinalities ?. For each ? ? ?, different linear maps of corresponding dimensions are trained (Eq. 13). These are trained jointly using a single cross-entropy loss, that uses the summed distances (Eq. 14), where the gradient is backpropagated for each ? (Eq. 10). The gradient is accumulated from each TRX, through the tuple representations, and backpropagated through a convolutional network to update frame representations. TRX is trained end-to-end with shared backbone parameters for all ? ? ?, and all tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. We evaluate our method on four datasets. The first two are Kinetics <ref type="bibr" target="#b4">[5]</ref> and Something-Something V2 (SSv2) <ref type="bibr" target="#b11">[12]</ref>, which have been frequently used to evaluate few-shot action recognition in previous works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref>. SSv2, in particular, has been shown to require temporal reasoning (e.g. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14]</ref>). We use the few-shot splits for both datasets proposed by the authors of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> which are publicly accessible <ref type="bibr" target="#b2">3</ref> . In this setup, 100 videos from 100 classes are selected, with 64, 12 and 24 classes used for train/val/test. We also provide results for the few-shot split of SSv2 used by <ref type="bibr" target="#b3">[4]</ref> which uses 10x more videos per class in the training set. Additionally, we evaluate our method on HMDB51 <ref type="bibr" target="#b15">[16]</ref> and UCF101 <ref type="bibr" target="#b20">[21]</ref>, using splits from <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Kinetics SSv2 ? SSv2 * HMDB UCF CMN <ref type="bibr" target="#b30">[31]</ref> 78.9 ----CMN-J <ref type="bibr" target="#b31">[32]</ref> 78.9 48.8 ---TARN <ref type="bibr" target="#b2">[3]</ref> 78.5 ----ARN <ref type="bibr" target="#b26">[27]</ref> 82.  <ref type="table">Table 1</ref>: Results on 5-way 5-shot benchmarks of Kinetics (split from <ref type="bibr" target="#b31">[32]</ref>), SSv2 ( ? : split from <ref type="bibr" target="#b31">[32]</ref>, * : split from <ref type="bibr" target="#b3">[4]</ref>), HMDB51 and UCF101 (both splits from <ref type="bibr" target="#b26">[27]</ref>).</p><p>Evaluation. TRX particularly benefits from the presence of a number of videos in the support set (i.e. few-shot rather than one-shot). We thus evaluate our method on the standard 5-way 5-shot benchmark, and report average results over 10,000 tasks randomly selected from the test sets. We provide an ablation on X-shot, including one-shot results in the ablation and appendices for completeness.</p><p>Baselines. We give comparisons against four seminal and recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b3">4]</ref>, which reported state-of-the-art in few-shot video action recognition. These have been discussed in Section 2. Implementation details. We train our TRX model, with all tuple cardinalities and frame-level backbones, end-toend. We use a ResNet-50 backbone <ref type="bibr" target="#b12">[13]</ref> with ImageNet pre-trained weights <ref type="bibr" target="#b6">[7]</ref>, so we are directly comparable to previous methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4]</ref>  <ref type="bibr" target="#b3">4</ref> . We initialise TRX parameters randomly and set d k =d v =1152. The last 2048 dimensional layer from the ResNet forms the frame-level input to the TRX. These are concatenated into tuples, depending on the length ?. Following <ref type="bibr" target="#b7">[8]</ref>, the query and key linear maps of each transformer share weights, to encourage similarity matching. Videos are re-scaled to height 256 and F =8 frames are sampled uniformly as in <ref type="bibr" target="#b24">[25]</ref>. They are augmented with random horizontal flipping and 224x224 crops. For testing, just a centre crop is used. We use SGD with a learning rate of 0.001, training for 10,000 tasks, which takes around 3 hours (apart from the larger SSv2 * split from <ref type="bibr" target="#b3">[4]</ref>, which uses 75,000 tasks). These hyperparameters were determined using the validation set. We train TRX on four NVidia 2080Ti GPUs. Due to the number of backbones (e.g. 48 ResNet-50 backbones when considering 5-shot support set, and a query, with 8 frames each), we can only fit a single task in memory. We thus average gradients and backpropagate once every 16 iterations. <ref type="table">Table 1</ref> shows our comparative results. TRX outperforms prior work on the four datasets. On the most challenging dataset (SSv2), TRX outperforms prior work by a wide margin (12% and 10% on different splits). The large <ref type="bibr" target="#b3">4</ref>  <ref type="bibr" target="#b26">[27]</ref> uses Conv-3D features. improvement is found on SSv2 because TRX is particularly beneficial when temporally ordered tuples can assist the discrimination between classes. It also outperforms prior work on HMDB51 and UCF101 by 15% and 13% respectively. On Kinetics, it exceeds the state-of-the-art (by 0.1%). Kinetics is more of an appearance-based dataset when used as a few-shot benchmark, where ordering is less important and single frame representation can be sufficient. We ablate this in Section 4.3.2. <ref type="figure" target="#fig_2">Figure 3</ref> shows qualitative results, highlighting tuple matches between the query and support set for ?={2, 3}. For each subfigure, we show query pairs (top) and triplets (bottom) with their corresponding tuples (same colour) in the support set. For example, the red pair of frames in the first example (frames 1 and 2) gets the maximum attention when compared to the second support set video (frames 2 and 3). We select three tuples to highlight in each case. The figure shows that tuples match to different videos in the support set, as well as tuples of varying positions and frame differences. A failure case <ref type="figure" target="#fig_2">(Fig. 3c</ref>) matches pair/triplet frames from the query "failing to put something into something because it doesn't fit", with pairs/triplets of the support set class "put something upright on the table". In each example, the putting action is correctly matched, but the query is closest to the wrong prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>Our motivation in proposing TRX is the importance of representing both the query and the support set videos by tuples of ordered frames, and that class prototypes should be constructed from multiple support set videos. We showcase this motivation experimentally through several ablations. We specifically evaluate: (4.3.1) the impact of ?, (4.3.2) the importance of ordered frames in the tuple, (4.3.3) the importance of multiple videos in the support set, and (4.3.4) whether tuples at various locations and frame positions are being matched within TRX. Additionally, (4.3.5) we compare performance and runtime as the number of sampled frames changes, and (4.3.6) compare exhaustive to random tuples, showcasing the potential to compress TRX models without a significant drop in performance. We primarily use the large-scale datasets Kinetics and SSv2 ? for these ablations using the splits from <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">TRX with different ? values</head><p>In our comparative analysis in Tab. 1, we reported results using ?={2, 3}. This is the combined TRX of pair and triplet frames demonstrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. We now evaluate each cardinality of ? ? {1, 2, 3, 4} independently as well as all their combinations on both datasets.</p><p>In Tab. 2, results demonstrate significant improvement in SSv2 moving from single frame comparisons to pair     <ref type="table">Table 3</ref>: Results assess the importance of temporal ordering. When the tuple orders are reversed for the query video, a large drop is observed for SSv2 ? , but not for Kinetics.</p><p>The improvement using TRX with the multiple cardinalities ?={2, 3} over frame-based comparisons (?={1}) is demonstrated per-class in <ref type="figure" target="#fig_3">Fig. 4</ref>. For SSv2, some classes see little improvement (e.g. "scooping something up with something", "opening something"), whereas others see a greater than 10% improvement (e.g. "pretending to take something from somewhere", "putting something next to something"). Aligned with the overall results on Kinetics, <ref type="figure" target="#fig_3">Fig. 4</ref> shows modest improvements per-class, including marginal drop in some classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The impact of ordered tuples</head><p>Up to this point, we have made the assumption that tuples should be temporally ordered to best represent actions. We  <ref type="figure">Figure 5</ref>: Comparing CMN <ref type="bibr" target="#b31">[32]</ref> results to TRX for X-shot 5-way, for 1 ? X ? 5 on SSv2 ? . TRX clearly benefits from increasing the number of of videos in the support set, both for ?={1} and using two CrossTransformers ?={2, 3}.</p><p>evaluate the extreme scenario, where frames in the support set are temporally ordered, but frames in the query take the reverse order during inference only. <ref type="table">Table 3</ref> shows a large drop for the reversed query sets on SSv2 (-7.8%). Supporting our prior observation that it is more an appearancebased dataset, no drop is observed for Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Matching to multiple support set videos</head><p>Our motivation for using CrossTransformers is that query tuples would match to tuples from multiple support set videos in order to create the query-specific class prototype. Note that this is not regularised during training -i.e. there is no encouragement to use more than one support set video. <ref type="figure">Figure 5</ref> ablates TRX (?={1} and ?={2, 3}) for the number of videos in the support set per class. We increase this from 1-shot to 5-shot reporting the performance for each on SSv2, as well as comparative results from CMN <ref type="bibr" target="#b31">[32]</ref>. Whilst all methods perform similarly for 1shot, TRX significantly increases the margin over the CMN  baseline as the number of shots increases. For our proposed model ?={2, 3}, we report improvements of +3.9%, +7.3%, +7.9% and +10.3% for 2-, 3-, 4-and 5-shots comparatively. Note that using a single frame representation also improves over CMN, by a smaller but significant margin. This ablation showcases TRX's ability to utilise tuples from the support set as the number of videos increases.</p><p>To analyse how many support videos are used, we train TRX with pairs (?={2}) and quadruples (?={4}) on SSv2 and Kinetics. For each query tuple, we find the support set tuple with the maximum attention value. We then count the number of support set videos per class which contain at least one maximal match, and average over all test tasks. <ref type="figure" target="#fig_5">Figure 6</ref> presents the results for true and false, positive and negative, results. The figure demonstrates that TRX successfully matches the query to tuples from multiple videos in the support set. Most queries (&gt; 50%) match to 2-3 videos in the support set. Very few queries match to all 5 videos in the support set, particularly for higher cardinality tuples. A similar distribution is seen for both datasets, however for SSv2, more true positive queries are matched to a single video in the support set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Visualising tuple matches</head><p>In addition to matching multiple support set videos, we visualise the tuple matches between the queries and the support set. Given a query tuple (row) and a support set tuple (col), we sum the attention values over the test set, and then normalise per row. <ref type="figure" target="#fig_7">Fig. 7</ref> shows the summed attention values between all sets of pairs. While query pairs match    frequently to corresponding support set pairs (i.e. same frame positions) in the support set, pairs are also matched to shifted locations (e.g. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> with <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>) as well as significantly different frame distances (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> with <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Varying the number of frames</head><p>All previous results sample 8 frames from each video, in the query and support set. This allows us to directly compare to previous few-shot works that all consider 8 uniformly sampled frames <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. To demonstrate TRX is scalable, we plot ? = {2, 3} results on SSv2 for the 5-way 5-shot task, sampling different numbers of frames <ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref>, and compare the accuracy and runtime in <ref type="figure" target="#fig_9">Fig. 8</ref>. Accuracy is comparable for ? 6 frames. Importantly, the method's runtime scales linearly with the number of frames. The TRX component only contributes a margin of the runtime and memory requirements of the network, with the ResNet-50 backbone dominating both needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Random tuples in TRX</head><p>All the above experiments have used exhaustive sets of tuples, e.g. every possible pair (n 1 , n 2 ) such that 1 ? n 1 &lt; n 2 ? F for ? = 2. To explore the impact of randomly sampling tuples, we experiment with 20, 40, 60 and 80% of tuples retained for ?={2}, {3} and {4}, as well as a combined ?={2, 3}. We report four runs for each percentage, each with a different random selection of tuples. <ref type="figure">Fig. 9</ref> shows that while retaining all tuples gives the best performance, some of the runs produce results compara-    <ref type="figure">Fig. 9</ref>.</p><p>ble to exhaustive tuple selections for ?={2, 3} and even outperform these for ?={4}. The performance degrades quicker for ?={2}. The associated Tab. 4 compares the corresponding GPU usage. This shows it is possible to utilise fewer resources with comparable performance. A method for selecting tuples that maintain performance is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduced Temporal-Relational CrossTransformers (TRX) for few-shot action recognition. TRX constructs query-specific class prototypes by comparing the query to sub-sequences of all support set videos. To model temporal relationships, videos are represented by ordered tuples of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared. TRX achieves state-of-the-art results on the few-shot versions of four datasets: Kinetics, Something-Something V2, HMDB51 and UCF101. An extensive set of ablations shows how TRX observes multiple support set videos, the importance of tuple representations over single-frame comparisons, and the benefits of exploiting tuples of different cardinalities. As future work, we aim to explore spatiotemporal versions of TRX.</p><p>In the main paper, we introduced Temporal-Realational CrossTransformers (TRX) for few-shot action recognition. They are designed specifically for K-shot problems where K &gt; 1, as TRX is able to match sub-sequences from the query against sub-sequences from multiple support set videos. <ref type="table">Table 1</ref> in the main paper shows results on the standard 5-way 5-shot benchmarks on Kinetics <ref type="bibr" target="#b4">[5]</ref>, Something-Something V2 (SSv2) <ref type="bibr" target="#b11">[12]</ref>, HMDB51 <ref type="bibr" target="#b15">[16]</ref> and UCF101 <ref type="bibr" target="#b20">[21]</ref>. For completeness we also provide 1-, 2-, 3-, 4-and 5shot results for TRX with ?={1} (i.e. frame-to-frame comparisons) and ?={2, 3} (i.e. pair and triplet comparisons) on the large-scale datasets Kinetics and SSv2. These are in <ref type="table" target="#tab_7">Table 5</ref> in this appendix, where we also list results from all other works which provide these scores.</p><p>For 1-shot, in Kinetics, TRX performs similarly to recent few-shot action-recognition methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref>, but these are all outperformed by OTAM <ref type="bibr" target="#b3">[4]</ref>. OTAM works by finding a strict alignment between the query and single support set video per class. It does not scale as well as TRX when K &gt; 1, shown by TRX performing better on the 5-shot benchmark. This is because TRX is able to match query sub-sequences against similar sub-sequences in the support set, and importantly ignore sub-sequences (or whole videos) which are not as useful. Compared to the strict alignment in OTAM <ref type="bibr" target="#b3">[4]</ref>, where the full video is considered in the alignment, TRX can exploit several subsequences from the same video, ignoring any distractors. Despite not being as well suited to 1-shot problems, on SSv2 TRX performs similarly to OTAM. 2-shot TRX even outperforms 5-shot OTAM. <ref type="table" target="#tab_7">Table 5</ref> again highlights the importance of tuples, shown in the main paper, where TRX with ?={2, 3} consistently outperforms ?={1}. <ref type="figure">Figure 5</ref> in the main paper shows how TRX scales on SSv2 compared to CMN <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, which also provides Xshot results (1 ? X ? 5). The equivalent graph for Kinetics is shown in <ref type="figure">Fig. 10</ref> here. This confirms TRX scales better as the shot increases. There is less of a difference between TRX with ?={1} and ?={2, 3}, as Kinetics requires less temporal knowledge to discriminate between the classes than SSv2 (ablated in Sec. 4.3.1 and 4.3.2 in the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The impact of positional encoding</head><p>TRX adds positional encodings to the individual frame representations before concatenating them into tuples. <ref type="table" target="#tab_8">Table 6</ref> shows that adding positional encodings improves SSv2 for both single frames and higher-order tuples (by +0.3% and +0.6% respectively). For Kinetics, performance   stays the same as single frames and improves slightly with tuples (+0.4%) for the proposed model. Overall, positional encoding improves the results marginally for TRX.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the Temporal-Relational CrossTransformer (TRX) on a 2-way 2-shot problem. First, pair and triple</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) SSv2 ? : Throwing something in the air and letting it fall.(b) Kinetics: Cutting watermelon. (c) SSv2 ? (False Positive): Query GT: Failing to put S into S because S does not fit, Support Set: putting something upright on the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples for TRX with ?={2, 3}. Colour-matching pairs (top) and triplets (bottom) are shown between the query and support set videos from one class. Three tuples are highlighted in each subfigure (red, green and blue). This figure demonstrates maximum attention matches to several videos in the support set, at different relative and absolute positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>o p in g S u p w it h S o p e n in g S m o v in g S to w a rd s th e c a m e ra s p in n in g S th a t q u ic k ly s to p s s p in n in g p u s h in g S o ff o f S ro lli n g S o n a fl a t s u rf a c e p u tt in g S o n th e e d g e o f S s o it is n o t s u p p o rt e d a n d fa lls u n fo ld in g S le tt in g S ro ll u p a s la n te d s u rf a c e , s o it ro lls b a c k d o w n p o k in g a s ta c k o f S s o th e s ta c k c o lla p s e s p o k in g a h o le in to S s o ft s q u e e z in g S p o k in g a h o le in to s o m e s u b s ta n c e fa ili n g to p u t S in to S b e c a u s e S d o e s n o t fi t tw is ti n g (w ri n g in g ) S w e t u n ti l w a te r c o m e s o u t p u tt in g S u p ri g h t o n th e ta b le ta k in g S o u t o f S d ro p p in g S in to S tw is ti n g S S fa lli n g lik e a fe a th e r o r p a p e r p u tt in g S o n a s u rf a c e p u s h in g S w it h S p re te n d in g to ta k e S fr o m s o m e w h e re p u tt in g S n e x in g a x e d iv in g c lif f h u la h o o p in g d a n c in g b a lle t p la y in g m o n o p o ly p a ra g lid in g u n b o x in g s h e a ri n g s h e e p p u s h in g c a r p la y in g d ru m s ri d in g e le p h a n t h u rl in g (s p o rt ) fo ld in g p a p e r s id e k ic k ic e s k a ti n g fi lli n g e y e b ro w s d a n c in g p la y in g tr u m p e t b u s k in g d a n c in g m a c a re n a c u tt in g ta p d a n c in g s tr e tc h in g a rm b la s ti n g s a n dKinetics Class improvement using tuples (?={2, 3}) compared to single frames (?={1}) for SSv2 ? and Kinetics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Percentage of queries that match a given number of support videos per class, with a max attention value, for SSv2 ? and Kinetics. True/False Positive/Negative query percentages are shown for ?={2} and ?={4}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc>Figure 7: Summed attention over the SSv2 ? test set, showing how query pairs (rows) match to support pairs (columns) from the TRX ?={2, 3}. Numbers in red show the distance between the frames in the pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Frames</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>SSv2 ? accuracy (left y-axis) vs runtime analysis (right y-axis in seconds/task) for TRX ? = {2, 3} as the number of sampled frames varies from 4 to 12 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparing all values of ? for TRX, noting the number of tuples for each model, given by ??? |? ? |.</figDesc><table><row><cell>comparisons, of (+4.5%). The performance increases further for triplets (+1.0%) and only marginally again for quadruples (+0.1%). Combining two CrossTrans-formers ?={2, 3} performs best. Using all cardinalities ?={2, 3, Method ?={2, 3} order reversed ?={2, 3}</cell><cell>Kinetics SSv2  ? 85.9 51.3 85.9 59.1</cell></row></table><note>4} results in a slight drop in performance (-0.2%). Comparatively, differences are smaller on Kinetics, and moving to quadruples drops the performance significantly (-1.4%) compared to the best TRX combination ?={2, 3}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Figure 9 :</head><label>9</label><figDesc>Effect of retaining % of tuples, selected randomly, on TRX, reported for SSv2 ? . Grey dots indicate results of different runs, with averages in red.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">% tuples retained</cell><cell></cell></row><row><cell>Cardinalities</cell><cell>20</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row><row><cell>?={2}</cell><cell cols="3">128 204 256</cell><cell>346</cell><cell>462</cell></row><row><cell>?={3}</cell><cell cols="3">228 390 540</cell><cell>702</cell><cell>844</cell></row><row><cell>?={4}</cell><cell cols="3">294 546 754</cell><cell>922</cell><cell>1152</cell></row><row><cell>?={2, 3}</cell><cell cols="5">356 594 796 1048 1306</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>GPU usage in MiB when randomly dropping tuples, corresponding to experiments in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison to few-shot video works on Kinetics (split from<ref type="bibr" target="#b31">[32]</ref>) and Something-Something V2 (SSv2) ( ? : split from [32] * : split from<ref type="bibr" target="#b3">[4]</ref>). Results are reported as the shot, i.e. number of support set videos per class, increases from 1 to 5. -: Results not available in published works. Comparing CMN<ref type="bibr" target="#b31">[32]</ref> results to TRX for X-shot 5-way, for 1 ? X ? 5 on Kinetics. TRX benefits from increasing the number of of videos in the support set, both for ?={1} and ?={2, 3}.</figDesc><table><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell>TRX</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?={2,3}</cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell>TRX ?={1}</cell></row><row><cell>Accuracy</cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell>CMN</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Shot</cell><cell></cell></row><row><cell cols="2">Figure 10: Method</cell><cell cols="4">Positional Encoding Kinetics SSv2  ?</cell></row><row><cell></cell><cell>?={1}</cell><cell>?</cell><cell></cell><cell>85.2</cell><cell>53.0</cell></row><row><cell></cell><cell>?={1}</cell><cell></cell><cell></cell><cell>85.2</cell><cell>53.3</cell></row><row><cell></cell><cell>?={2, 3}</cell><cell>?</cell><cell></cell><cell>85.5</cell><cell>58.5</cell></row><row><cell></cell><cell>?={2, 3}</cell><cell></cell><cell></cell><cell>85.9</cell><cell>59.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The importance of incorporating positional encoding for single frames and the proposed model ?={2, 3}.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We assume videos are uniformly sampled to be of the same length F for simplicity. Alternatively, we could set F to be the maximum video length and non-existent pairs could be masked out in the attention matrix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ffmpbgrnn/CMN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Publicly-available datasets were used for this work. This work was performed under the SPHERE Next Steps EPSRC Project EP/R005273/1. Damen is supported by EPSRC Fellowship UMPIRE (EP/T004991/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. X-Shot results</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Layer Normalization. arXiv</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved Few-Shot Visual Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Bishay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Zoumpourlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-Shot Video Classification via Temporal Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo Vadis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Action Recognition? A New Model and the Kinetics Dataset. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CrossTransformers: Spatially-Aware Few-Shot Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Nerual Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ProtoGAN: Towards Few Shot Learning for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Sai Kumar Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaib</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Mearning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The &quot;Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">STM: SpatioTemporal and Motion Encoding for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Few-Shot Object Detection via Feature Reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">On First-Order Meta-Learning Algorithms. arXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retro-Actions : Learning &apos;Close&apos; by Time-Reversing &apos;Open&apos; Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Nerual Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Few-Shot Object Recognition from Machine-Labeled Web Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Few-shot Action Recognition with Permutation-invariant Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Metagan: An Adversarial Approach to Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal Relational Reasoning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal Relational Reasoning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compound Memory Networks for Few-Shot Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Label Independent Memory for Semi-Supervised Few-shot Video Classification. Transactions on Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Toisoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Brais Martinez, and Tao Xiang. Few-shot Action Recognition with Prototype-centered Attentive Learning. arXiv, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
