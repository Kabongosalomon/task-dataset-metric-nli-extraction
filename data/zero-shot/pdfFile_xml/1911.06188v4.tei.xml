<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Xu</surname></persName>
							<email>xu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
							<email>wangzeyu0408@outlook.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc. yinda</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc. yinda</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc. yinda</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
							<email>yugang@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Megvii Inc. yinda</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual tracking problem demands to efficiently perform robust classification and accurate target state estimation over a given target at the same time. Former methods have proposed various ways of target state estimation, yet few of them took the particularity of the visual tracking problem itself into consideration. Based on a careful analysis, we propose a set of practical guidelines of target state estimation for high-performance generic object tracker design. Following these guidelines, we design our Fully Convolutional Siamese tracker++ (SiamFC++) by introducing both classification and target state estimation branch (G1), classification score without ambiguity (G2), tracking without prior knowledge (G3), and estimation quality score (G4). Extensive analysis and ablation studies demonstrate the effectiveness of our proposed guidelines. Without bells and whistles, our SiamFC++ tracker achieves state-of-the-art performance on five challenging benchmarks(OTB2015, VOT2018, La-SOT, GOT-10k, TrackingNet), which proves both the tracking and generalization ability of the tracker. Particularly, on the large-scale TrackingNet dataset, SiamFC++ achieves a previously unseen AUC score of 75.4 while running at over 90 FPS, which is far above the real-time requirement. Code and models are available at: https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generic Visual Tracking aims at locating a moving object sequentially in a video, given very limited information, often only the annotation of the first frame. Being a fundamental build block in various areas of computer vision, the task comes with a variety of applications such as UAV-based monitoring <ref type="bibr" target="#b16">(Mueller, Smith, and Ghanem 2016)</ref> and surveillance system <ref type="bibr" target="#b9">(Kokkeby et al. 2015)</ref>. One unique characteristic of generic object tracking is that no prior knowledge (e.g., the object class) about the object, as well as its surrounding environment, is allowed <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref>. <ref type="figure">Figure 1</ref>: A comparison of our approach (following the guidelines) with state-of-the-art SiamRPN++ tracker (which violates some of the guidelines). Score maps are visualized by red color (i.e. red parts represent regions with high scores and vice versa). In the case of a significant change of target appearance, SiamRPN++ fails due to anchor-object mismatch while our SiamFC++ successes by directly matching between objects. See Section 4 for analysis in detail.</p><p>Tracking problem can be treated as the combination of a classification task and an estimation task <ref type="bibr" target="#b3">(Danelljan et al. 2019)</ref>. The first task aims at providing a robust coarse location of the target via classification. The second task is then to estimate an accurate target state, often represented by a bounding box. While modern trackers have achieved significant progress, surprisingly their methods for the second task (i.e. target state estimation) largely differ. Based on this aspect, previous methods can be roughly divided into three categories. The first category, including Discriminative Correlation Filter (DCF) <ref type="bibr" target="#b6">(Henriques et al. 2014;</ref><ref type="bibr" target="#b1">Bolme et al. 2010)</ref> and <ref type="bibr">SiamFC (Bertinetto et al. 2016)</ref>, employs brutal multi-scale test which is inaccurate <ref type="bibr" target="#b3">(Danelljan et al. 2019</ref>) and inefficiency <ref type="bibr" target="#b10">(Li et al. 2018a)</ref>. Also, the prior assumption that target scale/ratio changes in a fixed rate in adjacent frames often does not hold in reality. For the second category, ATOM <ref type="bibr" target="#b3">(Danelljan et al. 2019</ref>) iteratively refines multiple initial bounding boxes via gradient ascending to estimate the target bounding box <ref type="bibr" target="#b8">(Jiang et al. 2018)</ref>, which yields a significant improvement on accuracy. However, this target estimation method brings not only a heavy computation burden but also many additional hyperparameters (e.g. the number of initial boxes, distribution of initial boxes) that requires careful tuning. The third category is SiamRPN tracker family <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019</ref>) that performs an accurate and efficient target state estimation by introducing the Region Proposal Network (RPN) <ref type="bibr" target="#b19">(Ren et al. 2015)</ref>. However, the pre-defined anchor settings not only introduce ambiguous similarity scoring that severely hinders the robustness (see Section 4) but also need access to prior information of data distribution, which is clearly against the spirit of generic object tracking <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref>.</p><p>Motivated by the aforementioned analysis, we propose a set of guidelines for high-performance generic object tracker design:</p><p>? G1: decomposition of classification and state estimation The tracker should perform two sub-tasks: classification and state estimation. Without a powerful classifier, the tracker cannot discriminate the target from background or distractors, which severely hinders its robustness <ref type="bibr" target="#b25">(Zhu et al. 2018)</ref>. Without an accurate estimation result, the accuracy of the tracker is fundamentally limited <ref type="bibr" target="#b3">(Danelljan et al. 2019)</ref>. Those brutal multi-scale test approaches largely ignore the latter task, suffering from inefficiency and low accuracy. ? G2: non-ambiguous scoring The classification score should represent the confidence score of target existence directly, in the "field of view", i.e. sub-window of the corresponding pixel, rather than the pre-defined settings like anchor boxes. As a negative example, matching between objects and anchors (e.g. the anchor-based RPN branch) is prone to deliver a false positive result, leading to tracking failure (see Section 4 for more details). ? G3: prior knowledge-free Tracking approaches should be free of prior knowledge like scale/ratio distribution, as is proposed by the spirit of generic object tracking <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref>. Dependency on prior knowledge of data distribution exists widely in existing methods, which hinders the generalization ability. ? G4: estimation quality assessment As is shown in previous researches <ref type="bibr" target="#b8">(Jiang et al. 2018;</ref><ref type="bibr" target="#b22">Tian et al. 2019)</ref>, using classification confidence for bounding box selection directly will result in degenerated performance. An estimation quality score independent of classification should be used, as in many previous pieces of research about both object detection and tracking <ref type="bibr" target="#b8">(Jiang et al. 2018;</ref><ref type="bibr" target="#b22">Tian et al. 2019;</ref><ref type="bibr" target="#b3">Danelljan et al. 2019)</ref>. The astonishing accuracy of the second branch (e.g. ATOM and DiMP) largely comes from this guideline. While the others still overlook it, leaving room for further estimation accuracy improvement. Following the guidelines above, we design our SiamFC++ method based on fully-convolutional siamese trackers <ref type="bibr" target="#b0">(Bertinetto et al. 2016)</ref>, where each pixel of the feature map directly corresponds to each translated sub-window on the search image due to its fully convolutional nature. We add a regression head for accurate target estimation, in parallel with the classification head (G1). Since the pre-defined anchor settings is removed, the matching ambiguity (G2) and prior knowledge (G3) about target scale/ratio distribution is also removed. Finally, following G4, an estimation quality assessment branch is added to privilege bounding boxes with high quality.</p><p>Our contribution can be summarized in three-fold: Concretely, by rescaling the search patch into multiple scales and assembling a mini-batch of scaled images, the algorithm picks the scale corresponding to the highest classification score as the predicted target scale in the current frame. This strategy is fundamentally limited since bounding box estimation is inherently a challenging task, requiring a high-level understanding of the pose of objects <ref type="bibr" target="#b3">(Danelljan et al. 2019)</ref>.</p><p>Inspired by DCF and IoU-Net (Jiang et al. 2018), ATOM <ref type="bibr" target="#b3">(Danelljan et al. 2019</ref>)tracks target by sequential classification and estimation. The coarse initial location of the target obtained by classification is iteratively refined for accurate box estimation. The Multiple random initializations of bounding boxes in each frame and multiple back propagations in iterative refinement greatly slows down the speed of ATOM. This approach yields a significant improvement on accuracy but also brings a heavy computation burden. What's more, ATOM introduces many additional hyperparameters that require careful tuning.</p><p>Another branch, named SiamRPN and its succeeding works <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019</ref>) append a Region Proposal Network after a siamese network, achieving a previously unseen accuracy. RPN regresses the location shift and size difference between pre-defined anchor boxes and target location. However, the RPN structure is much more fit for object detection, in which a high recall rate is required, while in visual tracking one and only one object should be tracked. Also, the ambiguous matching between anchor box and object severely hinders the robustness of tracker (see Section 4). Finally, the anchor setting does not comply with the spirit of generic object tracking, requiring pre-defined hyper-parameters describing its shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection Framework</head><p>With many unique characteristics, visual tracking task still has a lot in common with object detection, which makes each one task benefiting from each other possible. For example, the RPN structure first devised in Faster-RCNN <ref type="bibr" target="#b19">(Ren et al. 2015)</ref> achieves astonishing accuracy in SiamRPN <ref type="bibr" target="#b10">(Li et al. 2018a)</ref>. Inheriting from Faster-RCNN <ref type="bibr" target="#b19">(Ren et al. 2015)</ref>, most state-of-the-art modern detectors, named anchor-based detectors, have adopted the RPN structure and the anchor boxes setting <ref type="bibr" target="#b19">(Ren et al. 2015;</ref><ref type="bibr" target="#b15">Liu et al. 2016;</ref><ref type="bibr" target="#b11">Li et al. 2018b</ref>). The anchor-based detectors classifies pre-defined proposals called anchor as positive or negative patches, with an extra offsets regression to refine the prediction of bounding box locations. However, hyper-parameters introduced by anchor boxes (e.g. the scale/ratio of anchor boxes) have shown a great impact on the final accuracy, and require heuristic tuning (Cai and Vasconcelos 2018; <ref type="bibr" target="#b22">Tian et al. 2019)</ref>. Researchers have tried various ways to design anchor-free detectors, like predicting bounding boxes at points near the center of objects <ref type="bibr" target="#b18">(Redmon et al. 2016;</ref><ref type="bibr" target="#b8">Huang et al. 2015)</ref>, or detecting and grouping a pair of corners of a bounding box (Law and Deng 2018). In this paper, we show that a simple pipeline based on a carefully designed guidelines for target state estimation inspired by <ref type="bibr" target="#b7">(Huang et al. 2015;</ref><ref type="bibr" target="#b24">Yu et al. 2016;</ref><ref type="bibr" target="#b22">Tian et al. 2019)</ref> can achieve stateof-the-art tracking performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SiamFC++: Fully Convolutional Siamese Tracker for Object Tracking</head><p>In this section, we describe our Fully Convolutional Siamese tracker++ framework in detail. Our SiamFC++ is based on SiamFC and progressively refined according to the proposed guidelines. As shown in <ref type="figure">Figure 2</ref>, the SiamFC++ framework consists of a siamese subnetwork for feature extraction and a region proposal subnetwork for both classification and regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Siamese-based Feature Extraction and Matching</head><p>Object tracking task can be viewed as a similarity learning problem <ref type="bibr" target="#b10">(Li et al. 2018a)</ref>. Concretely speaking, a siamese network is trained offline and evaluated online to locate a template image within a larger search image. A siamese network consists of two branches. The template branch takes target patch in the first frame as input (denoted as z), while the search branch takes the current frame as input (denoted as x). The siamese backbone, which shares parameters between two branches, performs the same transform on the input z and x to embed them into a common feature space for subsequent tasks. A cross-correlation between template patch and search patch is performed in the embedding space ?:</p><formula xml:id="formula_0">f i (z, x) = ? i (? (z)) ? i (? (x)) , i ? {cls, reg}<label>(1)</label></formula><p>where denotes the cross-correlation operation, ?(.) denotes the siamese backbone for common feature extraction, ? i (.) denotes the task-specific layer and i denotes the subtask type ("cls" for classification and "reg" for regression).</p><p>In our implementation, We use two convolution layers for both ? cls and ? reg after common feature extraction to adjust the common features into task-specific feature space. Note that the extracted features of ? cls and ? reg are of the same size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application of Design Guidelines in Head Network</head><p>Based on SiamFC, we progressively refine each part of our trackers following our guidelines. Following G1, we design both classification head and regression head after the cross-correlation in the embedding space. For each pixel in feature maps, the classification head takes ? cls as input and classifies the corresponding image patch as either one positive or negative patch, while the regression head takes ? reg as input and outputs an extra offsets regression to refine the prediction of bounding box locations. The structure of heads is presented after the crosscorrelation operation of <ref type="figure">Figure 2</ref>.</p><p>Specifically, for classification, location (x, y) on feature map ? cls is considered as a positive sample if its corresponding location s 2 + xs, s 2 + ys on the input image falls into the ground-truth bounding box. Otherwise, it is a negative sample. Here s is the total stride of backbone (s = 8 in this paper). For the regression target of each positive location (x, y) on feature map ? reg , the final layer predicts the distances from the corresponding location s 2 + xs, s 2 + ys to the four sides of the ground-truth bounding box, denoted as a 4D vector t * = (l * , t * , r * , b * ). Hence, the regression targets for location (x, y) can be formulated as</p><formula xml:id="formula_1">l * = ( s 2 + xs) ? x 0 , t * = ( s 2 + ys) ? y 0 r * = x 1 ? ( s 2 + xs), b * = y 1 ? ( s 2 + ys)<label>(2)</label></formula><p>where (x 0 , y 0 ) and (x 1 , y 1 ) denote the left-top and rightbottom corners of the ground-truth bounding box B * associated with point (x, y). Each location (x, y) on the feature map of both classification and regression head, corresponds to an image patch on the input image centered at location s 2 + xs, s 2 + ys . Following G2, we directly classify corresponding image patch and regress the target bounding box at the location, as in many previous tracker <ref type="bibr" target="#b6">(Henriques et al. 2014;</ref><ref type="bibr" target="#b1">Bolme et al. 2010;</ref><ref type="bibr" target="#b0">Bertinetto et al. 2016)</ref>. In other words, our SiamFC++ directly views locations as training samples. While the anchor-based counterparts <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019)</ref>, which consider the location on the input image as the center of multiple anchor boxes, output multiple classification score at the same location and regress the target bounding box with respect to these anchor boxes, leading to ambiguous matching between anchor and object. Although <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019</ref>) have shown superior performance on various benmarks than <ref type="bibr" target="#b6">(Henriques et al. 2014;</ref><ref type="bibr" target="#b1">Bolme et al. 2010;</ref><ref type="bibr" target="#b0">Bertinetto et al. 2016)</ref>, we empirically show that the ambiguous matching could result in serious issues (see Section 4 for more details). In our per-pixel prediction fashion, only one prediction is made at each pixel on the final feature map. Hence it is clear that each classification score directly gives the confidence that the target is in the sub-window of the corresponding pixel and our design is free of ambiguity to this extent.</p><p>Since SiamFC++ does classification and regression w.r.t. the location, it is free of pre-defined anchor boxes, hence free of prior knowledge about target data distribution (e.g. scale/ratio), which comply with G3.</p><p>During the above sections, we do not take the target state estimation quality into consideration yet and directly use classification score to select the final box. That could cause the degradation of localization accuracy, as <ref type="bibr" target="#b8">(Jiang et al. 2018)</ref> shows that classification confidence is not well correlated with the localization accuracy. According to the analysis in <ref type="bibr" target="#b15">(Luo et al. 2016)</ref>, input pixels around the center of a sub-window will have larger importance on the corresponding output feature pixel than the rest. Thus we hypothesize that feature pixels around the center of objects will have a better estimation quality than others. Following G4, we add a simple yet effective quality assessment branch similar to <ref type="bibr" target="#b22">(Tian et al. 2019;</ref><ref type="bibr" target="#b8">Jiang et al. 2018</ref>) by appending a 1 ? 1 convolution layer in parallel with the 1 ? 1 convolution classification head, as shown in the right part of <ref type="figure">Figure 2</ref>. The output is supposed to estimate the Prior Spatial Score (PSS) which is defined as follows:</p><formula xml:id="formula_2">PSS * = min(l * , r * ) max(l * , r * ) ? min(t * , b * ) max(t * , b * )<label>(3)</label></formula><p>Note that PSS is not the only choice for quality assessment.</p><p>As a variant, we can also predict the IoU score between predicted boxes and ground-truth boxes similar to <ref type="bibr" target="#b8">(Jiang et al. 2018)</ref>:</p><formula xml:id="formula_3">IoU * = Intersection(B, B * ) Union(B, B * )<label>(4)</label></formula><p>where B is the predicted bounding box and B * is its corresponding ground-truth bounding box. During inference, the score used for final box selection is computed by multiplying the PSS with the corresponding predicted classification score. In this way, those bounding boxes far from the center of objects will be downweighted seriously. Thus the tracking accuracy is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Objective</head><p>We optimize a training objective as follows:</p><formula xml:id="formula_4">L ({p x,y } , q x,y , {t x,y }) = 1 N pos x,y L cls p x,y , c * x,y + ? N pos x,y 1 {c * x,y &gt;0} L quality q x,y , q * x,y + ? N pos x,y 1 {c * x,y &gt;0} L reg t x,y , t * x,y<label>(5)</label></formula><p>where 1 {?} is the indicator function that takes 1 if the condition in subscribe holds and takes 0 if not, L cls denote the focal loss <ref type="bibr" target="#b14">(Lin et al. 2017)</ref> for classification result, L quality denote the binary cross entropy (BCE) loss for quality assessment and L reg denote the IoU loss <ref type="bibr" target="#b24">(Yu et al. 2016</ref>) for bounding box result. We assign 1 to c * x,y if (x, y) is considered as a positive sample, and 0 if as a negative sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Implementation Details</head><p>Model settings In this work, we implement two versions of trackers with different backbone architectures: the one that adopts the modified version of AlexNet in the previous literature <ref type="bibr" target="#b0">(Bertinetto et al. 2016)</ref>, denoted as SiamFC++-AlexNet, and another one that uses GoogLeNet , denoted as SiamFC++-GoogLeNet. With lower computation cost, the later achieves the same or even better performance(see Section 4) on tracking benchmark than same previous methods using ResNet-50 <ref type="bibr" target="#b6">(He et al. 2016)</ref>. Both networks are pretrained on ImageNet (Krizhevsky, Sutskever, and Hinton 2012), which has been proven practical for tracking task <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018</ref>). We will release the code to facilitate further researches.</p><p>Training data We adopt ILSVRC-VID/DET <ref type="bibr" target="#b20">(Russakovsky et al. 2015)</ref>, COCO <ref type="bibr" target="#b13">(Lin et al. 2014</ref>) , Youtube-BB <ref type="bibr" target="#b17">(Real et al. 2017)</ref>, LaSOT <ref type="bibr" target="#b4">(Fan et al. 2019</ref>) and GOT-10k <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref> as our basic training set. Exceptions w.r.t. to specific benchmarks are detailed in the following subsections. For video datasets, we extract image pairs from VID, LaSOT, and GOT-10k by choosing frame pairs within an interval of less than 100 (5 for Youtube-BB). For image datasets (COCO/Imagenet-DET), we generate training samples by involving negative pairs <ref type="bibr" target="#b25">(Zhu et al. 2018)</ref> as part of training samples to enhance the capacity to distinguish distractors of our model. We perform random shifting and scaling following a uniform distribution on the search image as data augmentation techniques.</p><p>Training phase For the AlexNet version, we freeze the parameters from conv1 to conv3 and fine-tune conv4 and conv5. For those layers without pretraining, we adopt a zerocentered Gaussian distribution with a standard deviation of 0.01 for initialization. We first train our model with for 5 warm up epochs with learning rate linearly increased from 10 ?7 to 2 ? 10 ?3 , then use a cosine annealing learning rate schedule for the rest of 45 epochs, with 600k image pairs for each epoch. We choose stochastic gradient descent (SGD) with a momentum of 0.9 as the optimizer.</p><p>For the version implemented with GoogLeNet, we freeze stage 1 and 2, fine-tune stage 3 and 4, augment the base learning rate to 2 ? 10 ?2 , and multiply the learning rate of parameters in the backbone by 0. The proposed tracker with AlexNet backbone runs at 160 FPS on the VOT2018 short-term benchmark, while the one with GoogleNet backbone runs at about 90 FPS on the VOT2018 short-term benchmark, both evaluated on an NVIDIA RTX 2080Ti GPU.</p><p>Test phase The output of our model is a set of bounding boxes with their corresponding confidence scores s. Scores are penalized based on the scale/ratio change of corresponding boxes and distance away from the target position predicted in the last frame. Then the box with the highest penalized score is chosen and is used to update the target state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From SiamFC towards SiamFC++</head><p>While both employing a per-pixel prediction fashion, there exists a significant performance gap between SiamFC and our SiamFC++. In this subsection we perform an ablation study on VOT2018 dataset, with SiamFC as the baseline, aiming at identifying the key component for the improvement of tracking performance.</p><p>Results are shown in <ref type="table" target="#tab_2">Table 1</ref>. Concretely, in the SiamFC baseline, the tracker only performs classification tasks in its network and the target state estimation is done with multiscale test. We gradually update SiamFC tracker by using extra training data (Line 2/4), applying a better head structure (Line 3), and adding the regression branch for accurate estimation to yield our proposed SiamFC++ tracker (Line 5). We further replace the AlexNet backbone with GoogLeNet which is more powerful to extract visual feature (Line 6).</p><p>The key components for tracking performance can be listed in descending order as follows: the regression branch (0.094), data source diversity (0.063/0.010), stronger backbone (0.026), and better head structure (0.020), where the ?EAO brought by each part is noted in parentheses.</p><p>Note that these are the extra components of SiamRPN++ over SiamFC. After adding all the extra components into SiamFC, Our SiamFC++ achieves superior performance with less computation budget. Also, there are two things worth to mention: 1). the robustness (R) of Line 2 surpasses SiamRPN tracker (0.46 <ref type="bibr" target="#b10">(Li et al. 2018a)</ref>); 2). the R of Line 3 is at the same level of DaSiamRPN (0.337 <ref type="bibr" target="#b25">(Zhu et al. 2018)</ref>) while using less data (without COCO and DET) than the latter. These results indicate that, while the introduction of the RPN module and anchor boxes setting undoubtedly gives better accuracy, its robustness is not improved and even hindered. We owe this to its violation of our proposed guidelines.</p><p>Quality Assessment Choice On GOT-10k val subset, we obtain an AO of 77.8 for the tracker predicting PSS and an AO of 78.0 for the tracker predicting IoU. Experiments have been conducted with SiamFC++-GoogLeNet. We finally choose PSS in this paper as an implementation of our approach for its stability empirically observed across datasets during our experiment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Several Benchmarks</head><p>We test our tracker on several benchmarks and results are gathered in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Results on OTB2015 Benchmark As one of the most classical benchmarks for the object tracking task, the OTB benchmark <ref type="bibr" target="#b23">(Wu, Lim, and Yang 2013)</ref>   the protocol of GOT-10k and only trained our tracker on the train subset. Our tracker with AlexNet backbone reaches an AO of 53.5 surpassing SiamRPN++ by 1.7, while our tracker with GoogLeNet backbone yields 59.5 which is even superior to ATOM that uses online updating method. This result shows the ability of our tracker to generalize even the target classes are unseen during the training phase, which matches the demand of the generic tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on TrackingNet Benchmark</head><p>We evaluate our approach with 511 videos provided in the test split of <ref type="bibr">Track-ingNet (Muller et al. 2018)</ref>. We exclude the Youtube-BB dataset from our training data in order to avoid data leak. As is described in <ref type="bibr" target="#b16">(Muller et al. 2018)</ref>, the evaluation server calculates the following three indexes based on tracking results: success rate, precision, and normalized precision. Our SiamFC++-GoogLeNet outperforms the current stateof-the-art methods (including online-update methods like <ref type="bibr" target="#b3">(Danelljan et al. 2019)</ref>) in both precision and success rate dimensions, while our lightweight version SiamFC++ strikes a balance between performance and the speed. This result is achieved even without Youtube-BB containing a large portion of training data, which shows that the potential of our approach to be independent of large offline training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Trackers that Do not Apply Our Guidelines</head><p>The family of SiamRPN <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019)</ref> has achieved great success in visual tracking these years and drawn much attention from tracking community. Here we use state-of-the-art SiamRPN++ tracker as an example. Despite recent successes of the SiamRPN family, we have found that the SiamRPN tracker and its family do not follow our proposed guidelines entirely.</p><p>? (G2) the classification score of SiamRPN represents the similarity between anchor and object, rather than template object and objects in search image, which may cause matching ambiguity; ? (G3) the design of pre-set anchor boxes needs prior knowledge of the distribution of size and ratio of target; ? (G4) the choice of target state estimation does not take estimation quality into consideration. Note that the SiamRPN family adopts proposal refinement by the regression branch instead of the multi-scale test, and thus achieves astonishing tracking accuracy, which complies with our guideline G1.</p><p>As a consequence of the violation of guideline G2, we empirically find that the SiamRPN family is prone to deliver a false-positive result. In other words, SiamRPN will produce an unreasonable high score for nearby objects or background under large appearance variation of target object. As shown in <ref type="figure">Figure 1</ref>, we can see that SiamRPN++ fails to track the target object by giving very high scores for nearby objects (i.e. a rock or a face) under challenging scenarios like out-of-plane rotation and deformation. We hypothesize that SiamRPN matches objects and anchors rather than the object itself, which may deliver drifts and thus hinders its robustness. On the contrary, our proposed SiamFC++, which matches between template objects and objects in search image directly, gives accurate score predictions and successfully tracks the target.</p><p>To verify our hypothesis, we record the max score produced by SiamRPN++ and our proposed SiamFC++ on VOT2018 dataset. We then split them according to the tracking result, e.g., successful or failed. On VOT2018, a tracking result is considered failed if its overlap with the groundtruth box is zero. Otherwise, it is considered successful. The result is visualized in the first row in <ref type="figure" target="#fig_2">Figure 3</ref>. Comparing SiamRPN++ and SiamFC++ scores, we can see that most classification score of SiamRPN++ follows similar and highly overlapped distributions, successful or not, while the classification score of our SiamFC++ of failure state exhibit very different pattern with that of a successful state.</p><p>Another factor contributing to the ambiguity in SiamRPN++ is that the feature matching process is done with patches of fixed aspect ratio (multiple patches with different ratios will bring non-negligible computation cost), while each pixel of the feature after matching is assigned anchors whose aspect ratio varies.</p><p>As for the violation of G3, the performance of SiamRPN varies as the scales and ratios of anchors vary. As is shown in <ref type="table" target="#tab_6">Table 3</ref> from <ref type="bibr" target="#b10">(Li et al. 2018a)</ref>, three different ratio settings are tried and the performance of SiamRPN varies when using different anchor settings. Thus the best performance is achieved only by accessing prior knowledge of data distribution, which is against the spirit of generic object tracking <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref>.</p><p>Besides, in the second row of <ref type="figure" target="#fig_2">Figure 3</ref>, we also plot the histogram of SiamRPN++ statistics of IoU between output bounding box and ground truth and the histogram between anchor and ground truth, in both success and failure state. As is shown from the IoU distribution, the prior knowledge given by anchor settings (violation of G3) leads to a bias in target state estimation. Concretely, the predicted box of SiamRPN++ tends to overlap more with the anchor box than with the ground truth box which can lead to performance degradation.</p><p>As for the violation of G4, we can see that the SR .5 and SR .75 of SiamRPN++ on GOT-10k benchmark are 7.7 and 15.4 points lower than those of SiamFC++, respectively. In GOT-10k, the Success Rate (SR) measures the percentage of successfully tracked frames where the overlaps exceed a predefined threshold (i.e., 0.5 or 0.75). The higher the threshold, the more accurate the tracking result. Hence SR is a solid indicator for estimation quality. The SR .75 of SiamRPN++ is much lower than that of SiamFC++, indicating the lower estimation quality of SiamRPN++ caused by the violation of guideline G4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a set of guidelines for target state estimation in tracker design, by analyzing the unique characteristics of visual tracking tasks and the flaws of former trackers. Following these guidelines, we propose our approach that provides effective methods for both classification and target state estimation (G1), giving classification score without ambiguity (G2), tracking without prior knowledge (G3), and being aware of estimation quality (G4). We verify the effectiveness of proposed guidelines by extensive ablation study. And we show that our tracker based on these guidelines reaches state-of-the-art performance on five challenging benchmarks, while still running at 90 FPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Backbone Choice</head><p>Although with smaller MACs and parameter amounts (see <ref type="table" target="#tab_6">Table 3</ref> for statistics), our proposed tracker (version with GoogLeNet) achieves the same or even better performance of previous trackers using ResNet-50 as backbone across different benchmarks. On the other hand, this choice contributes significantly to the efficiency which is crucial for visual tracking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Test Phase Behavior</head><p>The output of our model is a set of bounding boxes B with their corresponding confidence scores s in the shape of N ? N ? 4 and N ? N , respectively. Following the tracking strategy of the SiamRPN family <ref type="bibr" target="#b0">(Bertinetto et al. 2016;</ref><ref type="bibr" target="#b10">Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018)</ref>, we adopt the following strategies to render our tracking result more robust.</p><p>? First, the score map s output by the model is multiplied by a penalization term p according to ratio(r and r ) and size changes(s and s ) of their corresponding boxes. Namely, p = e </p><p>where k is a hyperparameter that controls the magnitude of the penalization, denotes element-wise multiplication since the ratio(r and r ) and the size changes(s and s ) are all matrices in the shape of N ? N (refer to <ref type="bibr" target="#b10">(Li et al. 2018a</ref>) for more details).</p><p>? Second, a cosine window ? is added with a window influence coefficient ? to suppress large displacement since smooth motion is assumed.</p><formula xml:id="formula_6">?(x) = 0.5 ? 0.5 cos 2? ? dist(x, x c ) (N ? 1)/2 ? 1 s = s ? (1 ? ?) + ? ? ? (7)</formula><p>where ? is a hyperparameter that controls the influence of window, x is a point coordinate on the score map, x c is the center point coordinate on the score map, and dist(i, j) denotes the Euclidean distance between the point i and the point j.</p><p>? Third, the point with the highest score ons is chosen as x * and its corresponding bounding box B[x * ] is selected as the estimation B curr for the current frame.</p><p>x * = arg max</p><formula xml:id="formula_7">x?[0..N ?1] ?2s [x] B curr = B[x * ]<label>(8)</label></formula><p>where [0..N ? 1] ?2 refers to the coordinate space of the score map. ? Finally, the target size is updated by linear interpolation to make the shape change smoothly.  where B prev and B pred denotes the previous prediction and final prediction of target bounding box, respectively. Here ? is a hyperparameter to control the target size updating speed, multiplied by the penalized score on x * to make the box size updating speed match its confidence. The .size operation denotes the indexing on size dimensions(i.e. width and height).</p><formula xml:id="formula_8">? = s[x * ] ? ? B pred .size = (1 ? ? ) ? B prev .size + ? ? B curr .size<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Study over SiamFC++</head><p>Data source diversity We conduct our ablation study on the diversity of data sources to identify the influence of data diversity on tracker performance. We choose three different training data settings varying from low to high diversity. The results in Line 1, 2, and 3 of <ref type="table" target="#tab_7">Table 4</ref> show that the augmentation of the diversity of training data significantly brings higher performance, which demonstrates the ability of our method to exploit rich offline training data.</p><p>Backbone capacity To unveil the power of visual representation brought by deep CNN networks, we conduct ablation experiments by replacing the backbone by a larger base model, GoogLeNet-v2 <ref type="bibr" target="#b8">Ioffe and Szegedy 2015)</ref>. We adopt the technique of cropping the border of the backbone output feature map with a width of 4, which has been introduced in <ref type="bibr" target="#b12">(Li et al. 2019)</ref> for deep CNN architecture in the tracking task. The results in <ref type="table" target="#tab_7">Table 4</ref> show that the tracker with GoogLeNet outperforms the one with AlexNet by 0.017 on VOT2018 (Line 3 and 4) and 3.2 on GOT-10k (Line 10 and 6), which indicates that a stronger backbone brings a better tracking result. Additionally, the gain in tracking performance is mainly brought by the enhancement of accuracy.</p><p>Structure of head Due to the fact that padding may cause the drop of tracking performance by introducing noise, previous RPN-based siamese trackers <ref type="bibr" target="#b10">(Li et al. 2018a;</ref><ref type="bibr" target="#b25">Zhu et al. 2018;</ref><ref type="bibr" target="#b12">Li et al. 2019)</ref> do not use conv3 ? 3 layer after the cross-correlation operation in order to avoid the shrinkage of response region(the region on the search image which has a corresponding point on the score map) which may hinder the tracking performance, particularly in case of fast motion. However, in our SiamFC++ design, a trade-off can be achieved between the real search region size and the head capacity. An ablation study on the head structure is conducted and the results are shown in Line 5, 6, and 9 of   that the trackers see a search region of the same size for a fair comparison. The results show that the performance gets better as we add more conv3 ? 3 layers in the RPN head(0.4 brought by the second conv3 ? 3 layer and 0.3 brought by the third one) despite the shrinkage of response region. This proves that the gain from the enhancement of head capacity is superior to the gain from the increment of the response region range. In addition, the qualitative result in <ref type="figure" target="#fig_4">Figure 4</ref> also reveals that even with a small part of the target object falling in the score map covered zone, the model is able to predict the correct classification/regression response. Moreover, the enhanced robustness to pose change, occlusion, and the presence of distractors is also shown in the qualitative result.</p><p>Finally, we choose the tracker with two conv3 ? 3 layers in the head for it strikes a balance between the performance and the speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Data Distribution of Different Datasets</head><p>Data distribution varies between different datasets. We collect statistics over scale/ratio distribution across the involved datasets and the results are presented in <ref type="table" target="#tab_10">Table 5</ref>. Each dataset does not share the same diversity and the data distribution itself can be a sort of prior knowledge. Seeing that the access to prior knowledge is against the spirit of generic object tracking <ref type="bibr" target="#b8">(Huang, Zhao, and Huang 2018)</ref>, tracker design should prevent the involvement of factors related to scale/ratio information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Comment over Anchor-based Scoring: A Maxout Perspective</head><p>Under the post-processing scheme described in Section B, the scoring process in anchor-based tracking methods can be viewed as a maxout <ref type="bibr" target="#b5">(Goodfellow et al. 2013</ref>) of scores of foreground (scores of anchors) at each pixel location. According to the practice in the face detection task <ref type="bibr">(Tang et al. 2018)</ref>, the maxout of scores of foreground prefers to recall and thus avoid false negatives (FNs), while the maxout of scores of background is prone to select and thus suppress false positives (FPs).</p><p>In the SOT task, an FP is usually worse than an FN. The reason is that FP is likely to cause drift, while an FN just returns low scores at every pixel and the predicted bounding box of the target will be preserved as the same in the last frame due to the cosine window of the aforementioned postprocessing (see Section B for detail).</p><p>In this perspective of maxout, we can see that the avoidance of ambiguity brought by the per-pixel scoring avoids the maxout of scores of foreground, which contributes to the enhancement of the robustness of our proposed SiamFC++ tracker.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>correlation ?: element-wise production ?: argmax (taking left w.r.t. right) Figure 2: Our SiamFC++ pipeline (AlexNet version). Boxes denote feature maps. The intermediate layers in the common feature extractor have been omitted for clarity. For score visualization, deep green color represents the corresponding region on input image of score map, while the brightness of red color denotes the magnitude of scores (min-max normalized). Better viewed in color with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 w.r.t the global learning rate. We also reduce the number of image pairs per epoch to 300k, reduce the total epoch to 20 (thus 5 for warming-up, and 15 for training) and unfreeze the parameters in backbone at the 10th epoch to avoid overfitting. For the experiment on LaSOT benchmark (Fan et al. 2019) (protocol II), we freeze the parameters in the backbone and further reduce the number of image pairs per epoch to 150k so that the training with the relatively smaller amount of training data could be stabilized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The first row: score distribution of SiamRPN++ and SiamFC++. The second row: IoU distribution of SiamRPN++ in both success/failure state. Better visualized when zoomed in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>4: Ablation study: better understanding of SiamFC++. Experiments have been conducted on VOT-2018(A/R/EAO) and Tracking result by our tracker with multiple conv3 ? 3 layers in the head. The first two images show its performance for fast-motion objects, the third image presents its capacity to deal with pose change, and the fourth image demonstrates the robustness in case of occlusion and the presence of distractors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation study: from SiamFC towards SiamFC++. Experiments have been conducted on VOT-2018 (A/R/EAO). ?EAO denotes the augmentation of EAO w.r.t. the baseline (Line 1).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Following Protocol II under which trackers are trained on LaSOT train subset and evaluated on LaSOT test subset, the proposed SiamFC++ tracker achieves better performance, even w.r.t. those who have better performance than ours on the VOT2018 benchmark. This reveals the fact that the scale of the benchmark influences the rank of trackers.</figDesc><table><row><cell></cell><cell cols="2">Trackers</cell><cell>SiamFC (2016)</cell><cell>ECO (2017)</cell><cell>MDNet (2016)</cell><cell>SiamRPN++ (2019)</cell><cell>ATOM (2019)</cell><cell>SiamFC++-AlexNet</cell><cell>SiamFC++-GoogLeNet</cell></row><row><cell></cell><cell cols="2">OTB-15 Success</cell><cell>58.2</cell><cell>70.0</cell><cell>67.8</cell><cell>69.6</cell><cell>66.9</cell><cell>65.6</cell><cell>68.3</cell></row><row><cell></cell><cell></cell><cell>A</cell><cell cols="2">0.503 0.484</cell><cell>-</cell><cell>0.600</cell><cell>0.590</cell><cell>0.556</cell><cell>0.587</cell></row><row><cell></cell><cell>VOT-18</cell><cell>R</cell><cell cols="2">0.585 0.276</cell><cell>-</cell><cell>0.234</cell><cell>0.204</cell><cell>0.183</cell><cell>0.183</cell></row><row><cell></cell><cell></cell><cell>EAO</cell><cell cols="2">0.188 0.280</cell><cell>-</cell><cell>0.414</cell><cell>0.401</cell><cell>0.400</cell><cell>0.426</cell></row><row><cell></cell><cell cols="2">LaSOT Success</cell><cell>33.6</cell><cell>32.4</cell><cell>39.7</cell><cell>49.6</cell><cell>51.5</cell><cell>50.1</cell><cell>54.4</cell></row><row><cell>provides a fair test for all families of trackers. We conduct experiments on</cell><cell>GOT</cell><cell>SR .5 SR .75 AO</cell><cell>35.3 9.8 34.8</cell><cell>30.9 11.1 31.6</cell><cell>30.3 9.9 29.9</cell><cell>61.8 32.5 51.8</cell><cell>63.4 40.2 55.6</cell><cell>57.7 32.3 49.3</cell><cell>69.5 47.9 59.5</cell></row><row><cell>OTB2015 (Wu, Lim, and Yang 2013) which contains 100</cell><cell></cell><cell>Prec.</cell><cell>51.8</cell><cell>49.2</cell><cell>56.5</cell><cell>69.4</cell><cell>64.8</cell><cell>64.6</cell><cell>70.5</cell></row><row><cell>videos for tracker performance evaluation. With a success score of 0.682, our tracker reaches the state-of-the-art level w.r.t. other trackers in comparison.</cell><cell>T-Net</cell><cell cols="2">Norm. Prec. 65.2 Succ. 55.9 FPS 86</cell><cell>61.8 55.4 8</cell><cell>70.5 60.6 1</cell><cell>80.0 73.3 35</cell><cell>77.1 70.3 30</cell><cell>75.8 71.2 160</cell><cell>80.0 75.4 90</cell></row><row><cell>back-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bone yields a comparable score. Besides, our tracker has a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>significant advantage in the robustness among the trackers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>in comparison. To the best of our knowledge, this is the first</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tracker that achieves an EAO of 0.400 on VOT2018 (Kris-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tan et al. 2018) benchmark while running at a speed over 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPS, which demonstrate its potential of being applied in real</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>production cases.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results on LaSOT Benchmark With a large number of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>video sequences (1400 sequences under Protocol I while</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>280 under Protocol II), LaSOT (Fan et al. 2019) (Large-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>scale Single Object Tracking) benchmark makes it impos-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>sible for trackers to overfit the benchmark, which achieves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the purpose of testing the real performance of object track-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing. Results on GOT-10k Benchmark For target class gener-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>alization testing, we train and test our SiamFC++ model on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GOT-10k (Huang, Zhao, and Huang 2018) (Generic Object</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tracking-10k) benchmark. Not only as a large-scale dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Results on VOT Benchmark VOT2018 (Kristan et al. 2018) contains 60 video sequences with several challeng- ing topics including fast motion, occlusion, etc. We test our tracker on this benchmark and present the results in Ta- ble 2. Both versions of our trackers reaching comparable scores w.r.t. current state-of-the-art trackers, the tracker with AlexNet backbone outperforms other trackers with the same tracking speed and while the tracker with GoogLeNet(10,000 videos in train subset and 180 in both val and test subset), it also gives challenges in terms of the requirement of category-agnostic for generic object trackers as there is no class intersection between train and test subsets. We follow</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on several benchmarks. T-Net denotes TrackingNet. Top-3 results of each dimension (row) are col- ored in red, green, and blue, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Backbone characteristics. Input sizes for trackers indicated here are the real sizes during the test phase. In SiamFC++, the last stage of GoogLeNet have been pruned which results in a smaller parameter amount.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>The input size of the search image is fixed at 303 ? 303 to ensure</figDesc><table><row><cell>No.</cell><cell>VID&amp;Youtube</cell><cell>COCO&amp;Det</cell><cell>LaSOT</cell><cell>GOT</cell><cell>Backbone</cell><cell>Head type</cell><cell>Head structure</cell><cell>Quality assessment</cell><cell>A</cell><cell>R</cell><cell>EAO</cell><cell>SR .5</cell><cell>AO</cell></row><row><cell>1</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>AlexNet</cell><cell>cls+reg</cell><cell>3?conv3 ? 3</cell><cell>PSS</cell><cell>0.561</cell><cell>0.258</cell><cell>0.352</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>AlexNet</cell><cell>cls+reg</cell><cell>3?conv3 ? 3</cell><cell>PSS</cell><cell>0.548</cell><cell>0.215</cell><cell>0.378</cell><cell>-</cell><cell>-</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AlexNet</cell><cell>cls+reg</cell><cell>3?conv3 ? 3</cell><cell>PSS</cell><cell>0.556</cell><cell>0.183</cell><cell>0.400</cell><cell>-</cell><cell>-</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>2?conv3 ? 3</cell><cell>PSS</cell><cell>0.587</cell><cell>0.183</cell><cell>0.426</cell><cell>-</cell><cell>-</cell></row><row><cell>5</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>1?conv3 ? 3</cell><cell>PSS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.4</cell><cell>76.2</cell></row><row><cell>6</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>2?conv3 ? 3</cell><cell>PSS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.8</cell><cell>77.8</cell></row><row><cell>7</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>2?conv3 ? 3</cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.6</cell><cell>77.1</cell></row><row><cell>8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>2?conv3 ? 3</cell><cell>IoU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>89.3</cell><cell>78.0</cell></row><row><cell>9</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>GoogLeNet</cell><cell>cls+reg</cell><cell>3?conv3 ? 3</cell><cell>PSS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.0</cell><cell>78.1</cell></row><row><cell>10</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>AlexNet</cell><cell>cls+reg</cell><cell>3?conv3 ? 3</cell><cell>PSS</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>87.2</cell><cell>74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Scale/ratio statistics on involved datasets. Scale refer to the relative scale(i.e. scale changes T = s T /s T ?1 where s T = w T ? h T ) while ratio keeps its original definition(weight/height).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertinetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eco: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6638" to="6646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danelljan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lasot: A highquality benchmark for large-scale single object tracking</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5374" to="5383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ICML13, III1319III1327. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Densebox: Unifying landmark localization with end to end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04874</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11981</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
	<note type="report_type">JMLR.org</note>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Methods for autonomous tracking and surveillance. US Patent 9,026,272</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kokkeby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detnet: Design backbone for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
	<note>European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smith</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large highprecision human-annotated data set for object detection in video</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5296" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Russakovsky et al. 2015</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramidbox: A context-assisted single shot face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="797" to="813" />
		</imprint>
	</monogr>
	<note>Proceedings of the European Conference on Computer Vision (ECCV)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01355</idno>
		<title level="m">Fcos: Fully convolutional one-stage object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lim</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="516" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="101" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
