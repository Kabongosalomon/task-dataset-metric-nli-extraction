<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GEM-2: NEXT GENERATION MOLECULAR PROPERTY PREDICTION NETWORK BY MODELING FULL-RANGE MANY-BODY INTERACTIONS A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihang</forename><surname>Liu</surname></persName>
							<email>liulihang@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglong</forename><surname>He</surname></persName>
							<email>hedonglong@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhuo</forename><surname>Zhang</surname></persName>
							<email>zhangshanzhuo@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>wang.fan@baidu.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Xiaomin Fang ? Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Jingzhou He Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Hua Wu Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GEM-2: NEXT GENERATION MOLECULAR PROPERTY PREDICTION NETWORK BY MODELING FULL-RANGE MANY-BODY INTERACTIONS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Molecular property prediction ? Many-body interactions ? Full-range interactions</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Molecular property prediction is a fundamental task in the drug and material industries. Physically, the properties of a molecule are determined by its own electronic structure, which is a quantum many-body system and can be exactly described by the Schr?dinger equation. Full-range manybody interactions between electrons have been proven effective in obtaining an accurate solution of the Schr?dinger equation by classical computational chemistry methods, although modeling such interactions consumes an expensive computational cost. Meanwhile, deep learning methods have also demonstrated their competence in molecular property prediction tasks. Inspired by the classical computational chemistry methods, we design a novel method, namely GEM-2, which comprehensively considers full-range many-body interactions in molecules. Multiple tracks are utilized to model the full-range interactions between the many-bodies with different orders, and a novel axial attention mechanism is designed to approximate the full-range interaction modeling with much lower computational cost. Extensive experiments demonstrate the overwhelming superiority of GEM-2 over multiple baseline methods in quantum chemistry and drug discovery tasks. The ablation studies also verify the effectiveness of the full-range many-body interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Molecular property prediction is a fundamental task in drug and material industries, evaluating the physical, chemical, and biological properties to assist the researchers in making decisions. In essence, the properties of a molecule are determined by its own electronic structure. Theoretically, the electronic structure of molecules can be exactly described by the Schr?dinger equation. A molecule with multiple electrons is a many-body system with quantum full-range interactions <ref type="bibr" target="#b9">[10]</ref>. Modeling the full-range many-body interactions have been proven critical in obtaining an accurate solution of the Schr?dinger equation by classical computational chemistry methods. Two representative calculation methods to approximate the solution of the Schr?dinger equation are density functional theory (DFT) and coupled-cluster theory (CC). The core idea of DFT is to represent all electrons with a single electron density functional, thus reducing the dimension of the problem <ref type="bibr" target="#b3">[4]</ref>. Previous work <ref type="bibr" target="#b30">[31]</ref> argues that the error of DFT can be further improved by using functionals considering full-range interactions and/or hybrid functionals with higher-order gradient terms. The CC is currently regarded as the gold standard for many quantum problems <ref type="bibr">[1]</ref>. The calculation of CC is based on Interactions between many-bodies <ref type="figure">Figure 1</ref>: Demonstration of many-body interactions and full-range interactions for molecular modeling. Many-body interactions include not only (a) the interactions between the 1-bodies (atoms) but also (b) the interactions between the higher-order many-bodies, e.g., 2-bodies (atom pairs). Full-range interactions are the interactions between any two many-bodies, including both (1) short-range interactions, e.g., interactions between atoms that are spatially close or connected by chemical bonds, and (2) long-range interactions, i.e., many-bodies that are not directly connected. a solution of the ground state of the system and is further stacked with a series of cluster operators to approximate the interactions between all electrons. These cluster operators can be extended from single and/or double to infinite order to increase the accuracy <ref type="bibr" target="#b42">[43]</ref>. Although DFT and CC have completely different theoretical frameworks, they share the same idea: adding a term to describe the full-range and complex many-body interactions can further improve the accuracy of the solutions. Owing to the great success of deep learning methods in broad fields, many studies attempted to migrate widely used deep learning models to the field of molecular property prediction. Mainstream methods treat the compound as a graph and use a graph neural network (GNN) to model the interactions between nodes (nodes generally refer to atoms rather than electrons). Most of the methods <ref type="bibr">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> only take consideration of the interactions between atoms (1-bodies) in the molecule, as shown in column (a) in <ref type="figure">Figure 1</ref>. Meanwhile, as demonstrated in column (b) in <ref type="figure">Figure 1</ref>, several advanced works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> start to incorporate interactions between atom pairs to utilize angular information for the purpose of geometric learning, which can be seen as interactions between 2-bodies. Especially, our previous work GEM <ref type="bibr" target="#b12">[13]</ref> has demonstrated that interactions between the chemical bonds can also benefit the general molecular property prediction tasks. However, the studies that consider many-bodies mainly focus on the short-range interactions between the many-bodies, e.g., the interactions between the atoms that are close in space or connected by chemical bonds, as shown in row (1) in <ref type="figure">Figure 1</ref>. That means the long-range interactions, e.g., the many-bodies that are not directly connected (as shown in row <ref type="bibr">(2)</ref> in <ref type="figure">Figure 1</ref>), are disregarded. Although increasing deep learning methods <ref type="bibr">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> have verified that directly modeling the full-range interactions (including the short-range and long-range interactions) between the atoms (1-bodies) can effectively enhance the accuracy for property prediction, there is still no investigation in how to design a model architecture that can comprehensively integrate the full-range many-body interactions. We argue that the consideration of the full-range many-body interactions is indispensable because this is not only an inherent requirement when solving the Schr?dinger equation but has also been adopted by many computational chemistry methods as a basis to improve accuracy. Additionally, thoroughly describing the full-range many-body interactions is computationally expensive, especially when there are many atoms in the molecules, which is a huge challenge for the design of the learning mechanism.</p><p>To this end, we propose a novel molecular modeling framework, namely GEM-2, to comprehensively model the full-range many-body interactions for property prediction. GEM-2 introduces multiple tracks, where the m th -track learns the interactions between the m-bodies. For each track, a novel and efficient many-body axial attention mechanism is designed to capture the full-range interactions, i.e., the interactions between any two many-bodies. The designed mechanism approximates the effect of the way that directly models full-range interactions through stacking axial GEM-2 A PREPRINT attentions on multiple axes. We also exchange messages across different tracks, and in this way, the many-bodies with different orders can exploit the knowledge of each other.</p><p>To verify the effectiveness of GEM-2, we compare it with several competitive baselines on the tasks of quantum chemistry and drug discovery. In general, GEM-2 significantly outperforms the previous SOTA (state-of-the-art) methods on these tasks. Extensive ablation studies also demonstrate the advantages of incorporating the full-range many-body interactions for molecular property prediction.</p><p>Our contributions can be summarized as follows:</p><p>? We investigate the importance of full-range many-body interactions for molecular modeling and incorporate them in property prediction.</p><p>? A novel network architecture with multiple tracks is proposed to describe the full-range many-body interactions, and especially an efficient many-body axial attention mechanism is designed to model the full-range interactions.</p><p>? GEM-2 significantly outperforms the competitive baselines on quantum chemistry and drug discovery benchmarks. Extended ablation studies verify the contributions of full-range many-body interactions for molecular modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Framework of GEM-2</head><p>To start with, we define a m-body in a molecule as a group of m atoms that acts as a whole, denoted as (i 1 , ? ? ? , i m ), with i 1 , ..., i m representing the indexes of the atoms in the molecule. m is also called the order of that many-body. Aiming to incorporate the full-range many-body interactions into molecular modeling, we define a molecule with N atoms as</p><formula xml:id="formula_0">{V (m) }| M +1 m=1 . V (m) = {(i 1 , ? ? ? , i m )}| 1?i1,??? ,</formula><p>im?N is a set that contains all possible m-bodies of the molecule, i.e., the full permutation of m atoms, and the size of V (m) is N m . Especially, V <ref type="bibr">(1)</ref> represents the atom set, V <ref type="bibr">(2)</ref> represents the set of atom pairs, and V <ref type="bibr" target="#b2">(3)</ref> represents the set of atom triplets. We attempt to learn the interactions between the 1 st -bodies in V <ref type="bibr">(1)</ref> , between the 2 nd -bodies in V <ref type="bibr">(2)</ref> , ? ? ? , and between the M th -bodies in V (M ) , respectively. Note that the information of the (M + 1) th -bodies V (M +1) are additionally exploited to assist the modeling of many-body interactions.</p><p>The input features of a m-body (i 1 , ? ? ? , i m ) and a m-body set V (m) are denoted as x</p><formula xml:id="formula_1">(m) i1,??? ,im ? R c (m) feat and X (m) ? R N m ?c (m) feat respectively, where c (m)</formula><p>feat is the feature size of the m-body. For example, X (1) contains the features of single atoms, such as atom type; X <ref type="bibr">(2)</ref> contains the features of atom pairs, such as chemical bond and spatial distance between two atoms; and X (3) contains the features of atom triplets, such as angle formed by three atoms. (Please refer to <ref type="table">Table 3</ref> in the Appendix for a detailed feature list.)</p><p>Modeling the full-range many-body interactions has been proven effective by the classical computational chemical methods for describing molecules. We define the full-range many-body interactions of the m th -order as the interactions between any two m-bodies (i 1 , ? ? ? , i m ) and (i 1 , ? ? ? , i m ) in V (m) . Since V (m) contains N m m-bodies, the full-range many-body interactions, i.e., {((i 1 , ? ? ? , i m ), (i 1 , ? ? ? , i m ))}| 1?i1,??? ,im,i 1 ,??? ,i m ?N , contains N 2m interactions in theory. Consequently, directly learning the full-range many-body interactions is computationally expensive, especially when N and m are growing.</p><p>To effectively and also efficiently model the full-range many-body interactions, we propose a novel molecular modeling framework, namely GEM-2. The overall architecture of GEM-2 is demonstrated in <ref type="figure" target="#fig_3">Figure 2a</ref>. GEM-2 contains L stacked Optimus blocks to iteratively update the many-body representations {Z (m) }| M m=1 . Z (m) ? R N m ?cm and z (m) i1,??? ,im ? R cm denote representations of the m-body set V (m) and a m-body (i 1 , ? ? ? , i m ), respectively, where c m is the hidden size of the representations. The Optimus blocks are designed to learn the full-range interactions between the many-bodies with the same order and also transfer the messages between the many-bodies with different orders. First, the transformation of the input features {X (m) }| M m=1 are taken as the initial representations {Z (m) }| M m=1 . The Optimus blocks update the representations, and finally, the representations produced by the last Optimus block are utilized for molecular property prediction. The architecture details will be introduced in the following subsections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEM-2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-body Axial Attention</head><p>(on 1 st axis)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-body representation</head><p>( , , , )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-body representation</head><p>( , , , ) (a) The overall framework of GEM-2. First, a molecule is described by the representations of many-bodies of multiple orders. Then, Optimus blocks are designed to update the representations. Each Optimus block contains M tracks, and the m-th track contains a stack of many-body axial attentions to model the full-range interactions between the m-bodies. The many-body axial attentions and the Low2High module also play the roles of exchanging messages across the tracks. Finally, the molecular property prediction is made by pooling over the 1-body representations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-body Axial Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimus Block</head><p>As shown in <ref type="figure" target="#fig_3">Figure 2a</ref>  i1,??? ,im in the tensor representing a m-body (i 1 , ? ? ? , i m ). For full-range interactions shown on the right, each manybody (element) of order m collects the messages from all the many-bodies of the same order (all elements in the tensor). While for axial attention shown on the left, the target many-body (marked as purple) gradually aggregates the messages (marked as yellow) from all the many-bodies through m stacked axial attention on multiple axes, approximating the effect of full attention that collects the messages of all the many-bodies in one shot with a much lower computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Many-body Axial Attention</head><p>Many-body Axial Attention is designed to efficiently learn the full-range many-body interactions of the same order as well as incorporating the message from the higher-order track. Note that the many-body representations of order m in GEM-2 are organized as a multi-dimensional tensor Z (m) ? R N m ?cm , with each element z (m) i1,??? ,im in the tensor representing a m-body (i 1 , ? ? ? , i m ). Since the full-range many-body interactions, i.e., {((i 1 , ? ? ? , i m ), (i 1 , ? ? ? , i m ))}| 1?i1,??? ,im,i 1 ,??? ,i m ?N , contains N 2m interactions, directly modeling these interactions in one shot is computational expensive. Inspired by axial attention <ref type="bibr" target="#b15">[16]</ref>, we apply attention along a single axis of the tensor and run multiple times on different axes, learning the interactions of m-body pairs that share m ? 1 atoms, rather than applying attention to the flattened tensor elements.</p><p>The illustration of the principle of Many-body Axial Attention for full-range interaction modeling is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. For the sake of simplicity, we take the m th -order track and a target m-body (i 1 , ? ? ? , i m ) for example. In order to learn the interactions between the target m-body to all the m-bodies in V (m) (all the elements in the tensor), the original full attention (as shown on the right of <ref type="figure" target="#fig_1">Figure 3</ref>) takes that target m-body as the query of the attention mechanism, and all the m-bodies in V (m) as keys and values of the attention mechanism. While for our proposed Many-body Axial Attention (as shown on the left of <ref type="figure" target="#fig_1">Figure 3</ref>), we leverage m stacked axial attentions that operate on multiple axes (from the 1 st axis to the m th axis) to approximate the effect of full attention. The axial attention on the k th axis takes the target m-body as the query of the attention mechanism (marked by purple in <ref type="figure" target="#fig_1">Figure 3</ref>), and takes the m-bodies {(i 1 , ? ? ? , i k?1 , j, i k+1 , ? ? ? , i m )}| N j=1 (the elements marked by red in <ref type="figure" target="#fig_1">Figure 3</ref>) as keys and values of the attention mechanism, which shares (m ? 1) atoms (i.e. atom i 1 , ? ? ? , i k?1 , i k+1 , ? ? ? , i m ) with the target m-body except atom i k .</p><p>By stacking the axial attentions from the 1 st axis to the m th axis, the target m-body can gradually aggregates the messages from all the m-bodies in V (m) . More specifically, in the m-body Axial Attention on k th axis, the target m-body is capable of aggregating the messages from many-bodies in the set which we define as AggredMessage (k) (i 1 , ? ? ? , i m ) = {(i 1 , ? ? ? , i k , i k+1 , ? ? ? , i m )}| 1?i 1 ,??? ,i k ?N (marked by yellow in the figure). The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEM-2</head><p>A PREPRINT deduction is as follows:</p><formula xml:id="formula_2">AggredMessage (0) (i 1 , ? ? ? , i m ) ={(i 1 , ? ? ? , i m )}, AggredMessage (k) (i 1 , ? ? ? , i m ) = ? N i k =1 AggredMessage (k?1) (i 1 , ? ? ? , i k?1 , i k , i k+1 , ? ? ? , i m ) = ? N i k =1 ? N i k?1 =1 AggredMessage (k?2) (i 1 , ? ? ? , i k?2 , i k?1 , i k , i k+1 , ? ? ? , i m ) ? ? ? ={(i 1 , ? ? ? , i k , i k+1 , ? ? ? , i m )}| 1?i 1 ,??? ,i k ?N .<label>(1)</label></formula><p>Consequently, in the m-body Axial Attention on the m th axis, the target m-body can aggregate the messages from all the m-bodies in</p><formula xml:id="formula_3">V (m) = {(i 1 , ? ? ? , i m )}| 1?i 1 ,??? ,i m ?N = AggredMessage (m) (i 1 , ? ? ? , i m ). Computationally, it requires O(N m+1 )</formula><p>to calculate one axial attention on a single axis and O(mN m+1 ) for the total m stacked axial attentions. This is much more efficient than the original full attention which requires O(N 2m ). We provide the further discussion about the memory and computation cost in Section A.3 of Appendix. To formalize Many-body Axial Attention, we take m-body axial attention on the k th axis as an example:</p><formula xml:id="formula_4">q i = Linear(z (m) i1,??? ,i k?1 ,i,i k+1 ,??? ,im ), k j = Linear(z (m) i1,??? ,i k?1 ,j,i k+1 ,??? ,im ), v j = Linear(z (m) i1,??? ,i k?1 ,j,i k+1 ,??? ,im ), ? ij = softmax(q T i k j ), o i = j ? ij v j ,<label>(2)</label></formula><p>where q i , k j and v j are the query, key, and value of the attention mechanism, respectively. The m-body representation z</p><formula xml:id="formula_5">(m) i1,??? ,i k?1 ,i,i k+1</formula><p>,??? ,im is then updated by the output representation o i by the residual connection. In addition, to incorporate the knowledge from the track of higher order, we further reform the axial attention for better molecular modeling. For the two m-bodies (i 1 , ? ? ? , i k?1 , i, i k+1 , ? ? ? , i m ) and (i 1 , ? ? ? , i k?1 , j, i k+1 , ? ? ? , i m ) that share (m ? 1) atoms, their interaction involves (m + 1) atoms. Those (m + 1) atoms as a whole can be regarded as a (m + 1)-body (i 1 , ? ? ? , i k?1 , i, j, i k+1 , ? ? ? , i m ). We fuse the representations z (m+1) i1,??? ,i k?1 ,i,j,i k+1 ,??? ,im of the (m + 1)body into the axial attention to incorporate the knowledge from the higher-order track. The representation of the corresponding (m + 1)-body is taken as the additional keys and values of the attention. The m-body Axial Attention on the k th axis is redefined as</p><formula xml:id="formula_6">q i = Linear(z (m) i1,??? ,i k?1 ,i,i k+1 ,??? ,im ), k j = Linear(z (m) i1,??? ,i k?1 ,j,i k+1 ,??? ,im ), k ij = Linear(z (m+1) i1,??? ,i k?1 ,i,j,i k+1 ,??? ,im ), v j = Linear(z (m) i1,??? ,i k?1 ,j,i k+1 ,??? ,im ), v ij = Linear(z (m+1) i1,??? ,i k?1 ,i,j,i k+1 ,??? ,im ), ? ij = softmax(q T i k j + q T i k ij ) = softmax(q T i (k j + k ij )), o i = j ? ij (v j + v ij ),<label>(3)</label></formula><p>where k ij and v ij are the additional keys and values corresponding to the (m + 1)-body. That means both the m-th body (i 1 , ? ? ? , i k?1 , j, i k+1 , ? ? ? , i m ) and the (m + 1)-th body (i 1 , ? ? ? , i k?1 , i, j, i k+1 , ? ? ? , i m ) are served as the keys and values of the axial attention. Such a combination can further improve the capacity of axial attention in modeling many-body interactions. Examples of the detailed architectures of the many-body axial attentions are exhibited in <ref type="figure" target="#fig_3">Figure 2b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Exchanging Messages across the Tracks of Different Orders</head><p>Many-bodies of different orders describe a molecule from different levels. For example, the 1 st -order focuses on the atom-level knowledge, while the higher-orders focus on the motif-level or even higher-level knowledge. Exchanging the knowledge between the tracks of different orders facilitates each track to understand and utilize the knowledge at multiple levels.</p><p>Many-body Axial Attention has already built a bridge to transfer the messages from the higher-order track to the lower-order track. We further introduce a Low2High module to transfer the messages from the lower-order track to the higher-order track. We introduce the Low2High module to the 2 nd -to M th -order tracks. For the 2 nd -order track, the element-wise outer product operation is used as the Low2High module:</p><formula xml:id="formula_7">o (2) i1,i2 = OuterProduct(Linear(z (1) i1 ), Linear(z (1) i2 )),<label>(4)</label></formula><p>where o</p><p>i1,i2 denotes the updated 2-body representation. While for the m th -order track with 2 &lt; m ? M , we utilize the element-wise addition operation as the Low2High module:</p><formula xml:id="formula_9">o (m) i1,??? ,im = m k=1 Linear(z (m?1) i1,??? ,i k?1 ,i k+1 ,??? ,im ),<label>(5)</label></formula><p>where o (m) i1,??? ,im denotes the updated m-body representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Molecular Property Prediction</head><p>The messages of the many-bodies can be exchanged across the tracks with different orders in the Optimus block.</p><p>Through L stacked Optimus blocks where L is usually set to a value larger than M , the representations of the atoms (1-bodies) produced by the last Optimus block are able to integrate the information of the many-bodies with various orders and can be used for molecular property prediction. We first apply average pooling on the atom representations, Z <ref type="bibr">(1)</ref> , to obtain the representation of the whole molecule. Then, the molecular representation is fed into a Multilayer Perceptron (MLP) to output the predicted properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>To comprehensively evaluate the performance of GEM-2 for molecular property prediction, we compare it with multiple baseline methods on two kinds of benchmarks: quantum chemistry and drug discovery. Furthermore, we also conduct extensive ablation studies to investigate the impact of the full-range interactions and the orders of many-bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings of GEM-2</head><p>In all the experiments, GEM-2 contains 12 Optimus blocks. For computational efficiency, each Optimus block contains two tracks, i.e. the 1 st -order track and the 2 nd -order track, unless otherwise specified. That means we consider the 1-bodies, 2-bodies, and the 3-bodies for molecular modeling. In order to gain the best performance, the hyperparameters are slightly tuned, for example, we set c 1 = c 2 = c 3 = 256 for the quantum chemistry benchmark and c 1 = c 2 = c 3 = 128 for the drug discovery benchmark. While for each ablation study, in order to save computational consumption without losing fairness, we use smaller hidden sizes, and the hyper-parameters are fixed for different GEM-2 variants. For model optimization, we use Adam Optimizer <ref type="bibr" target="#b21">[22]</ref> to train GEM-2. Besides, exponential moving average (EMA) <ref type="bibr" target="#b31">[32]</ref> with a decay rate of 0.999 is exploited to smooth the model parameters for the sake of achieving more robust performance. The detailed settings are described in Section B.2 in Appendix.</p><p>The input features for GEM-2 can be generally divided into three types in our experiments: features for 1-body describing atoms; features for 2-body describing atom pairs, and features for 3-body describing atom triplets. All these features are fetched from the cheminformatics tool RDKit (https://www.rdkit.org), including the Merck molecular force field (MMFF94) <ref type="bibr" target="#b14">[15]</ref> function to obtain simulated three-dimensional coordinates of atoms. We utilize the same feature set for all our experiments. Please refer to Appendix A.2 for the detailed feature list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quantum Chemistry Benchmark</head><p>Dataset. PCQM4Mv2 <ref type="bibr" target="#b16">[17]</ref> is a large-scale quantum chemistry dataset containing the DFT-calculated HOMO-LUMO energy gaps of 3,746,619 molecules, which is originally curated under the PubChemQC project <ref type="bibr" target="#b28">[29]</ref>. Open Graph Benchmark (OGB) 3 split the dataset according to their PubChem ID (CID) into training, validation, test-dev, and test-challenge sets with a ratio of 90:2:4:4. The training set is used for training and the validation set is used for model selection. We report the performance of GEM-2 and other baseline methods on both validation and test-dev sets. Note that the results of test-dev set can only be obtained by submitting to the OGB server, thus some results on test-dev are not reported. Also, the test-challenge set has not been released by OGB so far.  <ref type="bibr" target="#b40">[41]</ref> 0.1185 0.1218 3.8M GCN-virtual <ref type="bibr" target="#b13">[14]</ref> 0.1153 0.1152 4.9M GIN-virtual <ref type="bibr" target="#b13">[14]</ref> 0.1083 0.1084 6.7M w/ full-range &amp; w/o many-body</p><p>TokenGT <ref type="bibr" target="#b20">[21]</ref> 0.0910 0.0919 48.5M GRPE-Large <ref type="bibr" target="#b29">[30]</ref> 0.0867 0.0876 118.3M Graphormer <ref type="bibr" target="#b41">[42]</ref> 0.0864 -a 48.3M GPS <ref type="bibr" target="#b32">[33]</ref> 0.0858 -a 19.4M EGT <ref type="bibr" target="#b18">[19]</ref> 0 Baselines. We compare GEM-2 with multiple baseline methods. They can be roughly classified into three categories according to whether considering full-range interactions and many-body interactions: 1) w/o full-range &amp; w/o manybody, 2) w/ full-range &amp; w/o many-body, and 3) w/o full-range &amp; w/ many-body. w/o full-range &amp; w/o many-body methods are based on graph neural networks (GNNs) and only consider the atom-level interactions between the atoms connected by chemical bonds, including Graph Isomorphism Network(GIN) <ref type="bibr" target="#b40">[41]</ref>, GIN-virtual, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b22">[23]</ref>, and GCN-virtual, where GIN-virtual and GCN-virtual are the revised versions of GIN and GCN that utilize virtual node <ref type="bibr" target="#b13">[14]</ref>. w/ full-range &amp; w/o many-body methods are Transformer-style networks that directly model the interactions between all the atoms, including TokenGT <ref type="bibr" target="#b20">[21]</ref>, GRPE-Large <ref type="bibr" target="#b29">[30]</ref>, EGT <ref type="bibr" target="#b17">[18]</ref>, Graphormer <ref type="bibr" target="#b41">[42]</ref> and GPS <ref type="bibr" target="#b32">[33]</ref>. They only consider the atom-level interactions without considering the interactions between the many-bodies. w/o full-range &amp; w/ many-body methods, including our previous work, GEM <ref type="bibr" target="#b12">[13]</ref>, take the many-body interactions into account but focus on the short-range interactions, e.g., interactions between the atoms connected by the chemical bonds.</p><p>Results. We regard the prediction of HOMO-LUMO energy gaps as a regression problem and apply L1 loss as the loss function and Mean Absolute Error (MAE) as the evaluation metric. The results of GEM-2 and the baseline methods are shown in <ref type="table" target="#tab_1">Table 1</ref> (The lower, the better). The results of GCN, GIN, GCN-vritual, GIN-virtual, TokenGT, GRPE-Large, and EDT are collected from the PCQM4Mv2 leaderboard 4 in OGB, the results of Graphormer and GPS are from their papers, and the results of GEM <ref type="bibr" target="#b12">[13]</ref> are obtained by adapting the open source code from the corresponding GitHub repositories to re-train the models on the PCQM4Mv2 dataset. From the results, we can draw the following conclusions:</p><p>1. Generally, GEM-2 significantly outperforms all the baseline methods that do not consider the full-range interactions or the many-body interactions for molecular modeling. GEM-2 achieves a relative improvement of 7.5% and 6.5% compared with the previous SOTA, i.e., EGT, on the validation and test-dev sets, respectively, demonstrating its superiority. 2. The MAE scores of the methods that model the full-range interactions are lower than those of the methods that only focus on the short-range interactions (the interactions between the locally closed atoms or many-bodies), no matter whether considering the many-body interactions or not. The results indicate that directly modeling full-range interactions is more effective in molecular modeling, enhancing the property prediction accuracy. 3. By comparing the methods that incorporate the many-body interactions for molecular modeling and the methods that only model the interactions between the atoms (1-bodies), we found that it is challenging for the methods that only model the interactions between the atoms to achieve further improvement, even though some of the baseline methods have already learned the full-range interactions through variant model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Drug Discovery Benchmark</head><p>Dataset. We further verify the effectiveness of GEM-2 on the virtual screening task for drug discovery. LIT-PCBA <ref type="bibr" target="#b36">[37]</ref> is a virtual screening dataset containing 15 protein targets, 9780 active compounds (positive samples), and 407,839 unique inactive compounds (negative samples) selected from high-confidence PubChem Bioassay data. Predicting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEM-2</head><p>A PREPRINT <ref type="table">Table 2</ref>: AUC scores (higher is better) of GEM-2 and the baseline methods on drug discovery dataset LIT-PCBA. The SOTA results are shown in bold. Standard deviations are shown in brackets. a These results are collected from <ref type="bibr" target="#b4">[5]</ref>, where standard deviations are not reported. the activities of the candidate compounds to a particular protein target can be regarded as a binary classification task. Due to the large prediction variance on the classification tasks with only a few positive samples, we only evaluate the methods on the targets (ALDH1, FEN1, GBA, KAT2A, MAPK1, PKM2, and VDR) with more than 150 active compounds. Following the previous work <ref type="bibr" target="#b4">[5]</ref>, we split the samples of each protein target into the training and test sets at the ratio of 3:1 with asymmetric validation embedding (AVE) method <ref type="bibr" target="#b38">[39]</ref>. The split dataset can be directly downloaded at the previous work's GitHub repository <ref type="bibr" target="#b4">5</ref> . Additionally, we reserve 1/9 samples in the training set as the validation set to select the best model and then evaluate the models' performance on the test set.</p><formula xml:id="formula_10">ALDH1 ? FEN1 ? GBA ? KAT2A ? MAPK1 ? PKM2 ? VDR ? Average ? No.</formula><p>Baselines. Similar to the experiments on PCQM4Mv2, we compare GEM-2 with multiple types of baselines: (1) traditional machine learning methods, including Naive Bayes(NB) <ref type="bibr" target="#b10">[11]</ref>, support vector machine (SVM) <ref type="bibr">[8]</ref>, random forest (RF) <ref type="bibr" target="#b26">[27]</ref> and extreme gradient boosting (XGBoost) <ref type="bibr" target="#b6">[7]</ref>; (2) w/o full-range &amp; w/o many-body methods, including GCN, Graph Attention Network (GAT) <ref type="bibr" target="#b37">[38]</ref>, and FP-GNN <ref type="bibr" target="#b4">[5]</ref>; (3) w/ full-range &amp; w/o many-body methods, including EGT and EGT pretrain <ref type="bibr" target="#b17">[18]</ref>; (4) w/o full-range &amp; w/ many-body methods, including GEM and GEM pretrain <ref type="bibr" target="#b12">[13]</ref>. GEM-2 and GEM-2 pretrain are our proposed methods. As the number of positive samples in LIT-PCBA is insufficient and may lead to over-fitting, we also implement the pretrained versions for methods EGT, GEM, and GEM-2 to alleviate the impact of over-fitting, marked with subscript "pretrain". For each method, we first pretrain it on the quantum chemistry benchmark PCQM4Mv2. Then, the pretrained model is finetuned on dataset LIT-PCBA.</p><p>Results. Since each target is taken as a binary classification task, binary cross entropy is used as the loss function to optimize the models. To compare the accuracy of GEM-2 and the baseline methods, we use ROC-AUC (area under the receiver operating characteristic curve) as the evaluation metric. The results of NB, SVM, RF, XGBoost, GCN, GAT, and FP-GNN are collected from <ref type="bibr" target="#b4">[5]</ref>. The results of EGT, EGT pretrain , GEM, GEM pretrain are obtained by running the released code with hyper-parameter searching on dataset LIT-PCBA. We ran each experiment three times and reported the average AUC scores to reduce the experimental variance caused by the limited positive samples. From <ref type="table">Table 2</ref>, we can observe that GEM-2 pretrain performs the best on 6 out of 7 targets, which is in line with the results on the quantum chemistry dataset, demonstrating its great potential in the drug discovery industry. Besides, the pretrained versions of EGT, GEM, and GEM-2 all gain further improvement compared with the corresponding no-pretraining versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>In this section, we conduct extensive ablation studies on dataset PCQM4Mv2 to investigate the contributions of many-body interactions and full-range interactions for molecular modeling. We further analyze the impact of the many-body interactions with different orders and the long-range interactions with different levels. We report the MAE scores of the ablation versions of GEM-2 on the PCQM4Mv2 valid set. Note that we use a smaller hidden size for    comparison in this subsection to save computational consumption without losing the fairness of comparison. Please refer to Appendix B.2 for the detailed settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Effects of Many-Body Interactions and Full-Range Interactions</head><p>GEM-2 incorporates the many-body interactions and the full-range interactions as a whole for molecular modeling. We compare four variants of GEM-2: 1) Baseline is the complete GEM-2 that models the full-range many-body interactions; 2) w/o full-range variant only models the short-range interactions of those many-bodies, e.g., the interactions between GEM-2 A PREPRINT those atoms that are connected by the chemical bonds; 3) w/o many-body variant only uses the 1 st order track in Optimus block, i.e. only considers the interactions between the atoms (1-bodies); 4) w/o full-range &amp; w/o many-body variant focuses on learning the short-range interactions between the atoms, which is a combination of w/o full-range variant and w/o many-body variant.</p><p>Besides, we suspect that the original GEM-2 has great advantages in modeling full-range interactions, especially long-range interactions. To verify this assumption, we further test molecules with different max_topo_dist where max_topo_dist stands for the maximal topological distance of any two atoms in the molecules. And the topological distance between two atoms refers to the minimal number of chemical bonds through which those atoms can be connected. Usually, the larger the maximum topological distance of a molecule, the greater the importance of longrange interaction modeling for that molecule. Besides the overall group that contains all the test molecules, we further divide the molecules into three groups with different level of max_topo_dist: (1) short max_topo_dist group, with max_topo_dist ? [1, 7], (2) moderate max_topo_dist group, with max_topo_dist ? <ref type="bibr">[8,</ref><ref type="bibr" target="#b10">11]</ref>, and (3) long max_topo_dist group, with max_topo_dist ? [12, ?).</p><p>We compare the MAE scores of multiple variants of GEM-2 in <ref type="figure" target="#fig_8">Figure 4a</ref>. In general, Baseline outperforms the other variants on all the molecular groups. Especially, the complete GEM-2, i.e., Baseline, shows excellent superiority on long max_topo_dist group. (1) Compared Baseline with w/o full-range, we can observe that Baseline achieves a significant improvement of 13.7% on long max_topo_dist group and a relatively slight improvement on short max_topo_dist group and moderate max_topo_dist group. Although methods only focusing on the local relations between the atoms can model those molecules with few long-range interactions, its modeling ability for large or complex molecules is unsatisfactory. (2) The improvement of many-body interaction modeling, by comparing Baseline and w/o many-body, is also significant and consistent (from 4.9% to 6.0%) on all the molecule groups. Many-body interaction modeling contributes a lot to molecular modeling, which is in line with the conclusion of the classical chemical computational methods and previously proposed deep-learning methods based on many-bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Effects of the Orders of Many-Bodies</head><p>We have demonstrated the overwhelming superiority over existing baseline methods in the previous subsections, where each Optimus block in GEM-2 involves two tracks for many-body modeling, i.e., 1 st -to-2 nd -order tracks. We believe incorporating higher-order many-bodies can further enhance the capacity of molecular modeling. To investigate the effects of the orders of many-bodies for molecular modeling, we conduct ablation studies on dataset PCQM4Mv2 to compare the following variants of GEM-2: 1) 1 st -order containing only the 1 st -order track; 2) 1 st -to-2 nd -order containing the 1 st -and 2 nd -order tracks; 3) 1 st -to-3 rd -order containing the 1 st -, 2 nd -and 3 rd -order tracks.</p><p>The MAE scores of the GEM-2 variants with different orders of many-bodies are shown in <ref type="figure" target="#fig_8">Figure 4b</ref>. As we expected, the MAE score of 1 st -order is the highest, and the MAE score of 1 st -to-3 rd -order is the lowest, which verifies that the introduction of higher-order many-body interactions is effective in improving the accuracy of molecular attribute prediction. Besides, the relative improvement brought by adding the 3 rd -order track is smaller than that by adding the 2 nd -order track, which we speculate is due to the marginal effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Effects of the Levels of Long-Range Interactions</head><p>We utilize the full-range interaction through the attention mechanism in the above experiments. It would also be interesting to investigate the contributions of different levels of long-range interactions. The level of the long-range interactions is simply controlled by applying attention mechanism to those atoms whose topological distance is within a specific value (level). For example, the level of k means the attention mechanism is only applied between atoms with topological distance less than or equal to k. The level of 1 means the attention mechanism is only applied between atoms connected by chemical bonds. The MAE scores of the GEM-2 variants of different levels of long-range interactions, from level from 1 to 13, are shown in <ref type="figure" target="#fig_8">Figure 4c</ref>. The MAE scores of GEM-2 get lower with the increase of levels, which again demonstrates the crucial of full-range interactions, especially long-range interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Our model has achieved SOTA results on multiple tasks, so an interesting question will be what our model has learned, especially in the high-order track, which is the extra part compared with other models. In order to answer this question, it is natural to regard the attention weights in the model as the predictive relevance of certain atoms and bonds. Then, the visualization of these attention weights makes the model explainable to some extent. This idea has been widely used to interpret attention-based models <ref type="bibr">[? ]</ref>. Here, we designed two comparable molecules. One is the n-Butylbenzene (nB, shown in <ref type="figure" target="#fig_9">Figure 5a</ref>, left), and another is the 1-Phenyl-1-butene (1P1B, <ref type="figure" target="#fig_9">Figure 5a</ref> between atom 6 and 7 in nB is modified to a double bond. The 3D structures of the two molecules are generated by the Merck molecular force field (MMFF94) <ref type="bibr" target="#b14">[15]</ref> function in the RDKit tool. The difference between the two molecules is that the double bond between atoms 6 and 7 in 1P1B forms a conjugation system with the benzene, making the whole molecular structure closer to the benzene plane (best view in the Side View of <ref type="figure" target="#fig_9">Figure 5a</ref>). In this comparable case, we expect the attention weights in our model can reflect two important differences between the two molecules: 1) the 3D structure of nB is symmetric while the 1P1B is not, and 2) the attention of the modified double bond in 1P1B should be more on the left benzene instead of the right two carbons. As shown in <ref type="figure" target="#fig_9">Figure 5b</ref>, we plot the attention weights of 2-body Axial Attention on 1 st axis given the atom pair (6, 7) as the query of attention. As we expected, the attention weights of nB <ref type="figure" target="#fig_9">(Figure 5b</ref>, left) are highly symmetric, where the attention weights on atoms 2 and 4 are nearly equal with those on atoms 1 and 0, respectively. On the contrary, the attention weights of 1P1B are quite different on these atoms <ref type="figure" target="#fig_9">(Figure 5b, right)</ref>. Moreover, we can see that the highest attention weights within 1P1B are on atoms 1 and 3, or on the benzene, reflecting the high attention of our model on the conjugated effect. To summarise, this visualization demonstrates that the information learned on the many-body track shows good agreement with the well-established chemistry knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Existing works for molecular modeling can be simply categorized from two prospective: whether considering the many-body interactions and whether considering long-range interactions.</p><p>Interactions between 1-Bodies v.s. Interactions between Many-Bodies. The mainstream works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref> focus on the interactions between 1-bodies (atoms), applying the graph neural networks (GNNs) or transformer-style models to learn the node representations, i.e., the atom representations. Through multiple stacked model blocks, the messages of the atoms are exchanged step by step. A few advanced works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> attempt to incorporate geometric information, e.g., bond angles, by modeling interactions between atom pairs (2-bodies). For example, DimeNet <ref type="bibr" target="#b24">[25]</ref> propose a message passing scheme that uses the directional information by transforming messages based on the angle between them. In particular, our previous work, GEM <ref type="bibr" target="#b12">[13]</ref>, has demonstrated the positive effect of modeling many-body interaction for general property prediction by constructing two graphs: an atom-bond graph and a bond-angle graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short-Range</head><p>Interactions v.s. Long-Range Interactions. Most studies <ref type="bibr">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref> regard a molecule as a graph by taking the atoms as the nodes of the graph and apply graph neural networks (GNNs) for molecular modeling. They concentrate on the local interactions, i.e., the short-range interactions, between the atoms. For instance, some works <ref type="bibr">[2,</ref><ref type="bibr" target="#b39">40]</ref> regard two atoms are connected if a chemical bond links them. While the studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref> connect two atoms if they are spatially close. Even though partial long-range interactions between the atoms can be inferred through the step-by-step message-passing technique, some information may be lost during the multi-step transition. Such an indirect manner could harm the modeling of the interactions between the indirectly connected atoms, especially those far away. To address this issue, exploiting the Transformer-style architectures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> to directly capture the fullrange interaction has attracted increasing attention. For example, EGT <ref type="bibr" target="#b17">[18]</ref> [6] applied the global self attention directly on molecular graphs with a generalized positional node encoding scheme. While CoMPT <ref type="bibr">[6]</ref> further reinforced the message interactions between nodes and edges by adding a node-edge interaction module on the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Molecular property prediction serves as a fundamental task in drug and material industries, which in essence, can be determined by its own electronic structure and described by the Schr?dinger equation. Inspired by the calculation methods of Schr?dinger equation, we clarify the importance of full-range many-body interactions for molecular property prediction and propose deep learning methods to comprehensively model such complex interactions for the improvement of molecular modeling. We propose a novel network architecture called GEM-2. The main component of GEM-2, i.e., Optimus block, simultaneously exploits multiple tracks to update the representations of the many-bodies of different orders, with each track learning the interactions of the many-bodies of a specific order. Many-body Axial Attention is designed to reduce the computational cost of the full-range interactions without loss of effectiveness. Extensive experiments on two kinds of benchmarks exhibit that GEM-2 significantly outperforms the mainstream baselines, demonstrating the effectiveness of full-range many-body interaction modeling for molecular property prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Availability</head><p>The source code of this study is available at GitHub repository (https://github.com/PaddlePaddle/ PaddleHelix/tree/dev/apps/pretrained_compound/ChemRL/GEM-2) to allow replication of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Availability</head><p>For Quantum Chemistry Benchmark, PCQM4Mv2 dataset is publicly available on OGB's official website(https:// ogb.stanford.edu/docs/lsc/pcqm4mv2/). For Drug Discovery Benchmark, LIT-PCBA dataset can be downloaded from <ref type="bibr" target="#b4">[5]</ref>'s Github repository(https://github.com/idrugLab/FP-GNN/blob/main/Data.rar). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GEM-2</head><p>where ? controls the shape of the radial kernel, and we set ? = 10. {? m } is a list of centers ranging from the minimum value to the maximum value of corresponding features with stride of 0.1. For model optimization of all our experiments, we utilize the Adam optimizer with a learning rate warm-up and a learning rate decay: the warm-up starts with 1% of the learning rate and linearly scales to 100% for 10 epochs, then the learning rate is kept for 40 epochs. Finally, we continuously decay the learning rate with 50% for every 10 epochs. We train our model for 100 epochs in total. Other settings of hyper-parameters are slightly tuned to gain the best performance for different benchmarks, including learning rate and hidden sizes of GEM-2, as shown in <ref type="table" target="#tab_5">Table 5</ref>. While for the ablation studies on PCQM4Mv2, we fix the hyper-parameters and use a smaller hidden size to save computational consumption. In Section 3.4.1, we set c 1 = c 2 = c 3 = 128 and learning rate as 8 ? 10 ?4 , while in Section 3.4.2 and 3.4.3, we set c 1 = c 2 = c 3 = 64, learning rate as 8 ? 10 ?4 and train for 50 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(on 3</head><label>3</label><figDesc>rd axis)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>*, *, +%) weight (*, *, *, +%) value (*, *, *, +%) probability (*, *, *) 1-body Axial Attention (on 1 st axis) 2-body Axial Attention (on 1 st axis) query in attention key or value in attention (b) Examples of the detailed model architectures of many-body axial attentions. Many-Body Axial Attentions regards the representations of the many-bodies as the queries, keys, and values of the attention mechanism.The knowledge of the many-bodies of higher-order are is also introduced for further improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Demonstration of GEM-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, an Optimus block consists of M tracks of different orders where the m th -order track updates m-body representations Z (m) . The m th -order track captures the interactions between the m-, (m ? 1)and (m + 1)bodies, i.e. Z (m) , Z (m?1) , and Z (m+1) . A novel Many-body Axial Attention module is designed to efficiently learn the full-range interactions within the m-bodies. The stacked Many-body Axial Attention on multiple axes can approximately model the full-range interactions at a much lower computational cost than directly modeling the full-range interactions of all the m-body pairs at one shot. Then, the Many-body Axial Attention module and the Low2High module aggregate representations from the high-order and lower-order track, i.e., the (m + 1) th -and (m ? 1) th -order tracks, to exchange the messages across the tracks of different orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Axial Attention (on 1 st axis) with ( ! ) 3-body Axial Attention (on 2 nd axis) with ( ! ) 3-body Axial Attention (on 3 rd axis) with ( ! ) 2-body Axial Attention (on 1 st axis) with ( " ) 2-body Axial Attention (on 2 nd axis) with ( " ) 1 st -order Track 1-body Axial Attention (on 1 st axis) with ( # ) Illustration of the principle of Many-body Axial Attention for full-range interaction modeling. The many-body representations of order m in GEM-2 are organized as a multi-dimensional tensor Z (m) ? R N m ?cm , with each element z (m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>a) MAE scores of GEM-2 variants to analyze the contributions of many-body interactions and full-range interactions. Besides the overall group, the test molecules are further divided into three groups with different max_topo_dist: (1) short max_topo_dist, (2) moderate max_topo_dist, and (3) long max_topo_dist. The relative improvement compared to Baseline is also shown at the top of each bar.1st-order 1st-to-2nd-order 1st-to-3rd-order 0MAE scores of GEM-2 variants with different orders of many-bodies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>MAE scores of GEM-2 variants with different long-range levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Ablation studies of GEM-2 on PCQM4Mv2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>, right), in which the single bond GEMVisualization of attention weights of Many-body Axial Attention of two molecules, n-Butylbenzene (nB) and 1-Phenyl-1-butene (1P1B). a) Smiles, 2D view, top view and side view of the molecules. b) Attention weights of 2-body Axial Attention on 1 st axis averaged over all heads. Each circle over atom j represents the attention weights between atom pair (6, 7) and atom pair (j, 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>A PREPRINT Algorithm 4 Algorithm 6 ij ? R cm 4 :</head><label>464</label><figDesc>Many-body Axial Attention def AxialAttention k (Z (m) , Z (m+1) , c m = 32, N (m) head = 8): 1: # Input Projections 2:Z (m) = LayerNorm(Z (m) ) 3: Z (m+1) = LayerNorm(Z (m+1) ) 4: q h i = Linear(z (m) i1,??? ,i k?1 ,i,i k+1 ,??? ,,im ) q h i ? R cm , h ? {1, ..., N ,,i k?1 ji k+1 ,??? ,,im ) + Linear(z (m+1) i1,??? ,i k?1 ,i,j,i k+1 ,??? ,im ) k h ij ? R cm 6: v h ij = Linear(z (m) i1,??? ,i k?1 ,j,i k+1 ,??? ,im ) + Linear(z (m+1) i1,??? ,i k?1 ,i,j,i k+1 ,??? ,im ) v h ij ? R cm 7: # Attention 8: ? h ij = softmax(q T i k ij ) 9: o h i = j ? ij v h ij 10: # Output projection 11:? i1,??? ,i k?1 ,i,i k+1 ,??? ,im = Linear(concat h (o h i )) z i1,??? ,i k?1 ,i,i k+1 ,??? ,im ? R cm?N (m)head 12: return? Algorithm 5 Low2High module def Low2High(Z (m?1) ,c m ): 1: if m == 1 then 2: Z (m) = 0 3: else if m == 2 then 4: Z (m) = OuterProduct(Z (m?1) ) 5: else 6: Z (m) = ElementwiseAdd(Z (m?1) ) 7: end if 8: return Z (m) Outer product def OuterProduct(Z (1) ,c outer ,c m ): return Z (2) Algorithm 7 Element-wise Addition def ElementwiseAdd(Z (m?1) ,c m ): ,im ) + Linear(z (m?1) i1,i3,??? ,im ) + ? ? ? + Linear(z ,im ? R cm 4: return Z (m) vectors. While for the continuous features, we use the Radial Basis Function[3] to expand each continuous value x into a vector e ? R M : e m (x) = exp(??||x ? ? m || 2 ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>A PREPRINT Optimus Block 1 st -order Track</head><label></label><figDesc></figDesc><table><row><cell>1-body representation ( , )</cell><cell></cell><cell>1-body Axial Attention (on 1 st axis)</cell><cell>Feed Forward</cell><cell></cell><cell>1-body representation ( , )</cell></row><row><cell></cell><cell>Low2High</cell><cell></cell><cell>2 nd -order Track</cell><cell></cell><cell></cell></row><row><cell>2-body representation ( , , )</cell><cell>Module</cell><cell>2-body Axial Attention</cell><cell>2-body Axial Attention</cell><cell>Feed Forward</cell><cell>2-body representation ( , , )</cell></row><row><cell></cell><cell></cell><cell>(on 1 st axis)</cell><cell>(on 2 nd axis)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Low2High</cell><cell></cell><cell>3 rd -order Track</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Module</cell><cell></cell><cell>3-body</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Axial</cell><cell>Feed</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Attention</cell><cell>Forward</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>(on 2 nd axis)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>? L</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>MAE scores (lower is better) of GEM-2 and multiple kinds of baselines on quantum chemistry dataset PCQM4Mv2.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">Validation MAE ? Test-dev MAE ? #Params</cell></row><row><cell></cell><cell>GCN[23]</cell><cell>0.1379</cell><cell>0.1398</cell><cell>2.0M</cell></row><row><cell>w/o full-range &amp; w/o many-body</cell><cell>GIN</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters of GEM-2 for training. parameter name PCQM4Mv2 LIT-PCBA</figDesc><table><row><cell>batch size</cell><cell>512</cell><cell>256</cell></row><row><cell>learning rate</cell><cell>4 ? 10 ?4</cell><cell>2 ? 10 ?4</cell></row><row><cell>p m</cell><cell>0.05</cell><cell>0.2</cell></row><row><cell>c m</cell><cell>256</cell><cell>128</cell></row><row><cell>B.2 Hyper-parameters</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://ogb.stanford.edu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://ogb.stanford.edu/docs/lsc/leaderboards/#pcqm4mv2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/idrugLab/FP-GNN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.paddlepaddle.org.cn/en</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Memory and Computation Cost</head><p>We take the m-body Axial Attention on k th -axis as an example to analyze the memory and computational cost. For memory consumption, it requires O(max (N m c m , N m+1 N (m) head )) to store the queries, keys, values, and the attention weights of the attention mechanism, where N  The description of the targets in LIT-PCBA in shown in <ref type="table">Table 4</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polishing the gold standard: The role of orbital choice in ccsd(t) vibrational frequency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">W</forename><surname>Bertels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Head-Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="742" to="755" />
			<date type="published" when="2021-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?my</forename><surname>Brossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriel</forename><surname>Frigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dehaene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15069</idno>
		<title level="m">Graph convolutions that can finally model local structure</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Radial basis functions: theory and implementations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buhmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dft in a nutshell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieron</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><forename type="middle">O</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Quantum Chemistry</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fp-gnn: a versatile deep learning architecture for enhanced molecular property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.03834</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning attributed graph representations with communicative message passing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuedong</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2107.08773</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemyslaw</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Smieja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Struski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Slowik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Maziarka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -27th International Conference</title>
		<editor>Haiqin Yang, Kitsuchart Pasupa, Andrew Chi-Sing Leung, James T. Kwok, Jonathan H. Chan, and Irwin King</editor>
		<meeting><address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="668" to="675" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Longrange interacting quantum systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?</forename><surname>Defenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Macr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Pagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ruffo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Trombettoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01063</idno>
		<idno>arXiv:2109.01063</idno>
		<imprint>
			<date type="published" when="2021-09" />
		</imprint>
	</monogr>
	<note>cond-mat, physics:quantph</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pattern classification and scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno>abs/2012.09699</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometry-enhanced molecular representation learning for property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieqiong</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="134" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-06-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Merck molecular force field. i. basis, form, scope, parameterization, and performance of mmff94</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas A Halgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="490" to="519" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Edge-augmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Shamim</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmashankar</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
		<idno>abs/2108.03348</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Edge-augmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Md Shamim Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmashankar</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Drug discovery with explainable artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Jim?nez-Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Grisoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gisbert</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="573" to="584" />
			<date type="published" when="1010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pure transformers are powerful graph learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><forename type="middle">Dat</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<idno>abs/2207.02505</idno>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Classification and regression by randomforest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
	<note>R news</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Molecule attention transformer. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Maziarka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Danel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Rataj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomomi</forename><surname>Shimazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1300" to="1308" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph self-attention for learning graph representation with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonggi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntae</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12787</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quest for a universal density functional: the accuracy of density functionals across a broad spectrum of databases in chemistry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Peverati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">G</forename><surname>Truhlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">20120476</biblScope>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislav</forename><surname>Ramp?sek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Prakash</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Recipe for a general, powerful, scalable graph transformer. CoRR, abs/2205.12454, 2022</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gastegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9377" to="9388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Heterogeneous molecular graph neural networks for predicting molecule properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeren</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on Data Mining</title>
		<editor>Claudia Plant, Haixun Wang, Alfredo Cuzzocrea, Carlo Zaniolo, and Xindong Wu</editor>
		<meeting><address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="492" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lit-pcba: An unbiased data set for machine learning and virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?lien</forename><surname>Viet-Khoa Tran-Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didier</forename><surname>Jacquemard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rognan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4263" to="4273" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Most ligand-based classification benchmarks reward memorization rather than generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhar</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Heifets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="916" to="932" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="PMID">31408336</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Medicinal Chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8749" to="8760" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28877" to="28888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coupled cluster theory in materials science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Igor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gr?neis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Materials</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">1 shows the overall algorithm of GEM-2, taking {V (m) } as</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alg</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>input to predict the property y. Here, p m is the dropout</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">def GEM_2({V (m) }): m ? {1</title>
		<imprint/>
	</monogr>
	<note>M } 1: X (m) = Dropout pm</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">L} do 4: {Z (m) } = OptimusBlock({Z (m) }) 5: end for 6: # make prediction 7: y = MLP</title>
		<imprint>
			<publisher>Pool</publisher>
		</imprint>
	</monogr>
	<note>Z (m) = LayerNorm(X (m) ) 3: for l ? {1</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">2 shows the details of a Optimus block, consisting of M tracks. Each track updates the representation of the many-bodies of the corresponding order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alg</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">M } do 3: # m th -order track 4:? (m) = OptimusTrack m (? (m?1) , Z (m)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Algorithm 2 Optimus block def OptimusBlock({Z (m) }): m ? {1. end for 6: return {? (m) }</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">3 shows the details of m th -order track, which consists of a Low2High module, m Many-body Axial Attention modules, and a Feed Forward module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alg</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">def OptimusTrack m (Z (m?1) , Z (m) , Z (m+1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dropout pm (Low2High(Z (m?1) )) 2: for a ? {1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? = Z</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">m} do 3:? += Dropout pm (AxialAttention k (?, Z (m+1) )) 4: end for 5:? += Dropout pm</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">This is a revised version of original attention module, which we apply self-attention along the k-th axis of Z (m) and efficiently leverage messages from the higher-order Z (m+1)</title>
	</analytic>
	<monogr>
		<title level="m">Alg. 4 shows the details of Many-body Axial on k-th axis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">When m = 2, we utilize an OuterProduct as the Low2High module, which is computationally expensive but has relatively high capacity</title>
	</analytic>
	<monogr>
		<title level="m">Alg. 5 shows the details of the Low2High module</title>
		<imprint/>
	</monogr>
	<note>When m = 1, there&apos;s no need for a Low2High module. When m &gt; 2, we utilize an ElementwiseAdd as the Low2High module with low computational cost</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Input Features All the features are extracted by RDKit. The feature list is shown in Table 3. There are two types of features: discrete features and continuous features. For discrete features, we use the one-hot operation to embed the features into one-hot</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
