<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
							<email>qianyi.wu@monash.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
							<email>zhouhang@link</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
							<email>bzhou@ie.cuhk.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">SenseTime Research 5 S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<email>bo.dai@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<email>xwzhou@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating speech-consistent body and gesture movements is a long-standing problem in virtual avatar creation. Previous studies often synthesize pose movement in a holistic manner, where poses of all joints are generated simultaneously. Such a straightforward pipeline fails to generate fine-grained co-speech gestures. One observation is that the hierarchical semantics in speech and the hierarchical structures of human gestures can be naturally described into multiple granularities and associated together. To fully utilize the rich connections between speech audio and human gestures, we propose a novel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech gesture generation. In HA2G, a Hierarchical Audio Learner extracts audio representations across semantic granularities. A Hierarchical Pose Inferer subsequently renders the entire human pose gradually in a hierarchical manner. To enhance the quality of synthesized gestures, we develop a contrastive learning strategy based on audio-text alignment for better audio representations. Extensive experiments and human evaluation demonstrate that the proposed method renders realistic co-speech gestures and outperforms previous methods in a clear margin. Project page: https://alvinliu0.github.io/projects/HA2G.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When communicating with other people, we spontaneously make co-speech gestures to help convey our thoughts. Such non-verbal behaviors supplement speech information, making the content clearer and more understandable to listeners <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b67">68]</ref>. Psycho-linguistic studies also suggest that virtual avatars with plausible speech gestures are more intimate and trustworthy <ref type="bibr" target="#b65">[66]</ref>. Therefore, actuating embodied AI agents such as social robots and digital humans with expressive body movements and gestures is of great importance to facilitating human machine interaction <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. To this end, researchers have explored the task of co-speech gesture synthesis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, which aims at generating a sequence of human gestures given the speech audio and transcripts as input.</p><p>Traditionally, the task is tackled through building oneto-one correspondences between speech and unit gesture pairs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49]</ref>. Such pipelines require huge human efforts, making them inapplicable to general scenarios of unseen speech. Recent studies leverage deep learning to solve this problem by training a neural network to map a compact representation of audio <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> and text <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref> to holistic human pose sequence. However, such a straightforward approach fails to capture the micro-scale motions and cross-modal information, e.g., the subtle finger movements and the rich meanings contained in speech audio. The problem of how to learn the fine-grained cross-modal association remains unsolved.</p><p>In order to fully exploit the rich multi-modal semantics, we identify two important observations from a human gesture study <ref type="bibr" target="#b49">[50]</ref>: 1) Different types of co-speech gestures are related to distinct levels of audio information. For example, the metaphorical gestures are strongly associated with the high-level speech semantics (e.g., when depicting a ravine, one would moving two outstretched hands apart and saying "gap"), while the low-level audio features of beat and volume lead to the rhythmic gestures. 2) The dynamic patterns of different human body parts in co-speech gestures are not the same, such as the flexible fingers and relatively still upper arms. Thus it is improper to generate the upper body pose as a whole like previous studies <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b73">74]</ref>.</p><p>Inspired by the discussions above, we develop the Hierarchical Audio-to-Gesture (HA2G) pipeline, which generates diverse co-speech gestures. Our key insight is to build hierarchical cross-modal associations across multiple lev-els between tri-modal information and generate gestures in a coarse-to-fine manner. Specifically, two modules are devised, namely the Hierarchical Audio Learner, and the Hierarchical Pose Inferer. In the Hierarchical Audio Learner, we argue that features extracted from different levels of the audio backbone capture different meanings. Additionally, text information can further strengthen the audio embedding through contrastive learning for more discriminative representations. Afterwards, based on the hypothesis that different levels of audio information contribute to different body joint movements, we associate the multi-level audio features with the hierarchical structure of human body in the Hierarchical Pose Inferer. In particular, the association is achieved in correlation with speaking styles encoded from speaker appearances. The hierarchy of human upper limb is predicted in a coarse-to-fine manner from shoulders to fingers like a tree structure by cascading multiple bi-directional GRU generators. In addition, we propose a novel physical regularization to enhance the realness of generated poses. Experiments demonstrate that our method synthesizes realistic and smooth co-speech gestures.</p><p>To summarize, our main contributions are three-fold: (1) We propose the Hierarchical Audio Learner to extract hierarchical audio features and render discriminative representations through contrastive learning. <ref type="bibr" target="#b1">(2)</ref> We propose the Hierarchical Pose Inferer to learn associations between multilevel features and human body parts. Human poses are thus generated in a cascaded manner. (3) Extensive experiments show that HA2G can generate fine-grained co-speech gestures, which outperform state-of-the-art methods on both objective evaluations and subjective human studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human-Centered Audio-Visual Learning. In recent years, human-centered audio-visual learning has been extensively studied <ref type="bibr">[22, 46, 62-64, 72, 77, 80]</ref>. While some works utilize audio-visual correspondence to solve the problems like music-to-dance <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>, and talking face generation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b80">81]</ref>, the modeling between speech and gesture remains largely unexplored. The difficulty of speech-based gesture generation lies in constructing the correspondence between speech and human gesture, which is more complicated and implicit than music-to-dance or talking face generation. Human Motion Synthesis. Synthesizing human motions has been of important interest in both computer vision and graphics, where spatial-temporal coherence of pose sequence is used to generate realistic motions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b81">82]</ref>. Earlier methods employ statistical models such as kernel-based probability distribution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b53">54]</ref> to synthesize human motions. Still, they fail to handle motion details, and the complicated training procedures essentially limit model capacity. Recently, the ability of deep models to generate hu-man motions has been proven on different network architectures, where CNN-based <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b72">73]</ref>, RNN-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b66">67]</ref> and GAN-based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref> methods have been explored. These methods are purely visual-based with the input of history motions, while our work focuses on identifying the strong correlations between speech and gestures in conversational settings to achieve speech-driven motion synthesis. Audio/Text-Driven Motion Generation. Early works on speech-driven motion generation are mostly rule-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b48">49]</ref>, where a predefined set of unit gestures and motion connecting rules are designed manually. With the development of deep learning, data-driven approaches have demonstrated superior performance. Some works map speech text information to co-speech gestures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b74">75]</ref>. Yoon et al. <ref type="bibr" target="#b74">[75]</ref> resort to RNN to map from utterance text to upper body gestures. Some methods use speech audio signals to drive gestures <ref type="bibr">[2, 20, 24-26, 43, 55]</ref>. For example, Ginosar et al. <ref type="bibr" target="#b23">[24]</ref> collect a 2D speaker-specific gesture dataset and train the model with an adversarial loss. To make gestures more expressive, Habibie et al. <ref type="bibr" target="#b24">[25]</ref> lift the 2D pose to 3D and generate facial expressions simultaneously. However, all of their methods learn a model for each speaker individually, which makes it hard to transfer to general scenes and limit speaker styles to a tiny number. Besides, either audio-or text-driven motion generation methods fail to consider messages from both modalities, which motivates recent methods to jointly tackle multi-modal information <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b73">74]</ref>. Specifically, Yoon et al. <ref type="bibr" target="#b73">[74]</ref> propose to encode the trimodal feature embeddings of text, audio, speaker identity and concatenate them together to pass a decoder. But they fail to fully make use of multi-level features. Further, the dynamic patterns of different human body parts are diverse when people talk, e.g., the range and frequency of co-speech finger and arm movement are not the same, which makes it unreasonable to learn holistic human pose directly. In this work, we propose to extract hierarchical audio features with a contrastive learning strategy to excavate cross-modal messages at multiple granularities and learn co-speech gestures in a coarse-to-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We present Hierarchical Audio-to-Gesture (HA2G) that generates a target person's co-speech gestures given speech audio. The generated poses are conditioned on speaker identity and initial poses. Following Yoon et al. <ref type="bibr" target="#b73">[74]</ref>, text information can be provided additionally. The whole pipeline is illustrated in <ref type="figure">Fig. 1</ref>  <ref type="figure">Figure 1</ref>. Illustration of the Hierarchical Audio-to-Gesture (HA2G). In Hierarchical Audio Learner, Ea encodes speech audio a into multi-level audio features f low a , f mid a and f high a (blue). The speech transcript t is encoded by Et into text features ft (grey). Then a contrastive learning strategy is used to enforce the discriminative audio feature extraction by attracting text feature and high-level audio feature (green) while repelling from low/mid-level features (red). In Hierarchical Pose Inferer, the reference frames I are encoded by EID to represent speaker's identity f id (orange), which is then transformed to style coordinator C for multi-level feature blending (f 1 a , ..., f 6 a ). Finally the co-speech gesturesp <ref type="bibr" target="#b5">6</ref> (1:N ) are generated by cascaded bi-GRU based on initial poses p 1 (1:M ) in a coarse-to-fine manner (purple).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Large amounts of speaking videos with clear co-speech gestures are used for training. Given a video with N frames V = {I 1 , . . . , I N }, the skeletal poses of the upper body can be denoted as p</p><formula xml:id="formula_0">= {p 1 , . . . , p N | p i = [d i,1 , d i,2 , . . . , d i,J?1 ]}.</formula><p>Each p i is represented as the concatenation of unit direction vectors d i,j between J joints. The goal of our model G is to use the video's accompanying speech audio sequence a = {a 1 , . . . , a N } to recover p according to target's identity representation f id and initial poses {p 1 , . . . , p M }. Following the setting of Yoon et al. <ref type="bibr" target="#b73">[74]</ref>, the text transcripts t = {t 1 , . . . , t N } are also provided for training. With encoder E a for audio information extraction, the overall objective can be written as:</p><formula xml:id="formula_1">arg min G,Ea ||p ? G(E a (a)|f id , p 1 , . . . , p M )||.</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Audio Learner</head><p>Hierarchical Audio Feature Extraction. In most previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b73">74]</ref>, only high-level audio features are extracted to guide the synthesis of desired movements. However, it has been discussed that different semantics in audios contribute to different granularities in the movements of human poses <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>, which has been mostly ignored in previous works. We identify that such multilevel audio information could be inferred from the hierarchy of an audio encoder E a to improve the generation resolution. Notably, the rich semantics of hierarchical feature maps embedded at different layers of a deep neural network have been explored in other deep learning tasks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b68">69]</ref>. Therefore, the output deep feature f high Contrastive Learning Strategy. Though we expect the audio features can be learned automatically given the property of the encoder, additional text can further enforce the embedding of our desired information. Transcripts, which represent high-level linguistic information, can be directly recognized by Automatic Speech Recognition (ASR) models <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b75">76]</ref> from speech. Thus we propose to learn the association between provided transcripts and audios in a simple yet effective manner with contrastive learning. Our strategy is to leverage the natural synchronization between text and audio. While the high-level audio features should reflect the temporally-aligned transcripts, text can in turn encourage mid-and low-level audio features to capture crucial speech content-irrelevant information such as tone and cadence.</p><p>Specifically, we denote the feature extracted by the text encoder from transcript t as f t = E t (t). In our contrastive learning formulation, the high-level audio features aligned to the transcript serve as positive examples, which are denoted as f high a+ . Then we design two types of negative samples: (1) Firstly, high-level features extracted at other time steps, or from other clips are selected as negative samples to enforce the high-level audio feature capture correct semantic information from the aligned text; (2) Secondly, the low/mid-level audio features are expected to be discriminative to reflect other audio information rather than high-level semantics. Therefore, we enforce them all to repel the text feature. With the similarity function defined as sim(f 1 , f 2 ) = f1?f2 |f1||f2| , we can compute the final multilevel contrastive loss as:</p><formula xml:id="formula_2">L multi = ? log exp(sim(f t , f high a+ )/? ) K i=1 l?L exp(sim(f t , f l a(i) )/? ) ,<label>(2)</label></formula><p>where L = {low, mid, high}, and f low a(i) , f mid a(i) , f high a(i) denote the i-th sample of low/mid/high-level audio feature, respectively. K is the number of samples and ? is the temperature parameter that controls the concentration of distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hierarchical Pose Inferer</head><p>As discussed in Sec. 1, different levels of audio features contribute to different hierarchies of human poses. Thus we propose to hierarchically infer gestures for more delicate audio-based control. To this end, we detach the joints from human body ends (fingers) to the main structure (spine) in H stages as illustrated in <ref type="figure">Fig. 1</ref> (right). However, two questions still remain: 1) How to associate multiple levels of audios with different levels of joints; 2) How to supervise coarse-to-fine generation process. Multi-Level Feature Blending with Style Coordinator. Our solution to the first question is to learn automatic feature blending schemes for different levels depending on a person-related style coordinator. As human gestures corresponding to the same speech are diverse across persons, the idea of learning person-specific styles has been adopted in various audio-driven animation tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b73">74]</ref>. In this work, the style coordinator should be responsible for finding the suitable ratio among hierarchical audio features that contributes to each level of motion hierarchy.</p><p>Different from <ref type="bibr" target="#b73">[74]</ref> that uses one-hot labels to represent identities, we leverage a more general form by learning from the appearances of reference frames. The encoder E ID is used to extract identity feature from a few frames, f id = E ID (I 1 , . . . , I M ). Then through a linear layer and softmax function, f id is transformed into the style coordi-</p><formula xml:id="formula_3">nator C ? R 3?H , where 3 i=1 C[i, h] = 1.</formula><p>In this way, we can associate multi-level audio features with hierarchical body parts by linear blending:</p><formula xml:id="formula_4">f h a = C[1, h] ? f low a + C[2, h] ? f mid a + C[3, h] ? f high a ,<label>(3)</label></formula><p>where f h a denotes the blended audio feature for the h-th motion hierarchy. The procedure is illustrated in the middle of <ref type="figure">Fig. 1</ref>. To further facilitate style sampling at the inference stage, the Kullback-Leibler (KL) divergence loss L KLD between the feature space of f id and N (0, I) is adopted to assume Gaussian style embedding distribution. Coarse-to-Fine Pose Generation. We follow the human body dynamic rules to design a H-level (H = 6) body hierarchy ( <ref type="figure">Fig. 1 right)</ref>. At each level, the generation is affected by both the inferred pose from the previous level and the current level's audio feature rendered by the style coordinator. Such an idea is also similar to previous coarse-to-fine network designs <ref type="bibr" target="#b51">[52]</ref>.</p><p>In particular, we leverage the bi-directional GRU as motion decoder since the recurrent structure effectively captures spatial-temporal dependency in human motion as proved in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b69">70]</ref>. With the hierarchical audio feature of</p><formula xml:id="formula_5">the h-th level f h a = {f h a(1) , . . . , f h a(N ) }, the h-th level co- speech gesturep h = {p h 1 , ...,p h N } is generated by: p h i = [h i ;p h?1 i ; f h a(i) ] * W h + b h , h i = GRU(h i?1 ,p h i?1 ),<label>(4)</label></formula><p>where h i is the i-th hidden state, [?; ?] is the concatenation operation and * is the matrix multiplication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Objectives for Gesture Generation</head><p>Reconstruction Huber Loss. The generation process is constrained via a hierarchical Huber loss <ref type="bibr" target="#b33">[34]</ref> by measuring the distances between generated samplesp h i and ground truth p h i :</p><formula xml:id="formula_6">L huber = E 1 HN H h=1 N i=1 HuberLoss(p h i ,p h i ) ,<label>(5)</label></formula><p>where H is the number of motion hierarchy and N is the length of gesture sequence. We feed the blended audio feature to cascaded bi-GRU as generator G and leverage an adversarial loss for preserving realism following <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b73">74]</ref>:</p><formula xml:id="formula_7">L GAN = min G max D E p [log D(p)] + E a [log(1 ? D(G(E a (a)|f id , p 1:M ))] .<label>(6)</label></formula><p>Style Diverging Loss. To further avoid posterior collapse on speaker identity f id , we guide the generator to synthesize different poses with diverse style input following <ref type="bibr" target="#b73">[74]</ref>.</p><p>Assuming thatp(f id ) is the predicted pose depending on identity feature f id , we have:</p><formula xml:id="formula_8">L style = ?E min HuberLoss(p(f id(1) ),p(f id(2) )) ?f id(1) ? f id(2) ? 1 , ? ,<label>(7)</label></formula><p>where f id(1) , f id(2) are two different speaker identities and ? is the numerical clipping parameter.</p><p>Physical Constraint. Previous methods on co-speech gesture generation mostly fail to consider human physical constraint, which leads to unnatural poses and incoherent results. Therefore, we propose to add restrictions on the included angle between bones to ensure reasonable human poses. Concretely, the pose is represented as directional vectors, thus the angle between consecutive bone vectors must obey physical rules. We specifically calculate the mean and variance of each angle within TED-Expressive dataset, and expect our generated ones to fall within such a Gaussian distribution. The loss function for the physics constraint is the log-likelihood function:</p><formula xml:id="formula_9">L phy = ? J?1 j=1 log N (? j ; ? j , ? 2 j )<label>(8)</label></formula><p>where ? j is the j-th bone angle value, ? j and ? 2 j are the mean and variance of the j-th angle, respectively.</p><p>The overall learning objective for the whole framework is as follows:</p><formula xml:id="formula_10">L total = L GAN + ? h L huber + ? p L phy + ? s L style + ? k L KLD + ? c L multi ,<label>(9)</label></formula><p>where the ? h , ? p , ? s , ? k , ? c are weight coefficients. At the training stage, the hierarchical audio encoder E a , text encoder E t , speaker identity encoder E ID and hierarchical pose decoder are trained with back-propagation from the above overall loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>At the inference stage, we use speech audio as guidance while text is not needed. We further extract initial poses and speaker identity from a few reference images. If the reference image is unavailable, we can sample initial poses from dataset and sample speaker identity from normal distribution to generate co-speech gestures since we constrain identity space with L KLD . In this way, we can generate diverse gestures with multiple styles by sampling style vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Annotation 1</head><p>TED Gesture. TED Gesture dataset <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref> is a large-scale English-language dataset for speech-driven motion synthesis, which contains 1,766 TED videos of different narrators covering various topics. The extracted 3D human skeletons, aligned English transcripts and speech audio are all available. Following <ref type="bibr" target="#b73">[74]</ref>, we resample human poses with 15 FPS and sample the consecutive 34 frames with stride of 10 frames as input segments. We finally get 252,109 segments with length of 106.1h. In this dataset, human pose p is represented by direction vectors of 10 upper body joints. TED-Expressive. The pose annotations of TED Gesture limit to 10 upper body keypoints without expressive cospeech finger movements. Hence, to harvest more detailed pose annotation as training data, we use the state-of-art 3D pose estimator ExPose <ref type="bibr" target="#b16">[17]</ref> to extract 3D human skeleton as pseudo ground truth. In particular, we first annotate the 3D coordinates of 43 keypoints, including 13 upper body joints and 30 finger joints. Then we convert 3D coordinates into 42 unit direction vectors following <ref type="bibr" target="#b73">[74]</ref> to represent each bone for eliminating the influence of various bone lengths in training data. In this way, our 3D representation is invariant to root joint motion and body shape. At the inference stage, the mean bone length over dataset is multiplied to the predicted bone vectors for visualized results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>Baselines. We compare our method with : (1) Attention Seq2Seq <ref type="bibr" target="#b74">[75]</ref> which generates gestures from speech text by attention mechanism; (2) Speech2Gesture [24] that takes the whole-length audio spectrogram as input and generates motion sequence with an encoder-decoder architecture and adversarial training scheme; (3) Joint Embedding <ref type="bibr" target="#b2">[3]</ref>, a representative method that maps the text and motion to the same embedding space and creates motion from description text; (4) Trimodal <ref type="bibr" target="#b73">[74]</ref>, the state-of-art method that considers the trimodal context of text, audio and speaker identity to learn co-speech gestures. Note that some recent works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b54">55]</ref> lack open-source codes so far, thus we do not compare with them. All works are trained on the TED Gesture and TED-Expressive datasets for the same number of epochs with hyper-parameters optimized by grid search for best evaluation results. We also show the evaluation directly on the pseudo Ground Truth annotated in the dataset. Implementation Details. <ref type="bibr" target="#b0">1</ref> Following the settings of <ref type="bibr" target="#b73">[74]</ref>, we set N = 34 and M = 4, so that the data are segmented into 34-frame sequences and the first 4 frames serve as reference frames. The number of joint J is 10 for TED Gesture dataset and 43 for TED-Expressive dataset as mentioned in Sec. 4.1. The audio encoder backbone is a ResNetSE34 <ref type="bibr" target="#b17">[18]</ref> and the structure of text encoder E t is borrowed from <ref type="bibr" target="#b4">[5]</ref>. The reference video frames are resized into 224 ? 224, then passed into the speaker identity encoder E ID with visual backbone of ResNet-18 <ref type="bibr" target="#b26">[27]</ref> to extract speaker identity. The raw audios are converted to mel-spectrograms with FFT window size 1024, hop length 512. The word sequence is inserted with padding tokens to align with gestures. For each frame, 16 padded words and 0.25s mel-spectrogram with the target frame time-step in the middle are sampled as condition. The pose decoder is a cascaded 4-layer bidirectional GRU with a hidden size d s of 300 for each level of pose hierarchy. Empirically, we set ? = 0.07, ? = 1000, d a = 32, ? h = 200, ? p = 0.1, ? s = 0.05, ? k = 0.1, ? c = 0.1. The models are trained using Adam Optimizer with the learning rate of 1e ? 4 on 1 GTX 1080Ti GPU. TED Gesture <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref> TED-Expressive </p><formula xml:id="formula_11">Methods FGD ? BC ? Diversity ? FGD ? BC ? Diversity ? Ground</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Evaluation</head><p>Evaluation Metrics. We take the evaluation metrics that have been previously used in the co-speech gesture generation and music2dance for quantitative analysis. Fr?chet Gesture Distance (FGD) is used in <ref type="bibr" target="#b73">[74]</ref> to measure how close the distribution of generated gesture is to the real one. Note that for the evaluation on TED Gesture dataset, we use the feature extractor provided in <ref type="bibr" target="#b73">[74]</ref> for fair comparison. For the TED-Expressive dataset, we similarly train an auto-encoder on the TED-Expressive dataset and take the encoder part for feature extraction. FGD is calculated as the fr?chet distance between the latent representations of real gesture and generated gesture.</p><p>Beat Consistency Score (BC) is a metric for motion-audio beat correlation as proposed in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44]</ref>. However, since the kinematic velocities vary from different joints, we propose to use the change of included angle between bones to track motion beats. Concretely, we calculate the mean absolute angle change (MAAC) of angle ? j in adjacent frames by:</p><formula xml:id="formula_12">MAAC(? j ) = S s=1 T ?1 t=1 ?? j,s,t+1 ? ? j,s,t ? 1 S * (T ? 1) ,<label>(10)</label></formula><p>where S is the total number of clips over dataset, T is the number of frames for a clip and ? j,s,t is included angle between the j-th and the (j+1)-th bone of the s-th clip at timestep t. In this way, the angle change rate of frame t for the s-th clip is 1</p><formula xml:id="formula_13">J?1 J?1 j=1 (?? j,s,t+1 ? ? j,s,t ? 1 /MAAC(? j )</formula><p>). Then we extract the local optima whose first-order difference is higher than a threshold 1 to get kinematic beats. We follow <ref type="bibr" target="#b40">[41]</ref> to detect audio beat by onset strength <ref type="bibr" target="#b18">[19]</ref> and compute the average distance between every audio beat and its nearest motion beat as Beat Consistency Score:</p><formula xml:id="formula_14">BC = 1 n n i=1 exp(? min ?t x j ?B x ?t x i ? t y j ? 2 2? 2 ),<label>(11)</label></formula><p>where B x = {t x i } are the kinematic beats, B y = {t y j } are the audio beats and ? is a parameter to normalize sequences that is empirically set to 0.1 for experiments.</p><p>Diversity evaluates the variations among generated gestures corresponding to various inputs <ref type="bibr" target="#b38">[39]</ref>. Similarly, we use the same feature extractor in measuring FGD to map synthesized gestures into latent feature vectors and calculate the average feature distance for evaluation. Concretely, we randomly sample 60 speech audios from the test set to generate co-speech gestures and compute the average feature distance between 500 random combinated pairs. Evaluation Results. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We can see that our HA2G framework outperforms existing methods on both datasets. Since our method establishes motion hierarchy and generates gestures in a coarse-to-fine manner, we can learn the diverse motion pattern of different human body parts and perform the best on FGD metric. Note that the improvement of FGD is smaller on TED Gesture dataset compared to TED-Expressive. This is due to the absence of finger information in TED Gesture dataset, which makes the motion hierarchy lower and the improvement brought by our hierarchical framework less significant. We can find that both Speech2Gesture <ref type="bibr" target="#b23">[24]</ref> and ours synthesize synchronous gestures to speech with high values on BC. But they tend to create unnatural poses and hence perform fair on FGD. In terms of Diversity, the discriminative feature extraction at multiple granularities enables us to excavate fine-grained audio-pose associations, thus capturing diverse speaking styles compared to baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Evaluation</head><p>Subjective evaluation is crucial for judging the quality of results in generation tasks. Here we show the key frames comparison of our method against ground truth and SOTA baselines (as listed in Sec. 4.2) in <ref type="figure">Fig. 2</ref>. For two cases, both Attention Seq2Seq <ref type="bibr" target="#b74">[75]</ref> and Joint Embedding <ref type="bibr" target="#b2">[3]</ref> generate slow and invariant motions that are misaligned to speech as demonstrated in red rectangles of <ref type="figure">Fig. 2</ref>. While Trimodal <ref type="bibr" target="#b73">[74]</ref> generates diverse gestures, the rigid motion pattern makes them mismatch to audio beats. For example, they stiffly move hands up and down with asynchronous beats to speech audio (see the red rectangle on the  <ref type="table">Table 2</ref>. User study results on motion naturalness, smoothness and synchrony. The rating is on a scale of 1-5, with the larger the better. right). Both our method and Speech2Gesture <ref type="bibr" target="#b23">[24]</ref> create synchronous motions, but they synthesize unnatural poses, e.g., the twisted hands in both cases as highlighted in <ref type="figure">Fig. 2</ref>. The hierarchical cross-modal association against singlelevel design also leads to more diverse results than <ref type="bibr" target="#b23">[24]</ref>. User Study. <ref type="bibr" target="#b1">2</ref> We conduct a user study on motion naturalness, smoothness and the generated co-speech gestures' synchrony to speech. In particular, we randomly sample 20 speech clips from test set of TED-Expressive to generate results for ground truth (tracked) annotations, baselines and our method. The study involves 24 participants. We adopt the widely-used Mean Opinion Scores (MOS) rating protocol, which requires the participants to rate three aspects of generated motions: (1) Naturalness; (2) Smoothness; (3) Synchrony between speech and generated gestures. The rating is based on a scale of 1 to 5, with 5 being the most plausible and 1 being the least plausible.</p><p>The results are shown in <ref type="table">Table 2</ref>. Since both Attention Seq2Seq <ref type="bibr" target="#b74">[75]</ref> and JointEmbedding <ref type="bibr" target="#b2">[3]</ref> generate slow and near-stationary results, they score reasonably low on naturalness and synchrony, and trivially perform well on smoothness, which is even better than ground truth due to the motion jitter in ExPose annotation. Although Speech2Gesture <ref type="bibr" target="#b23">[24]</ref> performs well on synchrony, unnat-2 Please refer to Supple. for more details about user study. ural poses lead to fair results on naturalness and smoothness. Moreover, as our hierarchical design can capture finegrained associations between multi-level features and diverse body parts, we score better than Trimodal <ref type="bibr" target="#b73">[74]</ref> on all three aspects, with comparable results against ground truth. Note that to measure the disagreement on scoring among the participants, we also calculate the Fleiss's-Kappa 3 statistic on 24 participants' ratings over all methods. The Fleiss-Kappa value is 0.837, which is comparatively high and can be interpreted as "almost perfect agreement".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this section, we present ablation studies on two key modules proposed in our framework. We report the results implemented on the TED-Expressive dataset. Hierarchical Audio Learner. To show the effect of multilevel audio feature in generating co-speech gesture, we conduct experiments on our model (1) f low a only, which means we only use low-level feature from hierarchical audio encoder, i.e., the weight for low-level is set as <ref type="bibr" target="#b0">1</ref>  , which states the situation without cross-level negative samples; <ref type="bibr" target="#b5">(6)</ref> w/o text, in this setting the input of speech text is not used, so we do not use the contrastive loss L multi for audio-text alignment and discriminative audio feature extraction. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>, which indicates the efficacy of Hierarchical Audio Learner. Concretely, the only use of single-level audio feature fails to excavate information at multiple granularities, thus leading to degradation in performance. Besides, the contrastive learning strategy further improves performance since it achieves discriminative audio feature extraction with the self-supervision of audio-text alignment. More importantly, we find that our method without text outperforms Yoon et al. <ref type="bibr" target="#b73">[74]</ref> with the input of text. This demonstrates that the hierarchical design and coarseto-fine generation manner can synthesize gestures of higher quality despite lack of text, enabling our method to handle general scenarios where video transcripts are unavailable.</p><p>Another ablation study relates to the Hierarchical Audio Learner is why we adopt contrastive learning strategy for discriminative feature extraction. We take inspiration from the fact that ASR models can semantically align text and audios, thus multi-level semantic information can be extracted from audio itself. However, the amount of data provided in the dataset is insufficient for training an expert ASR model, which leads to our choice of hierarchical contrastive design. For the ablation experiment, we use a welltrained ASR model <ref type="bibr" target="#b70">[71]</ref> as the audio encoder and generate co-speech gestures without contrastive strategy. The low, middle and high level features are also extracted from the backbone in a similar way as our method. We denote this variant of HA2G as HA2G-ASR. The comparisons on the TED-Expressive dataset are shown in the <ref type="table" target="#tab_3">Table 3</ref>. We can notice that the prior knowledge of pretrained ASR network prevents outlier predictions, which achieves competitive results compared to ours. This illustrates that using different levels of ASR features will benefit gesture generation. Note that the pretrained ASR network is trained on a large amount of additional data, while HA2G is trained with just a multi-level contrastive loss without involving other pretrained networks and additional data.  <ref type="table">Table 4</ref> shows the results, which verify that Hierarchical Pose Inferer improves the performance. The pose hierarchy and distinct audio feature of each level enable the model to grasp fine-grained audiopose associations of different body parts, making generated pose more vivid. The physical regularization L phy enhances FGD with more realistic human poses. Note that w/o body hierarchy outperforms w/o hand hierarchy. This is reasonable since the hand motion is more subtle, so hierarchical architecture's impact on hand is more significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Conclusion. In this paper, we propose a novel framework Hierarchical Audio-to-Gesture (HA2G) for co-speech gesture generation. We introduce Hierarchical Audio Learner with a contrastive learning strategy that extracts discriminative audio representations across semantic granularities. Then we propose Hierarchical Pose Inferer with a physical regularization to render the entire human pose gradually in a hierarchical manner. Extensive experiments demonstrate the superior performance of our proposed approach on cospeech gesture generation with high fidelity.</p><p>Limitation. From the dataset perspective, our model is trained on an English-based corpus, which brings inductive bias on language. How to build a versatile model to generate co-speech gesture of diverse languages is a worthy direction for the community to explore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Document: Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</head><p>A. More Details about Dataset</p><p>Choice of Data and Data Collection. Many learningbased approaches use motion and gesture training data captured in a MoCap studio with complex motion capture systems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>. They can acquire more accurate human motion data compared to automatic annotations on internet videos. However, such methods have the following drawbacks: 1) Owing to the high cost of MoCap data, it is hard to build a large-scale corpus of data covering various speaking contents and styles. For example, the length of MSP-AVATAR <ref type="bibr" target="#b57">[58]</ref> and Personality Dyads Corpus <ref type="bibr" target="#b64">[65]</ref> are less than 3h. 2) When capturing co-speech gesture data in the studio, the actors/actresses are asked to deliberately talk with their arms and hands moving, which contributes to the unnaturalness and exaggeration of captured motion data. Therefore, we follow the previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref> to collect internet videos and annotate 3D human pose as pseudo ground truth for later training. Specifically, Ginosar et al. <ref type="bibr" target="#b23">[24]</ref> and Habibie et al. <ref type="bibr" target="#b24">[25]</ref> use a speaker-specific gesture dataset of a very small number of speakers, i.e., 10 speakers in <ref type="bibr" target="#b23">[24]</ref> and 6 speakers in <ref type="bibr" target="#b24">[25]</ref>, making them unable to transfer to general speaking styles. TED Gesture dataset is proposed by Yoon et al. <ref type="bibr" target="#b73">[74]</ref> which contains over 1,700 TED talks covering diverse topics and speaker styles. Following Yoon et al. <ref type="bibr" target="#b73">[74]</ref>, we propose to build our TED-Expressive dataset based on the raw videos of TED talks. Differently, since the flexible finger movement matters a lot when people talk, we add the information of finger keypoints for more expressive co-speech gesture dataset establishment. We collect internet videos from the official TED channel on YouTube. <ref type="bibr" target="#b3">4</ref> We finally get 1,764 videos and their corresponding text transcripts. Pose Annotation and Post-Processing. To get the reliable pseudo ground truth of co-speech human upper body pose with finger keypoints, we leverage the state-of-art 3D human motion estimator ExPose <ref type="bibr" target="#b16">[17]</ref> for annotation. Similar to the step of <ref type="bibr" target="#b74">[75]</ref>, we segment videos into smaller shots by their scenes and annotate the 2D human pose of each frame by OpenPose <ref type="bibr" target="#b10">[11]</ref>. With the 2D pose prior provided by OpenPose, we use ExPose <ref type="bibr" target="#b16">[17]</ref> to annotate 3D upper body keypoints. Concretely, we use 43 keypoints, including 13 upper body joints (spine, neck, nose, left/right eyes, ears, shoulders, elbows and wrists, totally 13 = 3 + 5 * 2) and 30 finger joints (3 joints for each finger, totally 30 = 3 * 5 * 2). Then we select shots of interest under the following conditions: 1) the above mentioned 43 keypoints of speaker are visible for more than 50% frames of a clip; 2) the speaker should not remain almost still in the whole shot, i.e., the variance of motion is quite small; 3) the clip is longer than 5s. The statistics of TED Gesture and TED-Expressive dataset are recorded in <ref type="table" target="#tab_5">Table 5</ref>. For the TED Gesture dataset, we randomly split the segments into the 80% training set, 10% validation set, 10% test set and finally get 199,384; 26,795; and 25,930 segments in each partition.</p><p>Pose Representation and Quality. After the filtering process, we effectively eliminate the influence of bone length by normalizing them into 42 unit directional vectors to represent each bone. Such 3D representation is invariant to root joint motion and body shape, thus making it more stable in the training phase. At the inference stage, the mean bone length over the training set is multiplied to predicted bone vectors for visualized results. The whole pipeline is automated, which facilitates us to build a large corpus of co-speech gesture dataset. <ref type="figure">Figure 5</ref> shows the correspondence between keypoint index and joints. We can see that there are totally 43 annotated upper body keypoints, which are then transformed into 42 unit direction vectors as mentioned above.</p><p>As the pose annotations serve as pseudo ground truth in our pipeline, the quality of annotations is crucial for training. However, since the pose representation is 3D, we can not follow Ginosar et al. <ref type="bibr" target="#b23">[24]</ref> to evaluate the quality of annotations by automatic pipeline against human annotations. But the high performance of ExPose on benchmark datasets and our filtering algorithm guarantee that the data quality is good enough for utilization. Please refer to ExPose <ref type="bibr" target="#b16">[17]</ref> for the detailed quantitative 3D pose estimation results on benchmark dataset. Overall, we use the open-source code of Trimodal <ref type="bibr" target="#b73">[74]</ref>, OpenPose <ref type="bibr" target="#b10">[11]</ref> and ExPose <ref type="bibr" target="#b16">[17]</ref> following their licenses <ref type="bibr" target="#b4">5</ref> . Speech Audio Pre-Processing. The speech audios accompanied TED videos are raw waveforms, which are processed to 16kHZ and convert to mel-spectrograms as 2D  time-frequency representations for more compact information preservation. The FFT window size is 1024 and the hop length is 512. Speech Text Pre-Processing. We collect speech text input with the transcripts of TED videos. Then, the onset timestamps of each word are extracted by the Gentle forced aligner <ref type="bibr" target="#b55">[56]</ref> to insert padding tokens. For example, for the speech text "Good morning everyone", if there is a short pause between the word "morning" and "everyone", then the padded word sequence is "Good morning ? ? everyone" as padded by Gentle if the time of this sentence is 5. Following the process of <ref type="bibr" target="#b73">[74]</ref>, the padded word sequences are transformed into word vectors of 300 dimensions through a word embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture Details</head><p>Audio Encoder E a . The audio encoder is a ResNetSE34 borrowed from <ref type="bibr" target="#b17">[18]</ref>.  <ref type="table">Table 6</ref>. The detailed feature dimensions after each operation are shown in <ref type="table">Table 7</ref>. In this way, the hierarchical audio features are transformed into the same shape and the time dimension is exactly the frame number of a clip, i.e., 34 in our experiment, which is convenient for RNN-based model to take information of each time step as input. After the linear blending of multi-level features, hierarchical audio features for different levels of body parts are established and finally feed to cascaded bi-GRU for pose generation in a coarse-to-fine manner.</p><p>Text Encoder E t . With the speech text pre-processing mentioned above, the word sequences are transformed into word vectors. Next, these word vectors are encoded by an off-the-shelf temporal convolutional text encoder <ref type="bibr" target="#b4">[5]</ref>. The text encoder E t is 4-layered, the receptive field is 16 padded words centered at the current time step and the output dimension of text feature f t = E t (t) is 32. In this way, the high-level audio feature and text feature at time-step t are both of dimension 32, which enables the later contrastive learning strategy to leverage the natural audio-text correspondence for achieving discriminative cross-modal feature extraction. Speaker Identity Encoder E ID . The speaker identity encoder network E ID uses the standard ResNet-18-S5 model. And we do not load ImageNet pretrained weights. We also remove Global Average Pooling (GAP) and the final classifier to only leave the visual backbone for visual feature extraction. The input of the encoder is the first M reference frame images of a clip {I 1 , . . . , I M } and the output is speaker identity embedding f id = E ID (I 1 , . . . , I M ) ? R 18 . Then through a linear layer of FC <ref type="bibr" target="#b17">(18,</ref><ref type="bibr" target="#b17">18)</ref> and softmax function, f id is transformed into the style coordinator</p><formula xml:id="formula_15">C ? R 3?H , where 3 i=1 C[i, h] = 1.</formula><p>In this way, the speaker identity can affect hierarchical audio feature weight on motion hierarchies. Motion Hierarchy Establishment. The skeleton of human body is like a tree structure, where the father-joint carries the child-joint to move. To effectively learn the dynamic patterns of different human body parts, we propose to detach the joints from human body ends (fingers) to the main structure (spine) step-by-step and build a motion hierarchy of totally 6 hierarchies: 1) Nose, neck, spine and left/right eye, ear, shoulder; 2) Add left and right elbow; 3) Add left and right wrist; 4) Add left and right finger's first joint; 5) Add left and right finger's second joint; 6) Add left and right finger's third joint. Note that we do not separate more detailed motion hierarchy inside the skeleton of human head. Because there is hardly internal movement in the skeleton of head part, i.e., nose, left/right eye and ear. The dynamic among the keypoints of head part is quite trivial under our setting, so it is unreasonable to separate them into different levels of motion hierarchies. Coarse-to-Fine Pose Generator. According to the established motion hierarchy, the number of keypoints for 6 hierarchies is 9, 11, 13, 23, 33, 43. Since the poses are processed into 3D unit directional vectors representation, the pose dimension of 6 hierarchies is <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">96,</ref><ref type="bibr">126</ref>. The cascaded hierarchical pose generator contains 6 bi-directional GRU, with the input of corresponding hierarchy pose and audio feature and the hidden size of 300. Note that for the first motion hierarchy, the poses of the first M frames serve as initial poses and are denoted a? p 0 = {p 0 1 , ..., p 0 M , 0, ..., 0}; for the later hierarchies, the output from the last pose hierarchy is leveraged to initialize corresponding keypoints. Therefore, d s = 300, d a = 32, the whole pipeline to generate gestures. For the situations where reference frames and initial poses are unavailable, we can sample a style vector from normal distribution to serve as speaker identity f id . Then we can sample an arbitrary sequence of initial poses from the dataset to generate the gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Statistics in Physical Constraint</head><p>Previous methods on co-speech gesture generation mostly fail to consider human physics constraints, which contributes to unnatural pose and incoherent results. Therefore, we propose to add restrictions on the included angle between bones to ensure reasonable human pose. Concretely, the pose is represented as bone direction vector, which is rendered as p = [d 1 , d 2 , ? ? ? , d J?1 ] and J is the total number of joints. For the j-th bone vector d j ? R 3 and the (j+1)-th bone vector d j+1 ? R 3 , we can compute their included angle ? j by the arc-cosine function on their cosine value. Since there is no benchmark dataset with accurate finger keypoints annotations under co-speech settings, we use the hand pose estimator ExPose <ref type="bibr" target="#b16">[17]</ref> to annotate the TED-Expressive dataset. With the pseudo ground truth, we can calculate the mean and variance of each angle, which later serve as the mean and variance of Gaussian distribution. The loss function for the physics constraint is the loglikelihood function:</p><formula xml:id="formula_16">L phy = ? log J?1 j=1 N (? j ; ? j , ? 2 j ) = ? J?1 j=1 log N (? j ; ? j , ? 2 j ),<label>(12)</label></formula><p>where ? j = arccos dj ?dj+1 ?dj ??dj+1? is the j-th angle value, ? j and ? 2 j are the mean and variance of the j-th angle respectively. We illustrate in <ref type="table" target="#tab_1">Table 10</ref> the means and variances of the included angles (0-180 degrees) around two important joints. In particular, we use ? s to denote the included angle around the shoulder joint and ? e for the included angle around the elbow joint. Although some angles may not strictly follow the Gaussian distribution, the intention of physical constraint is to prevent outlier predictions. Thus the assumption of Gaussian distribution could play the role in regularizing generated poses. The ablation study in Table 4 (the setting of "w/o L phy ") shows the effectiveness of such a constraint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Analysis on Beat Consistency Score Metric</head><p>Beat Consistency Score (BC) is a metric adapted by us for motion-audio beat correlation. Previous methods detect motion beats by finding the local optima of kinematic velocity <ref type="bibr" target="#b40">[41]</ref>, while we propose to utilize the change of included angle between bones to track motion beats. The main reasons are two-fold: 1) previous methods are under the setting of music2dance, where human body involves a global body translation in a large scale. In other word, all of the human's body joints move fast when people dance and the velocity quickly drops when they stop to match a music beat. While in our co-speech gesture settings, the arms are comparatively still and the fingers are more flexible, so their moving scales vary a lot, we can not directly sum up them.</p><p>2) Compared to using the shifts of keypoints, we propose to use the included angle to detect motion beat. This is because the human body follow a tree structure. If the arm moves, hand and wrist will follow the movement of arm, which is similar to the process of orbital revolution and self rotation: the father-joint carry the child-joint to move like the orbital revolution and the internal movement of childjoint resembles self rotation. Therefore, directly calculating the Euclidean distance for each joint is unreasonable.</p><p>After calculating mean absolute angle change (MAAC) of angle ? j , we can calculate the sum angle change rate of a certain frame t for the n-th clip as:</p><formula xml:id="formula_17">1 J ? 1 J?1 j=1 ?? j,n,t+1 ? ? j,n,t ? 1 MAAC(? j ) .<label>(13)</label></formula><p>Then we propose to extract the kinematic beat through filtering the angle change rate by following conditions: 1) The angle change rate is a local optimum, e.g., the angle change rate of 9, 10, 11 time-step is 0.2, 0.1, 0.2, respectively. Then the time-step 10 is a local optimum. 2) The difference of the local optima with either side time-step is larger than a threshold. This is to filter the trivial situation where angle change rates are almost the same during a period of time and guarantee a sudden change of angle change rate as motion beat. For example, the angle change rate of 8, 9, 10, 11, 12 time-step is 0.11, 0.1, 0.11, 0.1, 0.11. It improper to take the time-step 9 and 11 as motion beat. The threshold controls what extent of angle change rate difference is perceived it as a motion beat. A very low threshold will detect the near-stationary motion sequence as many motion beats if there are many trivial beats of type 2 mentioned in the last paragraph. A very high threshold will ignore the normal motion beat. We present the influence of threshold over all baseline method in <ref type="figure" target="#fig_4">Fig. 3</ref>. We can see that our method can achieve superior performance on BC metric with high robustness to threshold compared to baseline methods. Note that both Attention Seq2Seq <ref type="bibr" target="#b74">[75]</ref> and Joint Embedding <ref type="bibr" target="#b2">[3]</ref> show low value of BC Score over all  <ref type="table">Table 9</ref>. Detailed structure and feature shape of Pose Auto-Encoder. ? Note that in the table, Conv1d/ConvTranspose1d (c, k, s) means the output of the convolution/transpose-convolution is c, kernel size is k and the stride is s; LeakyReLU, BatchNorm1d (c) means the leaky-relu and batch-norm operation on the feature of channel size c.</p><p>threshold, which also proves that they fail to generate results that are synchronous to speech since their gestures are almost still. Although Speech2Gesture <ref type="bibr" target="#b23">[24]</ref> shows higher performance on low threshold, they match the trivial beats and perform lower than our method on normal thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Choice of Speaker Identity Extraction</head><p>We leverage RGB frames rather than poses for identity information extraction. The appearances of different identities would vary significantly. Though dynamic information can hardly be inferred from the M inputs, our method focuses more on the appearance information like the speaker's height, age and nationality. Inferring speaking styles from appearances or identities only has also been proven effective in Yoon et al. <ref type="bibr" target="#b73">[74]</ref>. The only speaker-related information we can access is the initial frames, thus we are trying to make the best use of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Ablation Study on TED Gesture Dataset</head><p>We further conduct ablation study on TED Gesture and report results below, which shows the effectiveness of each module. The TED Gesture dataset lacks finger annotations, resulting in the lower motion hierarchy and less significant performance improvement brought by each module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Influence of Reference Frame Number M</head><p>All the models are implemented with the same amount of information given as the input, including the number of initial poses M . The setting of using M = 4 frames as seed pose is proposed in Trimodal <ref type="bibr" target="#b73">[74]</ref>. Our whole setting basically follows theirs. To investigate the influence of M , we further set M as 1 and 7. The results below suggest that the performance gain derived from additional initial poses is marginal, which shows the robustness of the proposed method to hyper-parameter M .</p><p>M FGD ? BC ? Diversity ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Randomness of Diversity Metric</head><p>The Diversity metric is adapted from <ref type="bibr" target="#b38">[39]</ref> and is popularly used in other works <ref type="bibr" target="#b32">[33]</ref>. In order to mitigate the influence of randomness, we randomly sample 500 pairs, which is much more than the number 200 in <ref type="bibr" target="#b38">[39]</ref>. To ensure and verify the reproducibility, we further conduct the evaluation 10 times (create random samples 10 times with different random seeds). The results are listed in the table below. We can see that the difference is comparatively small between each group, which proves that the Diversity metric can be reproduced and the sample number of 500 is enough to alleviate randomness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Limitations and Future Work</head><p>Our work mainly have the following limitations: 1) Since when people talk to others, the most important nonverbal behavior is upper body movements. Hence we only delve into the co-speech gesture generation of human upper body, without considering full body motions. This will make our trained avatars fail to walk around like TED Talk narrators. 2) In the TED Talk dataset, some data samples have very strong prior on human hand pose at the specific settings that will affect training, e.g., people with speaker or chalk in their hand as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. 3) Although our proposed approach can capture the fine-grained motions of co-speech finger movements and diverse dynamic patterns of different human body parts, we still find it difficult to capture some very subtle movements like "shrug". This is mainly due to the fact that there hardly exists such action samples in the dataset and it is very hard for our model to learn such dynamic patterns. In future work, we will improve our method to capture full-body co-speech gestures and some very minor pose movements and we will enhance the automatic dataset pipeline algorithm to filter samples with strong prior that may affect our training quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Social Impact</head><p>Making co-speech gestures to complement conversational information is a kind of innate non-verbal behavior for human, while this work encourages the machine intelligence to be equipped with such ability, especially learn to animate the subtle hand and arm motions. Therefore, this work can exert positive impacts on both machine learning research and application field. On the one hand, the proposed approach identifies the advantages of hierarchical architecture design to extract cross-modal information at multiple granularities and excavate the fine-grained audio-pose associations, which can further facilitate cross-modal animation tasks like talking face generation and music2dance prospectively. On the other hand, the speech-driven gesture generation technique has a wide range of beneficial applications for society, including digital human broadcaster and social robots. Specifically, it could also assist dumb people to learn communication skills by teaching sign language with expressive human-like motions. Since the generated motions are all skeleton-based, they hardly have detrimental impact in most cases. Still, it may potentially lead to the misuse of copyrighted 3D character models if we animate them without permission. Besides, the bias of the dataset may have some negative impact, e.g., some gestures may have negative meanings for some nations. But we believe the proper use of this technique will enhance positive societal development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Details of User Study</head><p>The study involves 24 participants. They take 25-35 min to complete the task. The participants are 12 females and 12 males, with age range of 18-24 years old. The users are unaware of which motion sequence corresponds to which method or even the ground truth. Specifically, we randomly shuffle the order of video placement for all methods every time, so that participants can concentrate only on the quality of generated results for fair comparison.</p><p>We have provided the users with instructions before conducting the study. The participants are asked to judge the three perspectives in the following manner: a) For "Naturalness", does motion look natural and like real human poses regardless of background speech? There should not be any strange angles and unnatural movements. b) For "Smoothness", does the generated motion maintain smoothness in temporal dimension, with no obvious rigid or stuck movements, regardless of background speech audio? c) For "Synchrony", does the generated motion match the corresponding speech audio both rhythmically and semantically? We also show the raw videos of TED Talk before participants' rating process to help them make more accurate judgement. 24 participants of 12 females and 12 males are involved in the study, covering 4 nationalities in order to bridge biases. 12 of them are researchers from the field of deep generative models and others are from other fields.  <ref type="table" target="#tab_1">13   14  15  16 17  18  19   20  21  22   23  24  25   26  27  28   29  30  31  32  33  34   35  36  37   38   39</ref> 40 41 42 7 6 <ref type="figure">Figure 5</ref>. The detailed 3D keypoints output annotation by Ex-Pose <ref type="bibr" target="#b16">[17]</ref>. In particular, we annotate 43 upper body keypoints, including: spine (0), neck (1), left shoulder (2), right shoulder <ref type="formula" target="#formula_4">(3)</ref>, left elbow (4), right elbow <ref type="formula" target="#formula_6">(5)</ref>, left wrist <ref type="formula" target="#formula_7">(6)</ref>, right wrist <ref type="formula" target="#formula_8">(7)</ref>, left index <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10)</ref>, left middle <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13)</ref>, left pinky <ref type="bibr" target="#b13">(14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16)</ref>, left ring <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19)</ref>, left thumb <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22)</ref>, right index <ref type="bibr" target="#b22">(23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25)</ref>, right middle <ref type="bibr" target="#b25">(26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28)</ref>, right pinky <ref type="bibr" target="#b28">(29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31)</ref>, right ring <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34)</ref>, right thumb <ref type="bibr" target="#b34">(35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37)</ref>, nose <ref type="bibr" target="#b37">(38)</ref>, right eye (39), left eye <ref type="formula" target="#formula_5">(40)</ref>, right ear <ref type="formula" target="#formula_5">(41)</ref>, left ear <ref type="bibr" target="#b41">(42)</ref>. Note that the holistic upper body with keypoints index is shown at the top of figure, the zoomin images of left hand and right hand with detailed annotations are shown at the bottom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>a</head><label></label><figDesc>of E a , the feature f mid a encoded in the middle of the audio encoder and the feature f low a encoded in the shallow of E a are specifically leveraged. We expect f low a , f mid a , f high a to represent the low, middle and high level audio features respectively, as shown in blue block of Fig. 1. These hierarchical features are used for inferring poses in Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>W h ? R (ds+d h?1 p +da)?d h p and b h ? R d h p are parameters where d s , d a and d h p are the dimensions of hidden state, audio feature and the h-th level posep h , respectively. Note that the poses of the first M frames serve as initial poses and are denoted asp 0 = {p 0 1 , ..., p 0 M , 0, ..., 0}.In this way, finegrained correspondences between audio sequence and cospeech gestures are jointly built in a coarse-to-fine manner. The last layer's outputp H from the hierarchy is our desired result. This procedure is depicted in the right part ofFig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hierarchical Pose Inferer. The experiments of Hierarchical Pose Inferer on our model contain: (1) Holistic, which means we do not use pose hierarchy and directly generate whole-body pose like previous methods [3, 24, 74, 75]; (2) w/o hand hierarchy, where the hand poses are generated holistically while body hierarchy remains; (3) w/o body hierarchy, where body poses are generated holistically while hand hierarchy remains; (4) Same audio f h a , which means we pass identical hierarchical audio features to each level of motion hierarchy, i.e., all columns of style coordinator C are same in Eq. 3; (5) w/o L phy .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Specifically, we define the features output from ResNet Stage-2 as shallow feature map, features output from ResNet Stage-3 as middle feature map, features output from ResNet Stage-4 as deep feature map. Then, a series of upsampling, convolution, batchnorm and linear layers transform corresponding audio feature maps into the same size. When the input audio mel-spectrogram of size 1 ? 128 ? 70, the channel dimension and frequency, time resolutions of different level features f low a , f mid a and f high a ? R 32 with their corresponding operations are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The influence of threshold on Beat Consistency (BC) Score metric. We present the BC value of baseline methods and ours under the threshold of range 0.01 to 0.3 with step size of 0.01. metric\setting f high a only Holistic w/o L phy HA2G-ASR HA2G Full FGD 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Examples of data samples at specific setting with very strong prior on hand pose. We implement the mosaic operations for all the images to eliminate personally identifiable information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. In this section, we first formulate the problem in Sec. 3.1, and then elaborate the Hierarchical Audio Learner which extracts hierarchical audio features in Sec. 3.2. Sec. 3.3 introduces the Hierarchical Pose Inferer to perform multi-level feature blending and co-speech gesture synthesis. Finally, training objectives for gesture generation are described in Sec. 3.4.</figDesc><table><row><cell>"A man I refer to as, hope and courage ?" = (1: ) = (1: )</cell><cell>low</cell><cell>negative</cell><cell>mid</cell><cell cols="2">positive ? multi high Hierarchical Audio Learner</cell><cell>(1: ) 1</cell><cell>2 3 4 1</cell><cell>GRU GRU GRU GRU</cell><cell>? ? ? ?</cell><cell>? (1: ) 2 ? (1: ) 3 ? (1: ) 4 ? (1: ) 1</cell><cell>Hierarchical Huber Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>? KLD</cell><cell cols="2">Hierarchical Pose Inferer</cell><cell>5</cell><cell>GRU</cell><cell>?</cell><cell>? (1: ) 5</cell></row><row><cell>= (1: )</cell><cell></cell><cell></cell><cell></cell><cell cols="2">style sampling ? style</cell><cell>FC, softmax</cell><cell>6</cell><cell>GRU</cell><cell>?</cell><cell>? (1: ) 6</cell><cell>? GAN , ? phy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The quantitative results on TED Gesture<ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75]</ref> and TED-Expressive. We compare the proposed Hierarchical Audio-to-Gesture (HA2G) against recent SOTA methods<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref> and ground truth under three metrics. For FGD the lower the better, and the higher the better for other metrics. Note that the FGD results of<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref> on TED Gesture are reported from<ref type="bibr" target="#b73">[74]</ref>.</figDesc><table><row><cell>Truth</cell><cell>0</cell><cell>0.795</cell><cell>110.821</cell><cell>0</cell><cell>0.723</cell><cell>175.231</cell></row><row><cell cols="3">Attention Seq2Seq [75] 18.154 0.186</cell><cell>92.176</cell><cell cols="2">54.920 0.155</cell><cell>122.693</cell></row><row><cell>Speech2Gesture [24]</cell><cell cols="2">19.254 0.764</cell><cell>98.095</cell><cell cols="2">54.650 0.714</cell><cell>142.489</cell></row><row><cell>Joint Embedding [3]</cell><cell cols="2">22.083 0.177</cell><cell>91.223</cell><cell cols="2">64.555 0.131</cell><cell>120.627</cell></row><row><cell>Trimodal [74]</cell><cell cols="2">3.729 0.688</cell><cell>102.539</cell><cell cols="2">12.613 0.592</cell><cell>154.088</cell></row><row><cell>HA2G (Ours)</cell><cell cols="2">3.072 0.769</cell><cell>108.086</cell><cell cols="2">5.306 0.715</cell><cell>173.899</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study results of Hierarchical Audio Learner.</figDesc><table><row><cell>and weights</cell></row></table><note>Table 4. Ablation study results of Hierarchical Pose Inferer.Sec. 3.2 for contrastive learning; (5) w/o f low,mid a?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Statistics # of Videos Interest Shots Length # of Segments Interest Ratio</figDesc><table><row><cell>TED Gesture [74, 75]</cell><cell>1,766</cell><cell>106.1h</cell><cell>252,109</cell><cell>25%</cell></row><row><cell>TED-Expressive</cell><cell>1,764</cell><cell>100.8h</cell><cell>240,447</cell><cell>21%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Statistics of the TED Gesture and TED-Expressive dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Statistics Left ? s Left ? e Right ? s Right ? e</figDesc><table><row><cell>mean( ? )</cell><cell>116.6</cell><cell>75.1</cell><cell>127.1</cell><cell>85.3</cell></row><row><cell>var( ? )</cell><cell>9.01</cell><cell>7.30</cell><cell>7.53</cell><cell>7.22</cell></row></table><note>Table 10. Statistics of important important joint angles.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Detailed structure and feature shape of Discriminator D. ? Note that in the table, the meanings of contents in operations</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Discriminator D</cell></row><row><cell>Feature</cell><cell cols="2">Feature Shapes</cell><cell>Operations</cell></row><row><cell>Input</cell><cell></cell><cell>34 ? 126</cell><cell>Transpose (0, 1)</cell></row><row><cell cols="2">Pre-Conv Layer-1</cell><cell>126 ? 34</cell><cell>Conv1d (126, 16, 3), BatchNorm1d (16), LeakyReLU (0.2)</cell></row><row><cell cols="2">Pre-Conv Layer-2</cell><cell>16 ? 32</cell><cell>Conv1d (16, 8, 3), BatchNorm1d (8), LeakyReLU (0.2)</cell></row><row><cell cols="2">Pre-Conv Layer-3</cell><cell>8 ? 30</cell><cell>Conv1d (8, 8, 3), Transpose (0, 1)</cell></row><row><cell cols="2">Bi-Directional GRU</cell><cell>28 ? 8</cell><cell>Bi-Directional GRU (8, 64)</cell></row><row><cell>FC-1</cell><cell></cell><cell>28 ? 64</cell><cell>FC (64, 1), Squeeze(1)</cell></row><row><cell>FC-2</cell><cell></cell><cell>28</cell><cell>FC (28, 1), Sigmoid</cell></row><row><cell>Output</cell><cell></cell><cell>1</cell><cell>-</cell></row><row><cell cols="4">column are: Conv1d (in channels, out channels, kernel size), BatchNorm1d (feature dim), LeakyReLU (alpha), Transpose (axis1, axis2),</cell></row><row><cell cols="4">Bi-directional GRU (in size, hidden size), FC (in size, out size), Squeeze (axis), Sigmoid.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pose Encoder</cell></row><row><cell cols="3">Feature Feature Shapes</cell><cell>Operations</cell></row><row><cell>Input</cell><cell>34 ? 126</cell><cell></cell><cell>Transpose (0, 1)</cell></row><row><cell>Layer-1</cell><cell>126 ? 34</cell><cell cols="2">Conv1d (32, 3, 1), BatchNorm1d (32), LeakyReLU (0.2)</cell></row><row><cell>Layer-2</cell><cell>32 ? 32</cell><cell cols="2">Conv1d (64, 3, 1), BatchNorm1d (64), LeakyReLU (0.2)</cell></row><row><cell>Layer-3</cell><cell>64 ? 30</cell><cell cols="2">Conv1d (64, 4, 2), BatchNorm1d (64), LeakyReLU (0.2)</cell></row><row><cell>Layer-4</cell><cell>64 ? 14</cell><cell></cell><cell>Conv1d (32, 3, 1)</cell></row><row><cell>Out1</cell><cell>32 ? 12</cell><cell cols="2">Flatten, FC (384, 256), BatchNorm1d (256), LeakyReLU (0.2)</cell></row><row><cell>Out2</cell><cell>256</cell><cell cols="2">FC (256, 128), BatchNorm1d (128), LeakyReLU (0.2), FC (128, 128)</cell></row><row><cell>Latent</cell><cell>128</cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Pose Decoder</cell></row><row><cell cols="3">Feature Feature Shapes</cell><cell>Operations</cell></row><row><cell>Input</cell><cell>128</cell><cell cols="2">FC (128, 64), BatchNorm1d (64), LeakyReLU (0.2), FC (64, 136)</cell></row><row><cell>reshape</cell><cell>136</cell><cell></cell><cell>Reshape (4, 34)</cell></row><row><cell>Layer-1</cell><cell>4 ? 34</cell><cell cols="2">ConvTranspose1d (32, 3, 1), BatchNorm1d (32), LeakyReLU (0.2)</cell></row><row><cell>Layer-2</cell><cell>32 ? 36</cell><cell cols="2">ConvTranspose1d (32, 3, 1), BatchNorm1d (32), LeakyReLU (0.2)</cell></row><row><cell>Layer-3</cell><cell>32 ? 38</cell><cell></cell><cell>Conv1d (32, 3, 1)</cell></row><row><cell>Layer-4</cell><cell>32 ? 36</cell><cell></cell><cell>Conv1d (126, 3, 1), Transpose(0, 1)</cell></row><row><cell>Pose</cell><cell>34 ? 126</cell><cell></cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please refer to Supplementary Material for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We obey the TED Talks Team's Creative Commons License (CC BY-NC-ND 4.0 International) by referencing all the video links shown in our papers. We sincerely thank the permission of the TED Talks Team for using the videos, audios and transcripts in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">ExPose License: https://github.com/vchoutas/expose/blob/master/LICE NSE; OpenPose License: https://github.com/CMU-Perceptual-Computing -Lab/openpose/blob/master/LICENSE; Trimodal: https://github.com/ai4r /Gesture-Generation-from-Trimodal-Context/blob/master/LICENSE.md</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work has been supported by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Fund, the RIE2020 Industry Alignment Fund-Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 7</ref><p>. Detailed feature shape after specific operations for the multi-level audio feature extraction. ? Note that in the table, Conv2d (c, k, s) means the output of the convolution is c, kernel size is k and the stride is s; ReLU, BatchNorm2d (c) means the relu and batch-norm operation on the feature of channel size c; Reshape operation combines the channel and frequency dimension together; FC (i, o) means the fully connected linear layer whose input dimension is i and output dimension is o; PixelShuffle (r) means the pixel shuffle operation <ref type="bibr" target="#b60">[61]</ref> with resolution r. Specifically, PixelShuffle (r) transforms a feature map of shape</p><p>d 0 p = 24, d 1 p = 24, d 2 p = 30, d 3 p = 36, d 4 p = 66, d 5 p = 96 and d 6 p = 126 for the parameters W h and b h of hierarchical GRU in the Eqn. 4 of main document. In this way, the next pose hierarchy can generate pose with information from the last level of pose, facilitating fine-grained correspondences between audio sequence and co-speech gestures in a coarseto-fine manner. The last layer's outputp H from the hierar-chy is our desired result. GAN Discriminator D. The architecture of discriminator D is borrowed from <ref type="bibr" target="#b73">[74]</ref>, with its detailed network design in the <ref type="table">Table 8</ref>. Pose Auto-Encoder. Since the generation is a multimodality problem, it is difficult to use evaluation metrics like L1 distance or L2 distance to judge whether the generated result is good or not. Fr?chet Inception Distance (FID) <ref type="bibr" target="#b28">[29]</ref> is widely leveraged to evaluate the image generation quality. It firstly pre-train a feature extractor to extract image latent features, then calculates the Fr?chet distance between the distributions of the latent feature space of real and generated images. The feature vectors contain more information about characteristics, which is more perceptually plausible than raw pixel space. Based on this, Yoon et al. <ref type="bibr" target="#b73">[74]</ref> propose a similar evaluation metric Fr?chet Gesture Distance (FGD) to evaluate gesture quality.</p><p>To further evaluate the pose with expressive finger movements, we train a pose auto-encoder with 43 keypoints on TED-Expressive dataset. The auto-encoder firstly maps 34frame poses into latent dimension of 128, and then reconstruct them. The detailed structure is borrowed from <ref type="bibr" target="#b73">[74]</ref> and recorded in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Stage and Inference Stage</head><p>At the training stage, the speech audios, transcripts and reference frames are all needed. The speech audio a is encoded by the hierarchical audio encoder E a to get multilevel audio features f low a , f mid a and f high a .</p><p>The speech transcript t is encoded by E t into text features f t , which are then used by contrastive learning strategy to achieve more discriminative audio feature extraction. Therefore, speech text takes an auxiliary effect in our proposed framework. The reference frames I = (I 1 , . . . , I M ) are encoded by E ID to represent speaker's identity f id , which is then transformed to style coordinator C for feature blending. Besides, reference frames are also used to extract initial poses and finally feed into cascaded bi-GRU to generate co-speech gestures in a coarse-to-fine manner.</p><p>At the inference stage, the speech transcript is not needed. This is the reason why we do not involve the variable t in the Eq. 1 in our main text. If the reference frames and initial poses are available, we can follow</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">No gestures left behind: Learning relationships between spoken language and freeform gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Dong Won Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1884" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukiko</forename><forename type="middle">I</forename><surname>Dong Won Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="248" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lan-guage2pose: Natural language grounded pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured prediction helps 3d human motion modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7144" to="7153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hp-gan: Probabilistic 3d human motion prediction via gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1418" to="1427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Glocalnet: Class-aware longterm human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Battan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yudhik</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Sai Soorya Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning statistical models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Human Modeling, Analysis and Synthesis, CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Style machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech-gesture mismatches: Evidence for one underlying representation of linguistic and nonlinguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl-Erik</forename><surname>Mccullough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pragmatics &amp; cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Animated conversation: rulebased generation of facial expression, gesture &amp; spoken intonation for multiple conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Achorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tripp</forename><surname>Becket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Douville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 21st annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beat: the behavior expression animation toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>H?gni Vilhj?lmsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bickmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Life-Like Characters</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="163" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What comprises a good talking-head video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03201</idno>
	</analytic>
	<monogr>
		<title level="m">A survey and benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7832" to="7841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">In defence of metric learning for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongkyu</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><forename type="middle">Soo</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soyeon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheon</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bong-Jin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Icksang</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.11982</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beat tracking by dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial gesture generation with realistic gesture phasing. Computers &amp; Graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ylva</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Mcdonnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning variable-length markov models of behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aphrodite</forename><surname>Galata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="398" to="413" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2.5 d visual sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning human motion models for long-term predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="458" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning individual styles of conversational gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gefen</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning speech-driven 3d conversational gestures from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjie</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06837</idno>
		<editor>Hans-Peter Seidel, Gerard Pons-Moll, Mohamed Elgharib, and Christian Theobalt</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of speech-to-gesture generation using bi-directional lstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dai</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoshi</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Sakuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Intelligent Virtual Agents</title>
		<meeting>the 18th International Conference on Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatio-temporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robot behavior toolkit: generating effective social behaviors for robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Dance revolution: Long-term dance generation with music via curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruozi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06119</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A speech-driven hand gesture generation method and evaluation in android robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">T</forename><surname>Ishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Machiyashiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryusuke</forename><surname>Mikata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3757" to="3764" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Audio-driven emotional video portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gesticulator: A framework for semantically-aware speech-driven gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Sanne Van Waveren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iolanda</forename><surname>Alexandersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedvig</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimodal Interaction, ICMI &apos;20</title>
		<meeting>the 2020 International Conference on Multimodal Interaction, ICMI &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Talking with hands 16.2 m: A large-scale dataset of synchronized bodyfinger motion and audio for conversational motion analysis and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddhartha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="763" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dancing to music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ding</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gesture controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2010 papers</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dancenet3d: Music based dance generation with parametric motion transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10206</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Audio2gestures: Generating diverse gestures from speech audio with conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06720</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learn to dance with aist++: Music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08779</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Visual sound localization in the wild by cross-modal interference erasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06406</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semantic-aware implicit neural audio-driven video portrait generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07786</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Temporal, structural, and pragmatic synchrony between intonation and gesture. Laboratory phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loehr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Virtual character performance from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaux</forename><surname>Lhommet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</title>
		<meeting>the 12th ACM SIG-GRAPH/Eurographics Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Gruyter Mouton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dynamic bayesian networks for audio-visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhong</forename><surname>Ara V Nefian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxing</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A lip sync expert is all you need for speech to lip generation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Kr Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Animating by multi-level sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Computer Animation</title>
		<meeting>Computer Animation</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Speech drives templates: Co-speech gesture synthesis with learned templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shenhan Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08020</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Gentle: A forced aligner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ochshorn</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hawkin</forename><surname>Max</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Msp-avatar corpus: Motion capture recordings to study the role of discourse functions in the design of intelligent virtual agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najmeh</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generation and evaluation of communicative robot gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ipke</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Rohlfing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Joublin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Social Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A friendly gesture: Investigating the effect of multimodal robot behavior in human-robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Rohlfing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Joublin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ro-Man</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="247" to="252" />
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cyclic co-learning of sounding object visual grounding and sound separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unified multisensory perception: Weakly-supervised audio-visual video parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Audio-visual event localization in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A multimodal motioncaptured corpus of matched and mismatched extravertintrovert conversational pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Tolins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean E Fox</forename><surname>Tree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Neff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3469" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The persona effect: how substantial is it?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Van Mulken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeth</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">People and computers XIII</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="53" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural kinematic networks for unsupervised motion retargetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8639" to="8648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gesture and speech in interaction: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Malisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning trajectory dependencies for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Miaomiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salzemann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hongdong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Lightweight and efficient end-to-end speech recognition using low-rank transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Cahyawijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6144" to="6148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visually informed binaural audio generation without binaural audios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeletonbased action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Speech gesture generation from the trimodal context of text, audio, and speaker identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bok</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Haeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwoo</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo-Ri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geehyuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Audio-visual recognition of overlapped speech for the lrs2 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shansong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6984" to="6988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The sound of motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1735" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Talking face generation by adversarially disentangled audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pose-controllable talking face generation by implicitly modularized audio-visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Sep-stereo: Visually guided stereophonic audio generation by associating source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Makelttalk: speaker-aware talking-head animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Echevarria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08891</idno>
		<title level="m">Generative tweening: Long-term inbetweening of 3d human motions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
