<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Zatsarynna</surname></persName>
							<email>zatsarynna@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><forename type="middle">Abu</forename><surname>Farha</surname></persName>
							<email>abufarha@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Modal Temporal Convolutional Network for Anticipating Actions in Egocentric Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anticipating human actions is an important task that needs to be addressed for the development of reliable intelligent agents, such as self-driving cars or robot assistants. While the ability to make future predictions with high accuracy is crucial for designing the anticipation approaches, the speed at which the inference is performed is not less important. Methods that are accurate but not sufficiently fast would introduce a high latency into the decision process. Thus, this will increase the reaction time of the underlying system. This poses a problem for domains such as autonomous driving, where the reaction time is crucial. In this work, we propose a simple and effective multi-modal architecture based on temporal convolutions. Our approach stacks a hierarchy of temporal convolutional layers and does not rely on recurrent layers to ensure a fast prediction. We further introduce a multi-modal fusion mechanism that captures the pairwise interactions between RGB, flow, and object modalities. Results on two large-scale datasets of egocentric videos, EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves comparable performance to the state-of-the-art approaches while being significantly faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anticipating future events is of great importance for intelligent agents. There are many real-world scenarios, where apart from recognizing what is happening in the current moment, one also needs to make predictions about the future. For example, autonomous driving systems need to anticipate pedestrians movement to avoid collisions. Another field of application is assistive robotics, where the ability of robots to anticipate future human activities allows for smoother and more productive interactions. In our work, we focus on human activity anticipation since it is a challenging yet crucial task for an intelligent system to be <ref type="figure">Figure 1</ref>. The short-term action anticipation task predicts the next unobserved action Ta seconds before it occurs. To address this task, we propose a multi-modal approach based on temporal convolutional networks that achieves state-of-the-art results while being faster than traditional RNN-based approaches. deemed as such.</p><p>In recent years, the number of works addressing the task of action anticipation has experienced a substantial increase. Generally, one could subdivide these works into two categories based on the time horizon of anticipation they tackle. While some works try to anticipate several actions into the future (long-term anticipation) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>, others aim at anticipating only the next action at a fixed anticipation time based on the recent observations preceding it (short-term anticipation) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45]</ref>. In our work, we deal with the second setting as illustrated in <ref type="figure">Figure 1</ref>.</p><p>Initially, this task has been addressed by predicting representations of the future frames and anticipating the actions by training a classifier on them <ref type="bibr" target="#b44">[45]</ref>. Such approaches, while being successful on videos shot from the third-person view, do not perform well on egocentric videos captured from the first-person view. Egocentric action anticipation has been addressed in the work of Furnari et al. <ref type="bibr" target="#b13">[14]</ref>, who introduced a multi-modal LSTM-based <ref type="bibr" target="#b18">[19]</ref>  proposed <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref> that show improved performance in the egocentric action anticipation.</p><p>While these methods demonstrate better performance in the egocentric short-term action anticipation, little attention has been paid to the effectiveness of the training and inference procedures of the methods. Many of the above mentioned works, in particular <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48]</ref>, use recurrent layers for performing temporal sequence modelling. However, in recent years it has been repeatedly indicated <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> that for many sequence processing tasks convolution-based methods are much faster than canonical recurrent layers such as LSTMs, and still show a similar or even superior performance. Convolution-based architectures are also easier to train, since they do not possess notorious drawbacks of recurrent layers such as vanishing gradients and inability to model long-term dependencies. On a different note, some of the action anticipation works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> also gather and incorporate additional data or annotations into training, which is a costly and labor intensive process.</p><p>In our work, we propose a model that addresses the previously mentioned limitations. We introduce a multimodal network based on a hierarchy of temporal convolutions. Our network consists of three parallel branches where each branch operates on features extracted from RGB, optical flow or object modalities. To fuse these modalities, we introduce a multi-modal fusion mechanism that captures both mutual and pairwise interactions between the different branches. In contrast to previous approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>, our model does not require any additional data or annotations. We evaluate our approach on two large-scale datasets of egocentric videos: EPIC-Kitchens-55 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> and EPIC-Kitchens-100 <ref type="bibr" target="#b7">[8]</ref>. We show that our model achieves comparable results to the state-of-the-art while being at least two times faster during both training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Action Anticipation in Videos</head><p>There are several lines of work in the area of action anticipation that differ in the time horizon of predictions. Some approaches focus on long-term predictions. That is, given a subset of observed actions, they aim to predict multiple actions into the future or even all subsequent actions. In the work by Abu Farha et al. <ref type="bibr" target="#b0">[1]</ref>, two approaches for long-term anticipation have been proposed, based on RNN and CNN. The RNN-based method predicts labels and lengths of the upcoming actions by feeding its predicted values back to the network for future predictions. The CNN-based approach, unlike the previous one, predicts all future actions in a single step. It makes predictions by encoding both its input and output in a matrix form. To avoid intermediate computations and accumulation of errors, Ke et al. <ref type="bibr" target="#b24">[25]</ref> introduced a time-conditioned method that anticipates long-term actions in one shot. Gammulle et al. <ref type="bibr" target="#b14">[15]</ref> proposed a network that models long-term relationships within the input sequence with the help of a neural memory module, whose refined output is then used to make action predictions. In <ref type="bibr" target="#b10">[11]</ref>, a sequence-to-sequence model is used to predict future activities and their durations. Additionally, the authors leveraged cycle consistency over time by predicting past actions on the basis of the future predictions made by the network. In contrast to these approaches, we focus on short-term action anticipation from egocentric videos.</p><p>For the task of short-term action anticipation, the goal is to forecast an action several seconds prior to its occurrence. State-of-the-art methods usually take the most recent observations into account and predict actions up to several seconds into the future. For example, Vondrick et al. <ref type="bibr" target="#b44">[45]</ref> proposed a mixture of regression networks to learn a representation of a frame one second in the future based on the frame at the current time step. Then, to predict the action, they categorize the predicted representations with a classifier network. Gao et al. <ref type="bibr" target="#b22">[23]</ref> further extended the previous idea and introduced an encoder-decoder network that anticipates a sequence of future representations based on an observed sequence of representations, instead of just a single representation. In the work of Jain et al. <ref type="bibr" target="#b19">[20]</ref>, the authors also used an encoder-decoder network, albeit with several modalities and a loss that exponentially increases with time to prevent overfitting and encourage early anticipation. Different from previous works, Damen et al. <ref type="bibr" target="#b5">[6]</ref> leveraged an action recognition network based on TSN <ref type="bibr" target="#b45">[46]</ref> for the task of action anticipation. During training, the network receives the observed segment preceding the action of interest as input, while the corresponding label is set to the category of the action that needs to be predicted. Miech et al. <ref type="bibr" target="#b32">[33]</ref> proposed to predict future actions by averaging predictions of two complementary modules: predictive and transitional, where the predictive model directly anticipates the upcoming action, and the transitional model is constrained to at first output the current action and then use the acquired information to anticipate the future. In <ref type="bibr" target="#b13">[14]</ref>, the RU-LSTM network was introduced, that consists of two LSTM networks. The first LSTM summarizes the past observations whereas the second one predicts the future actions. Camporese et al. <ref type="bibr" target="#b4">[5]</ref> further proposed to extend the RU-LSTM with label smoothing to mitigate over-confident predictions and make their system more uncertainty-aware. Sener et al. <ref type="bibr" target="#b37">[38]</ref> proposed a framework based on non-local blocks <ref type="bibr" target="#b46">[47]</ref>, that aggregates multi-scale features from the video by computing interaction between recent and distant observations. The resulting features are then used to anticipate both short-term and long-term actions. Liu et al. <ref type="bibr" target="#b27">[28]</ref> explicitly incorporate intentional hand movement as an anticipatory representation of actions. They jointly model and predict hand trajectories, interaction hotspots and labels of future actions. Dessalene et al. <ref type="bibr" target="#b9">[10]</ref> proposed to use a Graph Convolutional Network (GCN) to model long-term temporal semantic relations between actions based on contact information. They use the constructed graph representations along with appearance features to make anticipation about the future actions. In contrast to these approaches, our approach relies on temporal convolutions to capture dependencies in the input sequence and predict the future action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anticipation of other Modalities</head><p>While in our work we address the anticipation of human activities, there are numerous efforts that address the anticipation of other modalities. Anticipation of human trajectories and motion is a popular task that has been addressed in many works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49]</ref>. Another line of work deals with predictions of future human poses <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Also, many approaches have been proposed for prediction of future semantic segmentation maps of images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> or even semantic instance segmentation maps <ref type="bibr" target="#b28">[29]</ref>. A more difficult problem of future frame prediction has also been explored in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. Some works have also addressed the task of generating sentences for describing future frames or upcoming steps in recipes <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>We introduce a multi-modal temporal convolutional network for the task of action anticipation. We start by defining the task of action anticipation in Section 3.1. Then, we discuss the video processing procedure in Section 3.2. Finally, we introduce our uni-modal anticipation branch in Section 3.3 and discuss the multi-modal fusion strategy in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Anticipation Task</head><p>We adopt the problem definition of action anticipation from <ref type="bibr" target="#b5">[6]</ref>. Let T a be the anticipation time, i.e. how many seconds in advance an action is predicted, and T o be the observation time, i.e. the length of the observed video segment that precedes the action of interest. For a given action video segment A = [t s , t e ], let t s and t e denote times of action start and end respectively. Then, the goal of the action anticipation task is to predict the action label of A by observing a video segment of length T o preceding the action start time t s by the anticipation time of T a seconds, that is</p><formula xml:id="formula_0">[t s ? (T a + T o ), t s ? T a ].</formula><p>The action anticipation task is illustrated in <ref type="figure">Figure 1</ref>.</p><p>In our work, the anticipation time T a is one second. I.e. an action is anticipated one second before its occurrence. The observation time T o is set to 5.25 seconds. We will show in the experiments the effect of varying the length of the observed video segment on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Processing</head><p>We process the observed video segments in the same way as proposed in <ref type="bibr" target="#b13">[14]</ref>. Let the observed video segment be denoted by V . As mentioned previously, we set the length of V to 5.25 seconds (i.e. T o = 5.25). During processing, we break V down into snippets that have a duration of ? = 0.25 seconds, which results in a total of N = 21 snippets {V 1 , V 2 , . . . , V N }. Since the anticipation time is equal to 1.0 second, the resulting video snippets are located at {6.0, 5.75, . . . , 1.0} seconds before the action of interest, where the location is defined by the last frame of the snippet. Each snippet contains several video frames</p><formula xml:id="formula_1">V i = {I 1 i , I 2 i , . . . , I m i },</formula><p>where the exact number of frames m depends on the frame rate of the videos. From each snippet, we sample one or several frames, depending on the modality, which are then used to extract features.</p><p>In our model, we use three types of modalities: RGB, optical flow and object features. For extracting the RGB features, each snippet is represented by its last frame I m i . For optical flow, the last 5 frames {I m?4 i , . . . , I m i } of horizontal and vertical flow are used. To get object features, as for the RGB features, the last frame of each video snippet is used, that is I m i . Having collected the frames, we extract RGB and optical flow features using TBN <ref type="bibr" target="#b23">[24]</ref> pre-trained for action recognition, while for object features we use the representation proposed by <ref type="bibr" target="#b13">[14]</ref>. We elaborate on the feature extraction procedure in the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Uni-modal Branch</head><p>Our proposed branch for uni-modal action anticipation is inspired by the temporal convolutional network (TCN) proposed in <ref type="bibr" target="#b26">[27]</ref>. Similar to <ref type="bibr" target="#b26">[27]</ref>, the uni-modal branch stacks several layers of temporal convolutional residual blocks. An overview of the uni-modal branch is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>The first layer of the proposed branch is a one dimensional convolution with kernel size one, that adjusts the number of channels in the features of the input sequence to match the number of the features maps in the network. After that, the embedded sequence is processed by the residual block layers, that contain dilated one dimensional convolutional filters. These blocks are applied to the input hierarchically, meaning that each layer processes the output of the previous one.</p><p>All blocks follow the structure depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. It consists of a temporal convolution with kernel size K and C convolutional filters. In our work, we set the kernel size K = 3 and the number of filters C = 1024. Depending on the layer number l, the convolutional filters within the residual blocks have different dilation factors. We increase the dilation factor for the convolutional kernels linearly with the number of layers (i.e. 1, 2, 3, 4).</p><p>Also, within each block, we normalize the output of the convolution using batch normalization <ref type="bibr" target="#b41">[42]</ref> and apply The increasing dilation factor allows to increase the receptive field of the model without increasing its depth. (Right) Overview of the convolutional block with the residual connection. Since the length of the input and output sequences for the residual blocks differ, we only consider the most recent elements of the input sequence for residual connections. dropout <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> to avoid overfitting. We also apply dropout to the input of the first dimension-adjustment layer. Similar to <ref type="bibr" target="#b2">[3]</ref>, we used a spatial dropout, that is at each training step a whole channel across all time steps is zeroed out. To facilitate the gradient flow, we further introduce residual connections into the blocks that are followed by ReLU non-linearity. Since the temporal length of the output of the convolutional layer is less than the input sequence, we only use the most recent elements of the input sequence for the residual connections. Formally, the output at the l th level is computed as follows:</p><formula xml:id="formula_2">Z l = BN (W l * Z l?1 + b l ) Z l = Dropout(Z l ) Z l = ReLU (? l + Z [N l?1 ?N l +1,...,N l?1 ] l?1 ),</formula><p>where Z l is the output of layer l, * denotes the convolution operator with convolutional filters parametrized by a weight matrix W l ? R K?C?C and a bias vector b l ? R C , Z</p><formula xml:id="formula_3">[N l?1 ?N l +1,...,N l?1 ] l?1</formula><p>is the sub-sequence of the most recent N l elements of the sequence Z l?1 , where N l denotes length of the sequence at level l.</p><p>Overall, the branch contains L layers of the previously discussed blocks. We set L = 4 for our model. Given an input sequence, we pass it through the hierarchy of the residual block layers until the whole sequence is summarized in a single feature vector F at the bottom of the pyramid. Based on the final feature vector F , we perform action anticipation using a fully-connected layer. In addition to the action classification layer, similar to <ref type="bibr" target="#b37">[38]</ref>, we also add two more layers for verb and noun classification that solve the auxiliary classification tasks to help anticipation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-Modal Fusion</head><p>Our approach fuses three modalities for anticipating the future action: RGB, flow, and object modalities. <ref type="figure">Figure 3</ref> illustrates the proposed multi-modal fusion strategy. We fuse the branches by constructing a mutual multi-modal feature and use it for performing the final future prediction. To do so, we at first separately pre-train modality-specific branches for action anticipation. Then, we extract features F mod from each modality by taking the output of the last convolutional block from the pre-trained branches.</p><p>We construct a cross-branch multi-modal feature by combining pairwise and mutual embeddings of the features computed by individual branches. To compute a pairwise embedding, we at first apply corresponding fully-connected layers to the pairwise concatenations of the features from the three branches. After that, these intermediate representations are merged by another feed-forward layer. A mutual embedding is constructed by projecting the concatenation of the features from the three branches with a fullyconnected layer. The output dimension of both pairwise and mutual embeddings is 1024. Finally, the two computed embeddings are combined by taking their elementwise sum. Based on the resulting feature, three parallel fully-connected classification layers predict action, verb, <ref type="figure">Figure 3</ref>. Overview of the multi-modal fusion strategy. Given the output of the individual uni-modal branches, we construct pairwise and mutual embeddings. Finally, these two embeddings are combined and passed to the final classification layers. and noun, respectively. We demonstrate the effect of using different fusion strategies in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Features Extraction</head><p>To extract the RGB and optical flow features, we employ the TBN action recognition network <ref type="bibr" target="#b23">[24]</ref>. We use the TBN pre-trained for action recognition, where for pre-training we follow the procedure recommended in <ref type="bibr" target="#b23">[24]</ref>. After pretraining, similar to <ref type="bibr" target="#b13">[14]</ref>, we take RGB frames and stacks of 5 horizontal and 5 vertical optical flow frames of size 456 ? 256 and pass them to the network to extract features. As features, we consider the output produced by the global average pooling layer of the BN-Inception network for the corresponding modality stream, with the resulting representation for each modality containing 1024 channels.</p><p>For object features, we use the representation proposed by <ref type="bibr" target="#b13">[14]</ref>. They are extracted using a Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> object detector with ResNet-101 backbone <ref type="bibr" target="#b16">[17]</ref>. For all modalities, the corresponding features extraction models are fixed and not fine-tuned for the anticipation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training Details</head><p>We implemented our model using the Pytorch framework <ref type="bibr" target="#b34">[35]</ref>. For optimization we use Stochastic Gradient Descent (SGD) with momentum equal to 0.9 and weight decay of 5 ? 10 ?4 . We trained both the uni-modal branches and the fusion layers for 80 epochs. At first, we separately pre-train uni-modal branches for action anticipation. Then, during fusion, we fix the weights of the individual branches, and optimize only the paramters of the fusion layers.</p><p>For EPIC-Kitchens-55, we use a batch size of 64 examples. The starting learning rate is set to 0.005 for the object branch and 0.0005 for the RGB and flow branches as well as for the fusion layers. At each epoch we adjust the learning rate l according to the following schedule: (1 ? e E ) 0.99 , where e denotes the number of the current epoch and E is the total number of epochs. For the residual blocks within the individual branches we use a dropout ratio of 0.5. We also apply dropout of 0.3 to the input sequence, before applying the first layer. Fully-connected classification layers for uni-modal and multi-modal predictions have dropout rates of 0.7 and 0.8, respectively.</p><p>For EPIC-Kitchens-100, we use a batch size of 128. We use the same starting learning rates for the uni-modal branches, while for the fusion layers we increase the learning rate to 0.00075. The schedule for the update of the learning rate remains the same. The dropout ratio for the blocks within the individual branches is decreased to 0.3 since the dataset is larger and thus the model is less prone to overfitting. As previously, we also apply dropout of 0.3 to the input sequence. Fully-connected classification layers for both uni-modal and multi-modal predictions have the dropout rate of 0.7.</p><p>For reporting results on the test sets, we use models that are pre-trained on both validation and train splits, after optimizing for hyper-parameters on the validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>We perform our experiments on two large-scale datasets of egocentric videos: EPIC-Kitchens-55 <ref type="bibr" target="#b5">[6]</ref> and EPIC-Kitchens-100 <ref type="bibr" target="#b7">[8]</ref>.</p><p>EPIC-Kitchens-55 contains videos collected by 32 participants, capturing their daily kitchen activities, including cooking, cleaning, doing laundry, etc. The RGB frames and pre-computed optical flow frames are publicly available with the dataset. In total, there are 55 hours of recordings and 39596 action annotations. In annotations, there are 125 verb and 352 noun classes. For action classes, similar to <ref type="bibr" target="#b13">[14]</ref>, we considered all unique (verb, noun) pairs that are present in the public training set. This amounts to the total of 2513 action classes.</p><p>For evaluation purposes, the authors of the dataset defined two test splits: seen and unseen kitchens. The seen split (S1) contains videos from the kitchens present both in training and test sets, while the unseen test split (S2) contains videos only from such kitchens that have not been observed during training. As a validation set, we use the same subset of training videos as proposed by <ref type="bibr" target="#b13">[14]</ref>, who created training and validation sets from the publicly available training set by randomly choosing 232 and 40 videos for each set, respectively.</p><p>EPIC-Kitchens-100 <ref type="bibr" target="#b7">[8]</ref> extends upon EPIC-Kitchens-55 dataset, with the total of 100 hours of footage. Videos in EPIC-Kitchens-100 were collected by 37 participants in 45 environments. It contains 89977 fine-grained action annotations, with 97 verb and 300 noun classes. By using the same principle as for EPIC-Kitchens-55, there are 3806 action classes. All videos in the dataset are split into train, validation and test sets with a ratio of approximately 75/10/15. The validation and test splits contain two subsets, on which the results are reported separately: unseen participants and tail classes. The unseen subset contains videos of participants that are not present in the train set. The subset of tail classes for verbs and nouns contains the set of classes that have the fewest instances and that account for 20% of the instances in the whole dataset. An action class is considered to be a tail class, if it contains either a tail noun or verb. Overall, there are 86/228/3729 verb/noun/action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>For both EPIC-Kitchens-55 and EPIC-Kitchens-100, we evaluate our approach using the official dataset metrics. For EPIC-Kitchens-55, we use top-1 and top-5 verb, noun and action accuracy (the prediction is deemed correct if the ground-truth action falls into the top-1 or top-5 predictions, respectively). For EPIC-Kitchens-100, we report class-mean top-5 recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Anticipation Results on EPIC-Kitchens-55</head><p>We compare our proposed model to the state-of-the-art methods on the test splits of the EPIC-Kitchens-55 dataset in <ref type="table">Table 1</ref>. In our model, the RGB and optical flow features are extracted using TBN. For a fair comparison with the methods that use TSN features proposed by <ref type="bibr" target="#b13">[14]</ref>, namely <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, we also trained a separate model for which we used appearance and motion features provided by the authors. As one can see, both models perform similarly on the unseen test split, with the top-1 action accuracy of 8.9%, while on the seen test split, the model trained with the TBN features outperforms the model trained with the TSN features by a margin of 0.5%. Comparing to other methods, on the seen test split, both models trained with TSN and TBN features outperform LSTM-based methods in top-1 action accuracy: RU-LSTM by 0.5% and 1.0% accordingly and ImagineRNN <ref type="bibr" target="#b47">[48]</ref> by 0.2% and 0.7%. On the unseen test split, we outperform RU-LSTM by 0.7%, while ImagineRNN performs better by 0.4%. Apart from achieving similar or better accuracy, our proposed model is also more efficient during training and inference stages. In <ref type="table" target="#tab_2">Table 2</ref> we compare average training time per epoch and time required for inference on the validation set for our RGB branch and that of RU-LSTM. For measuring training and inference times of the RU-LSTM network, we used the official code made publicly available by the authors. Since RU-LSTM makes predictions at several time steps, to ensure a fair comparison, we performed an experiment where we modified the RU-LSTM training procedure to make predictions at the anticipation time of one second only. This however, resulted in a decrease of both Top-1 and Top-5 accuracy of the model. Therefore, we compare the training time of our method to the original setup of RU-LSTM. Then, for measuring the time required for inference, we considered predictions made by RU-LSTM only for the anticipation time of one second, without performing computations for different anticipation times. To further minimize the effect of factors not related to the direct effectiveness of the models, for both methods we used TSN features, identical training and validation sets, as well as the same GPU and data loading procedure. We conducted the experiments using an Nvidia Titan Xp GPU. <ref type="table" target="#tab_2">Table 2</ref>, our method is more than two times faster than RU-LSTM during training and almost two times faster during inference stage. Apart from shorter per-epoch training time, our method also does not use an additional teacher-forcing pre-training stage, as well as the total number of epochs required for convergence is 80 compared to 100 for the RU-LSTM. So, all things considered, our proposed branch can be trained approximately five times faster. ImagineRNN builds upon the RU-LSTM baseline by extending it with contrastive learning, while the underlying architecture, along with training and inference procedures are identical to those of RU-LSTM, except for the absence of the additional pre-training stage. Therefore, based on the measurements made for the RU-LSTM, we can expect that both methods require a similar amount of time perepoch for training and inference. Thus, we can also expect our method to be faster than ImagineRNN. Finally, our model is also more effective than LSTM-based models in terms of memory-usage. Our branch needs less than 60% of the memory requirement for the one of the RU-LSTM. Higher memory efficiency of convolution-based networks over RNNs has also been discussed in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Concerning the other methods, our model performs on par with Liu et al. <ref type="bibr" target="#b27">[28]</ref> on the seen test split, while Ego-OMG and Sener et al. <ref type="bibr" target="#b37">[38]</ref> outperform our approach in top-1 action accuracy by 0.6% and 1.2% respectively. On the unseen test split, Liu et al. <ref type="bibr" target="#b27">[28]</ref>, Ego-OMG and Sener et al. <ref type="bibr" target="#b37">[38]</ref> perform better than our model by 1.0%, 2.9% and 1.2% respectively. Notice, however, that both Ego-OMG <ref type="bibr" target="#b9">[10]</ref> and Liu et al. <ref type="bibr" target="#b27">[28]</ref> use additional annotations to train their proposed approaches. Ego-OMG uses additional supervision in the form of progression time of directed hand movements, as well as ground truth segmentation masks of  interaction objects, while Liu et al. <ref type="bibr" target="#b27">[28]</ref> use additional annotations for interaction hotspots and hand trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Anticipation Results on EPIC-Kitchens-100</head><p>We compare our proposed model to the baseline methods on the test set of the EPIC-Kitchens-100 dataset in <ref type="table" target="#tab_3">Table 3</ref>. Since EPIC-Kitchens-100 has been introduced only recently, for many of the previously mentioned methods no evaluation results are available. Therefore, we compare our method to the officially reported baselines.</p><p>As the table shows, our approach performs on par with RU-LSTM on both overall and tail-class splits. The performance is also consistent using different types of features and our model works well with both TBN and TSN features. Furthermore, our approach shows better generalization behavior as indicated by the results on the unseen environments. Our approach outperforms RU-LSTM in the mean top-5 action recall on the unseen split by 2.5% and 1.4% using TBN and TSN features, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>In this section, we provide a set of ablation experiments to analyze the different components of our approach. As our main motivation is to develop a model that has a good tradeoff between accuracy and efficiency for the task of action anticipation, we verify how much past is really necessary for the network to achieve a good accuracy. Additionally, we also study different fusion strategies for the uni-modal branches. For the ablation experiments, we report results on the validation set of the EPIC-Kitchens-55 dataset using TBN features. As previously, we use the validation set constructed by <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Observation length</head><p>We report the top-1 action accuracy of our RGB branch trained on observation intervals of different lengths in <ref type="table">Table 4</ref>. We vary the observation length starting from 0.75 seconds up to 7.75 seconds. As shown in the table, the accuracy of the predictions increases with the length of the observation interval until it saturates at 5.25 seconds. Similar findings have been reported in <ref type="bibr" target="#b37">[38]</ref>. Based on this observation, we fix the length of the observed interval to 5.25 seconds which corresponds to an input sequence of 21 snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Fusion strategy</head><p>We report the top-1 action accuracy of the individual branches and different multi-modal fusion strategies in Ta-  <ref type="table">Table 4</ref>. Effect of the observation length on the prediction accuracy. We report top-1 action accuracy for the RGB branch using TBN features on the validation set of EPIC-Kitchens-55. ble 5. Among the uni-modal branches, the appearance branch has the highest top-1 action accuracy, whereas the flow branch has the lowest. All fusion schemes improve over the performance of uni-modal branches. In total, we experimented with five different fusion methods:</p><p>? Late fusion: Averaging predictions made by individual branches.</p><p>? Attention fusion: Learning weights for predictions of the individual branches based on the final features from the three branches: {F RGB , F F low , F Object }.</p><p>? Mutual fusion: Applying only the mutual fusion path from the proposed fusion scheme (see <ref type="figure">Figure 3</ref>).</p><p>? Pairwise fusion: Applying only the pairwise fusion path from the proposed fusion scheme (see <ref type="figure">Figure 3</ref>).</p><p>? Mutual + Pairwise: Applying both pairwise and mutual fusion paths and merging them with element-wise addition (see <ref type="figure">Figure 3</ref>).</p><p>We observed that merging predictions of individual branches either via late fusion (top-1 action accuracy 14.1%) or attention fusion (top-1 action accuracy 14.3%) achieves lower results than merging individual features of the uni-modal branches and making a cross-branch prediction based on the constructed representations. Furthermore, learning both pairwise and mutual embeddings of the unimodal features and combining them via the element-wise addition is better than making the final prediction based only either on the pairwise or mutual embeddings.  <ref type="table">Table 5</ref>. Comparison of different multi-modal fusion methods on the EPIC-Kitchens-55 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a multi-modal architecture based on temporal convolutional layers for the short-term action anticipation task. Instead of relying on recurrent layers for temporal modelling, we use a stack of temporal convolutional layers, which allows our approach to perform anticipation faster. We further proposed a multi-modal fusion strategy that combines both mutual and pairwise interactions between the different branches. Results on two large-scale datasets of egocentric videos, EPIC-Kitchens-55 and EPIC-Kitchens-100, show that our approach achieves performance comparable to the state-of-the-art approaches while being at least two times faster and more efficient compared to RNN-based approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Architectural elements of our uni-modal branch. (Left) Our branch consists of a set of dilated temporal convolutions (with kernel size K) that process the input sequence (of length N ) iteratively allowing our model to learn hierarchical feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>encoderdecoder network. Recently, several new methods have been arXiv:2107.09504v1 [cs.CV] 18 Jul 2021</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average training time per epoch and inference time on EPIC-Kitchens-55. 'Training' represents training time on the training split. 'Inference' represents average inference time per sample on the validation split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results for action anticipation at anticipation time Ta = 1 on the EPIC-Kitchens-100 test set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Mean Top-5 Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Overall (%)</cell><cell></cell><cell cols="2">Unseen (%)</cell><cell></cell><cell></cell><cell>Tail (%)</cell></row><row><cell></cell><cell>Method</cell><cell cols="9">Verb Noun Act. Verb Noun Act. Verb Noun Act.</cell></row><row><cell></cell><cell>Random</cell><cell>6.2</cell><cell>2.3</cell><cell>0.1</cell><cell>8.1</cell><cell>3.3</cell><cell>0.3</cell><cell>1.9</cell><cell>0.7</cell><cell>0.03</cell></row><row><cell></cell><cell cols="2">RU-LSTM [14] 25.3</cell><cell cols="3">26.7 11.2 19.4</cell><cell>26.9</cell><cell>9.7</cell><cell>17.6</cell><cell>15.9</cell><cell>7.9</cell></row><row><cell></cell><cell>Ours (TSN)</cell><cell>20.4</cell><cell cols="3">26.6 10.9 17.9</cell><cell cols="3">26.9 11.1 11.7</cell><cell>15.2</cell><cell>7.0</cell></row><row><cell></cell><cell>Ours (TBN)</cell><cell>21.5</cell><cell cols="3">26.8 11.0 20.8</cell><cell cols="3">28.3 12.2 13.2</cell><cell>15.4</cell><cell>7.2</cell></row><row><cell cols="5">No. of Snippets Obs. Time (sec) Top-1 Act. Acc (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>0.75</cell><cell></cell><cell>11.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>7</cell><cell>1.75</cell><cell></cell><cell>11.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>3.25</cell><cell></cell><cell>11.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>21</cell><cell>5.25</cell><cell></cell><cell>12.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>31</cell><cell>7.75</cell><cell></cell><cell>12.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When will you do what? -anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yazan Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian prediction of future street scenes using synthetic likelihoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge distillation for action anticipation via label smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guglielmo</forename><surname>Camporese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamberto</forename><surname>Ballan</surname></persName>
		</author>
		<idno>abs/2004.07711</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The epic-kitchens dataset: Collection, challenges and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forecasting action through contact representations from first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eadom</forename><surname>Dessalene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmaya</forename><surname>Devaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maynord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Egocentric object manipulation graphs. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eadom</forename><surname>Dessalene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maynord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmaya</forename><surname>Devaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term anticipation of activities with cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Yazan Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM German Conference on Pattern Recognition (GCPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting object dynamics in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rollingunrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Forecasting future action sequences with neural memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshala</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatio-temporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting scene parsing and motion dynamics in the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audio-visual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Timeconditioned action anticipation in one shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting human-object interaction: Joint prediction of motor attention and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Future semantic segmentation with convolutional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Seyed Shahabeddin Nabavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biritsh Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<idno>abs/1412.6604</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot anticipation for instructional activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio</title>
		<editor>Arxiv</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc Val</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to anticipate egocentric actions by imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1143" to="1152" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Planning-based prediction for pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garratt</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
