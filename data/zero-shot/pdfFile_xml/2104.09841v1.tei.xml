<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SelfReg: Self-supervised Contrastive Regularization for Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehee</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Kookmin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekoo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Kookmin University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SelfReg: Self-supervised Contrastive Regularization for Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In general, an experimental environment for deep learning assumes that the training and the test dataset are sampled from the same distribution. However, in real-world situations, a difference in the distribution between two datasets, domain shift, may occur, which becomes a major factor impeding the generalization performance of the model. The research field to solve this problem is called domain generalization, and it alleviates the domain shift problem by extracting domain-invariant features explicitly or implicitly. In recent studies, contrastive learning-based domain generalization approaches have been proposed and achieved high performance. These approaches require sampling of the negative data pair. However, the performance of contrastive learning fundamentally depends on quality and quantity of negative data pairs. To address this issue, we propose a new regularization method for domain generalization based on contrastive learning, self-supervised contrastive regularization (SelfReg). The proposed approach use only positive data pairs, thus it resolves various problems caused by negative pair sampling. Moreover, we propose a class-specific domain perturbation layer (CDPL), which makes it possible to effectively apply mixup augmentation even when only positive data pairs are used. The experimental results show that the techniques incorporated by SelfReg contributed to the performance in a compatible manner. In the recent benchmark, DomainBed, the proposed method shows comparable performance to the conventional state-of-the-art alternatives. Codes are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning systems often fail to generalize out-ofsample distribution as they assume that in-samples and outof-samples are independent and identically distributed -this assumption rarely holds during deployment in real-world scenarios where the data is highly likely to change over time and space. Deep convolutional neural network features are often domain-invariant to low-level visual cues <ref type="bibr" target="#b34">[35]</ref>, some studies <ref type="bibr" target="#b9">[10]</ref> suggest that they are still susceptible to domain shift.</p><p>There have been increasing efforts to develop models that can generalize well to out-of-distribution. The literature in domain generalization (DG) aims to learn the invariances across multiple different domains so that a classifier can robustly leverage such invariances in unseen test domains <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. In the domain generalization task, it is assumed that multiple source domains are accessible during training, but the target domains are not <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>. This is different from domain adaptation (DA), semi-supervised domain adaptation (SSDA), and unsupervised domain adaptation (UDA) problems, where examples from the target domain are available during training. In this paper, we focus on the domain generalization task.</p><p>Some recent studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref> suggest that contrastive learning can be successfully used in a self-supervised learning task by mapping the latent representations of the positive pair samples close together, while that of negative pair samples further away in the embedding space. Such a contrastive learning strategy has also been utilized for the do-main generalization tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b10">11]</ref>, similarly aiming to reduce the distance of same-class features in the embedding space, while increasing the distance of different-class features. However, such negative pairs often make the training unstable unless useful negative samples are available in the same batch, which is but often challenging.</p><p>In this work, we revisit contrastive learning for the domain generalization task, but only with positive pair samples, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As it is generally known that using positive pair samples only causes the performance drop, which is often called representation collapse <ref type="bibr" target="#b16">[17]</ref>. Inspired by recent studies on self-supervised learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, which successfully avoids representation collapse by placing one more projection layer at the end of the network, we successfully learn domain-invariant features and our model trained with self-supervised contrastive losses shows the matched or better performance against alternative state-ofthe-art methods, where ours is ranked at 2nd places in the domain generalization benchmarks, i.e. DomainBed <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, self-supervised contrastive losses are only part of the story. As we generally use a linear form of the loss function, properly balancing gradients is required so that network parameters converge to generate domain-invariant features. To mitigate this issue, we advocate for applying the following three gradient stabilization techniques: (i) loss clipping, (ii) stochastic weights averaging (SWA), and (iii) inter-domain curriculum learning (IDCL). We observe that the combined use of these techniques further improves the model's generalization power.</p><p>To effectively evaluate our proposed model, we first use the publicly available domain generalization data set called PACS <ref type="bibr" target="#b25">[26]</ref>, where we analyzed our model in detail to support our claims. We further experiment with much larger benchmarks called DomainBed <ref type="bibr" target="#b17">[18]</ref> where our model shows matched or better performance against alternative state-of-the-art methods.</p><p>We summarize our main contributions as follows:</p><p>? SelfReg facilitates the application of metric learning using only positive pairs without negative pairs. ? We devised a CDPL by exploiting a condition that use only positive pairs. The combination of CDPL and mixup improves the weakness of mixup approach. ? The performance comparable to that of the SOTA DG methods was confirmed in the DomainBed that facilitated the comparison of DG performance in the fair and realistic environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The main goal of domain generalization (DG) is to generate domain-invariant features so that the model is generalizable to unseen target domains, which are generally outside the training distribution. Of a landmark work, Vap-nik et al. <ref type="bibr" target="#b39">[40]</ref> introduces Empirical Risk Minimization (ERM) that minimizes the sum of errors across domains. Notable variants have been introduced to learn domaininvariant features by matching distributions across different domains. Ganin et al. <ref type="bibr" target="#b14">[15]</ref> utilizes an adversarial network to match such distributions, while Li et al. <ref type="bibr" target="#b28">[29]</ref> instead matches the conditional distributions across domains. Such a shared feature space is optimized by minimizing maximum mean discrepancy <ref type="bibr" target="#b27">[28]</ref>, transformed feature distribution distance <ref type="bibr" target="#b30">[31]</ref>, or covariances <ref type="bibr" target="#b37">[38]</ref>. In this work, we also follow this stream of work, but we explore the benefit of self-supervised contrastive learning that can inherently learn to domain-invariant discriminating feature by explicitly mapping the "same-class" latent representations close together.</p><p>To our best knowledge, there are few that applied contrastive learning in the domain generalization setting. Classification and contrastive semantic alignment (CCSA) <ref type="bibr" target="#b29">[30]</ref> and model-agnostic learning of semantic features (MASF) <ref type="bibr" target="#b10">[11]</ref> aimed to reduce the distance of sameclass (positive pair) feature distributions while increasing the distance of different-class (negative pair) feature distributions. However, using such negative pairs often make the training unstable unless useful negative samples are available in the same batch, which is often challenging. To address this issue, we focus on minimizing a distance between the same-class (positive pair) features in the embedding space as recently studied for the self-supervised learning task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, including BYOL <ref type="bibr" target="#b16">[17]</ref> and SimSiam <ref type="bibr" target="#b7">[8]</ref>.</p><p>Inter-domain mixup <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b42">43]</ref> techniques are introduced to perform empirical risk minimization on linearly interpolated examples from random pairs across domains. We also utilize such a mixup, but we only interpolate sameclass features to preserve the class-specific features. We observe that such a same-class mixup help obtaining robust performance for unseen domain data.</p><p>As another branch, JiGen <ref type="bibr" target="#b4">[5]</ref> utilizes a self-supervised signal by solving a jigsaw puzzle as a secondary task to improve generalization. Meta-learning frameworks <ref type="bibr" target="#b26">[27]</ref> are also explored for domain generalization to meta-learn how to generalize across domains by leveraging MAML <ref type="bibr" target="#b13">[14]</ref>. Some also explored splitting the model into domaininvariant and domain-variant components by low-rank parameterization <ref type="bibr" target="#b25">[26]</ref>, style-agnostic network <ref type="bibr" target="#b31">[32]</ref>, domainspecific aggregation modules <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We start by motivating our method before explaining its details. The main goal of domain generalization is to learn a domain-invariant representation from multiple source domains so that a model can generalize well across unseen target domains. While domain-variant representation can be achieved to some degree through deep network architec-  Here, we propose to use the self-supervised (in-batch) contrastive losses to regularize the model to learn domain-invariant representations. These losses regularize the model to map the representations of the "same-class" samples close together in the embedding space. We compute the following two dissimilarities in the embedding space: (i) individualized and (ii) heterogenerous self-supervised dissimilarity losses. We further use the stochastic weight average (SWA) technique and the inter-domain curriculum learning (IDCL) to optimize gradients in conflict directions.</p><p>tures, invariant representations are often harder to achieve and are usually implicitly learned with the task. To address this, we argue that a model should learn a domain-invariant discriminating feature by comparing among different samples -the comparison can be performed between positive pairs of same-class inputs and negative pairs of differentclass inputs.</p><p>Here we propose the self-supervised contrastive losses to regularize the model to learn domain-invariant representation by mapping the representations of the "same-class" samples close together, while that of "different-class" samples further away in the embedding space. This may share a similar idea with contrastive learning, which trains a discriminative model on multiple input pairs according to some notion of similarity. Thus, we start with the recent batch contrastive approaches and extend them to the domain generalization setting, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. While some domain generalization approaches need to modify the model architecture during learning, our proposed contrastive method is much simpler where no modification to the model architecture is needed.</p><p>In the next section, we explain our proposed selfsupervised contrastive losses for domain generalization tasks, which mainly measures the following two featurelevel dissimilarities in the embedding space: (i) Individualized In-batch Dissimilarity Loss (Section 3.1) and (ii) Heterogeneous In-batch Dissimilarity Loss (Section 3.2). Note that these losses can be applied to both the intermediate features and the logits from the classifier (Section 3.3). In fact, in our ablation study (Section 4.4), the combined use of both regularization achieves the best performance. In Section 3.4, we also discuss the stochastic weight average (SWA) technique that we use with our self-supervised con-trastive losses and observe a further performance improvement, which is possibly due to SWA provides the more flatness in loss surface by ensembling domain-specific models. We exclude contents of the inter-domain curriculum learning (IDCL) strategy in this paper because of publication copyright issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Individualized In-batch Dissimilarity Loss</head><p>Given latent representations z c i = f ? (x i ) for i ? {1, 2, . . . , N } and a class label c ? C, we compute the individualized in-batch dissimilarity loss L ind . Note that we use a feature generator f ? parameterized by ? and we use a batch size of N . The dissimilarity between a positive pair of the "same-class" latent representations is measured as in the following Eq. 1:</p><formula xml:id="formula_0">L ind (z) = 1 N N i=1 z c i ? f CDPL z c j?[1,N ] 2 2<label>(1)</label></formula><p>where z c j is randomly chosen from other in-batch latent representations {z c i } that has the same class label c ? C. Note that we only consider optimizing the alignment of positive pairs and the uniformity of the representation distribution at the same time. As discussed in <ref type="bibr" target="#b16">[17]</ref>, we use an additional an MLP layer f CDPL , called Class-specific Domain Perturbation Layer, to prevent the performance drop caused by so-called representation collapse. We provide an ablation study in Section 4.4 to confirm the use of f CDPL achieves better performance.</p><p>For better computational efficiency, we use the following two steps to find all positive pairs, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Heterogeneous In-batch Dissimilarity Loss</head><p>To further push the model to learn domain-invariant representations, we use an additional loss, called heterogeneous in-batch dissimilarity loss. Given latent representations u i = f CDPL (z c i ) from the previous step, we apply a two-domain Mix-up layer to obtain the interpolated latent representationz i across different domains. This regularizes the model on the mixup distribution <ref type="bibr" target="#b45">[46]</ref>, i.e. a convex combination of samples from different domains. This is similar to a layer proposed by Wang et al. <ref type="bibr" target="#b42">[43]</ref> as defined as follows:?</p><formula xml:id="formula_1">c i = ?u c i + (1 ? ?)u c j?[1,N ]<label>(2)</label></formula><p>where ? ? Beta(?, ?) for ? = ? ? (0, ?). Similarly, u c j is randomly chosen from {u c i } for i ? {1, 2, . . . , N } that have the same class label. Note that ? ? [0, 1] is controlled by hyper-parameters ? and ?.</p><p>Finally, we compute the heterogeneous in-batch dissimilarity loss L hdl (z) as follows:</p><formula xml:id="formula_2">L hdl (z) = 1 N N i=1 z c i ?? c i 2 2<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature and Logit-level Self-supervised Contrastive Losses</head><p>The proposed individualized and heterogeneous inbatch dissimilarity losses can be applied to both the intermediate features and the logits from the classifier. We use the loss function L SelfReg as follows:</p><formula xml:id="formula_3">L SelfReg = ? feature L feature + ? logit L logit<label>(4)</label></formula><p>where we use ? feature and ? logit to control the strength of each term. As we use a linear form of the loss function, which often needs to be properly balanced so that network parameters converge to generate domain-invariant features that are also useful for the original classification task. We observe that our self-supervised contrastive losses L SelfReg become dominant after the initial training stage, inducing gradient imbalances to impede proper training. To mitigate this issue, we apply two gradient stabilization techniques: (i) loss clipping and (ii) stochastic weights averaging (SWA), and (iii) inter-domain curriculum learning (IDCL). For (i), we modify gradient magnitudes to be dependent on the magnitude of the classification loss L c -i.e. we use the gradient magnitude modifier min(1.0, L c ) and</p><formula xml:id="formula_4">thus L feature = min(1.0, L c ) ?L ind + (1 ? ?)L hdl . For (ii)</formula><p>we discuss details in Section 3.4.</p><p>Loss Function Ultimately, we use the following loss function L that consists of classification loss L c as well as our self-supervised contrastive loss L SelfReg :</p><formula xml:id="formula_5">L = L c + L SelfReg (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Stochastic Weights Averaging (SWA)</head><p>Stochastic Weight Average (SWA) is an ensembling technique to find a flatter minimum in loss space by averaging snapshots of model parameters derived from multiple local minima in the training procedure <ref type="bibr" target="#b22">[23]</ref>. It is known that finding a flatter minima guarantees better generalization performance <ref type="bibr" target="#b18">[19]</ref>, and thus it has been used in domain adaptation and generalization fields that require high generalization performance <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Given model weight space ? = {? 0 , ? 1 , . . . , ? N }, where N is the number of training steps. There is no specific constraint for sampling model weights, however, in general, sampling process is performed at a specific period while the model is sufficiently converged. We use c as a cyclic step length and sample weight space for SWA is ? swa = {? m+kc } for k ? 0, 0 ? m ? m + kc ? N , where m indicates the initial step for SWA. Then we can derive the averaged weight w swa as follows:</p><formula xml:id="formula_6">? swa = 1 k + 1 k i=0 ? m+ic .<label>(6)</label></formula><p>Note that we use SWA to examine whether the proposed method is compatible with other existing techniques. Therefore, we apply SWA only for ablation study (Section 4.4), not DomainBed (Section 5), for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proof-of-Concept Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation and Evaluation Details</head><p>Following Huang et al. <ref type="bibr" target="#b21">[22]</ref>, we train our model, for approximately 30 epochs, with a SGD optimizer using   <ref type="bibr" target="#b21">[22]</ref>, and (c) ours. For better understanding, we also provide sample images of house from all target domains. Note that we differently color-coded each points according to its class. Data: PACS <ref type="bibr" target="#b25">[26]</ref> ResNet18 <ref type="bibr" target="#b20">[21]</ref> as a backbone, which is pretrained on Im-ageNet <ref type="bibr" target="#b8">[9]</ref>. Our backbone produces 512-dimensional latent representation from the last layer. The batch size is set to 128 and learning rate to 0.004, which is decayed to 0.1 at 24 epochs. Note that such a decaying learning rate is not used when it combined with the Stochastic Weights Averaging technique, where we instead compute the averaged weight w swa at the every end of each epoch.</p><p>The loss weights are ? feature = 0.3 and ? logit = 1.0 were determined using grid-search. For a two-domain Mix-up layer, we use ? = ? = 0.5. The model architecture for the class-specific domain perturbation layer f CDPL is a 2-layer MLPs with the number of hidden units set to 1024, where we apply batch normalization followed by ReLU activation function. Following RSC <ref type="bibr" target="#b21">[22]</ref>, data augmentation is used in our experiments to improve model generalizability. This is done by randomly cropping, flipping horizontally, jittering color, and changing the intensity.</p><p>Dataset To verify the effectiveness of the proposed method, we evaluate our proposed method on the publicly available PACS <ref type="bibr" target="#b25">[26]</ref>. This benchmark dataset contains the overall 10k images from four different domains: P hoto, Art P ainting, Cartoon, and Sketch. This dataset is particularly useful in domain generalization research as it provides a bigger domain shift than existing photo-only benchmarks. This dataset provides seven object categories: i.e. dog, elephant, giraffe, guitar, horse, house, and person. We follow the same train-test split strategy from <ref type="bibr" target="#b25">[26]</ref>, we split examples from training domains to 9:1 (train:val) and test on the whole held-out domain. Note that we use the bestperformed model on validation for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Evaluation</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we first compare our model with the stateof-the-art method, called Representation Self-Challenging (RSC) <ref type="bibr" target="#b21">[22]</ref>, which iteratively discards the dominant features during training and thus encourages the network to fully use remaining features for the final verdict. For a fair comparison, all models use the identical backbone ConvNet, i.e. ResNet18. To see the performance variance, we trained each model 20 times for each test domain and report the average image recognition accuracy and its standard deviation. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our proposed model clearly outperforms the other approaches in all test domains (compare the model B vs. model C), and the average image recognition accuracy is 1.52% better than RSC <ref type="bibr" target="#b27">[28]</ref>, while produces lower model variance (0.9 vs. 0.3 on average).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis by t-SNE</head><p>We use t-SNE <ref type="bibr" target="#b38">[39]</ref> to compute pairwise similarities in the latent space and visualize in a low dimensional space by matching the distributions by KL divergence. In <ref type="figure" target="#fig_5">Figure 4</ref>, we provide a comparison of t-SNE visualizations of baseline, RSC, and ours. The better a model generalizes well, the points in the t-SNE should be more clustered. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, (a) the baseline model and (b) RSC <ref type="bibr" target="#b21">[22]</ref> produce scattered multiple clusters for each domain and class (see houses in the different clusters according to their domain). Ours is not the case for this. As shown in <ref type="figure" target="#fig_5">Figure 4</ref> (c), objects from the same class tend to form a merged cluster, making latent representations close to each other in the high-dimensional space. The Effect of Dissimilarity Loss We propose two types of self-supervised contrastive loss that map the "same-class" samples close together. We observe in <ref type="figure" target="#fig_6">Figure 5</ref> that "sameclass" pairwise distance is effectively regularized in both latent (a) feature and (b) logit space (compare dotted (baseline) vs. red solid line (ours)). This was not the case for the baseline. Note that we use Euclidean-based distance to measure the pairwise difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis with GradCAM</head><p>We use GradCAM <ref type="bibr" target="#b36">[37]</ref> to visualize image regions where the network attends to. In <ref type="figure">Fig. 6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Single-source Domain Generalization</head><p>We also evaluate our model in an extreme case for the domain generalization task. We train our model with ex-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photo</head><p>Art Painting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cartoon Sketch</head><p>Input image SelfReg (ours) RSC <ref type="figure">Figure 6</ref>. Original images with a giraffe for different domains (1st row). We provide visualizations of Grad-CAM <ref type="bibr" target="#b36">[37]</ref> for ours and RSC <ref type="bibr" target="#b21">[22]</ref> , which localizes class-discriminative regions. Data: PACS <ref type="bibr" target="#b25">[26]</ref> amples from a single source domain (not multiple source domains as we see in a previous experimental setting), and then we evaluate with examples from other remaining target domains. As shown in <ref type="table">Table 2</ref>, we report scores for all source-target combinations, i.e. rows and columns for source and target domains, respectively. As a baseline, we compare ours with those of RSC <ref type="bibr" target="#b21">[22]</ref> evaluated in the same setting (compare scores in left and right tables). We also report their differences in the last row ('+' indicates that ours performs better). We observe in <ref type="table">Table 2</ref> that ours generally outperform alternative, where the average accuracy is improved by 0.93%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare variants of our model by removing each component: i.e. (i) feature-level in-batch dissimilarity regularization, (ii) logit-level in-batch dissimilarity regularization, (iii) a two-domain mix-up layer, (iv) a classspecific domain perturbation layer (CDPL), (v) stochastic weights averaging (SWA), and (vi) inter-domain curriculum learning (IDCL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Inter-domain Curriculum Learning (IDCL)</head><p>We observe in <ref type="table" target="#tab_3">Table 3</ref> that applying our inter-domain curriculum learning (IDCL) provides the recognition accuracy (compare model A vs. B). Scores are generally improved in all target domains, i.e. the average accuracy is improved by 0.32%. <ref type="table" target="#tab_3">Table 3</ref>, the use of stochastic weight average technique further provides better performance (compare model B vs. <ref type="table">Table 2</ref>. As an extreme case for the domain generalization task, we train our model with a single source domain (rows) and evaluate with other remaining target domains (columns). As a baseline, we also compare with RSC <ref type="bibr" target="#b21">[22]</ref> of the same setting (compare left and right tables). We also report their differences in the last row (+ indicates that ours performs better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Stochastic Weights Averaging (SWA) As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RSC [22]</head><p>Target  C) in all target domains, i.e. the average accuracy is improved by 0.25%. This is probably due to SWA provides the flatness in loss surface by ensembling domain-specific models, which generally have multiple local-minima during the training procedure. <ref type="table" target="#tab_3">Table 3</ref>, we observe that both CDPL and Mixup components contribute to improve the overall performance (compare Model C vs. D for CDPL, and Model D vs. E for Mixup). Such improvement is more noticeable for the Sketch domain, which may support that CDPL reinforces the overall effect of mixup and makes DG performance more robust for target domains that are significantly distanced from their source domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Mixup and CDPL As shown in</head><p>Feature and Logit-level Contrastive Losses Model F , as defined as the baseline model (Model G) plus R l , had an average performance improvement of 1.50%. Accuracy improved and variance decreased across all of the domains. Therefore, regularization to minimize the logit vector-wise distance on positive pairs appears effective in extracting domain invariant features. Furthermore, Model E, which adds R f and R l to the baseline model, exhibited even greater performance increase. Minimizing feature distances of positive pairs as well as logit distances, was observed to be effective in improving DG performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on DomainBed</head><p>We further conduct experiments using DomainBed <ref type="bibr" target="#b17">[18]</ref>, which is a unified testbed useful for evaluating domain generalization algorithms. This testbed currently provides seven multi-domain datasets (i.e. ColoredMNIST <ref type="bibr" target="#b0">[1]</ref>, RotatedMNIST <ref type="bibr" target="#b15">[16]</ref>, VLCS <ref type="bibr" target="#b12">[13]</ref>, PACS <ref type="bibr" target="#b25">[26]</ref>, Office-Home <ref type="bibr" target="#b41">[42]</ref>, and TerraIncognita <ref type="bibr" target="#b1">[2]</ref>, DomainNet <ref type="bibr" target="#b33">[34]</ref>) and provides benchmarks results of 14 baseline approaches (i.e. ERM <ref type="bibr" target="#b40">[41]</ref>, IRM <ref type="bibr" target="#b0">[1]</ref>, GroupDRO <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr">Mixup [45]</ref>, MLDG <ref type="bibr" target="#b26">[27]</ref>, CORAL <ref type="bibr" target="#b37">[38]</ref>, MMD <ref type="bibr" target="#b27">[28]</ref>, DANN <ref type="bibr" target="#b14">[15]</ref>, CDANN <ref type="bibr" target="#b28">[29]</ref>, MTL <ref type="bibr" target="#b2">[3]</ref>, SagNet <ref type="bibr" target="#b31">[32]</ref>, ARM <ref type="bibr" target="#b46">[47]</ref>, VREx <ref type="bibr" target="#b24">[25]</ref>, RSC <ref type="bibr" target="#b21">[22]</ref>).</p><p>As shown in <ref type="table" target="#tab_4">Table 4</ref>, we also report scores for our model evaluated in the setting of DomainBed. We observe in Table 4 that ours generally shows matched or better performance against alternative state-of-the-art methods, where ours is ranked 2nd places in terms of average of all seven benchmarks. Note that ours does not use IDCL and SWA <ref type="bibr">(3.4)</ref> techniques, which we confirmed that further improvements are highly achievable combined with these techniques. We provide more detailed scores for each domain in the appendix. Note that DANN <ref type="bibr" target="#b14">[15]</ref> and CORAL <ref type="bibr" target="#b37">[38]</ref>   <ref type="bibr" target="#b37">[38]</ref> is trained to minimize the distance between covariances of the source and target features. Note also that some studies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> use the adversarial learning setting to obtain an unknown domaininvariant feature by fitting implicit generative models, such as GAN (generative adversarial networks). Though GAN is a powerful framework, the alternating gradient updates procedure is often highly unstable and often results in mode collapse <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed SelfReg, a new regularization method for domain generalization that leverages a selfsupervised contrastive regularization loss with only positive data pairs, mitigating problems caused by negative pair sampling. Our experiments on PACS dataset and Do-mainBed benchmarks show that our model matches or outperforms prior work under the standard domain generalization evaluation setting. In future work, it would be interesting to extend SelfReg with the siamese network, enabling the model to choose better positive data pairs.  <ref type="table">Table 8</ref>. Detailed scores on PACS <ref type="bibr" target="#b25">[26]</ref> in DomainBed <ref type="bibr" target="#b17">[18]</ref>. Note that we provide the performance of our SelfReg applied with SWA <ref type="bibr" target="#b22">[23]</ref> technique also.  <ref type="table" target="#tab_0">Table 11</ref>. Detailed scores on DomainNet <ref type="bibr" target="#b33">[34]</ref> in DomainBed <ref type="bibr" target="#b17">[18]</ref>. Note that SelfReg ? achived the-state-of-the-art. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our model utilizes the self-supervised contrastive losses for the model to learn domain-invariant representation by mapping the latent representation of the same-class samples close together. Note that different shapes (i.e. circles, stars, and squares) indicate different classes C i?{1,2,3} , and we differently color-code according to their domain D i?{1,2,3,4} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An overview of our proposed SelfReg.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .Figure 3 .</head><label>33</label><figDesc>(i) We first cluster and order latent representations z i into a same-class group, i.e. {z c i } for c ? C. (ii) For each same-An overview of our proposed self-supervised contrastive regularization losses. class group, we modify its order by random shuffling and obtain SHUFFLE{z c i }. (iii) We finally form a positive pair in order from {z c i } and SHUFFLE{z c i }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualizations by t-SNE [39] for (a) baseline (no DG techniques), (b) RSC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Distance between (a) a pair of same-class features and (b) a pair of same-class logits. We measure such distance at every epoch during training and compare ours (solid red) with baseline (dotted). Euclidean-based distance is used to measure distance in feature space. Data: PACS<ref type="bibr" target="#b25">[26]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Image recognition accuracy (%) comparison with the state-of-the-art approach, RSC<ref type="bibr" target="#b21">[22]</ref>, on PACS<ref type="bibr" target="#b25">[26]</ref> test set. We also report standard deviation from a set of 20 models individually trained for each model and each test domain.</figDesc><table><row><cell>Model</cell><cell cols="2">Test Domain</cell><cell>Average</cell></row><row><cell>Photo</cell><cell>Art Painting</cell><cell>Cartoon</cell><cell>Sketch</cell></row></table><note>A. DeepAll 95.66 ? 0.4 79.89 ? 1.3 75.61 ? 1.5 73.33 ? 2.8 81.12 ? 0.8 B. RSC [22] 94.56 ? 0.4 79.88 ? 1.7 76.87 ? 1.2 77.11 ? 2.7 82.10 ? 0.9 C. A + SelfReg (ours) 96.22 ? 0.3 82.34 ? 0.5 78.43 ? 0.7 77.47 ? 0.8 83.62 ? 0.3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, we provide examples for different target domains where we compare the model's attention maps. We observe ours better captures the class-invariant feature (i.e. the long neck of the giraffe), while RSC [22] does not. Red is the attended region for the network's final verdict.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of SelfReg on PACS. Abbr. R f : feature-level in-batch dissimilarity loss, R ? 0.3 81.77 ? 1.1 77.45 ? 1.1 75.74 ? 1.6 82.75 ? 0.7 F. E w/o L feature 96.19 ? 0.3 81.59 ? 1.2 76.98 ? 1.3 75.71 ? 1.3 82.62 ? 0.5</figDesc><table><row><cell>: logit-level in-batch dissimilarity</cell></row></table><note>lG. F w/o L logit (baseline) 95.66 ? 0.4 79.89 ? 1.3 75.61 ? 1.5 73.33 ? 2.8 81.12 ? 0.8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average out-of-distribution test accuracies on the DomainBed setting. Here we compare 14 domain generalization algorithms in the exact same conditions. Note that we train domain validation set as a model selection method. ? : Ours does not use IDCL and SWA techniques due to implementational inflexibility on the DomainBed environment. Abbr. D: learning domain-invariant features by matching distributions across different domains, A: Adversarial learning strategy, M : inter-domain mix-up, C: contrastive learning, U : unsupervised domain adaptation, which is originally designed to take examples from the target domain during training.</figDesc><table><row><cell>Model</cell><cell cols="7">D A M C U CMNIST [1] RMNIST [16] VLCS [13] PACS [26] OfficeHome [42] TerraIncognita [2] DomainNet [34] Average</cell></row><row><cell>CORAL [38]</cell><cell>51.5 ? 0.1</cell><cell>98.0 ? 0.1</cell><cell>78.8 ? 0.6 86.2 ? 0.3</cell><cell>68.7 ? 0.3</cell><cell>47.6 ? 1.0</cell><cell>41.5 ? 0.1</cell><cell>67.5</cell></row><row><cell>SelfReg (ours)  ?</cell><cell>52.1 ? 0.2</cell><cell>98.0 ? 0.1</cell><cell>77.8 ? 0.9 85.6 ? 0.4</cell><cell>67.9 ? 0.7</cell><cell>47.0 ? 0.3</cell><cell>42.8 ? 0.0</cell><cell>67.3</cell></row><row><cell>SagNet [32]</cell><cell>51.7 ? 0.0</cell><cell>98.0 ? 0.0</cell><cell>77.8 ? 0.5 86.3 ? 0.2</cell><cell>68.1 ? 0.1</cell><cell>48.6 ? 1.0</cell><cell>40.3 ? 0.1</cell><cell>67.2</cell></row><row><cell>Mixup [45]</cell><cell>52.1 ? 0.2</cell><cell>98.0 ? 0.1</cell><cell>77.4 ? 0.6 84.6 ? 0.6</cell><cell>68.1 ? 0.3</cell><cell>47.9 ? 0.8</cell><cell>39.2 ? 0.1</cell><cell>66.7</cell></row><row><cell>MLDG [27]</cell><cell>51.5 ? 0.1</cell><cell>97.9 ? 0.0</cell><cell>77.2 ? 0.4 84.9 ? 1.0</cell><cell>66.8 ? 0.6</cell><cell>47.7 ? 0.9</cell><cell>41.2 ? 0.1</cell><cell>66.7</cell></row><row><cell>ERM [41]</cell><cell>51.5 ? 0.1</cell><cell>98.0 ? 0.0</cell><cell>77.5 ? 0.4 85.5 ? 0.2</cell><cell>66.5 ? 0.3</cell><cell>46.1 ? 1.8</cell><cell>40.9 ? 0.1</cell><cell>66.6</cell></row><row><cell>MTL [3]</cell><cell>51.4 ? 0.1</cell><cell>97.9 ? 0.0</cell><cell>77.2 ? 0.4 84.6 ? 0.5</cell><cell>66.4 ? 0.5</cell><cell>45.6 ? 1.2</cell><cell>40.6 ? 0.1</cell><cell>66.2</cell></row><row><cell>RSC [22]</cell><cell>51.7 ? 0.2</cell><cell>97.6 ? 0.1</cell><cell>77.1 ? 0.5 85.2 ? 0.9</cell><cell>65.5 ? 0.9</cell><cell>46.6 ? 1.0</cell><cell>38.9 ? 0.5</cell><cell>66.1</cell></row><row><cell>ARM [47]</cell><cell>56.2 ? 0.2</cell><cell>98.2 ? 0.1</cell><cell>77.6 ? 0.3 85.1 ? 0.4</cell><cell>64.8 ? 0.3</cell><cell>45.5 ? 0.3</cell><cell>35.5 ? 0.2</cell><cell>66.1</cell></row><row><cell>DANN [15]</cell><cell>51.5 ? 0.3</cell><cell>97.8 ? 0.1</cell><cell>78.6 ? 0.4 83.6 ? 0.4</cell><cell>65.9 ? 0.6</cell><cell>46.7 ? 0.5</cell><cell>38.3 ? 0.1</cell><cell>66.1</cell></row><row><cell>VREx [25]</cell><cell>51.8 ? 0.1</cell><cell>97.9 ? 0.1</cell><cell>78.3 ? 0.2 84.9 ? 0.6</cell><cell>66.4 ? 0.6</cell><cell>46.4 ? 0.6</cell><cell>33.6 ? 2.9</cell><cell>65.6</cell></row><row><cell>CDANN [29]</cell><cell>51.7 ? 0.1</cell><cell>97.9 ? 0.1</cell><cell>77.5 ? 0.1 82.6 ? 0.9</cell><cell>65.8 ? 1.3</cell><cell>45.8 ? 1.6</cell><cell>38.3 ? 0.3</cell><cell>65.6</cell></row><row><cell>IRM [1]</cell><cell>52.0 ? 0.1</cell><cell>97.7 ? 0.1</cell><cell>78.5 ? 0.5 83.5 ? 0.8</cell><cell>64.3 ? 2.2</cell><cell>47.6 ? 0.8</cell><cell>33.9 ? 2.8</cell><cell>65.4</cell></row><row><cell>GroupDRO [36]</cell><cell>52.1 ? 0.0</cell><cell>98.0 ? 0.0</cell><cell>76.7 ? 0.6 84.4 ? 0.8</cell><cell>66.0 ? 0.7</cell><cell>43.2 ? 1.1</cell><cell>33.3 ? 0.2</cell><cell>64.8</cell></row><row><cell>MMD [28]</cell><cell>51.5 ? 0.2</cell><cell>97.9 ? 0.0</cell><cell>77.5 ? 0.9 84.6 ? 0.5</cell><cell>66.3 ? 0.1</cell><cell>42.2 ? 1.6</cell><cell>23.4 ? 9.5</cell><cell>63.3</cell></row><row><cell cols="4">are designed to take examples from the target domain dur-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ing training -i.e. CORAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 7 .</head><label>57</label><figDesc>Detailed scores on ColoredMNIST<ref type="bibr" target="#b0">[1]</ref> in DomainBed<ref type="bibr" target="#b17">[18]</ref>. Detailed scores on VLCS<ref type="bibr" target="#b12">[13]</ref> in DomainBed<ref type="bibr" target="#b17">[18]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Model selection: training-domain validation set</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell>+90%</cell><cell>+80%</cell><cell>-90%</cell><cell>Avg</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">SelfReg (ours)  ? 72.2 ? 0.5 73.7 ? 0.2 10.5 ? 0.3 52.1 ? 0.2</cell><cell></cell></row><row><cell></cell><cell>ERM [41]</cell><cell cols="3">71.7 ? 0.1 72.9 ? 0.2 10.0 ? 0.1</cell><cell>51.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Model selection: test-domain validation set (oracle)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell>+90%</cell><cell>+80%</cell><cell>-90%</cell><cell>Avg</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">SelfReg (ours)  ? 71.3 ? 0.4 73.4 ? 0.2 29.3 ? 2.1 58.0 ? 0.7</cell><cell></cell></row><row><cell></cell><cell>ERM [41]</cell><cell cols="3">71.8 ? 0.4 72.9 ? 0.1 28.7 ? 0.5</cell><cell>57.8</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Table 6. Detailed scores on RotatedMNIST [16] in DomainBed [18].</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Model selection: training-domain validation set</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>60</cell><cell>75</cell><cell>Avg</cell></row><row><cell cols="8">SelfReg (ours)  ? 95.7 ? 0.3 99.0 ? 0.1 98.9 ? 0.1 99.0 ? 0.1 98.9 ? 0.1 96.6 ? 0.1 98.0 ? 0.2</cell></row><row><cell>ERM [41]</cell><cell cols="6">95.9 ? 0.1 98.9 ? 0.0 98.8 ? 0.0 98.9 ? 0.0 98.9 ? 0.0 96.4 ? 0.0</cell><cell>98.0</cell></row><row><cell></cell><cell cols="5">Model selection: test-domain validation set (oracle)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>0</cell><cell>15</cell><cell>30</cell><cell>45</cell><cell>60</cell><cell>75</cell><cell>Avg</cell></row><row><cell cols="8">SelfReg (ours)  ? 96.0 ? 0.3 98.9 ? 0.1 98.9 ? 0.1 98.9 ? 0.1 98.9 ? 0.1 96.8 ? 0.1 98.1 ? 0.7</cell></row><row><cell>ERM [41]</cell><cell cols="6">95.3 ? 0.2 98.7 ? 0.1 98.9 ? 0.1 98.7 ? 0.2 98.9 ? 0.0 96.2 ? 0.2</cell><cell>97.8</cell></row><row><cell></cell><cell></cell><cell cols="4">Model selection: training-domain validation set</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell cols="6">SelfReg (ours)  ? 96.7 ? 0.4 65.2 ? 1.2 73.1 ? 1.3 76.2 ? 0.7 77.8 ? 0.9</cell><cell></cell></row><row><cell></cell><cell>ERM [41]</cell><cell cols="4">97.7 ? 0.4 64.3 ? 0.9 73.4 ? 0.5 74.6 ? 1.3</cell><cell>77.5</cell><cell></cell></row><row><cell></cell><cell cols="5">Model selection: test-domain validation set (oracle)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell>C</cell><cell>L</cell><cell>S</cell><cell>V</cell><cell>Avg</cell><cell></cell></row><row><cell></cell><cell cols="6">SelfReg (ours)  ? 97.9 ? 0.4 66.7 ? 0.1 73.5 ? 0.7 74.7 ? 0.7 78.2 ? 0.1</cell><cell></cell></row><row><cell></cell><cell>ERM [41]</cell><cell cols="4">97.6 ? 0.3 67.9 ? 0.7 70.9 ? 0.2 74.0 ? 0.6</cell><cell>77.6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>? 0.8 77.1 ? 0.5 96.4 ? 0.8 77.3 ? 1.8 84.6 GroupDRO [36] 83.5 ? 0.9 79.1 ? 0.6 96.7 ? 0.3 78.3 ? 2.0 84.4 DANN [15] 86.4 ? 0.8 77.4 ? 0.8 97.3 ? 0.4 73.5 ? 2.3 83.6 IRM [1] 84.8 ? 1.3 76.4 ? 1.1 96.7 ? 0.6 76.1 ? 1.0 83.5 CDANN [29] 84.6 ? 1.8 75.5 ? 0.9 96.8 ? 0.3 73.5 ? 0.6 82.6 ? 0.1 83.0 ? 0.1 97.6 ? 0.1 82.8 ? 0.2 87.7 ? 0.1 Detailed scores on OfficeHome [42] in DomainBed [18]. ? 63.6 ? 1.4 53.1 ? 1.0 76.9 ? 0.4 78.1 ? 0.4 67.9 ? 0.7 ERM [41] 61.3 ? 0.7 52.4 ? 0.3 75.8 ? 0.1 76.6 ? 0.3 ? 64.2 ? 0.6 53.6 ? 0.7 76.7 ? 0.3 77.9 ? 0.5 68.1 ? 0.3 ERM [41] 61.7 ? 0.7 53.4 ? 0.3 74.1 ? 0.4 76.2 ? 0.6 66.4 Detailed scores on TerraIncognita [2] in DomainBed [18]. ? 48.8 ? 0.9 41.3 ? 1.8 57.3 ? 0.7 40.6 ? 0.9 47.0 ? 0.3 ERM [41] 49.8 ? 4.4 42.1 ? 1.4 56.9 ? 1.8 35.7 ? 3.9 46.1 ? 60.0 ? 2.3 48.8 ? 1.0 58.6 ? 0.8 44.0 ? 0.6 52.8 ? 0.9 ERM [41] 59.4 ? 0.9 49.3 ? 0.6 60.1 ? 1.1 43.2 ? 0.5 53.0</figDesc><table><row><cell></cell><cell cols="3">Model selection: training-domain validation set</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell>Avg</cell></row><row><cell cols="6">SelfReg with SWA (ours) 85.9 ? 0.6 81.9 ? 0.4 96.8 ? 0.1 81.4 ? 0.6 86.5 ? 0.3</cell></row><row><cell>SagNet [32]</cell><cell cols="4">87.4 ? 1.0 80.7 ? 0.6 97.1 ? 0.1 80.0 ? 0.4</cell><cell>86.3</cell></row><row><cell>CORAL [38]</cell><cell cols="4">88.3 ? 0.2 80.0 ? 0.5 97.5 ? 0.3 78.8 ? 1.3</cell><cell>86.2</cell></row><row><cell>SelfReg (ours)  ?</cell><cell cols="5">87.9 ? 1.0 79.4 ? 1.4 96.8 ? 0.7 78.3 ? 1.2 85.6 ? 0.4</cell></row><row><cell>ERM [41]</cell><cell cols="4">84.7 ? 0.4 80.8 ? 0.6 97.2 ? 0.3 79.3 ? 1.0</cell><cell>85.5</cell></row><row><cell>RSC [22]</cell><cell cols="4">85.4 ? 0.8 79.7 ? 1.8 97.6 ? 0.3 78.2 ? 1.2</cell><cell>85.2</cell></row><row><cell>ARM [47]</cell><cell cols="4">86.8 ? 0.6 76.8 ? 0.5 97.4 ? 0.3 79.3 ? 1.2</cell><cell>85.1</cell></row><row><cell>VREx [25]</cell><cell cols="4">86.0 ? 1.6 79.1 ? 0.6 96.9 ? 0.5 77.7 ? 1.7</cell><cell>84.9</cell></row><row><cell>MLDG [27]</cell><cell cols="4">85.5 ? 1.4 80.1 ? 1.7 97.4 ? 0.3 76.6 ? 1.1</cell><cell>84.9</cell></row><row><cell>MMD [28]</cell><cell cols="4">86.1 ? 1.4 79.4 ? 0.9 96.6 ? 0.2 76.5 ? 0.5</cell><cell>84.6</cell></row><row><cell>Mixup [45]</cell><cell cols="4">86.1 ? 0.5 78.9 ? 0.8 97.6 ? 0.1 75.8 ? 1.8</cell><cell>84.6</cell></row><row><cell>MTL [3]</cell><cell cols="3">87.5 Model selection: test-domain validation set (oracle)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>A</cell><cell>C</cell><cell>P</cell><cell>S</cell><cell>Avg</cell></row><row><cell cols="2">SelfReg with SWA (ours) 87.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>? 60.7 ? 0.1 21.6 ? 0.1 49.4 ? 0.2 12.7 ? 0.1 60.7 ? 0.1 51.7 ? 0.1 42.8 ? 0.0 ERM [41] 58.1 ? 0.3 18.8 ? 0.3 46.7 ? 0.3 12.2 ? 0.4 59.6 ? 0.1 49.8 ? 0.4 ? 60.7 ? 0.1 21.6 ? 0.1 49.5 ? 0.1 14.2 ? 0.3 60.7 ? 0.1 51.7 ? 0.1 43.1 ? 0.1 ERM [41] 58.6 ? 0.3 19.2 ? 0.2 47.0 ? 0.3 13.2 ? 0.2 59.9 ? 0.3 49.8 ? 0.4 41.3</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Model selection: training-domain validation set</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Clip</cell><cell>Info</cell><cell>Paint</cell><cell>Quick</cell><cell>Real</cell><cell>Sketch</cell><cell>Avg</cell></row><row><cell cols="8">SelfReg (ours)  40.9</cell></row><row><cell></cell><cell></cell><cell cols="4">Model selection: test-domain validation set (oracle)</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>Clip</cell><cell>Info</cell><cell>Paint</cell><cell>Quick</cell><cell>Real</cell><cell>Sketch</cell><cell>Avg</cell></row><row><cell>SelfReg (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In <ref type="table">Table 5</ref>-11, SelfReg (ours) ? does not include Interdomain Curriculum Learning (IDCL) and SelfReg with stochastic weight averaging (SWA) <ref type="bibr" target="#b22">[23]</ref> techniques. However, note that <ref type="table">Table 8</ref> provides the performance of our Sel-fReg with SWA technique also. Since DomainBed is supposed to be evaluated every N steps, we needed to modify the code to apply the SWA technique. We modified the code to evaluate model on the test set after completing 5000 steps learning with SWA techniques. We used "-single test envs" option because the required amount of computation for the cross-validation model selection method was too much for us.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Domain generalization by marginal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Anand Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urun</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07910</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generalizing from several related classification tasks to a new unlabeled sample. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2178" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain generalization needs stochastic weight averaging for robustness on domain shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hancheol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<idno>2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566,2020.2</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UC Berkeley &amp; ICSI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Domain generalization via modelagnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13580</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Antonio D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00744</idno>
		<title level="m">Asymmetric valleys: Beyond sharp and flat local minima</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-challenging improves cross-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<title level="m">On convergence and stability of gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Out-ofdistribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00688</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5715" to="5725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reducing domain gap via style-agnostic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11645</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Statistical learning theory new york</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An overview of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improve unsupervised domain adaptation with mixup training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Shen Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lincan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adaptive risk minimization: A meta-learning approach for tackling group shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02931</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On learning invariant representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="7523" to="7532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selfreg</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>9 ? 0.5 80.6 ? 1.1 97.1 ? 0.4 81.1 ? 1.3 86.7 ? 0.8 ERM [41] 86.5 ? 1.0 81.3 ? 0.6 96.2 ? 0.3 82.7 ? 1.1 86</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vrex</surname></persName>
		</author>
		<idno>25] 87.8 ? 1.2 81.8 ? 0.7 97.4 ? 0.2 82.1 ? 0.7</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
		<idno>45] 87.5 ? 0.4 81.6 ? 0.7 97.4 ? 0.2 80.8 ? 0.9 86.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groupdro</surname></persName>
		</author>
		<idno>36] 87.5 ? 0.5 82.9 ? 0.6 97.1 ? 0.3 81.1 ? 1.2 87.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
