<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Answering Open-Domain Questions of Varying Reasoning Steps from Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<email>pengqi@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haejun</forename><surname>Lee</surname></persName>
							<email>haejun82.lee@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Oghenetegiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Tg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Sido</surname></persName>
							<email>osido@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Christopher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University ? JD AI Research ? Samsung Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Answering Open-Domain Questions of Varying Reasoning Steps from Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks-retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents-in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called B QA, by combining existing one-and twostep datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https: //beerqa.github.io/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using knowledge to solve problems is a hallmark of intelligence. Since human knowledge is often containned in large text collections, open-domain question answering (QA) is an important means for intelligent systems to make use of the knowledge in large text collections. With the help of large-scale datasets based on Wikipedia <ref type="bibr" target="#b11">(Rajpurkar et al., 2016</ref><ref type="bibr">(Rajpurkar et al., , 2018</ref> and other large corpora <ref type="bibr" target="#b15">(Trischler et al., 2016;</ref><ref type="bibr" target="#b9">Dunn et al., 2017;</ref><ref type="bibr" target="#b14">Talmor and Berant, 2018)</ref>, the research community has made substantial progress on tackling this problem in recent years, including * These authors contributed equally. in the direction of complex reasoning over multiple pieces of evidence, or multi-hop reasoning <ref type="bibr" target="#b23">(Yang et al., 2018;</ref><ref type="bibr" target="#b20">Welbl et al., 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>.</p><p>Despite this success, most previous systems are developed with, and evaluated on, datasets that contain exclusively single-hop questions (ones that require a single document or paragraph to answer) or two-hop ones. As a result, their design is often tailored exclusively to single-hop (e.g., <ref type="bibr" target="#b2">Chen et al., 2017;</ref><ref type="bibr" target="#b18">Wang et al., 2018b)</ref> or multi-hop questions (e.g., <ref type="bibr">Nie et al., 2019;</ref><ref type="bibr">Min et al., 2019;</ref><ref type="bibr" target="#b10">Feldman and El-Yaniv, 2019;</ref><ref type="bibr" target="#b25">Zhao et al., 2020a;</ref><ref type="bibr" target="#b21">Xiong et al., 2021)</ref>. Even when the model is designed to work with both, it is often trained and evaluated on exclusively single-hop or multi-hop settings (e.g., <ref type="bibr" target="#b0">Asai et al., 2020)</ref>. In practice, not only can we not expect open-domain QA systems to receive exclusively single-or multi-hop questions from users, but it is also non-trivial to judge reliably whether a question requires one or multiple pieces of evidence to answer a priori. For instance, "In which U.S. state was Facebook founded?" appears to be single-hop, but its answer cannot be found in the main text of a single English Wikipedia page.</p><p>Besides the impractical assumption about reasoning hops, previous work often also assumes access to non-textual metadata such as knowledge bases, entity linking, and Wikipedia hyperlinks when retrieving supporting facts, especially in answering complex questions <ref type="bibr">(Nie et al., 2019;</ref><ref type="bibr" target="#b10">Feldman and El-Yaniv, 2019;</ref><ref type="bibr" target="#b26">Zhao et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2020;</ref><ref type="bibr" target="#b8">Dhingra et al., 2020;</ref><ref type="bibr" target="#b25">Zhao et al., 2020a)</ref>. While this information is helpful, it is not always available in text collections we might be interested in getting answers from, such as news or academic research articles, besides being labor-intensive and time-consuming to collect and maintain. It is therefore desirable to design a system that is capable of extracting knowledge from text without using such metadata, to maximally emphasize using knowledge available to us in the form of text.  <ref type="figure">Figure 1</ref>: The IRRR question answering pipeline answers a complex question in the HotpotQA dataset by iteratively retrieving, reading, and reranking paragraphs from Wikipedia. In this example, the question is answered in five steps: 1. the retriever model selects the words "Ingerophrynus gollum" from the question as an initial search query; 2. the question answering model attempts to answer the question by combining the question with each of the retrieved paragraphs and fails to find an answer; 3. the reranker picks the paragraph about the Ingerophrynus gollum toad to extend the reasoning path; 4. the retriever generates an updated query "Lord of the Rings" to retrieve new paragraphs; 5. the reader correctly predicts the answer "150 million copies" by combining the reasoning path (question + "Ingerophrynus gollum") with the newly retrieved paragraph about "The Lord of the Rings".</p><p>To address these limitations, we propose Iterative Retriever, Reader, and Reranker (IRRR), which features a single neural network model that performs all of the subtasks required to answer questions from a large collection of text (see <ref type="figure">Figure 1</ref>). IRRR is designed to leverage off-the-shelf information retrieval systems by generating natural language search queries, which allows it to easily adapt to arbitrary collections of text without requiring welltuned neural retrieval systems or extra metadata. This further allows users to understand and control IRRR, if necessary, to facilitate trust. Moreover, IRRR iteratively retrieves more context to answer the question, which allows it to easily accommodate questions of different number of reasoning steps.</p><p>To evaluate the performance of open-domain QA systems in a more realistic setting, we construct a new benchmark called B QA 1 by combining the questions from the single-hop SQuAD Open <ref type="bibr" target="#b11">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017)</ref> and the two-hop HotpotQA <ref type="bibr" target="#b23">(Yang et al., 2018)</ref> with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer. We map all questions to a unified version of the English Wikipedia to reduce stylistic differences that might provide statistical shortcuts to models. As a result, B QA provides a more realistic evaluation of open-ended question answering systems in their ability to answer questions without knowledge of the number of reasoning steps required ahead of time. We show that IRRR not only achieves competitive performance with stateof-the-art models on the original SQuAD Open and HotpotQA datasets, but also establishes a strong baseline for this new dataset.</p><p>To recap, our contributions in this paper are: <ref type="formula" target="#formula_6">(1)</ref>   <ref type="bibr">(retrieval, reranking, and reading comprehension)</ref>, which not only achieves strong results on SQuAD and HotpotQA, but also establishes a strong baseline on this new benchmark. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Open-Domain Question Answering</head><p>The task of open-domain question answering is concerned with finding the answer to a question from a large text collection D. Successful solutions to this task usually involve two crucial components: an information retrieval system that finds a small set of relevant documents D from D, and a reading comprehension system that extracts the answer from it. <ref type="bibr">3</ref>  <ref type="bibr" target="#b2">Chen et al. (2017)</ref> presented the first neuralnetwork-based approach to this problem, which was later extended by <ref type="bibr" target="#b17">Wang et al. (2018a)</ref> with a reranking system to further reduce the amount of context the reading comprehension component has to consider to improve answer accuracy.</p><p>More recently, <ref type="bibr" target="#b23">Yang et al. (2018)</ref> showed that this single-step retrieve-and-read approach to opendomain question answering is inadequate for more complex questions that require multiple pieces of evidence to answer (e.g., "What is the population of Mark Twain's hometown?"). While later work approaches these by extending supporting fact retrieval beyond one step, most assumes that all questions are either exclusively single-hop or multihop during training and evaluation. We propose IRRR, a system that performs variable-hop retrieval for open-domain QA to address these issues, and present a new benchmark, B QA, to evaluate systems in a more realistic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IRRR: Iterative Retriever, Reader, and Reranker</head><p>In this section, we present a unified model to perform all of the subtasks necessary for open-domain question answering-Iterative Retriever, Reader, and Reranker (IRRR), which performs the subtasks involved in an iterative manner to accommodate questions with a varying number of steps. IRRR aims at building a reasoning path from the question , through all the necessary supporting documents or paragraphs ? D gold to the answer (where D gold is the set of gold supporting facts). 4 As shown in <ref type="figure">Figure 1</ref>, IRRR operates in a loop of retrieval, reading, and reranking to expand the reasoning path with new documents from ? D. Specifically, given a question , we initialize the reasoning path with the question itself, i.e., 0 = [ ], and generate from it a search query with IRRR's retriever. Once a set of relevant documents D 1 ? D is retrieved, they might either help answer the question, or reveal clues about the next piece of evidence we need to answer . The reader model then attempts to read each of the documents in D 1 to answer the question combined with the current reasoning path . If more than one answer can be found from these candidate reasoning paths, we predict the answer with the highest answerability score, which we will detail in section 3.2. If no answer can be found, then IRRR's reranker scores each retrieved paragraph against the current reasoning path, and appends the top-ranked paragraph to the current reasoning path, <ref type="bibr">4</ref> For simplicity, we assume that there is a single set of relevant supporting facts that helps answer each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retriever (Query Generator)</head><p>Reader Reranker i.e., +1 = + [arg max ? 1 reranker( , )], before the updated reasoning path is presented to the retriever to generate new search queries. This iterative process is repeated until an answer is predicted from one of the reasoning paths, or until the reasoning path has reached a cap of documents.</p><p>To reduce computational cost and improve model representations of reasoning paths from shared statistical learning, IRRR is implemented as a multitask model built on a pretrained Transformer model that performs all three subtasks. At a high level, it consists of a Transformer encoder <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> which takes the reasoning path (the question and all retrieved paragraphs so far) as input, and one set of task-specific parameters for each task of retrieval, reranking, and reading comprehension (see <ref type="figure">Figure 2</ref>). The retriever generates natural language search queries by selecting words from the reasoning path, the reader extracts answers from the reasoning path and abstains if its confidence is not high enough, and the reranker assigns a scalar score for each retrieved paragraph as a potential continuation of the current reasoning path.</p><p>The input to our Transformer encoder is formatted similarly to that of the BERT model <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>. For a reasoning path that consists of the question and retrieved paragraphs, the input is formatted as " <ref type="bibr">[CLS]</ref>  We will detail each of the task-specific components in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retriever</head><p>The goal of the retriever is to generate natural language queries to retrieve relevant documents from an off-the-shelf text-based retrieval engine. 5 This allows IRRR to perform open-domain QA in an explainable and controllable manner, where a user can easily understand the model's behavior and intervene if necessary. We extract search queries from the current reasoning path, i.e., the original question and all of the paragraphs that we have already retrieved, similar to G E Retriever's approach <ref type="bibr">(Qi et al., 2019)</ref>. This is based on the observation that there is usually a strong semantic overlap between the reasoning path and the next paragraph to retrieve, which helps reduce the search space of potential queries. We note, though, that IRRR differs from G E Retriever in two important ways: (1) we allow search queries to be any subsequence of the reasoning path instead of limiting it to substrings to allow for more flexible combinations of search phrases; (2) more importantly, we employ the same retriever model across reasoning steps to generate queries instead of training separate ones for each reasoning step, which is crucial for IRRR to generalize to arbitrary reasoning steps.</p><p>To predict these search queries from the reasoning path, we apply a token-wise binary classifier on top of the shared Transformer encoder model, to decide whether each token is included in the final query. At training time, we derive supervision signal to train these classifiers with a binary cross entropy loss (which we detail in Section 3.4.1); at test time, we select a cutoff threshold for query words to be included from the reasoning path. In practice, we find that boosting the model to predict more query terms is beneficial to increase the recall of the target paragraphs in retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reader</head><p>The reader model attempts to find the answer given a reasoning path comprised of the question and retrieved paragraphs. To support unanswerable questions and the special non-extractive answers yes and no from HotpotQA, we train a classifier conditioned on the Transformer encoder representation of the [CLS] token to predict one of the 4 classes SPAN/YES/NO/NOANSWER. The classifier thus simultaneously assigns an answerability score to this reasoning path to assess the likelihood of the document having the answer to the original question on this reasoning path. Span answers are predicted from the context using a span start classifier and a span end classifier, following <ref type="bibr" target="#b7">Devlin et al. (2019)</ref>.</p><p>We define answerability as the log likelihood ratio between the most likely positive answer and the NOANSWER prediction, and use it to pick the best answer from all the candidate reasoning paths to stop IRRR's iterative process, if found. We find that this likelihood ratio formulation is less affected by sequence length compared to prediction probability, thus making it easier to assign a global threshold across reasoning paths of different lengths to stop further retrieval. We include further details about answerability calculation in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reranker</head><p>When the reader fails to find an answer from the reasoning path, the reranker selects one of the retrieved paragraphs to expand it, so that the retriever can generate new search queries to retrieve new context to answer the question. To achieve this, we assign each potential extended reasoning path a score by linearly transforming the hidden representation of the [CLS] token, and picking the extension that has the highest score. At training time, we normalize the reranker scores across top retrieved paragraphs with softmax, and maximize the log likelihood of selecting gold supporting paragraphs from retrieved ones, which is a noise contrastive estimation <ref type="bibr">(NCE;</ref><ref type="bibr">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr">Jean et al., 2015)</ref> of the reranker likelihood over all retrieved paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training IRRR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Dynamic Oracle for Query Generation</head><p>Since existing open-domain QA datasets do not include human-annotated search queries, we need to derive supervision signal to train the retriever with a dynamic oracle. Similar to G E Retriever, we derive search queries from overlapping terms between the reasoning path and the target paragraph with the goal of maximizing retrieval performance.</p><p>To reduce computational cost, we limit our attention to overlapping spans of text between the reasoning path and the target document when generating oracle queries. For instance, when "David" is part of the overlapping span "David Dunn", the entire span is either included or excluded from the oracle query to reduce the search space. Once overlapping spans are found, we approximate the importance of each with the following "importance" metric to avoid enumerating all 2 combinations to generate the oracle query</p><formula xml:id="formula_0">Imp( ) = Rank( , { } =1, ? ) ? Rank( , { }),</formula><p>where are overlapping spans, and Rank( , ) is the rank of target document in the search result when spans are used as search queries (the smaller, the closer is to the top). Intuitively, the second term captures the importance of the search term when used alone, and the first captures its importance when combined with all other overlapping spans, which helps us capture query terms that are only effective when combined. After estimating importance of each overlapping span, we determine the final oracle query by first sorting all spans by descending importance, then including each in the final oracle query until the search rank of stops improving. The resulting time complexity for generating these oracle queries is thus ( ), i.e., linear in the number of overlapping spans between the reasoning path and the target paragraph. <ref type="figure" target="#fig_0">Figure 3</ref> shows that the added flexibility of nonspan queries in IRRR significantly improves retrieval performance compared to that of G E Retriever, which is only able to extract contiguous spans from the reasoning path as queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Reducing Exposure Bias with Data Augmentation</head><p>With the dynamic oracle, we are able to generate target queries to train the retriever model, retrieve documents to train the reranker model, and expand reasoning paths in the training set by always choosing a gold paragraph, following Qi et al. <ref type="bibr">(2019)</ref>. However, this might prevent the model from generalizing to cases where model behavior deviates from the oracle. To address this, we augment the training data by occasionally selecting non-gold Question: How many counties are on the island that is home to the fictional setting of the novel in which Daisy Buchanan is a supporting character?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Page 1: Daisy Buchanan</head><p>Daisy Fay Buchanan is a fictional character in F. Scott Fitzgerald's magnum opus "The Great Gatsby" (1925)...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Page 2: The Great Gatsby</head><p>The Great Gatsby is a 1925 novel ... that follows a cast of characters living in the fictional town of West Egg on prosperous Long Island ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wikipedia Page 3: Long Island</head><p>The Long Island ... comprises four counties in the U.S. state of New York: Kings and Queens ... to the west; and Nassau and Suffolk to the east... paragraphs to expand reasoning paths, and use the dynamic oracle to generate queries for the model to "recover" from these synthesized retrieval mistakes. We found that this data augmentation significantly improves the performance of IRRR in preliminary experiments, and thus report main results with augmented training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer: four</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Standard Benchmarks. We test IRRR on two standard benchmarks, SQuAD Open and HotpotQA. SQuAD Open <ref type="bibr" target="#b2">(Chen et al., 2017)</ref> designates the development set of the original SQuAD dataset as its test set, which features more than 10,000 questions, each based on a single paragraph in a Wikipedia article. For this dataset, we follow previous work and use the 2016 English Wikipedia as the corpus for evaluation. Since the authors did not present a standard development set, we further split part of the training set to construct a development set roughly as large as the test set. HotpotQA <ref type="bibr" target="#b23">(Yang et al., 2018)</ref> features more than 100,000 questions that require the introductory paragraphs of two Wikipedia articles to answer, and we focus on its open-domain "fullwiki" setting in this work. For HotpotQA, we use the introductory paragraphs provided by the authors for training and evaluation, which is based on a 2017 Wikipedia dump.</p><p>New Benchmark. To evaluate the performance of IRRR as well as future QA systems in a more realistic open-domain setting without a pre-specified number of reasoning steps for each question, we further combine SQuAD Open and HotpotQA with 530 newly collected challenge questions (see <ref type="figure" target="#fig_1">Figure  4</ref> for an example, and Appendix E for more details) to construct a new benchmark. Note that naively combining the datasets by merging the questions and the underlying corpora is problematic, as the corpora not only feature repeated and sometimes contradicting information, but also make them available in two distinct forms (full Wikipedia pages in one and just the introductory paragraphs in the other). This could result in models taking corpus style as a shortcut to determine question complexity, or even result in plausible false answers due to corpus inconsistency.</p><p>To construct a high-quality unified benchmark, we begin by mapping the paragraphs each question is based on to a more recent version of Wikipedia. <ref type="bibr">6</ref> We discarded examples where the Wikipedia pages have either been removed or significantly edited such that the answer can no longer be found from paragraphs that are similar enough to the original contexts the questions are based on. 7 As a result, we filtered out 22,328 examples from SQuAD Open, and 18,649 examples from HotpotQA's fullwiki setting. We add newly annotated challenge questions to the test set of the new benchmark, which require at least three steps of reasoning to answer. This allows us to test the generalization capabilities of QA models to this unseen scenario. The statistics of the final dataset, which we name B QA, can be found in <ref type="table">Table 1</ref>. For all benchmark datasets, we report standard answer exact match (EM) and unigram F 1 metrics.</p><p>Training details. We use ELECTRA LARGE <ref type="bibr" target="#b5">(Clark et al., 2020)</ref> as the pre-trained initialization for our Transformer encoder. We train the model on a combined dataset of SQuAD Open and HotpotQA questions where we optimize the joint loss of the retriever, reader, and reranker components simultaneously in an multi-task learning <ref type="bibr">6</ref> In this work, we used the English Wikipedia dump from August 1st, 2020. <ref type="bibr">7</ref> We refer the reader to Appendix A for further details about these Wikipedia corpora and how we process and map between them. fashion. Training data for the retriever and reranker components is derived from the dynamic oracle on the training set of these datasets, where reasoning paths are expanded with oracle queries and by picking the gold paragraphs as they are retrieved for the reader component. We augment the training data with the technique in Section 3.4.2 and expand reasoning paths up to 3 reasoning steps on HotpotQA and 2 on SQuAD Open, and find that this results in a more robust model. After an initial model is finetuned on this expanded training set, we apply our iterative training technique to further reduce exposure bias of the model by generating more data with the trained model and the dynamic oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In this section, we present the performance of IRRR when evaluated against previous systems on standard benchmarks, and demonstrate its efficacy on our new, unified benchmark, especially with the help of iterative training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance on Standard Benchmarks</head><p>We first compare IRRR against previous systems on SQuAD Open and the fullwiki setting of HotpotQA. On each dataset, we compare the performance of IRRR against best previously published systems, as well as unpublished ones on public leaderboards. For a fairer comparison to previous work, we make use of their respective Wikipedia corpora, and limit the retriever to retrieve 150 paragraphs of text from Wikipedia at each step of reasoning. We also compare IRRR against the Graph Recurrent Retriever (GRR; Asai et al., 2020) on our newly collected 3-hop question challenge test set, using the author's released code and models trained on HotpotQA. In these experiments, we report IRRR performance both from training on the dataset it is evaluated on, and from combining the training data we derived from both SQuAD Open and HotpotQA.</p><p>As can be seen in <ref type="table" target="#tab_6">Tables 2 and 3</ref>, IRRR achieves competitive performance with previous work, and further outperforms previously published work on SQuAD Open by a large margin when trained on combined data. It also outperforms systems that were submitted after IRRR was initially submitted to the HotpotQA leaderboard. On the 3+ hop challenge set, we similarly notice a large performance margin between IRRR and GRR, although neither is trained with questions requiring three or more hops, demonstrating that IRRR generalizes well to System SQuAD Open EM F 1</p><p>DrQA <ref type="bibr" target="#b2">(Chen et al., 2017)</ref> 27.1 -DensePR <ref type="bibr">(Karpukhin et al., 2020)</ref> 38.1 -BERTserini <ref type="bibr" target="#b22">(Yang et al., 2019)</ref> 38.6 46.1 MUPPET <ref type="bibr" target="#b10">(Feldman and El-Yaniv, 2019)</ref>    questions that require more retrieval steps than the ones seen during training. We note that the systems that outperform IRRR on these datasets typically make use of trainable neural retrieval components, which IRRR can potentially benefit from adopting as well. Specifically, SPARTA <ref type="bibr" target="#b27">(Zhao et al., 2020b)</ref> introduces a neural sparse retrieval system that potentially works well with IRRR's oracle query generation procedure to further improve retrieval performance, thanks to its use of natural language queries. HopRetriever (Li et al., 2020) introduces a novel representation of documents for retrieval that is particularly suitable for discovering documents connected by the same entity to answer multi-hop questions, which IRRR could benefit from as well. We leave exploration of these directions to future work.</p><p>To better understand the behavior of IRRR on (1)</p><p>(1)</p><p>(1) </p><p>(2)</p><p>(2) these benchmarks, we analyze the number of paragraphs retrieved by the model when varying the number of paragraphs retrieved at each reasoning step among {50, 100, 150}. As can be seen in <ref type="figure" target="#fig_2">Figure 5</ref>, IRRR stops its iterative process as soon as all necessary paragraphs to answer the question have been retrieved, effectively reducing the total number of paragraphs retrieved and read by the model compared to always retrieving a fixed number of paragraphs for each question. Further, we note that the optimal cap for the number of reasoning steps is larger than the number of gold paragraphs necessary to answer the question on each benchmark, which we find is due to IRRR's ability to recover from retrieving and selecting non-gold paragraphs (see the example in <ref type="figure">Figure 6</ref>). Finally, we note that increasing the number of paragraphs retrieved at each reasoning step remains an effective, if computationally expensive, strategy, to improve the end-to-end performance of IRRR. However, the tradeoff between retrieval budget and model performance is more effective than that of previous work (e.g., GRR), and we note that the queries generated by IRRR are explainable to humans and can help humans easily control its behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance on the Unified Benchmark</head><p>To demonstrate the performance of IRRR in a more realistic setting of open-domain QA, we evaluate it on the new, unified benchmark. As is shown in <ref type="table" target="#tab_11">Table 4</ref>, IRRR's performance remains competitive on all questions from different origins in the unified benchmark, despite the difference in reasoning complexity when answering these questions.    <ref type="table" target="#tab_11">Table  4</ref> because fewer reasoning steps are used (3 vs. 5) and fewer paragraphs retrieved at each step (50 vs. 150).</p><p>The model also generalizes to the 3-hop questions despite having never been trained on them. We note that the large performance gap between the development and test settings for SQuAD Open questions is due to the fact that test set questions (the original SQuAD dev set) are annotated with multiple human answers, while the dev set ones (originally from the SQuAD training set) are not.</p><p>To better understand the contribution of the various components and techniques we proposed for IRRR, we performed ablation studies on the model iterating up to 3 reasoning steps with 50 paragraphs for each step, and present the results in <ref type="table" target="#tab_12">Table 5</ref>. First of all, we find it is important to allow IRRR to dynamically stop retrieving paragraphs to answer the question. Compared to its fixed-step retrieval counterpart, dynamically stopping IRRR improves F 1 on both SQuAD and HotpotQA questions by 27.0 and 2.1 points respectively (we include further analyses for dynamic stopping in Appendix D). We also find combining SQuAD and HotpotQA datasets beneficial for both datasets in an opendomain setting, and that ELECTRA is an effective alternative to BERT for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The availability of large-scale question answering (QA) datasets has greatly contributed to the research progress on open-domain QA. SQuAD <ref type="bibr">(Rajpurkar Question</ref> The Ingerophrynus gollum is named after a character in a book that sold how many copies?</p><p>Step 1 (Non-Gold)</p><p>Ingerophrynus is a genus of true toads with 12 species. ... In 2007 a new species, "Ingerophrynus gollum", was added to this genus. This species is named after the character Gollum created by J. R. R. Tolkien." Query Ingerophrynus gollum book sold copies J. R. R. Tolkien</p><p>Step 2 (Gold)</p><p>Ingerophrynus gollum (Gollum's toad) is a species of true toad. ... It is called "gollum" with reference of the eponymous character of The Lord of the Rings by J. R. R. Tolkien. Query</p><p>Ingerophrynus gollum character book sold copies J. R. R. Tolkien true Lord of the Rings</p><p>Step 3 (Gold)</p><p>The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. ... is one of the best-selling novels ever written, with 150 million copies sold. Answer/GT 150 million copies <ref type="figure">Figure 6</ref>: An example of IRRR answering a question from HotpotQA by generating natural language queries to retrieve paragraphs, then rerank them to compose reasoning paths and read them to predict the answer. Here, IRRR recovers from an initial retrieval/reranking mistake by retrieving more paragraphs, before arriving at the gold supporting facts and the correct answer. <ref type="bibr">et al., 2016, 2018)</ref> is among the first question answering datasets adopted for this purpose by <ref type="bibr" target="#b2">Chen et al. (2017)</ref>  Inspired by the TREC QA challenge, <ref type="bibr">8 Chen et al. (2017)</ref> were the first to combine information retrieval systems with accurate neural network-based reading comprehension models for open-domain QA. Recent work has improved open-domain QA performance by enhancing various components in this retrieve-and-read approach. While much research focused on improving the reading comprehension model <ref type="bibr" target="#b13">(Seo et al., 2017;</ref><ref type="bibr" target="#b4">Clark and Gardner, 2018)</ref>, especially with pretrained langauge models like BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, researchers have also demonstrated that neural network-based information retrieval systems achieve competitive, if not better, performance compared to traditional IR engines <ref type="bibr" target="#b7">(Lee et al., 2019;</ref><ref type="bibr">Khattab et al., 2020;</ref><ref type="bibr">Guu et al., 2020;</ref><ref type="bibr" target="#b21">Xiong et al., 2021)</ref>. Aside from the reading comprehension and retrieval components, researchers have also found value from reranking search results <ref type="bibr" target="#b17">(Wang et al., 2018a)</ref> or answer candidates <ref type="bibr" target="#b18">(Wang et al., 2018b;</ref><ref type="bibr">Hu et al., 2019)</ref>.</p><p>While most work focuses on questions that require only a local context of supporting facts to answer, <ref type="bibr" target="#b23">Yang et al. (2018)</ref> presented HotpotQA, which tests whether open-domain QA systems can generalize to more complex questions that require evidence from multiple documents to answer. Researchers have explored various techniques to extend retrieveand-read systems to this problem, including making use of hyperlinks between Wikipedia articles <ref type="bibr">(Nie et al., 2019;</ref><ref type="bibr" target="#b10">Feldman and El-Yaniv, 2019;</ref><ref type="bibr" target="#b26">Zhao et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2020;</ref><ref type="bibr" target="#b8">Dhingra et al., 2020;</ref><ref type="bibr" target="#b26">Zhao et al., 2019)</ref> and iterative retrieval <ref type="bibr" target="#b14">(Talmor and Berant, 2018;</ref><ref type="bibr" target="#b6">Das et al., 2019;</ref><ref type="bibr">Qi et al., 2019)</ref>. While most previous work on iterative retrieval makes use of neural retrieval systems that directly accept real vectors as input, our work is similar to that of <ref type="bibr">Qi et al. (2019)</ref> in using natural language search queries. A crucial distinction between our work and previous work on multi-hop open-domain QA, however, is that we don't train models to exclusively answer single-hop or multi-hop questions, but demonstrate that one single set of parameters performs well on both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented Iterative Retriever, Reader, and Reranker (IRRR), a system that uses </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data processing</head><p>In this section, we describe how we process the English Wikipedia and the SQuAD dataset for training and evaluating IRRR. For the standard benchmarks (SQuAD Open and HotpotQA fullwiki), we use the Wikipedia corpora prepared by <ref type="bibr" target="#b2">Chen et al. (2017)</ref> and <ref type="bibr" target="#b23">Yang et al. (2018)</ref>, respectively, so that our results are comparable with previous work on these benchmarks. Specifically, for SQuAD Open, we use the processed English Wikipedia released by <ref type="bibr" target="#b2">Chen et al. (2017)</ref> which was accessed in 2016, and contains 5,075,182 documents. 9 For HotpotQA, <ref type="bibr" target="#b23">Yang et al. (2018)</ref> released a processed set of Wikipedia introductory paragraphs from the English Wikipedia originally accessed in October 2017. <ref type="bibr">10</ref> While it is established that the SQuAD dev set is repurposed as the test set for SQuAD Open for ease of evaluation, most previous work make use of the entire training set during training, and as a result a proper development set for SQuAD Open does not exist. <ref type="bibr">11</ref> We therefore resplit the SQuAD training set into a proper development set that is not used during training, and a reduced training set that we use for all of our experiments. As a result, although IRRR is evaluated on the same test set as previous systems, it is likely disadvantaged due to the reduced amount of training data and hyperparameter tuning on this new dev set. We split the training set by first grouping questions and paragraphs by the Wikipedia entity/title they belong to, then randomly selecting entities to add to the dev set until the dev set contains roughly as many questions as the test set (original SQuAD dev set). The statistics of our resplit of SQuAD can be found in <ref type="table" target="#tab_14">Table 6</ref>. We make our resplit publicly available to the community at https://beerqa.github.io/.</p><p>For the unified benchmark, we started by processing the English Wikipedia 12 with the WikiExtractor <ref type="bibr" target="#b1">(Attardi, 2015)</ref>. We then tokenized this dump and the supporting context used in SQuAD and HotpotQA with Stanford CoreNLP 4.0.0 <ref type="bibr">(Manning et al., 2014)</ref> to look for paragraphs in the Split Origin <ref type="table" target="#tab_6"># Entities #QAs   train  train  387  77,087  dev  train  55  10,512  test  dev  48</ref> 10,570 2020 Wikipedia dump that might correspond to the context paragraphs in these datasets. Since many Wikipedia articles have been renamed or removed since, we begin by following Wikipedia redirect links to locate the current title of the corresponding Wikipedia page (e.g., the page "Madonna (entertainer)" has been renamed "Madonna"). After the correct Wikipedia article is located, we look for combinations of one to two consecutive paragraphs in the 2020 Wikipedia dump that have high overlap with context paragraphs in these datasets.</p><p>We calculate the recall of words and phrases in the original context paragraph (because Wikipedia paragraphs are often expanded with more details), and pick the best combination of paragraphs from the article. If the best candidate has either more than 66% unigrams in the original context, or if there is a common subsequence between the two that covers more than 50% of the original context, we consider the matching successful, and map the answers to the new context paragraphs. The main causes of mismatches are a) Wikipedia pages that have been permanently removed (due to copyright issues, unable to meet notability standards, etc.); b) significantly edited to improve presentation (see <ref type="figure">Figure 7</ref>(a)); c) significantly edited because the world has changed (see <ref type="figure">Figure 7(b)</ref>). As a result, 20,182/2,146 SQuAD train/dev examples (that is, 17,802/2,380/2,146 train/dev/test examples after data resplit) and 15,806/1,416/1,427 HotpotQA train/dev/fullwiki test examples have been excluded from the unified benchmark. To understand the data quality after converting SQuAD Open and HotpotQA to the newer version of Wikipedia, we sampled 100 examples from the training split of each dataset. We find that 6% of SQuAD questions and 10% of HotpotQA questions are no longer answerable from their context paragraphs due to edits in Wikipedia or changes in the world, despite the presence of the answer span. We also find that 43% of HotpotQA examples contain more than the minimal set of necessary paragraphs <ref type="bibr">Madonna Louise Ciccone (born August 16, 1958)</ref> is an American singer, songwriter, actress, and businesswoman. She achieved popularity by pushing the boundaries of lyrical content in mainstream popular music and imagery in her music videos, which became a fixture on MTV. Madonna is known for reinventing both her music and image, and for maintaining her autonomy within the recording industry. Music critics have acclaimed her musical productions, which have generated some controversy. Referred to as the "Queen of Pop", Madonna is often cited as an influence by other artists.</p><p>Madonna Louise Ciccone <ref type="bibr">(born August 16, 1958)</ref> is an American singer-songwriter, author, actress and record executive. She has been referred to as the "Queen of Pop" since the 1980s. Madonna is noted for her continual reinvention and versatility in music production, songwriting, and visual presentation. She has pushed the boundaries of artistic expression in popular culture, while remaining completely in charge of every aspect of her career. Her works, which incorporate social, political, sexual, and religious themes, have made a cultural impact which has generated both critical acclaim and controversy. Madonna is often cited as an influence by other artists.</p><p>(a) The Wikipedia page about Madonna, on December 20, 2016 (on the left, which is in the version SQuAD Open used) versus July 31, 2020 (on the right, which is in the version B QA used).</p><p>Peter Langkjaer Madsen (born 12 January 1971) is a Danish aerospace engineering enthusiast, "art engineer", submarine builder, entrepreneur, co-founder of the non-profit organization Copenhagen Suborbitals, and founder and CEO of RML Spacelab ApS. He was arrested in August 2017 for involvement in the death of Swedish journalist Kim Wall; the investigation is ongoing.</p><p>Peter Langkjaer Madsen (]; born 12 January 1971) is a Danish convicted murderer. In April 2018 he was convicted of the 2017 murder of Swedish journalist Kim Wall on board his submarine, UC3 Nautilus, and sentenced to life imprisonment. He had previously been an engineer and entrepreneur.</p><p>(b) The Wikipedia page about Peter Madsen, on September 27, 2017 (on the left, which is in the version HotpotQA used) versus July 26, 2020 (on the right, which is in the version B QA used). <ref type="figure">Figure 7</ref>: Changes in Wikipedia that present challenges in matching them across years. We highlight portions of the text that have been deleted in red underlined text, that have been added in green boldface text, and that have been significantly paraphrased in orange italics, and leave near-verbatim text in the normal font and color.</p><p>to answer the question as a result of the mapping process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Elasticsearch Setup</head><p>We set up Elasticsearch in standard benchmark settings (SQuAD Open and HotpotQA fullwiki) following practices in previous work <ref type="bibr" target="#b2">(Chen et al., 2017;</ref><ref type="bibr">Qi et al., 2019)</ref>, with minor modifications to unify these approaches.</p><p>Specifically, to reduce the context size for the Transformer encoder in IRRR to avoid unnecessary computational cost, we primarily index the individual paragraphs in the English Wikipedia. To incorporate the broader context from the entire article, as was done by <ref type="bibr" target="#b2">Chen et al. (2017)</ref>, we also index the full text for each Wikipedia article to help with scoring candidate paragraphs. Each paragraph is associated with the full text of the Wikipedia article it originated from, and the search score is calculated as the summation of two parts: the similarity between query terms and the paragraph text, and the similarity between the query terms and the full text of the article.</p><p>For query-paragraph similarity, we use the standard BM25 similarity function <ref type="bibr" target="#b12">(Robertson et al., 1994)</ref> with default hyperparameters ( 1 = 1.2, = 0.75). For query-article similarity, we find BM25 to be less effective, since the length of these articles overwhelm the similarity score stemming from important rare query terms, which has also been reported in the information retrieval literature <ref type="bibr">(Lv and Zhai, 2011)</ref>. Instead of boosting the term frequenty score as considered by Lv and Zhai (2011), we extend BM25 by taking the square of the IDF term and setting the TF normalization term to zero ( = 0), which is similar to the TF-IDF implementation by <ref type="bibr" target="#b2">Chen et al. (2017)</ref> that is shown effective for SQuAD Open.</p><p>Specifically, given a document and query , the score is calculated as</p><formula xml:id="formula_6">score( , ) = ?? =1 IDF 2 + ( ) ? ( , ) ? (1 + 1 ) ( , ) + 1 ,<label>(1)</label></formula><p>where IDF + ( ) = max(0, log(( ? ( ) + 0.5)/( ( ) + 0.5)), with denoting the total numberr of documents and ( ) the document frequency of query term , and ( , ) is the term frequency of query term in document . We set  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Further Training and Prediction Details</head><p>We include the hyperparameters used to train the IRRR model in <ref type="table" target="#tab_16">Table 7</ref> for reproducibility.</p><p>For our experiments using SQuAD for training, we also follow the practice of <ref type="bibr" target="#b0">Asai et al. (2020)</ref> to include the data for SQuAD 2.0 <ref type="bibr">(Rajpurkar et al., 2018)</ref> as negative examples for the reader component. Hyperparameters like the prediction threshold of binary classifiers in the query generator are chosen on the development set to optimize end-to-end QA performance.</p><p>We also include how we use the reader model's prediction to stop the IRRR pipeline for completeness. Specifically, when the most likely answer is yes or no, the answerability of the reasoning path is the difference between the yes/no logit and the NOANSWER logit. For reasoning paths that are not answerable, we further train the span classifiers to predict the [CLS] token as the "output span", and thus we also include the likelihood ratio between the best span and the [CLS] span if the positive answer is a span. Therefore, when the best predicted answer is a span, its answerability score is computed by including the score of the "[CLS] span" as well, i.e., Answerability span ( ) = logit span ? logit NOANSWER + logit start ? logit start</p><formula xml:id="formula_7">[CLS] 2 + logit end ? logit end [CLS] 2 ,<label>(2)</label></formula><p>where logit span is the logit of predicting span answers from the 4-way classifier, while logit start and logit end are logits from the span classifiers for selecting the predicted span from the reasoning path.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Further Analyses of Model Behavior</head><p>In this section, we perform further analyses and introduce further case studies to demonstrate the behavior of the IRRR system. We start by analyzing the effect of the dynamic stopping criterion for reasoning path retrieval, then move on to the endto-end performance and leakages in the pipeline, and end with a few examples to demonstrate typical failure modes we have identified that might point to limitations with the data.</p><p>Effect of Dynamic Stopping. We begin by studying the effect of using the answerability score as a criterion to stop the iterative retrieval, reading, and reranking process within IRRR. We compare the performance of a model with dynamic stopping to one that is forced to stop at exactly steps of reasoning, neither more nor fewer, where = 1, 2, . . . , 5. As can be seen in <ref type="table" target="#tab_19">Table 8</ref>, IRRR's dynamic stopping criterion based on the answerability score is very effective in achieving good end-to-end question answering performance for questions of arbitrary complexity without having to specify the complexity of questions ahead of time. On both SQuAD Open and HotpotQA, it achieves competitive, if not superior question answering performance, even without knowing the true number of gold paragraphs necessary to answer each question.</p><p>Aside from this, we note four interesting findings: (1) the performance of HotpotQA does not peak at two steps of reasoning, but instead is helped by performing a third step of retrieval for the average question; (2) for both datasets, forcing the model to retrieve more paragraphs after a point consistently hurt QA performance; (3) dynamic stopping slightly hurts QA performance on SQuAD Open compared to a fixed number of reasoning steps ( = 1);  (4) when IRRR is allowed to select a dynamic stopping criterion for each example independently, the resulting question answering performance is better than a one-size-fits-all solution of applying the same number of reasoning steps to all examples. While the last confirms the effectiveness of our answerability-based stopping criterion, the cause behind the first three warrants further investigation. We will present further analyses to shed light on potential causes of these in the remainder of this section.</p><p>Case Study for Failure Cases. Besides model inaccuracy, one common reason for IRRR to fail at finding the correct answer provided with the datasets is the existence of false negatives (see <ref type="figure" target="#fig_4">Figure 8</ref> for an example from SQuAD Open). We estimate that there are about 9% such cases in the HotpotQA part of the training set, and 26% in the SQuAD part of the training set. These false negatives hurt the quality of data generation as well, especially when generating the SQuAD part of the training set. We investigate randomly selected question-context pairs in the training set and find 24% of our SQuAD training set and 13% of GRR's SQuAD training set are false negatives. This means our methods find better candidate documents but true answers in those documents become false positives. That results in worse performance for our model when it is trained with only the SQuAD part of training set as shown in <ref type="table" target="#tab_6">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Three+ Hop Challenge Set Analysis</head><p>Although SQuAD Open and HotpotQA probe our model's ability on single and two-hop questions, we lacked insight into the ability of our model to   generalize to questions that require three or more reasoning steps/hops, which is more than what our model is trained on. Therefore, we built a challenge set comprised of questions that require at least three hops of reasoning to answer (see <ref type="table" target="#tab_20">Table 9</ref> for a breakdown of the number of documents required to answer each question in the challenge set). While the vast majority of challenge set questions require three documents, questions that require four or more documents are also present, hence the "Three+ Hop Challenge Set" name. Although we intend to use the challenge set for testing only, we will share a few key insights into the question sourcing process, the reasoning types required, and the answer types present.</p><p>Question Sourcing Process. We annotated 530 examples that require three or more paragraphs to be answered on the 2020 Wikipedia dump. We developed roughly 50-100 question templates that cover a diverse set of topics, including science, literature, film, music, history, sports, technology, politics, and geography. We then annotated approximately ten to twenty examples from each of these question templates to ensure that the resulting challenge set contained a diverse set of topics and questions.</p><p>Reasoning Types. During the annotation process for the challenge set, we recorded the types of reasoning required to answer each question (Table 10). Roughly half of the questions require chain reasoning (Bridge), where the reader must identify bridge entities that link the question to the first context paragraph, the first context paragraph to the second, and finally the second to the third where the answer can be found. In the case  that four or more hops of reasoning are required, this chain of reasoning will extend past the third paragraph to the -th paragraph where the answer can be found. Additionally, approximately 25% of the questions require the comparison of three or more entities (Comparison). For these questions, the reader needs to retrieve three or more context paragraphs identified in the question that are not directly connected to each other and then compare them on certain aspects specified in the question, similar to the comparison questions in HotpotQA. The remaining 25% of the questions require both chain reasoning and the comparison of two or more entities (Bridge-Comparison). For these questions, the reader must first identify a bridge entity that links the question to the first context paragraph. They then must identify two or more entities to compare within the first context paragraph. Afterwards, they retrieve context paragraphs for each of the aforementioned entities and compare them on certain aspects specified in the question.</p><p>Answer Types. We also analyze the types of answers present in the challenge set. As shown in <ref type="table" target="#tab_23">Table 11</ref>, the challenge set features a diverse set of answers. We find that roughly half of the questions ask about people (29%) and numeric quantities (20%). Additionally, we find a considerable number of questions that require a yes or no answer (15%), ask about groups or organizations (11%), dates (8%), and other proper nouns (7%). The challenge set also contains a non-negligible amount of questions that ask about creative works (5%), locations (4%), and common nouns (1%).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Recall of the two gold supporting documents by the oracle queries of G E Retriever and IRRR on the HotpotQA dataset, where each question corresponds to two supporting documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>An example of the newly collected challenge questions. This particular question requires three pieces of evidence to answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The retrieval behavior of IRRR and its relation to the performance of end-to-end question answering. Top: The distribution of reasoning path lengths as determined by IRRR. Bottom: Total number of paragraphs retrieved by IRRR vs. the end-to-end question answering performance as measured by answer F 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to build QA systems over Wikipedia articles. Similarly, TriviaQA (Joshi et al., 2017) and Natural Questions (Kwiatkowski et al., 2019) feature Wikipedia-based questions that are written by trivia enthusiasts and extracted from Google search queries, respectively. More recently, Petroni et al. (2021) presented, KILT, a new benchmark based on Wikipedia where many knowledge-based tasks are evaluated in a unified version of Wikipedia, including open-domain question answering, entity linking, dialogue, etc. Unlike B QA, however, single-hop and multi-hop QA are held completely separate during evaluation in KILT, which makes the evaluation of open-domain QA less realistic.Aside from Wikipedia, researchers have also used news articles<ref type="bibr" target="#b15">(Trischler et al., 2016)</ref> and search results from the web<ref type="bibr" target="#b9">(Dunn et al., 2017;</ref><ref type="bibr" target="#b14">Talmor and Berant, 2018)</ref> as the corpus for open-domain QA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>An example where there are false negative answers in Wikipedia for the question from SQuAD Open.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2010.12527v4 [cs.CL] 29 Oct 2021 ? WIKIPEDIA search ? Q ? "Ingerophrynus Gollum" ? Q + Ingerophrynus Gollum ? "Lord of the Rings" ? Q + retrieved paras ? NOANSWER ? Q + Ingerophrynus Gollum + The Lord of the Rings ? "150 million copies"</figDesc><table><row><cell>W</cell><cell></cell><cell>W</cell></row><row><cell></cell><cell></cell><cell>W</cell></row><row><cell>? Q + retrieved paras ?</cell><cell>W</cell><cell>Ingerophrynus Gollum</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>End-to-end question answering performance on SQuAD Open, evaluated on the same set of documents as<ref type="bibr" target="#b2">Chen et al. (2017)</ref>.</figDesc><table><row><cell>System</cell><cell>HotpotQA EM F 1</cell><cell cols="2">3+ hop EM F 1</cell></row><row><cell>GRR (Asai et al., 2020)</cell><cell>60.0 73.0</cell><cell cols="2">27.2  ? 31.9  ?</cell></row><row><cell>Step-by-step ?</cell><cell>63.0 75.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DDRQA (Zhang et al., 2021) 62.3 75.3</cell><cell>-</cell><cell>-</cell></row><row><cell>MDR (Xiong et al., 2021)</cell><cell>62.3 75.3</cell><cell>-</cell><cell>-</cell></row><row><cell>EBS-SH ?</cell><cell>65.5 78.6</cell><cell>-</cell><cell>-</cell></row><row><cell>TPRR ?</cell><cell>67.0 79.5</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HopRetriever (Li et al., 2020) 67.1 79.9</cell><cell>-</cell><cell>-</cell></row><row><cell>IRRR (HotpotQA)</cell><cell>65.2 78.0</cell><cell>29.2</cell><cell>34.2</cell></row><row><cell cols="2">IRRR (SQuAD + HotpotQA) 65.7 78.2</cell><cell>32.5</cell><cell>36.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>End-to-end question answering performance on HotpotQA and the new 3+ hop challenge questions, evaluated on the official HotpotQA Wikipedia paragraphs. ? denotes anonymous/preprint unavailable at the time of writing of this paper. ? indicates results we obtained using the publicly available code and pretrained models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>End-to-end question answering performance of IRRR on the unified benchmark, evaluated on the 2020 copy of Wikipedia. These results are not directly comparable with those in Tables 2 and 3 because the set of questions and Wikipedia documents differ.</figDesc><table><row><cell>System</cell><cell cols="2">SQuAD HotpotQA</cell></row><row><cell>Ours (joint dataset)</cell><cell>58.69</cell><cell>68.74</cell></row><row><cell>vs. fixed retrieval steps ( = 3)</cell><cell>31.70</cell><cell>66.60</cell></row><row><cell>vs. remove HotpotQA / SQuAD data</cell><cell>54.35</cell><cell>66.91</cell></row><row><cell>replace ELECTRA w/ BERT LARGE-WWM</cell><cell>57.19</cell><cell>63.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Ablation study of different design choices in IRRR, as evaluated by Answer F 1 on the dev set of the unified benchmark. Results differ from those in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps. IRRR achieves competitive results on standard opendomain QA benchmarks, and establishes a strong baseline on B QA, the new unified benchmark we present, which features questions with mixed levels of complexity. Clinton Gormley and Zachary Tong. 2015. Elasticsearch: The definitive guide: A distributed real-time search and analytics engine. O'Reilly Media, Inc. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909. Yuanhua Lv and ChengXiang Zhai. 2011. When documents are very long, BM25 fails! In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 1103-1104.</figDesc><table><row><cell></cell><cell>Christopher D. Manning, Mihai Surdeanu, John Bauer,</cell></row><row><cell></cell><cell>Jenny Finkel, Steven J. Bethard, and David Mc-</cell></row><row><cell>Minghao Hu, Yuxing Peng, Zhen Huang, and Dong-sheng Li. 2019. Retrieve, read, rerank: Towards end-to-end multi-document reading comprehension. In Proceedings of the 57th Annual Meeting of the</cell><cell>Closky. 2014. The Stanford CoreNLP natural lan-guage processing toolkit. In Association for Compu-tational Linguistics (ACL) System Demonstrations, pages 55-60.</cell></row><row><cell>Association for Computational Linguistics.</cell><cell>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Han-</cell></row><row><cell>Gautier Izacard and Edouard Grave. 2020. Leverag-ing passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.</cell><cell>naneh Hajishirzi. 2019. Multi-hop reading compre-hension through question decomposition and rescor-ing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097-6109, Florence, Italy. Association for Compu-</cell></row><row><cell>S?bastien Jean, Kyunghyun Cho, Roland Memisevic,</cell><cell>tational Linguistics.</cell></row><row><cell>and Yoshua Bengio. 2015. On using very large tar-get vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7th International Joint Conference on Natural Language</cell><cell>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in neural information pro-cessing systems, pages 2265-2273.</cell></row><row><cell>Processing (Volume 1: Long Papers).</cell><cell>Yixin Nie, Songhe Wang, and Mohit Bansal. 2019.</cell></row><row><cell>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehen-sion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers).</cell><cell>Revealing the importance of semantic retrieval for machine reading at scale. In Proceedings of the 2019 Conference on Empirical Methods in Natu-ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-?CNLP).</cell></row><row><cell>V. Karpukhin, Barlas Ouguz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv, abs/2004.04906.</cell><cell>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt?schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge</cell></row><row><cell>Omar Khattab, Christopher Potts, and Matei Zaharia. 2020. Relevance-guided supervision for OpenQA with ColBERT. arXiv preprint arXiv:2007.00814.</cell><cell>intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544.</cell></row><row><cell>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-</cell><cell></cell></row><row><cell>field, Michael Collins, Ankur Parikh, Chris Al-</cell><cell>Peng Qi, Xiaowen Lin, Leo Mehr, Z?ian Wang, and</cell></row><row><cell>berti, Danielle Epstein, Illia Polosukhin, Jacob De-</cell><cell>Christopher D. Manning. 2019. Answering complex</cell></row><row><cell>vlin, Kenton Lee, Kristina Toutanova, Llion Jones,</cell><cell>open-domain questions through iterative query gen-</cell></row><row><cell>Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,</cell><cell>eration. In Proceedings of the 2019 Conference on</cell></row><row><cell>Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.</cell><cell>Empirical Methods in Natural Language Processing</cell></row><row><cell>Natural questions: A benchmark for question an-</cell><cell>and the 9th International Joint Conference on Natu-</cell></row><row><cell>swering research. Transactions of the Association</cell><cell>ral Language Processing (EMNLP-?CNLP).</cell></row><row><cell>for Computational Linguistics, 7.</cell><cell>Colin Raffel, Noam Shazeer, Adam Roberts, Kather-</cell></row><row><cell>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.</cell><cell>ine Lee, Sharan Narang, Michael Matena, Yanqi</cell></row><row><cell>2019. Latent retrieval for weakly supervised open</cell><cell>Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the</cell></row><row><cell>domain question answering. In Proceedings of the</cell><cell>limits of transfer learning with a unified text-to-text</cell></row><row><cell>57th Annual Meeting of the Association for Compu-</cell><cell>transformer. Journal of Machine Learning Research,</cell></row><row><cell>tational Linguistics.</cell><cell>21(140):1-67.</cell></row><row><cell>Shaobo Li, Xiaoguang Li, Lifeng Shang, Xin Jiang,</cell><cell>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.</cell></row><row><cell>Qun Liu, Chengjie Sun, Zhenzhou Ji, and Bingquan</cell><cell>Know what you don't know: Unanswerable ques-</cell></row><row><cell>Liu. 2020. HopRetriever: Retrieve hops over</cell><cell>tions for SQuAD. In Proceedings of the 56th Annual</cell></row><row><cell>wikipedia to answer complex questions. arXiv</cell><cell>Meeting of the Association for Computational Lin-</cell></row><row><cell>preprint arXiv:2012.15534.</cell><cell>guistics (Volume 2: Short Papers).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Statistics of the resplit SQuAD dataset for proper training and evaluation on the SQuAD Open setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameter setting for IRRR training.</figDesc><table><row><cell>laps while it is less dampened by document length,</cell></row><row><cell>making it ideal for an initial sift to find relevant</cell></row><row><cell>documents for open-domain question answering.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>However, the eventual-AFC Champion Cincinnati Bengals, playing in their first AFC Championship Game, defeated the Chargers 27-7 in what became known as the Freezer Bowl. ... Super Bowl XXVII was an American football game between the American Football Conference (AFC) champion Buffalo Bills and the National Football Conference (NFC) champion Dallas Cowboys to decide the National Football League (NFL) champion for the 1992 season. ...</figDesc><table><row><cell>Question</cell><cell>What team was the AFC champion?</cell></row><row><cell>Step1</cell><cell></cell></row><row><cell>(Non-Gold)</cell><cell></cell></row><row><cell>Step2</cell><cell></cell></row><row><cell>(Non-Gold)</cell><cell></cell></row><row><cell>Gold</cell><cell>Super Bowl 50 was an American football game to determine</cell></row><row><cell></cell><cell>the champion of the National Football League (NFL) for</cell></row><row><cell></cell><cell>the 2015 season. The American Football Conference (AFC)</cell></row><row><cell></cell><cell>champion Denver Broncos defeated the National Football</cell></row><row><cell></cell><cell>Conference (NFC) champion Carolina Panthers 24-10 to earn</cell></row><row><cell></cell><cell>their third Super Bowl title. ...</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: SQuAD and HotpotQA performance using</cell></row><row><cell>adaptive vs. fixed-length reasoning paths, as measured</cell></row><row><cell>by answer exact match (EM) and F 1 . The dynamic</cell></row><row><cell>stopping criterion employed by IRRR achieves compa-</cell></row><row><cell>rable performance to its fixed-step counterparts, without</cell></row><row><cell>knowledge of the true number of gold paragraphs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 :</head><label>9</label><figDesc>Distribution of reasoning steps for questions in Three+ Hop Challenge Set.</figDesc><table><row><cell>Reasoning Type</cell><cell>%</cell></row><row><cell>Comparison</cell><cell>25.6</cell></row><row><cell cols="2">Bridge-Comparison 25.3</cell></row><row><cell>Bridge</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 10 :</head><label>10</label><figDesc>Reasoning types required for Three+ Hop Challenge Set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 11 :</head><label>11</label><figDesc>Types of answers in Three+ Hop Challenge Set. These statistics are based on 100 randomly sampled examples.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://beerqa.github.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code for the model can be found at: https:// github.com/beerqa/IRRR.3 Some recent work breaks away from this mold, and use large pretrained language models (e.g.,T5; Raffel et al., 2020)   to directly generate answers from knowledge stored in model parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We employ Elasticsearch(Gormley and Tong, 2015)  as our text-based search engine, and follow previous work to process Wikipedia and search results, which we detail in Appendix B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://trec.nist.gov/data/qamain.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://github.com/facebookresearch/DrQA 10 https://hotpotqa.github.io/wiki-readme. html 11 Thus, if any hyperparameter tuning has been performed, it is usually done to directly maximize the performance on this held-out test set, inflating the performance on this set as a result.12 Accessed on August 1st, 2020, which contains 6,133,150 articles in total.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">= 1.2 in all of our experiments. Intuitively, compared to the standard BM25, this scoring function puts more emphasis on important, rare term over-</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for discussions and comments on earlier versions of this paper. This research is funded in part by Samsung Electronics Co., Ltd. and in part by the SAIL-JD Research Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over Wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HybridQA: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-step retrieverreader interaction for scalable open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Differentiable reasoning over a virtual knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidhisha</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">SearchQA: A new Q&amp;A dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-hop paragraph retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>NIST Special Publication</publisher>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">NewsQA: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">R 3 : Reinforced reader-ranker for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evidence aggregation for answer re-ranking in opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-passage BERT: A globally normalized BERT model for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-?CNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00021</idno>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end open-domain question answering with BERTserini</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aileen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Minneapolis</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), Minneapolis<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">IDRQA: Iterative document reranking for open-domain multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<editor>SI-GIR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Complex factoid question answering with a free-text knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1205" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformer-XH: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sparta: Efficient open-domain question answering via sparse transformer matching retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyusong</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13013</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Knowledge-aided opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouxing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05244</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
