<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Retrieval Augmented Classification for Long-Tail Visual Recognition *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of New</orgName>
								<address>
									<country>South Wales</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulak</forename><surname>Purkait</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Blair</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of New</orgName>
								<address>
									<country>South Wales</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Retrieval Augmented Classification for Long-Tail Visual Recognition *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large Transformer <ref type="bibr" target="#b49">[50]</ref> models have arrived in Computer Vision, with parameter counts and pretraining dataset size increasing rapidly <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b53">54]</ref>. The distributed representations learned by such models result in significant performance gains on a range of tasks, however come with the drawback of storing world knowledge implicitly within their parameters, making post-hoc modification <ref type="bibr" target="#b8">[9]</ref> and interpretability <ref type="bibr" target="#b3">[4]</ref> challenging. In addition, real-world data is long-tailed by nature, and implicitly storing every visual cue present in the world appears futile with current hardware constraints. As an alternative to this fully parametric approach, we propose augmenting standard classification pipelines with an explicit external memory, thus separating model performance from parameter count, and facilitating * Part of this work was done when WY was with Amazon and CS was with The University of Adelaide. the dynamic addition and removal of information explicitly with no changes to model weights.</p><p>To evaluate our approach, we focus on the problem of Long-Tail visual recognition, as it shares many of the properties likely to be encountered by a general agent. Specifically, the data distributions are highly skewed on a per-class basis, with a majority of classes containing a small number of samples. The number of samples in these small classes, commonly referred to as the "tail", can far outweigh those in the relative minority of high sample classes (referred to as the "head"). In this situation, learning is challenging due to both the lack of information provided for tail classes, and the tendency for head classes to dominate the learning process. Long-tail learning is a well-studied <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> instance of the more general label shift problem <ref type="bibr" target="#b42">[43]</ref>, where the shift is static and known during both training and testing. Despite being well-studied, commonly occurring, and of great practical importance, classification performance on long-tail distributions lags significantly behind the state-ofthe-art for better balanced classes <ref type="bibr" target="#b24">[25]</ref>.</p><p>Base approaches are largely variants of the same core idea-that of "adjustment", where the learner is encouraged to focus on the tail of the distribution. This can be achieved implicitly, via over/under-weighting samples during training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> or cluster-based sampling <ref type="bibr" target="#b6">[7]</ref>, or explicitly via logit <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58]</ref> or loss <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref> modification. Such approaches largely focus on consistency, ensuring minimizing the training loss corresponds to a minimal error on the known, balanced, test distribution.</p><p>An alternative approach focuses on ensembling models. Instead of disregarding knowledge of the test distribution, recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60]</ref> use ensembling models to induce invariance to the test distribution. This is typically done by training separate models under different losses or resampling techniques, and combining them at test time.</p><p>We introduce a third approach, Retrieval Augmented Classification (RAC), motivated by the desire to explicitly store tail knowledge, as a retrieval-based augmentation to standard classification pipelines.</p><p>RAC's retrieval module is multi-modal, making use of image representations as retrieval keys, and returning en-  coded textual information associated with each image. We place no limitation on the nature of this text; it may be the labels from a supervised training set, descriptions, captions etc. In the simplest case, the images in the index, and associated text, can be the images and labels from the dataset of interest alone. RAC jointly trains a standard base encoder, and a separate retrieval branch. We demonstrate empirically that the retrieval branch learns, without explicit prompting, to focus on tail classes. This frees the base encoder from modelling these sparse classes, as they are already effectively represented by the non-parametric memory of the retrieval module. This in turn allows the base encoder to achieve a higher level of performance on the head classes.</p><p>RAC achieves state-of-the-art performance on common Long-Tail classification benchmarks, even out-performing approaches such as LACE <ref type="bibr" target="#b37">[38]</ref> that are provably consistent with regard to the class-balanced error, and Bayes-optimal under Gaussian class priors. A major benefit of RAC is its ability to use large, pretrained models for inference (for index and retrieval encoding), leveraging their rich representations to improve the classification performance of a base learner. This broadens the applicability of such models due to the large cost of fine-tuning.</p><p>Our contributions are summarised as follows:</p><p>1. The first demonstration of effective external memory within long-tail visual recognition setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A novel method for Long-Tail Classification that sig-</head><p>nificantly improves on the current state-of-the-art. 3. Insight into the proposed method, with the reimplementation of strong baselines that also exceed current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Resampling and Logit Adjustment Over-sampling sparse classes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref> is one of the oldest approaches to addressing distribution bias, but one that is still in common use. Under-sampling common classes <ref type="bibr" target="#b12">[13]</ref>, applying additional data-augmentation to sparse classes in pixel, or feature space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, or sampling uniformly from pre-computed clusters <ref type="bibr" target="#b22">[23]</ref>, have also been suggested. Hong et al. <ref type="bibr" target="#b21">[22]</ref> propose a distribution aware weight regularizer that is applied more heavily to head classes than tail classes, in a similar vein to weight normalization. However, empirically, the resulting model (LADE) only produces marginal gains over straight-forward balanced softmax. Recent work <ref type="bibr" target="#b37">[38]</ref> has unified many empirically successfully approaches under a Fisher-consistent scorer for the balanced error, and additionally shown that weight normalization fails when used with the ADAM optimizer. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> adopt a two stage approach, and propose a class-specific learnable (from the samples) reweighting (via a single layer NN) of the frozen pretrained logits based on a generalized formulation of the class-balanced softmax. They show that transforming the classification head, as opposed to re-training it, performs better. PaCo <ref type="bibr" target="#b6">[7]</ref>, the current state-of-the-art for longtail classification, combines learnable logit adjustment with contrastive learning <ref type="bibr" target="#b39">[40]</ref>. Despite their simplicity, adjusted logit methods (LACE, LDAM, LADE) remain strong solutions to the long tail problem, typically achieving within 1-2% top-1 accuracy of state-of-the-art ensemble approaches (see <ref type="table">Table 1</ref>, <ref type="table">Table 2</ref>).  <ref type="bibr" target="#b51">[52]</ref> in contrast combine multiple independently trained classification heads that are pushed to be decorrelated in their predictions via a (class balanced) KL loss, with a small routing network that improves computational efficiency during inference. External Memory One of the first models to successfully combine deep networks with external memory was the Neural Turing Machine <ref type="bibr" target="#b15">[16]</ref>. The purpose of that model was symbolic manipulation, however, which renders its architecture quite different to that of RAC. Gong et al. <ref type="bibr" target="#b14">[15]</ref> proposed a similar retrieval-module architecture for anomaly detection, but without RAC's corresponding base module. Recently, in the NLP domain, several works have proposed the augmentation of large language models with a non-parametric memory to allow explicit access to external data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31]</ref>. While such approaches make use of differential retrievers, which introduces the problem of lookup/representation drift, they are still closely related to RAC. k-NN Language Models (LMs) <ref type="bibr" target="#b27">[28]</ref> are most similar to our work, which directly interpolate a retrieval distribution with the next token distribution produced by a base LM, resulting in reduced combined model perplexity. Latent retrieval has been applied to textual open-domain QA <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. The central difference is such approaches return information that is most similar to the retrieval key, whereas RAC returns information (text) attached to retrieved samples. An approach similar to that of RAC has been applied to knowledge-intensive QA <ref type="bibr" target="#b50">[51]</ref>, where a 'fact memory' consisting of triples from a symbolic Knowledge Base (KB) is directly encoded and queried using the final representation of a language model as keys. In computer vision, non-parametric retrieval has been used to assist in addressing the fine-grained retrieval problem, such as in enforcing instance-level retrieval loss in <ref type="bibr" target="#b47">[48]</ref>. The Openworld Long-tail model proposed in <ref type="bibr" target="#b33">[34]</ref> also makes use of a retrieval module, but primarily as a mechanism to distinguish between seen, and unseen samples in the 'open world' setting, not to boost performance on seen classes as we do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>In long-tailed visual recognition, the model has access to a set of N training samples S = {(x n , y n )} N n=1 , where x n ? X ? R D and labels Y = {1, 2, .., L}. Training class frequencies are defined as N y = (xn,yn)?S 1 yn=y and the test-class distribution is assumed to be sampled from a uniform distribution over Y 1 , but is not explicitly provided <ref type="bibr" target="#b0">1</ref> While this is true for Places365-LT, iNaturalist2018 has a fixed number of test samples for each class (N test</p><formula xml:id="formula_0">i = 3, ?i ? Y)</formula><p>during training. The goal is thus to minimize the balanced error, of a scorer f : X ? R L , defined as;</p><formula xml:id="formula_1">BE(x, f (?)) = y?Y P x|y y / ? arg max y ?Y f y (x)<label>(1)</label></formula><p>where f y (x) is the logit produced for true label y for sample x. Traditionally this is done by minimizing a proxy loss, the Balanced Softmax Cross Entropy (BalCE):</p><formula xml:id="formula_2">BalCE (x, y, f y (?)) = ? 1 N y log e fy(x) y ?Y e f y (x) . (2)</formula><p>This is a form of re-weighting, where the contribution of each label's individual loss is scaled by an approximation of P(y), which for BalCE , is the inverse class frequency. BalCE remains a strong baseline in this domain (see Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LACE Loss</head><p>An alternative to re-weighting is to adjust the logits themselves, however the two can be done in conjunction, resulting in the general form of the re-weighted (via ? y ) and adjusted (via ?y) softmax cross entropy loss;</p><formula xml:id="formula_3">(x, y, f y (?)) = ?? y log e fy(x)+? ??y y ?Y e f y (x)+? ??y<label>(3)</label></formula><p>where ? is a constant temperature scaling parameter. Several recent works focusing on long-tail learning exploit special cases of this loss. If ? y = 1 and, Logit Adjusted Cross-Entropy (LACE) <ref type="bibr" target="#b37">[38]</ref>, can be recovered with ? y = log (N y /N ) and LDAM <ref type="bibr" target="#b0">[1]</ref> with ? y = 1 /Ny and</p><formula xml:id="formula_4">?y = N ?1/4 y if y = y, 0 otherwise.<label>(4)</label></formula><p>Both are Fisher-consistent with respect to the balanced loss. The optimal ? can be found with a holdout set, or set to 1 if the logits are calibrated <ref type="bibr" target="#b16">[17]</ref>. In our experiments, this calibration is achieved through label smoothing <ref type="bibr" target="#b38">[39]</ref>, which has been shown to implicitly calibrate neural networks <ref type="bibr" target="#b46">[47]</ref>. This property is important in the design of RAC as we use LACE as the base loss and do not apply manual temperature adjustment, setting ? = 1, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Retrieval Augmented Classification</head><p>The overall idea of RAC is very simple-Split the scorer into two branches (see <ref type="figure" target="#fig_1">Fig. 1</ref>), where one branch (retrieval) exhibits implicit invariance to class frequency. The two branches are trained under a common LACE loss, with their individual logits combined with a norm, addition and rescale operation to ensure that one does not override the other during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The base branch encoder B(?) can be any choice of a standard backbone network. In our experiments, we primarily use the ViT-B-16 variant of Visual Image Transformer <ref type="bibr" target="#b11">[12]</ref>, transforming the final token embedding via a standard linear layer. The retrieval module (see Sec. 3.4) takes a raw image and performs a latent-space lookup on an index of precomputed embeddings, returning the text attached to the top k most similar images to the image currently being considered, x q . This text is then fed through a text encoder and transformed by another linear layer into logits f ret (x q ).</p><p>We make use of the pretrained BERT-like text encoder (63M parameters, 12-layer 512-wide model with 8 attention heads) from CLIP <ref type="bibr" target="#b43">[44]</ref>, which we choose due to the broad (400M) range of images, alt-text pairs used during pretraining, and the compatibility with other language models due to the preservation of masked self attention in the architecture. In our experiments, the choice of text encoder is not critical as the textual information being retrieved (labels) is not highly complex, and off-the-shelf word embeddings, and even random encodings still perform reasonably well (see <ref type="figure" target="#fig_6">Fig. 4</ref>). This choice does increase training time due to the larger parameter count (see <ref type="table" target="#tab_5">Table 5</ref>), but allows RAC to scale to more complex retrieved text.</p><p>To combine base and retrieval branches, we normalize each branch's outputs to the unit norm and add them together. To ensure training dynamics are not altered (via lower logit magnitudes) in comparison to the baselines we rescale the combined logits by a constant factor (dependent on L due to final layer Xavier initialization <ref type="bibr" target="#b13">[14]</ref> also being dependent on L).</p><formula xml:id="formula_5">f (x) = L 2 f ret (x) ||f ret (x)|| 2 + f base (x) ||f base (x)|| 2 ,<label>(5)</label></formula><p>where f base (x) represents the logits produced by the image encoder backbone, and f ret (x) is the output of the retrieval module. This straightforward setup has the benefit of being able to treat the branch outputs as individual logits, increasing the interpretability of RAC, and allowing us to precisely evaluate the per-class accuracy of each branch (see <ref type="figure" target="#fig_3">Fig. 2</ref>). While there are many ways to combine the branches such as confidence or distance based weightings, attention mechanisms etc., we found this approach sufficient, with the weighting of each branch done implicitly by the learned sharpness of the logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Retrieval Module</head><p>The retrieval module consists of a frozen pretrained image encoder E(?), a pre-existing set of external images I = {i j } J j=1 , with associated text T = {t j } J j=1 , which may be labels, descriptions, captions etc. Unless otherwise specified E(?) is a ViT-B-16, pretrained on ImageNet following <ref type="bibr" target="#b44">[45]</ref>. Prior to training RAC, the retrieval module is  initialized by producing image keys Z = {z j } such that z j = E(i j ) ?j, and storing the resultant representations in a fast approximate k-NN index.</p><p>During training, we produce features z q = E(x q ) for each image x q in the training batch. The k-NN is queried for each z q and returns a list of indices of the k closest keys in Z, where cosine similarity is the distance metric. The text element in T is recovered for every such index, generating k text elements for each query. These text elements are then encoded by a text encoder T(?) which produces the retrieval branch's (fixed length) logits, f ret (x q ).</p><p>Text strings are truncated after 76 tokens, and the resultant batches are zero-padded. This approach allows for a single text-encoder call per batch, as opposed to k which would be required if each text snippet was encoded separately, and would result in a significant slowdown. The use of a large-scale transformer ensures that RAC can scale to longer text snippets if the external information is expanded to contain additional sources beyond simply labels.</p><p>A key feature of the retrieval module is its ability to include otherwise unconnected data-sources simply via their labels. In this way, we can dynamically add or remove datasets from I, and if new examples are similar (from the point of view of the encoder), they can directly impact clas- sification accuracy, providing an alternative to fine-tuning in order to incorporate new information.</p><p>For fast querying of the index, we make use of the FAISS implementation <ref type="bibr" target="#b23">[24]</ref> of the Hierarchical Navigable Small World (HNSW) approximate k-NN lookup <ref type="bibr" target="#b36">[37]</ref>. We construct the index with default settings aside from the hyperparameter M = 32, which sets the number of bidirectional links per node and increases the complexity of the index, but allows for higher recall. During training, we drop the first result, as when training data is included in the index, the first result is often the original image, which causes the text encoder to place undue weight on the first retrieved label when creating predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We establish RAC's high level of performance on common benchmark datasets iNaturalist2018 <ref type="table">(Table 1)</ref> and Places365-LT <ref type="table">(Table 2</ref>) 2 with no additional external information aside from the datasets used to pretrain the individual encoders. Note that these tables report results from the literature which were obtained under varying architectures and training schemes. We ablate the benefit of RAC's improved training pipeline in <ref type="table" target="#tab_2">Table 3</ref> where we reimplement class-balanced softmax Cross Entropy (BalCE) and LACE <ref type="bibr" target="#b37">[38]</ref> as baselines. We consider 'Base' trained under the LACE loss <ref type="bibr" target="#b37">[38]</ref> as our primary baseline, due to LACE's strong theoretical grounding, provable consistency, and high level of previously reported empirical performance. We report overall accuracy as well as per-class accuracy bucketed into the few (&lt; 20), medium (? 100) and many (&gt; 100) shot categories. The full per-class distribution curve is also shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. We then focus specifically on the design choices of the retrieval module and how the choice of data for the index affects RAC in Sec. 4.7.</p><p>In all experiments, unless otherwise indicated, E is a ViT-B-16 encoder, with the weights from <ref type="bibr" target="#b11">[12]</ref>. The weights are obtained from pretraining on ImageNet21k (IM21k), a larger (11M samples) variant of the original 1.2M images ImageNet <ref type="bibr" target="#b9">[10]</ref> dataset, with more granular classes. We make use of IM21k to expand the index in some experiments, and use the variant introduced in [29].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Places365-LT</head><p>Places365-LT is a synthetic long-tail variant of Places-2 <ref type="bibr" target="#b60">[61]</ref> introduced in <ref type="bibr" target="#b33">[34]</ref>. It consists of 365 high-level scene classes such as 'airport', 'basement', etc. across 62.5K samples at 256 ? 256 resolution. The minimum number of samples per class is 5, with a training set that, while balanced, is not perfectly uniform. The dataset contains a significant amount of label noise, which makes it appealing, as logit adjustment methods typically assume fully separable classes in their theoretical motivation.</p><p>We observe that with no explicit prompting, the retrieval network learns to highly skew its accuracy towards the fewshot classes <ref type="figure" target="#fig_1">(Fig. 1)</ref>, confirming our hypothesis that it will be beneficial in this case. Note that there is no explicit signal pushing the retrieval network to learn infrequent classes over common ones, or for the supervised network to prefer common classes, as both are trained under the common LACE loss. Interestingly, RAC's learned strategy is similar to the hard-coded ensembling utilized in TADE <ref type="bibr" target="#b56">[57]</ref>, which is the previous state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">iNaturalist-2018</head><p>iNaturalist-2018 (iNat) <ref type="bibr" target="#b48">[49]</ref> consists of 437K images and 6 levels of label granularity (kingdom, genus etc.). Following other work, we consider only the most granular labels (species), which constitutes 8142 unique classes with a naturally occurring class imbalance. In many cases the labels are very fine-grained, making it a challenging dataset even without the long-tailed property. The test set is perfectly balanced, with 3 samples per class.</p><p>In addition to the 224 ? 224 resolution commonly studied, we report the results with 384 ? 384 images, which was used in GRAFIT <ref type="bibr" target="#b47">[48]</ref> and is currently state-of-the-art for this task. We found the use of 16 ? 16 patch size to be of major importance on iNat, boosting retrieval accuracy by 21.6% (see <ref type="table" target="#tab_4">Table 4</ref>), likely due to the fine-grained nature of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation</head><p>We baseline RAC's performance against class-balanced softmax cross entropy (BalCE) and with the retrieval branch removed (Base only variant) under a common training setup in <ref type="table" target="#tab_2">Table 3</ref>. We also include final accuracies for ResNet models for comparison. RAC consistently increases allclass top-1 accuracy by 8.04% on Places365-LT and 7.72% on iNat over BalCE, and by 25.48% over standard cross entropy on Places365-LT. These improvements are most  One question is how RAC is able to so outperform methods that are provably consistent, such as LACE <ref type="bibr" target="#b37">[38]</ref>. We hypothesize that, in addition to the non-convexity introduced by using Neural Networks as the scorer, this is due to the fact that sample frequency alone does not indicate classification 'difficulty' from the perspective of a balanced learner <ref type="bibr" target="#b58">[59]</ref>. Instead, a small number of samples may still define a sufficiently clear decision boundary if the volume of semantic space covered by that class is small and distinct <ref type="bibr" target="#b7">[8]</ref>, and hence in a truly balanced model, both inter-and intra-class distributions must be considered. Accounting for the intraclass distribution being difficult, however, given no prior on this quantity is typically provided, aside from the labels themselves. Instead, the majority of prior work has either ignored this factor, or assumed the class distributions to be Gaussian. In our formulation, these "easy" classes get picked up by the retrieval model, leaving the base branch to focus on examples that are difficult, where the difficulty is a combination of presentation frequency and class complexity. Previous methods have attempted this by correlating stability under augmentation with confidence <ref type="bibr" target="#b56">[57]</ref>, however, this correlation is weak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Retrieval</head><p>Quantifying retrieval accuracy is important because if standard retrieval performance is significantly lower than that of a balanced supervised learner such as the LACE baseline, it is unlikely to be beneficial. In <ref type="table" target="#tab_4">Table 4</ref> we perform standard retrieval with ImageNet pretrained encoders on both datasets, encoding the training set and then query- ing it with encoded test images, returning the label of the closest image in the training set as the prediction. All comparisons are done on exact match indexes with the 2 distance, no data augmentation and consistent crop, interpolation and normalization constants. z q has length 2048 for the ResNet models, and 768 for ViTs. Despite being trained on the same data, we show that ViTs significantly outperform ResNets, and are hence critical to RAC's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Importance of the Text Encoder</head><p>RAC makes use of a large BERT-like text encoder to learn a mapping from retrieved labels to class logits. Here we quantify the importance of this model relative to two  alternatives: (i) Bag-of-words (BoW) GLoVe <ref type="bibr" target="#b41">[42]</ref> embeddings, and (ii) BoW cached random embeddings.</p><p>Both are of dimension 300 vs. 512 for the CLIP encoder. The random embeddings are sampled from a uniform distribution over the interval [0, 1) and cached for each word in the input string. That is, the embeddings for individual words are consistent, but have no inherent semantics.</p><p>We observe that a higher capacity T does improve performance, particularly on the Places365-LT few-shot classes, but that overall this benefit is minor. This is likely due to the input to the encoder not being overly complex, and more detailed information such as captions were returned, this effect may be more pronounced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effect of k</head><p>Given that our choice of k in approximate k-NN search is larger than the minimum number of samples present perclass for both Places365-LT and iNat, we question whether the additional returned samples, which cannot be the correct class (in the few-shot case), degrade retrieval performance.</p><p>To study this, we experimented on only the retrieval branch, with no base encoder, and utilized an index that contained the training set only. As can be seen in <ref type="figure" target="#fig_4">Fig 3,</ref> increasing k consistently increases accuracy, indicating the text encoder T(?) is able to learn to disregard the common classes. Note the few-shot performance is low here, as the retrieval branch is still trained under the LACE loss, and hence pushed towards balanced performance across all classes. It is thus not free (via the base branch) to focus on the tail classes. This indicates that newer transformer architectures, that facilitate longer sequence length, may be beneficial when applied to RAC, especially when the associated text snippets themselves are longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Impact of Index Content</head><p>To quantify how index content affects RAC, we carried out three experiments in which we trained only the retrieval module on the Places365-LT dataset, with variants of the ImageNet21k dataset used for the index. Training was done with the same final LACE loss as complete RAC, with a ViT-B-16 as E. Specifically, we alter: Results are shown in <ref type="figure" target="#fig_8">Fig. 5</ref>. While naively increasing index size does increase performance, this effect diminishes as more samples are added. This is likely caused by the information content of the labels passed to T not increasingas once most labels are present E is more likely to find a 7 10 3 similar image, however from the perspective of T, which is not distance or image aware, the information is the same. This is supported by sub-figures (b) and (c), in which the total amount of samples in the index is increased consistently between both plots, but adding samples by via new labels has a disproportionately larger effect than adding new samples with the number of labels constant. This is promising in that it indicates increasing label granularity, through the use of image captions or associated text, is likely to increase RAC's performance even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Runtime Consideration</head><p>Nearest neighbour searches can be computationally infeasible for large datasets. We show here, however, that a lookup over a sample index of size &gt;10M can be performed for each training sample with negligible overhead, although the additional label encoding (and subsequent backprop), does increase the training time by a factor of 1.5 ? 2?. We report the precise run-times of models with and without retrieval augmentation on Places365-LT in <ref type="table" target="#tab_5">Table 5</ref>. Given that the index is static, the number of iterations per second is constant throughout training. All training is carried out on a single node, containing 8? A100 GPUs (32GB Mem).</p><p>Moving a tensor from the GPU to CPU, querying the index, then moving the resultant tensor back to GPU maybe expected to slowdown training. However, we find that the impact is minor with the majority of overhead coming from the additional text encoder (the random encoder, 'Rand.', contains no additional parameters). To facilitate multi-node training, RAC keeps separate, complete copies of each index in memory for each node, ensuring querying the index is never the bottleneck, which we found to be essential. While it is possible to do the search entirely on GPU, due to the low overhead we do not do this, instead using the free GPU memory to facilitate a large batch size. Due</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Data</head><p>Size Text Enc. Speed (s/epoch) to the use of HNSW, index query time is logarithmic with respect to the index size, and a standard exhaustive search is prohibitively slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>While RAC demonstrates robust performance for both naturally occurring (iNat) and constructed (Places365-LT) long-tailed class distributions, the analysis could be further expanded to include additional long-tailed datasets. The performance of RAC on balanced datasets is also of interest and not explored. Finally, while RAC clearly demonstrates the benefit of an explicit retrieval component, the data being retrieved (labels) is of limited value and imposes a cap on RACs performance-a natural extension is to query for whole paragraphs or captions. However, the 76 token limit imposed by the CLIP text encoder prevents this, and would need to be increased. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced RAC, a generic approach to augmenting standard classification pipelines with an explicit retrieval module. RAC's retrieval module, without prompt-ing, achieves a high level of accuracy on tail classes, freeing up the base encoder to focus on common classes. RAC improves upon the state-of-the-art results on the iNat and Places365-LT benchmarks by a large margin for the task of long-tail image classification. We hope that RAC represents a step towards more effectively making use of external memory within common computer vision architectures, and we predict its use for other vision tasks, particularly, such as one/few shot learning, and continual learning without catastrophic forgetting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide more information here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experiments</head><p>A.1. Re-weighting and Temperature Scaling As a top level loss for RAC, we make use of unscaled logit adjustment exclusively, with no reweighting (i.e. ? y = 1 ?y ? Y) and no temperature scaling (? = 1) in Eq. (3). This loss is theoretically well-grounded <ref type="bibr" target="#b37">[38]</ref>, and is appealing due to its simplicity. Nevertheless, other works have noted that the combination of logit adjustment with reweighting often leads to higher empirical performance <ref type="bibr" target="#b0">[1]</ref>. While optimizing the top-level loss is not the focus of RAC, we include here a comparison of RAC performance under various re-weighting schemes when combined with logit adjustment for completeness. Specifically, we consider inverse log</p><formula xml:id="formula_6">? y = 1 log N y<label>(6)</label></formula><p>and inverse square-root</p><formula xml:id="formula_7">? y = 1 N y<label>(7)</label></formula><p>class-frequency based re-weighting of individual sample losses. We confirm that the same effect is present for RAC on Places365-LT, with overall accuracy increasing with the use of both re-weighting schemes, with improvement most pronounced for tail classes <ref type="table">(Table S1</ref>). However, this trend does not hold on iNat.</p><p>For Places365-LT, we also we perform a sweep across ? , to evaluate if the same effect can be achieved with manual temperature scaling <ref type="figure" target="#fig_1">(Fig. S1</ref>). Higher ? does result in slightly higher overall accuracy, however this effect is minor in comparison to re-weighting. This divergence from  theory is likely due to the non-separability of many classes in Places365-LT due to the high label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Places365-LT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Index Ablations</head><p>We examine the effect of the distance metric and index type on RAC's lookup performance and speed in <ref type="table" target="#tab_7">Table S2</ref>. To quantify error induced by an approximate index, we include the lookup accuracy on the index content itself (training set) in addition to the validation accuracy. Query Time (QT) includes encoding samples with a ViT-B-16 model, which is the primarily overhead on small indexes.</p><p>We observe that the choice of distance metric ( 2 vs. cosine) has little effect, as may be expected, give the high dimentionality of the index. Cosine distance does introduce a minor computational overhead due to the need to normalize the embeddings prior to querying. The drop in accuracy due to use of an approximate (HNSW) instead of exact k-NN is also minor, but comes with a significant (2?) speedup on large index's. Construction time is constant across all indexes at 0.02?s per sample, with the exception of largeindex HNSW, which requires 0.05? per sample. In summary, performance differences are minor when querying small indexes, however as the index size grows, the choice of HNSW becomes critical to ensure lookup time does not bottleneck training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Per-class Accuracy on iNat</head><p>We include the same per-class visualization presented in the main body for Places365-LT <ref type="figure" target="#fig_3">(Fig. 2)</ref>, for iNat. Note that for iNat only 3 samples are present for each class in the validation set, hence the square-wave appearance of the plots <ref type="figure" target="#fig_3">(Fig. S2)</ref>. Nevertheless, the same trend is clearly visible in the sliding window moving average, with the retrieval module performing best on tail classes and the base network largely focusing on the many and mid-frequency classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Further Details</head><p>For Places365-LT we use the training and validation splits during development and report final numbers on the test set, with no validation samples used during final training. For iNaturalist2018, following prior work, we report results on the released validation split, as labels for the test set are not publicly available.</p><p>In the main text, we use several common model variants. While the architectures for these models are standard, we   All training is carried out on 8? 32GB A100 GPU's. We largely follow the procedures outlined in <ref type="bibr" target="#b44">[45]</ref>, with the following alterations. We finetune with AdamW <ref type="bibr" target="#b34">[35]</ref> instead of SGD, and make use of low-magnitude RandAugment <ref type="bibr" target="#b5">[6]</ref> alone for data augmentation, with no Color-Jitter, Mixup,    <ref type="table" target="#tab_12">Table S7</ref>. While we found the use of Mixup and Cutmix does boost performance on standard ViTs trained under a BalCE loss, their use of combined targets requires special treatment to make compatible with the LACE loss, which requires hard targets in order to assign the class adjustment. While one approach may be to apply the 'merged' class adjustment, the performance benefit is marginal and hence we simply did not include either approach in RAC's data augmentation pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Retrieval Branch Visualization</head><p>While <ref type="table" target="#tab_4">Table 4</ref> quantifies top-1 performance of the retrieval branch, non-exact match snippets will also effect RAC as they are still likely to be informative. If 'plane', 'runway', 'concrete', 'sky', 'propeller' etc. are returned for example, it is not difficult for B to place a high score on 'airport'. We visualize the returned strings for random samples by distance and frequency in Figs. S3 and S4. For all runs, k = 30 as in the main work.</p><p>Each column displays (from left to right):  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>(a) RAC overview. RAC consists of a retrieval module that augments a standard encoder B(?) with explicit external memory. (b) The retrieval module consists of external images I encoded by a fixed, pretrained image encoder E(?), and associated text T queried using an approximate k-NN and encoded via a text encoder T(?). The logits of the retrieval encoder are then combined with those of the base network. In our instantiation, B and E are ViT's, and T is a BERT-like text encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>f y per-class accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Per-class top-1 accuracy on Places365-LT from each branch's output. Without prompting, the retrieval module learns to focus on tail classes. The 20 sample moving average over classes (solid line) is shown for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Effect of the number of retrieved text snippets, k, on Places365-LT top-1 accuracy for the retrieval only branch, querying an index containing only the Places365-LT training set. Higher k consistently improves performance until the cutoff induced by the text encoding truncation (76 tokens), however it does come at the cost of (linearly) higher training time. We choose k = 30 in our experiments. x-axis is log-scaled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Effect of the choice of text encoder on performance. The overall impact is minor, however the CLIP LM significantly boosts few-shot performance on Places365-LT, where labels are natural language terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) the index size via directly sub-sampling from the full ImageNet21k dataset. (b) the number of training examples per-class while keeping the number of classes constant. (c) the number of classes while keeping the number of sample per class constant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Total classes (N i = 10) Effect of index content on performance (k = 30) on the retrieval branch only, trained under the LACE loss on Places365-LT. Here, the index contains no Places365-LT data, only variants of the ImageNet21k dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S1 .</head><label>S1</label><figDesc>Effect of ? within the LACE loss on balanced performance on the Places365-LT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>f</head><label></label><figDesc>y per-class accuracy Figure S2. Per-class top-1 accuracy on iNat from each branch's output. The 300 sample moving average over classes (solid line) is shown for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure S3 .</head><label>S3</label><figDesc>0.73) coast 2. (0.70) beach 3. (0.69) beach 4. (0.68) beach 5. (0.66) beach 6. (0.66) beach 7. (0.65) beach 8Retrieval branch visualization for randomly drawn samples from Places365-LT. For detailed explanation see Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Image 1 Figure S4 .</head><label>1S4</label><figDesc>0.70) Anthopleura artemisia 2. (0.70) Strongylocentrotus purpuratus 3. (0.70) Anthopleura artemisia 4. (0.69) Strongylocentrotus purpuratus 5. (0.69) Strongylocentrotus purpuratus 6. (0.69) Oulactis magna 7. (0.69) Strongylocentrotus purpuratus 8. (0.69) Strongylocentrotus purpuratus 0 Retrieval branch visualization for randomly drawn samples from iNaturalist2018. For detailed explanation see Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Historical performance on iNat, ? Results reproduced from<ref type="bibr" target="#b57">[58]</ref>. Historical performance on Places365-LT.</figDesc><table><row><cell>Method</cell><cell cols="2">Many Med</cell><cell>Few</cell><cell>All</cell></row><row><cell></cell><cell cols="2">Input: 224 ? 224</cell><cell></cell><cell></cell></row><row><cell>OLTR [34]  ?</cell><cell>59</cell><cell>64.1</cell><cell>64.9</cell><cell>63.9</cell></row><row><cell cols="2">Decouple-LWS [25]  ? -</cell><cell>-</cell><cell>-</cell><cell>65.9</cell></row><row><cell>LADE [22]  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.3</cell></row><row><cell>Grafit [48]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.9</cell></row><row><cell>ALA [58]</cell><cell>71.3</cell><cell>70.8</cell><cell>70.4</cell><cell>70.7</cell></row><row><cell cols="2">RIDE [52] (2 experts) 70.2</cell><cell>71.3</cell><cell>71.7</cell><cell>71.4</cell></row><row><cell>LACE [38]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.9</cell></row><row><cell cols="2">RIDE [52] (4 experts) 70.9</cell><cell>72.4</cell><cell>73.1</cell><cell>72.6</cell></row><row><cell>TADE [57]</cell><cell>74.4</cell><cell>72.5</cell><cell>73.1</cell><cell>72.9</cell></row><row><cell>DisAlign [55]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>74.1</cell></row><row><cell>PaCo [7]</cell><cell>75.0</cell><cell>75.5</cell><cell>74.7</cell><cell>75.2</cell></row><row><cell>RAC (ours)</cell><cell cols="4">75.92 80.47 81.07 80.24</cell></row><row><cell></cell><cell cols="2">Input: 384 ? 384</cell><cell></cell><cell></cell></row><row><cell>Grafit</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.2</cell></row><row><cell>RAC (ours)</cell><cell cols="4">82.91 85.71 86.06 85.56</cell></row><row><cell>Method</cell><cell cols="2">Many Med</cell><cell>Few</cell><cell>All</cell></row><row><cell>Focal Loss [32]  ?</cell><cell>41.1</cell><cell>34.8</cell><cell>22.4</cell><cell>34.6</cell></row><row><cell>Range Loss [56]  ?</cell><cell>41.1</cell><cell>35.4</cell><cell>23.2</cell><cell>35.1</cell></row><row><cell>OLTR [34]  ?</cell><cell>44.7</cell><cell>37</cell><cell>25.3</cell><cell>35.9</cell></row><row><cell cols="2">Decouple-LWS [25]  ? 40.6</cell><cell>39.1</cell><cell>28.6</cell><cell>37.6</cell></row><row><cell>LADE [22]  ?</cell><cell>42.8</cell><cell>39</cell><cell>31.2</cell><cell>38.8</cell></row><row><cell>DisAlign [55]</cell><cell>40.4</cell><cell>42.4</cell><cell>30.1</cell><cell>39.3</cell></row><row><cell>ALA [58]</cell><cell>43.9</cell><cell>40.1</cell><cell>32.9</cell><cell>40.1</cell></row><row><cell>TADE [57]</cell><cell>43.1</cell><cell>42.4</cell><cell>33.2</cell><cell>40.9</cell></row><row><cell>PaCo [7]</cell><cell>36.1</cell><cell>47.9</cell><cell>35.3</cell><cell>41.2</cell></row><row><cell>RAC (ours)</cell><cell cols="4">48.69 48.31 41.76 47.17</cell></row></table><note>? Results repro- duced from [58].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of top-1 accuracy against baselines under a common training scheme. Column B indicates the architecture of the base branch.</figDesc><table><row><cell cols="2">Method B</cell><cell>Many</cell><cell>Med</cell><cell>Few</cell><cell>All</cell></row><row><cell></cell><cell></cell><cell cols="2">Places365-LT</cell><cell></cell><cell></cell></row><row><cell>CE</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell cols="2">-32.14</cell></row><row><cell>BalCE</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell cols="2">-38.31</cell></row><row><cell>CE</cell><cell cols="5">ViT-B-16 50.81 33.83 19.51 37.16</cell></row><row><cell>BalCE</cell><cell cols="5">ViT-B-16 49.03 45.72 29.05 43.67</cell></row><row><cell cols="2">Retrieval -</cell><cell cols="4">43.50 41.99 26.83 39.58</cell></row><row><cell>Base</cell><cell cols="5">ViT-B-16 44.57 45.06 40.77 44.05</cell></row><row><cell>RAC</cell><cell cols="5">ViT-B-16 48.69 48.31 41.76 47.17</cell></row><row><cell></cell><cell></cell><cell cols="2">iNaturalist 2018</cell><cell></cell><cell></cell></row><row><cell>CE</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.7</cell></row><row><cell>BalCE</cell><cell>RN50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.8</cell></row><row><cell>CE</cell><cell cols="5">ViT-B-16 81.53 76.62 69.82 74.44</cell></row><row><cell>BalCE</cell><cell cols="5">ViT-B-16 72.39 76.06 73.05 74.49</cell></row><row><cell cols="2">Retrieval -</cell><cell cols="4">50.10 52.77 52.45 52.37</cell></row><row><cell>Base</cell><cell cols="5">ViT-B-16 74.41 78.95 78.55 78.32</cell></row><row><cell>RAC</cell><cell cols="5">ViT-B-16 75.92 80.48 81.07 80.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Analysis of standard retrieval performance and Construction Time (CT) which includes both image encoding and HNSW in- dexing.* 384 ? 384 resolution variants.pronounced on the tail classes where RAC improves over BalCE by 30.42% on Places365-LT and 10.98% on iNat.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Effect of additional text encoder, and lookup on training wall-time for RAC (k = 50) on Places365-LT. Top row indicates the use of the base encoder only. The majority of added overhead comes from use of the text encoder, rather than the lookup itself.</figDesc><table><row><cell>None</cell><cell>None None</cell><cell>23.6</cell></row><row><cell>Places</cell><cell>184K Rand.</cell><cell>28.3</cell></row><row><cell>Places</cell><cell>184K CLIP</cell><cell>44.3</cell></row><row><cell cols="2">Places, IM21k 11.2M CLIP</cell><cell>47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 .</head><label>S2</label><figDesc>Index ablations on Places365-LT. QT indicates Query Time. Large-sample indices are filled with the ImageNet21k dataset.</figDesc><table><row><cell cols="2">Index Size Index Type Distance</cell><cell>Many</cell><cell>Med</cell><cell cols="5">Test Few Top-1 Top-5 Top-1 Train QT (ms/sample)</cell></row><row><cell>62.5k Exact</cell><cell>L 2</cell><cell cols="6">39.97 26.74 18.65 29.91 53.81 99.97</cell><cell>1.12</cell></row><row><cell>62.5k Exact</cell><cell>Cosine</cell><cell cols="6">39.84 26.51 18.03 29.64 53.31 99.96</cell><cell>1.14</cell></row><row><cell>62.5k HNSW</cell><cell>L 2</cell><cell cols="6">39.89 26.51 18.03 29.66 53.32 99.79</cell><cell>1.06</cell></row><row><cell>62.5k HNSW</cell><cell>Cosine</cell><cell cols="6">39.57 26.26 17.68 29.37 52.83 99.79</cell><cell>1.07</cell></row><row><cell>11.2M Exact</cell><cell>L 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>7.96</cell></row><row><cell>11.2M HNSW</cell><cell>L 2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S3 .Table S4 .Table S5 .</head><label>S3S4S5</label><figDesc>Dataset Details ViT model architecture details. ResNet model architecture details. specify the high-level design choices in Tables S4 and S5. These choices are consistent across both the 224 ? 224 and 384 ? 384 variants.</figDesc><table><row><cell>Hyperparameter</cell><cell></cell><cell cols="2">ViT-B-16 ViT-B-32</cell></row><row><cell>Patch size</cell><cell></cell><cell>16</cell><cell>32</cell></row><row><cell>Depth</cell><cell></cell><cell>12</cell><cell>12</cell></row><row><cell cols="2">Embedding Dimension</cell><cell>768</cell><cell>768</cell></row><row><cell>Attention Heads</cell><cell></cell><cell>12</cell><cell>12</cell></row><row><cell>Parameters</cell><cell></cell><cell>85.8M</cell><cell>87.4M</cell></row><row><cell>Hyperparameter</cell><cell></cell><cell>RN50</cell><cell>RN152d</cell></row><row><cell>Input size</cell><cell cols="3">244 ? 224 ? 3 256 ? 256 ? 3</cell></row><row><cell>Head</cell><cell cols="2">Avg. Pool, FC</cell><cell>Avg. Pool, FC</cell></row><row><cell>Convolutions</cell><cell></cell><cell>standard</cell><cell>standard</cell></row><row><cell>Stem Convolutions</cell><cell cols="2">1 layer, 3 ? 3</cell><cell>3 layer, 3 ? 3</cell></row><row><cell>Stem width</cell><cell></cell><cell cols="2">32 (128, 128, 128)</cell></row><row><cell>Layers</cell><cell></cell><cell>[3, 4, 6, 3]</cell><cell>[3, 8, 36, 3]</cell></row><row><cell>Pool size</cell><cell></cell><cell>7 ? 7</cell><cell>8 ? 8</cell></row><row><cell>Num. features</cell><cell></cell><cell>2048</cell><cell>2048</cell></row><row><cell>Parameters</cell><cell></cell><cell>23.5M</cell><cell>58.2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Places365-LT iNat</figDesc><table><row><cell>Batch Size</cell><cell>200</cell><cell>50</cell></row><row><cell>Learning Rate</cell><cell cols="2">5e-5 1e-4</cell></row><row><cell>Epochs</cell><cell>30</cell><cell>20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S6 .</head><label>S6</label><figDesc>Dataset specific training hyperparameters</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Global normalization means</cell><cell>[0.5, 0.5, 0.5]</cell></row><row><cell>Global normalization stds</cell><cell>[0.5, 0.5, 0.5]</cell></row><row><cell>Crop (train and test)</cell><cell>0.95</cell></row><row><cell>Distributed</cell><cell>DDP 8 GPU's</cell></row><row><cell>LR schedule</cell><cell>cosine</cell></row><row><cell>Min LR</cell><cell>1e-7</cell></row><row><cell>Warmup LR</cell><cell>1e-7</cell></row><row><cell>Warmup epochs</cell><cell>5</cell></row><row><cell>Optimizer</cell><cell>adamw</cell></row><row><cell>Beta 1</cell><cell>0.9</cell></row><row><cell>Beta 2</cell><cell>0.999</cell></row><row><cell>Eps.</cell><cell>1e-8</cell></row><row><cell>Gradient clipping</cell><cell>L 2 Norm</cell></row><row><cell>Gradient clipping magnitude</cell><cell>1.0</cell></row><row><cell>RandAugment magnitude</cell><cell>1</cell></row><row><cell>RandAugment layers</cell><cell>3</cell></row><row><cell>RandAugment noise std.</cell><cell>0.5</cell></row><row><cell>Weight decay</cell><cell>0.02</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell></row><row><cell>Stochastic depth</cell><cell>0.1</cell></row><row><cell>Random erase prob.</cell><cell>0.0</cell></row><row><cell>Color jitter</cell><cell>0.0</cell></row><row><cell>Random scale</cell><cell>[0.75, 1.33]</cell></row><row><cell>Random crop</cell><cell>No</cell></row><row><cell>Horizontal flip prob.</cell><cell>0.5</cell></row><row><cell>Random rotation</cell><cell>No</cell></row><row><cell>Mixed precision level</cell><cell>O2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S7 .</head><label>S7</label><figDesc>Training hyperparameters for the ViT models Cutmix, RandomErase or Augmix applied. Full hyperparameters are shown in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We do not compare against CIFARLT and ImageNetLT, as in these scenarios training is typically performed from scratch, and RAC requires a pretrained network for the retrieval module. While it is possible to train the base network from scratch, this is not a fair comparison and RAC significantly outperforms other methods due to the information present in E.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving minority class prediction using case-specific feature weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="782" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="694" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn. Workshops</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn. Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="715" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<title level="m">Editing factual knowledge in language models. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PML: Progressive margin loss for long-tailed age classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10503" to="10512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv: Comp. Res</title>
		<imprint/>
	</monogr>
	<note>Repository, 2020. 1, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Class imbalance and cost sensitivity: Why undersampling beats oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drumnond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-KDD Workshop: Learning from Imbalanced Datasets</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Intelligence and Statistics</title>
		<meeting>Int. Conf. Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<title level="m">Neural turing machines. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long-tailed multi-label visual recognition by collaborative training on uniform and rebalanced samplings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15089" to="15098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">Realm: Retrieval-augmented language model pre-training. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Intelligent Computing</title>
		<meeting>Int. Conf. Intelligent Computing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge &amp; Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Disentangling label distribution for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="6626" to="6636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<title level="m">Billionscale similarity search with GPUs. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<title level="m">Transformers in vision: A survey. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tim Rockt?schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2970" to="2979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Decoupled weight decay regularization. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dmitry A Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<title level="m">Long-tail learning via logit adjustment. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">When does label smoothing help? arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The long tail of recommender systems and how to leverage it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon-Joo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conf. Recommender Systems</title>
		<meeting>ACM Conf. Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Process</title>
		<meeting>Conf. Empirical Methods in Natural Language ess</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Qui?onero-Candela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How to train your vit? Data, augmentation, and regularization in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grafit: Learning finegrained image representations with coarse labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptable and interpretable neural MemoryOver symbolic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3678" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="247" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<title level="m">Scaling vision transformers. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distribution alignment: A unified framework for long-tail visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with longtailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Test-agnostic long-tailed recognition by test-time aggregating diverse experts with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive logit adjustment loss for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository, 2021</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving long-tailed classification from instance level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weicong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv: Comp. Res. Repository</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9719" to="9728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">1. query image x q , 2. retrieved labels sorted by distance (distance shown in brackets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Places: A 10 million image database for scene recognition. exact matches colored green</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Note that as the distance metric is cosine, a higher value corresponds to more similar samples</title>
	</analytic>
	<monogr>
		<title level="m">We restrict the lists of retrieved labels to the top eight for visualization purposes</title>
		<imprint/>
	</monogr>
	<note>Image 1. (0.79) airfield 2. (0.79) airfield 3. (0.79) airfield 4. (0.78) airfield 5. (0.78) runway 6. (0.78) airfield 7. (0.78) airfield 8. (0.78) airfield 1. (0.72) dining room 2. (0.71) patio 3. (0.70) balcony-interior 4. (0.69) balcony-interior 5. (0.69) beach 6. (0.68) beach house 7. (0.68) restaurant patio 8. (0.67) porch</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
