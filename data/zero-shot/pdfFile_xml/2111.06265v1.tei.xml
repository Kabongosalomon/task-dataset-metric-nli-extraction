<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Unsupervised Learning for Video Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Araslanov</surname></persName>
							<email>nikita.araslanov@visinf.tu-darmstadt.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Darmstadt 2 hessian.AI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Schaub-Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Darmstadt 2 hessian.AI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
							<email>stefan.roth@visinf.tu-darmstadt.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Darmstadt 2 hessian.AI</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Unsupervised Learning for Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel approach to unsupervised learning for video object segmentation (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter-and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power.</p><p>Implementing this process in a novel framework, we attain a new state of the art in video segmentation accuracy, with compelling computational and data efficiency. Our framework can be trained on Code (Apache-2.0 License) available at https://github.com/visinf/dense-ulearn-vos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised learning of visual representations has recently made considerable and expeditious advances, in part already outperforming even supervised feature learning methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Most of these works, however, require substantial computational resources <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, and only few accommodate one of the most ubiquitous types of visual data: videos <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to image sets, video data embeds ample information about typical transformations occurring in nature. Exploiting such cues may allow systems to learn more task-relevant invariances, instead of relying only on hand-engineered data augmentation <ref type="bibr" target="#b29">[30]</ref>. In this work, we propose to learn such invariances with a novel framework for the task of video object segmentation (VOS). In contrast to previous works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, we learn dense representations efficiently in a fully convolutional manner, previously considered prone to degenerate solutions <ref type="bibr" target="#b16">[17]</ref>. This is achieved by leveraging the assumption of feature equivariance to similarity transformations (e. g., multi-scale cropping) applied to the input frame. <ref type="figure">Fig. 1</ref> presents an overview of our approach. For each video sequence, we designate one of the frames as the reference frame. We sample a set of features, produced from the reference frame by a fully convolutional network, on a spatially uniform grid, and refer to them as anchors. Using a contrastive formulation <ref type="bibr" target="#b11">[12]</ref>, our approach learns to represent the temporally proximate frames to the reference in terms of these anchors. Importantly, this representation is learned to be equivariant to similarity transformations. Towards this goal, we devise a self-training objective <ref type="bibr" target="#b19">[20]</ref> that generates pseudo labels by leveraging the second, transformed view of the original video sequence. These pseudo labels contain dominant assignments of the features from the second view to the anchors. Following the self-training mechanism, we learn to assign the features in the original view consistently with the transformed view. Our self-supervised loss further disambiguates the anchors themselves spatially and between independent video sequences, while also ensuring their transformation equivariance.  <ref type="figure">Figure 1</ref>: Illustration of the main idea. We learn dense visual feature representations in an unsupervised way by (i) discriminating the features both spatially and at the video level, and (ii) embedding video representations in terms of a temporally persistent set of video-specific anchors. We devise a simple and effective approach that sidesteps trivial solutions by exploiting feature equivariance to similarity transformations of the input frame, which allows to efficiently learn dense representations in a fully convolutional manner.</p><p>a single commodity GPU, yet achieves higher segmentation accuracy than previous work, while requiring orders of magnitude less data. Improving the accuracy even further with more data, our approach also exhibits favourable scalability in a more comprehensive testing scenario, which has not yet been shown in prior work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The research domains most relevant to our work are video object segmentation (VOS) without supervision and representation learning.</p><p>VOS. The inference setting of semi-supervised VOS, which is the task considered here, is to densely track object masks provided in the first frame. It is typically approached by learning dense feature representations to establish temporal correspondences for propagating the semantic labels. While most previous methods are supervised by pixel-wise mask annotations [e. g., <ref type="bibr" target="#b26">27]</ref>, an emerging direction is to leverage unsupervised learning. Our work contributes to these research efforts. The Contrastive Random Walk (CRW) of Jabri et al. <ref type="bibr" target="#b16">[17]</ref> and its predecessor <ref type="bibr" target="#b38">[39]</ref> exploit a time-cycle consistency constraint, akin to the forward-backward consistency in unsupervised optical flow estimation <ref type="bibr" target="#b23">[24]</ref>. Other methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> learn the label propagation process at training time by substituting the target mask (which is unavailable at training time) with the image, an idea inspired by video colourisation <ref type="bibr" target="#b36">[37]</ref>. The algorithm for label propagation itself significantly affects the VOS accuracy. Mask propagation with naive pixel-wise correspondences usually benefits greatly from leveraging more sophisticated region-level propagation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>, which is not the focus here.</p><p>Representation learning. Although previous work on unsupervised representation learning pursued spatially view-invariant objectives <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>, a concurrent study <ref type="bibr" target="#b9">[10]</ref> finds that equipping these methods with temporally-invariant, or temporally-persistent constraints, yields sizeable gains in terms of accuracy on action recognition benchmarks. These findings align well with previous and concurrent work tailored specifically to learning spatio-temporal representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> for video recognition and retrieval tasks. These works additionally distinguish the representations of frames from different videos (or, at different timesteps). The limitation of these works in the context of VOS is that they do not learn dense, but global feature representations at the image or video level, and typically require considerable computational budgets. By contrast, Pinheiro et al. <ref type="bibr" target="#b30">[31]</ref> learn dense representations and exploit the equivariance constraint, as in our work. However, their approach is limited to image sets of static scenes, whereas we address videos of (potentially) dynamic scenes here.</p><p>Semi-supervised representation learning. Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref> leverage optical flow <ref type="bibr" target="#b33">[34]</ref> to ensure the representation similarity of temporally corresponding features. Such an objective can be alternatively supervised by motion segments <ref type="bibr" target="#b27">[28]</ref>. Spatial equivariance has been also explored in part co-segmentation <ref type="bibr" target="#b15">[16]</ref>, a setup that requires semantically aligned image pairs, as well as in weakly supervised semantic segmentation <ref type="bibr" target="#b39">[40]</ref> and unsupervised domain adaptation <ref type="bibr" target="#b0">[1]</ref>. In contrast to this body of work, we learn to establish temporal correspondences via a suitable feature representations learned in a completely unsupervised way.</p><p>Clustering. Contrastive learning methods are inherently expensive due to the size of pairwise feature comparisons they consider in training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. Clustering provides a more computationally efficient alternative <ref type="bibr" target="#b5">[6]</ref>, since pairwise distances need to be computed only w. r. t. a (moderately sized) set of pre-defined clusters. Combining these ideas, Caron et al. <ref type="bibr" target="#b4">[5]</ref> propose to cluster the embeddings and then enforce consistency between the cluster assignments by contrastive learning. The main challenge in this approach is to impose an expressive prior on the cluster assignments. For example, a uniform prior prevents trivial solutions where feature assignments collapse to a single cluster.</p><p>Here, we neither explicitly define the clusters nor the prior. Instead, we sample attractor points, called anchors, directly from the available feature representations using a spatial grid sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Learning feature representations from unlabelled data is an inherently ill-posed problem and requires specifying assumptions about what constitutes a useful feature embedding with a downstream task in mind. We begin with outlining the assumptions guiding our unsupervised learning method for video segmentation, before delving into the technical specifics of their implementation. We then look in more detail into the mechanisms that our method implements to comply with those assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Assumptions</head><p>Assumption 1: Non-local feature diversity. We assume that the distinctness of visual artefacts in a natural scene exhibits spatial correlation: the more distant two visual phenomena are on the pixel grid, the more likely they are to belong to different semantic entities, and vice versa. Dense representations of the images should mirror this property, i. e. be spatially distinguishable. Note that this assumption is weaker than that of spatial smoothness, which is problematic at object boundaries, where it would encourage the same representation of the object and the background. In contrast, non-local feature diversity is agnostic to the spatial arrangement of the objects and the background in the scene. On a feature grid, where every cell corresponds to its own receptive field in the image, we can discriminate the representation on all levels of the semantic hierarchy (provided sufficient grid resolution): between the objects and the background, as well as between object parts. Jabri et al. <ref type="bibr" target="#b16">[17]</ref> implicitly relied on this assumption to spatially distinguish the nodes of the space-time graph.</p><p>Assumption 2: Temporal coherence. We posit that the feature representation extracted from one frame in a video should be closer in the embedding space (e. g., in terms of the cosine similarity) to a feature vector from another temporally close frame of the same sequence, rather than to a feature embedding derived from another video. The premise for this assumption is the temporal coherence in videos: the changes in appearance within a video sequence are seldom as significant as between two independent video shots. Also leveraged by Wang et al. <ref type="bibr" target="#b37">[38]</ref>, this is a desirable property for dense tracking, which is our focus in this work.</p><p>Assumption 3: Temporal persistence of semantic content. We further assume that the semantic content of video clips remains unchanged, at least for a short time span. Specifically, if we represent a given reference frame with a set of distinct features (following Assumption 1), the semantic content of temporally close frames can be faithfully represented with the same feature set. Note that the contrastive random walk (CRW) <ref type="bibr" target="#b16">[17]</ref> makes a somewhat stronger assumption that every frame in a sequence must contain the same feature set as the reference frame. By contrast, under our assumption every frame may comprise only a (proper) subset of the reference feature set. Although the edge dropout technique improves model robustness to partial occlusions in <ref type="bibr" target="#b16">[17]</ref>, following our assumption allows to handle full occlusions, provided the occluded object is visible in the reference frame.</p><p>Assumption 4: Equivariance to similarity transformations. Finally, we require the feature representation be equivariant to similarity transformations, i. e. scaling or flipping of an input video frame should produce a correspondingly scaled or flipped feature embedding. This assumption has not been yet explored in dense unsupervised learning from videos, and is in contrast to the common practice of unsupervised learning from static image sets, where two views produced by a random similarity transform need to yield the same global, hence invariant representation <ref type="bibr" target="#b7">[8]</ref>.</p><p>While Assumptions 1 and 2 have been studied independently in prior work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>, we investigate their combination here, and further complement them with our occlusion-aware variant of Assumption 3 and the yet unexplored Assumption 4 on equivariance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview</head><p>The core of our training procedure is a self-supervised loss imposing four assumed properties on the feature representation. First, we learn to densely represent every video frame in terms of spatially discriminative features (Assumption 1). Second, the set of discriminative features representing a video is learned to be distinguishable from the set of another video sequence (Assumption 2). Third, our method learns to represent every frame in a video sequence as a composite of discriminative features extracted from a single reference frame of the same sequence (Assumption 3). Last, we learn a feature representation satisfying these properties under the equivariance constraint (Assumption 4). Specifically, we minimise the distance between every pair of spatially corresponding features extracted from two views and related by a similarity transform w. r. t. non-corresponding pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Framework</head><p>The feature extractor in our framework, illustrated in <ref type="figure" target="#fig_1">Fig. 2a</ref>, is a fully convolutional network <ref type="bibr" target="#b21">[22]</ref>. It processes two copies of the input image batch, comprising the same set of video sequences, in the main and regularising branches. The purpose of the regularising branch is to prevent degenerate solutions observed in previous attempts to learn feature representations in a fully convolutional manner <ref type="bibr" target="#b16">[17]</ref>. While the main branch receives the original video frames, we feed a transformed version of these frames to the regularising branch. Specifically, we extract random multi-scale crops and apply horizontal flipping at random. We denote this random similarity transform as T (?). Both the main and the regularising branch produce dense L 2 -normalised K-dimensional feature tensors k,k ? R B,T,K,h,w , respectively, where k i,j,:,l,m = k i,j,:,l,m = 1, B is the batch size, T is the number of selected frames from a video sequence, and h, w are the spatial dimensions of these embeddings. Terms related to the regularising branch are denoted with the hat notation (?) in the following. In the next step, we compute pairwise feature distances to leverage contrastive learning <ref type="bibr" target="#b11">[12]</ref>. Recall from Sec. 3.2 that our goal is to discriminate features (i) spatially within individual frames under the equivariance constraint, and (ii) temporally, to represent each frame in a video sequence in terms of the same feature set while distinguishing between independent video sequences.</p><p>Anchor sampling. We take a bi-level sampling approach based on clustering for improved training efficiency, illustrated in <ref type="figure" target="#fig_1">Fig. 2b</ref>. First, for each video sequence in the batch, we sample one frame at random. We define it as the reference frame for the video sequence from which it originates. Relying on Assumption 3 that the frames of the same sequence share semantic content, we leverage the randomly chosen reference frame for extracting a set of video-level features, i. e. encouraged to be shared between temporally close video frames. In more detail, instead of computing the pairwise distances between every feature vector in the batch, we define a spatially uniform grid of size N ?N on the feature tensor of the reference frame, and draw one sample per grid cell. This is to collect features that are spatially distinct (Assumption 1) and cover the full image. As a result, we obtain (B ?N 2 ) K-dimensional feature embeddings, which we will refer to as anchors and denote as q.</p><p>Defining this grid sampling operation as G N (?), we can write this step as q := G N (k). We then compute pairwise distances between the features k andk w. r. t. these anchors, rather than to the features themselves. The outcome is a distance matrix of size (B ? T ? h ? w)?(B ? N 2 ). Compared to dense sampling, this reduces memory and the computational budget by a factor of O(T ? h ? w / N 2 ).</p><p>Computing affinity to anchors. Following our cluster-based analogy, we compute affinities of the features k andk from the two branches w. r. t. the anchors q. Note that we extract the anchors only from the main branch and share them with the regularising branch. Since the anchors q ? R BN 2 ?K are sampled from k, they also have unit L 2 -norm, i. e. q i,: = 1 for all i. Let v,v ? R BT hw?BN 2 denote the affinities of k andk w. r. t. q, respectively, which we compute using a softmax over cosine similarities w. r. t. all anchors, i. e.</p><formula xml:id="formula_0">v i,j = exp(ki?qj /? ) l exp(ki?q l /? ) ,v i,j = exp(ki?qj /? ) l exp(ki?q l /? ) ,<label>(1)</label></formula><p>where ? ? R + is a scalar temperature hyperparameter. Put simply, v i,j ? [0, 1] BT hw?BN <ref type="bibr" target="#b1">2</ref> represents the similarity between feature k i and anchor q j (analogously forv i,j ).</p><p>Space-time self-training. We first generate pseudo labels of anchor assignments for self-training by acquiring the dominant anchor index for each feature from the regularising branch,</p><formula xml:id="formula_1">u i = arg max j?N (i)v i,j ,<label>(2)</label></formula><p>where N (i) is the index set of the anchors that stem from the same video clip as the feature vector with index i. Observe that v and? are in spatial correspondence via a similarity transformation T . The former represents soft assignments of the feature vectors from the original view to the anchors, while the latter contains hard assignments of the features from the regularising branch. Our primary self-supervised loss minimises the distance of the features extracted from the other temporally close frames to the anchors,</p><formula xml:id="formula_2">L ST = ? i ?R log T (v i,?i ) ,<label>(3)</label></formula><p>where R is the index set of the features extracted from the reference frames; T (?) spatially aligns the soft affinities v of the main branch with the pseudo labels obtained with the help of the regularising branch. As we analyse in Sec. 3.4, this loss term relates to the spatial and temporal properties of the learned representation within the same view (since we do not propagate the gradient in Eq. (2) following our self-training approach), hence we refer to this objective as space-time self-training. However, implementing Assumptions 1-3, this objective only implicitly fulfils Assumption 4 on equivariance by means of seeking a consistent anchor assignment between the two views.</p><p>Cross-view consistency. To generate the pseudo labels in Eq. <ref type="formula" target="#formula_1">(2)</ref>, we assume that the dominant assignment ofk to the anchors q is meaningful, despite the two representations originating from different synthetic views (related by T (?)). By following our space-time self-training in Eq. <ref type="formula" target="#formula_2">(3)</ref>, we already find this assumption largely to hold for datasets of modest size (cf. Sec. 4.3). However, we observed additional accuracy benefits and improved training stability from explicitly facilitating the equivariance, our Assumption 4, especially when training on larger datasets. We impose a cross-view consistency term on the reference features only. Note that it complements the self-supervision with the pseudo labels in Eq. <ref type="formula" target="#formula_2">(3)</ref>, which omits the reference features. Re-using our grid sampling operator G M (?) parametrised by a cross-view grid size M , we subsample the features from the main and the regularising branches as r = G M (T (k)) andr = G M (k) such that r,r ? R BM 2 ?K . Similarly to Eq. (1), we compute pairwise affinities between these features:</p><formula xml:id="formula_3">h i = exp(ri?ri/? ) l =i exp(ri?r l /? ) .<label>(4)</label></formula><p>This affinity contrasts the cosine similarity between the corresponding features (since r andr are spatially aligned) w. r. t. non-corresponding pairs. We define the cross-view loss term to uphold this correspondence in the embedding space by minimising</p><formula xml:id="formula_4">L CV = ? i?R log h i .<label>(5)</label></formula><p>A hyperparameter ? weights its contribution to the total loss used for training our framework:</p><formula xml:id="formula_5">L = L ST + ?L CV .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>We now take detailed look at how our framework implements the assumptions from Sec. 3.1.</p><p>Non-local feature diversity (Assumption 1). Our loss disambiguates feature representations by clustering, with the anchors acting as attractors. Recall that our formulation of the feature distance to the anchors in Eq. <ref type="formula" target="#formula_0">(1)</ref> is contrastive, i. e. it is measured relative to all other anchors. As a result, minimising such distance would not only encourage an increased cosine similarity of the features to the anchors, but also a decreased cosine similarity between the anchors themselves. Recall that the anchors are a subset of features sampled spatially from a randomly selected video frame on a uniform grid. Consequently, our loss learns to discriminate between spatially distinct features.</p><p>Temporal coherence (Assumption 2). In Eq. (1), we compute the affinity of the features to the anchors extracted from multiple videos in the training batch. Note that we select the dominant anchors in Eq. (2) for self-training only from the same video sequence as the feature itself. This implies that (i) the features will be attracted only to the anchors originating from the same video, and (ii) the distance between the anchors and the features from different video sequences will increase by virtue of our contrastive formulation of the affinity. Note that the latter also implements inter-video discrimination, analogous to instance-specific learning from image sets, which was shown to result in class-specific representations [e. g., 8].</p><p>Temporal persistence of semantic content (Assumption 3). Our anchor sampling strategy ensures that all anchors stem from a single video frame, the reference frame. By design of the pseudo labels (cf. Eq. <ref type="formula" target="#formula_1">(2)</ref>), our self-training loss in Eq. <ref type="formula" target="#formula_2">(3)</ref> aligns the representation of the other frames only with the reference originating from the same video.</p><p>Equivariance (Assumption 4). We generate a random similarity transformation to synthesise the input to the regularising branch. The task of video object segmentation (VOS), studied here, is equivariant under this family of transformations: flipping or scaling the image should result in a corresponding change in the segmentation output. Our cross-view consistency loss in Eq. (5) explicitly facilitates this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>Preventing degenerate solutions. In our preliminary experiments without the regularising branch and Assumption 4, we observed that the model would rapidly converge to a trivial solution, previously also reported by Jabri et al. <ref type="bibr" target="#b16">[17]</ref>. Our investigation suggested that the network was encoding positional cues into a degenerate feature representation. We hypothesise that such solutions may be a consequence of the limited spatial invariance in contemporary CNN implementations <ref type="bibr" target="#b1">[2]</ref> and the widely adopted padding. The padding type is, in fact, irrelevant, since any deterministic padding strategy provides a stable and predictable pattern. We also found it necessary to spatially jitter the grid for sampling the anchors, rather than to maintain a fixed offset, to prevent further shortcut solutions.</p><p>Computational scalability. Using a grid sampling strategy to extract anchors allows for considerable computational savings. For example, if h ? w = 32 ? 32, and we use a grid of size N ?N = 8 ? 8, this will have a computational and memory saving factor of 16. This saving becomes even more valuable if we scale up the storage costs of the affinities v andv, which have the size of B ? T ? h ? w ? B ? N 2 , w. r. t. B and T . Improving the computational footprint with such subsampling is non-trivial in MAST <ref type="bibr" target="#b18">[19]</ref> due to the use of the dense reconstruction loss, and in CRW <ref type="bibr" target="#b16">[17]</ref> as it extracts image patches with 50% overlap, hence propagates the same pixels through the network up to three times.</p><p>Practical considerations. The order of the frames from a sequence in the batch is irrelevant and can be randomly selected, since we do not require any assumptions about motion continuity. Further, we do not use any momentum network or queue buffers, e. g. such as in <ref type="bibr" target="#b13">[14]</ref>. Our training is surprisingly stable without any of them. Further, we only use similarity transformations to augment the training data, but no appearance-based augmentations such as photometric noise <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>. Instead, we rely on learning natural changes in the appearance directly from video data.</p><p>Comparison to Caron et al. <ref type="bibr" target="#b4">[5]</ref>. In contrast to <ref type="bibr" target="#b4">[5]</ref>, the representation of our clusters (anchors) is non-parametric. Specifically, we do not learn a global set of cluster features shared by the complete video dataset. The advantage is that we neither need to set the number of clusters a priori, nor to specify a prior on the cluster assignment. Comparison to CRW <ref type="bibr" target="#b16">[17]</ref>. The implementation of our approach is simpler, since the frame ordering within the batch is irrelevant, and more computationally efficient, as we compute feature affinities in parallel rather than sequentially (see <ref type="table" target="#tab_5">Table 3</ref> for detail). We also use a weaker assumption on semantic persistence (Assumption 3), which can be advantageous in video sequences with occlusions. Additionally, we learn to discriminate features between different videos, which is not part of CRW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the learned feature representations, we conduct experiments in the setting of semisupervised VOS. The task provides a set of segmentation masks for the first frame in a video sequence and requires the evaluated algorithm to densely track the demarcated objects in the remaining frames. We largely follow the VOS setup of Jabri et al. <ref type="bibr" target="#b16">[17]</ref> and evaluate our method on DAVIS-2017 <ref type="bibr" target="#b34">[35]</ref>. Following Lai et al. <ref type="bibr" target="#b18">[19]</ref>, we additionally test our approach on the YouTube-VOS val by submitting our results to an evaluation server <ref type="bibr" target="#b41">[42]</ref>.</p><p>Implementation details. Similarly to Jabri et al. <ref type="bibr" target="#b16">[17]</ref>, we use ResNet-18 as the backbone network for our feature extractor. We evaluate the correspondences using the output of the fourth (the last) residual block. To obtain the output stride of 8 in our feature extractor, we remove the strides of the last two residual blocks, as in previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>. At training time, we first scale the video frames such that the lowest side is between 256 and 320 pixels, and extract random crops of size 256 ? 256. We train our network with Adam and the learning rate 10 ?4 on the smaller YouTube-VOS and OxUvA <ref type="bibr" target="#b34">[35]</ref>, whereas we found SGD with the learning rate 10 ?3 to work better on the larger Kinetics-400 and TrackingNet datasets. We set the temperature ? = 0.05 throughout our experiments; we observed its influence on the accuracy to not be significant. The hyperparameter ?, trading off the influence of the cross-view consistency, equals 0.1 by default, and we empirically study its role in Sec. 4.3. We train our models on one A100 GPU, although training our most accurate configuration of the framework requires only 12GB of memory, hence a single Titan X GPU is actually sufficient.</p><p>Label propagation. To propagate the semantic labels from the initial ground-truth annotation, we rely on the label propagation algorithm implemented by Jabri et al. <ref type="bibr" target="#b16">[17]</ref>, detailed in Appendix C.</p><p>In particular, for every feature embedding in the current frame, we compute its cosine similarity w. r. t. the features in the previous N T = 20 context frames and the first reference frame. We select N K = 10 feature embeddings with the highest similarity, divide the similarity by ? and compute a softmax to obtain normalised affinity values. The mask prediction for this feature embedding is a convex combination of the mask predictions of its neighbours, weighted by this affinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on DAVIS-2017</head><p>To evaluate on DAVIS-2017 <ref type="bibr" target="#b28">[29]</ref> val, we independently train our feature extractor on 4 datasets. The OxUvA dataset <ref type="bibr" target="#b34">[35]</ref> spans 366 video sequences with a total duration of 14 hours. The second dataset is YouTube-VOS <ref type="bibr" target="#b41">[42]</ref>, which by comparison to OxUvA contains more videos (around 4.5K sequences), but has a shorter overall duration of 5.6 hours. In addition, we train on larger datasets, TrackingNet <ref type="bibr" target="#b25">[26]</ref> and Kinetics-400 <ref type="bibr" target="#b6">[7]</ref>. These datasets contain significantly more video sequences, albeit of lower resolution and quality (e. g., due to compression artefacts). While running the inference on DAVIS-2017 <ref type="bibr" target="#b28">[29]</ref>, we process the frames at a resolution of 480p for fair comparison. <ref type="table" target="#tab_1">Table 1</ref> reports the segmentation accuracy in terms of two types of metrics: The Jaccard's index measures the intersection-over-union (IoU) of the object mask respectively the contour; we report the mean of mask and contour IoU, J m and F m , as well as the recall J r and F r (with an IoU threshold of 0.5) <ref type="bibr" target="#b28">[29]</ref>; J &amp;F m is the average of J m and F m . In a comparable setting where all methods use the same dataset for training, our approach clearly improves over the state-of-the-art accuracy. When trained on OxUvA, our approach outperforms MAST [19] by 1.6% J &amp;F m . This improvement is even more pronounced when trained on YouTube-VOS, where we outperform MAST <ref type="bibr" target="#b18">[19]</ref> by 3.8% in J &amp;F m score. This improvement is especially notable, since MAST <ref type="bibr" target="#b18">[19]</ref> produces its feature embeddings at twice the resolution of our method, hence has a larger memory footprint. These results are consistent when training our approach on larger but less carefully curated datasets. Using TrackingNet, we improve over the previous result of Wang et al. <ref type="bibr" target="#b37">[38]</ref> by 6.4% J &amp;F m . Compared to CRW <ref type="bibr" target="#b16">[17]</ref>, our approach reaches a higher J &amp;F m score by 1.1%. Remarkably, using the smaller YouTube-VOS dataset for training, our method already achieves comparable or higher VOS accuracy compared to previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref> that required significantly larger datasets for training. We also observe that the diversity and the quality of the dataset, i. e. the number of the videos and their resolution, tends to improve the quality of our learned features in terms of the segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on YouTube-VOS</head><p>Following Lai et al. <ref type="bibr" target="#b18">[19]</ref>, we additionally evaluate our features on the YouTube-VOS 2018 valid split. With 474 video sequences containing 91 unique object classes, its diversity represents a significantly more challenging and comprehensive test bed for VOS than DAVIS-2017. We train our model on three datasets as before: YouTube-VOS, TrackingNet, and Kinetics-400. The train set of YouTube-VOS contains only some of the object classes present in valid and the benchmark distinguishes between these as "seen" and "unseen" categories. We submit our results to the official evaluation server to obtain the quantitative metrics J m and F m . YouTube-VOS valid has additional specifics compared to the DAVIS benchmark and previous work tends to use a different label propagation strategy to address them (e. g., the initial object masks may appear in different intermediate frames, rather than only in the first frame at once). Since we use the label propagation from CRW <ref type="bibr" target="#b16">[17]</ref>, we also evaluate this model trained on Kinetics-400 for a fair comparison. Note that the label propagation of MAST <ref type="bibr" target="#b18">[19]</ref> benefits from a larger feature resolution and a more advanced two-stage inference process: first detecting a ROI and then computing feature correlations bounded by the ROI.</p><p>The results in <ref type="table" target="#tab_2">Table 2</ref> show that our approach sets a new state of the art, improving over CRW <ref type="bibr" target="#b16">[17]</ref> by 0.8% mean score. In a comparable setting when our method and CRW use the same training data, Kinetics-400, we improve by 0.7%. With larger training datasets, Kinetics and TrackingNet, we reach higher VOS accuracy compared to training on the smaller YouTube-VOS. Such scalability has not been shown in previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>, where the training dataset was limited to a single instance, or two instances of comparable (small) scale <ref type="bibr" target="#b18">[19]</ref>. Notably, the accuracy from training on YouTube-VOS already matches that of CRW when trained on much more data. Finally, we also outperform fully supervised methods, OSVOS <ref type="bibr" target="#b3">[4]</ref> and PreMVOS <ref type="bibr" target="#b22">[23]</ref>, especially on unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further analysis</head><p>We study (i) the interplay between the number of video sequences B per batch and the number of frames  We compare our models pre-trained on YouTube-VOS, TrackingNet, and Kinetics to previous work. CRW <ref type="bibr" target="#b16">[17]</ref> and Colorize <ref type="bibr" target="#b36">[37]</ref> were trained on Kinetics-400; CorrFlow <ref type="bibr" target="#b17">[18]</ref> and MAST <ref type="bibr" target="#b18">[19]</ref> used OxUvA and YouTube-VOS, respectively. CRW <ref type="bibr" target="#b16">[17]</ref> uses the same label propagation as ours.  Balance between the number of videos and their size. The effective batch size of our method is B ?T , hence depends on the number of videos and their duration. Here, we investigate the balance between the size of the video set T and the number of distinct video sequences B used in a single training batch. We fix B ? T to 80 as the invariant, and evaluate three additional configurations of B and T . Our main observation in <ref type="figure" target="#fig_3">Fig. 3</ref> is that using multiple frames, i. e. ensuring T &gt; 1, is crucial: training with only a single frame per video (T = 1) reduces the quality of learned feature representations significantly, by 9.3% J &amp;F m . In light of Assumption 3, this is expected, as only in a multi-frame setting our model can learn a shared representation of the video set.</p><p>The influence of the cross-view consistency ?. By setting ? = 0, we train our approach without the cross-view consistency (cf. Eq. <ref type="formula" target="#formula_4">(5)</ref>). We find that this results in an accuracy drop of 2.3% J &amp;F m , confirming its accuracy benefits. On the other extreme, we increase its values from 0.1 to 1.0 and find it dominating over the space-time self-training term, hence inhibiting learning of temporal coherence: J &amp;F m decreases by 0.8%. Overall, we confirm that the benefits of data augmentation used via cross-view consistency is only complementary to our self-training. This is in contrast to the pivotal role of data augmentation in prior work [e. g., <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. Our main hypothesis, elaborated in Appendix B.2, is that artificial image transformations do not reflect the transformations occurring in time in video sequences, which is important to establish accurate temporal correspondences.</p><p>Grid resolution. We vary the size of the sampling grid for the anchors N and the cross-view consistency M (Eq. <ref type="formula" target="#formula_4">(5)</ref>). Since a high grid density (i. e. large N and M ) implies an increased computational footprint (cf. Sec. 3.5), we seek to reduce it without compromising VOS accuracy. A low grid resolution may be too coarse in terms of discriminating a sufficient level of detail. Conversely, a high grid resolution may focus the learning more on discriminating between object parts, which may be detrimental to object-level disambiguation. We find that both of these extremes lead to a tangibly lower VOS accuracy, e. g. using N = 4 leads to a drop in J &amp;F m score by 8.2%. Varying M leads to a more moderate decrease in J &amp;F m , by at most 1.5% for M = 8. Regarding the discrepancy between the optimal N = 8 and M = 4, we hypothesise that while a grid size of N = 4 may be sufficient to satisfy our Assumption 1, it may undermine Assumption 3, since a sparser grid may miss feature vectors representing the object of interest, visible in the temporally close frames.</p><p>Sampling more frames T per video. We now fix B = 16 and increase the number of frames per video T . Higher T have the appeal of providing larger degrees of natural transformations. However, larger videos also contain more occlusion, which our method can handle, but also disocclusions, which violate our Assumption 3 on temporal persistence, hence currently problematic. The results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-2017</head><p>YouTube-VOS  <ref type="figure">Figure 4</ref>: Qualitative examples on DAVIS-2017 and YouTube-VOS. The representation learned by our method is robust to occlusions, naturally occurring non-rigid transformations and deals well with object-background disambiguation.</p><p>confirm this expectation: using more frames does not further improve the VOS accuracy. For example, increasing T to 15, we match J &amp;F m of the baseline, yet at additional computational costs. Qualitative examples. <ref type="figure">Fig. 4</ref> shows examples of mask propagation, leveraging our learned feature representation. We observe temporal correspondences to remain stable and accurate despite complex self-occlusions (e. g., torso of dancer), non-rigid deformations, and appearance changes due to shadows (e. g., giraffes). Our method does not exhibit "bleeding" artefacts of MAST <ref type="bibr" target="#b18">[19]</ref> and produces more complete segmentation masks than <ref type="bibr" target="#b16">[17]</ref> despite using less data for training (YouTube-VOS for the DAVIS-2017 example, TrackingNet for the YouTube-VOS result). See Appendix D for more visual results.</p><p>Computational efficiency. <ref type="table" target="#tab_5">Table 3</ref> summarises the computational benefits of our approach w. r. t. previous work. Regardless of the training data, our approach requires less time for convergence, both in terms of the number of training iterations (#) and wall clock duration ?t. This improvement comes with modest memory requirements: to train our framework we require only 12GB of GPU memory, a twofold decrease compared to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a simple and computationally efficient unsupervised method for learning dense spacetime representations from unlabelled videos. Our approach exhibits fast training convergence and compelling data efficiency. We achieve VOS accuracy surpassing previous results despite using only a fractional amount of the training data required in prior work. We recognise the possibility of our research results to be used with malicious intents, such as for unauthorised surveillance. However, as a product of fundamental research with low technology readiness levels of 1 to 3, this is highly unlikely. Our method is yet unable to handle disocclusions. We are excited to explore such capability to improve learning a wider spectrum of invariances by leveraging larger temporal windows in videos containing complex (ego-)motion, where disocclusions are more likely to occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Overview</head><p>This appendix provides further details on training, the label propagation algorithm used at inference time, as well as additional qualitative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training details</head><p>Our feature encoder has a ResNet-18 <ref type="bibr" target="#b45">[46]</ref> architecture consisting of four residual blocks. Similarly to CRW <ref type="bibr" target="#b16">[17]</ref>, we remove the strides in the res3 and res4 blocks. To produce the embeddings, we additionally pass the output from res4 through a multilayer perceptron (MLP). The MLP contains the standard Conv1x1-BatchNorm-ReLU block, which preserves the feature dimensionality (512), followed by a Conv1x1 operation, reducing the feature dimensionality to 128. We experimented with other MLP architectures as well, including replacing Batch Normalisation (BN) <ref type="bibr" target="#b46">[47]</ref> with the Layer Normalisation <ref type="bibr" target="#b42">[43]</ref> layers, but found the effect on the final accuracy insignificant.</p><p>In each training epoch we sample only one video set (of size T , cf. main text) per video in the complete dataset. In total, our training requires 150K ? 300K iterations for convergence (depending on the training data, cf. <ref type="table" target="#tab_5">Table 3)</ref>, which is only a fraction of the training time required by other unsupervised methods (e. g., 2M iterations in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>). The computational footprint per iteration is comparable. As in previous work, we train with an input resolution of 256 ? 256. CRW <ref type="bibr" target="#b16">[17]</ref> samples 20 video clips with the length of 4 yielding B ? T = 20 ? 4 = 80 frames per forward pass, which is equivalent to our setup of using B = 16 video sets T = 5 frames each. MAST <ref type="bibr" target="#b18">[19]</ref> uses a smaller batch size of 24 in the first 1M iterations, but processes frames with the output stride reduced by a factor of 2, hence the forward pass alone increases both the memory and the computational overhead by at least the same factor. For validation and model selection, we use 5 video sequences selected randomly from DAVIS-2017 valid. <ref type="figure" target="#fig_5">Fig. 5</ref> demonstrates the pseudo code of the learning algorithm. We observe that our algorithm shares the simplicity of other unsupervised learning algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref>, and, in contrast to CRW <ref type="bibr" target="#b16">[17]</ref>, learns temporally consistent embeddings without iterative structures. All operations support efficient implementation in popular libraries, such as PyTorch. Note that the space-time loss term does not propagate the gradient to the regularising branch. This allows for memory-efficient implementations by means of always maintaining the regularising branch in "evaluation mode". We achieve this by simply using the main branch to also process the reference frame (one frame per video), for which the gradient is required for the cross-view consistency, and then re-combining the output into the same-sized k andk. As a result, our framework has a reduced memory footprint compared to Siamese architectures <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Data augmentation</head><p>We experimented with rotations and sheering initially, but found their benefits insignificant at the cost of increased implementation complexity. The main disadvantage of these transformations is the need to carefully handle the boundaries: both rotation and sheering require image padding, which needs to be removed post-hoc; otherwise, fully convolutional training would result in a degenerate solution.</p><p>We also experimented with two versions of incorporating appearance-base augmentation (e. g., colour jittering). On one hand, using frame-level augmentation (i. e. different augmentation per frame), we observed a decrease in VOS accuracy. We hypothesise that artificial augmentation techniques, such as sudden changes in contrast, saturation, or scale, poorly reflect natural transformations occurring in real-world video sequences. Using different augmentation per frame may also challenge a meaningful association between the anchors, extracted from one frame, and the features from the other frames, especially at the beginning of training, on which we rely for generating the pseudo labels in selftraining. On the other hand, using video-level augmentation (i. e. the same augmentation for every frame, but different across video clips), we did not observe a significant change in accuracy. This is expected, since the framework would be additionally required to cope with distinguishing visually perturbed video clips (following our implementation of Assumption 2), which does not provide useful information for VOS.   <ref type="figure">Fig. 6</ref> illustrates the label propagation algorithm we use in our experiments. To predict mask m t for the current timestep t, we make use of context embeddings and masks, accumulated from the previous frames. Following <ref type="bibr" target="#b16">[17]</ref>, we use the output from the penultimate residual block res4 to obtain the embedding for frame t, denoted here as h t ? R h,w,K , where h, w are the spatial dimensions and K is the dimension of the feature embedding. An embedding context E t = {h 0 , h t?N T +1 , ..., h t?1 }, |E t | = N T , maintains embedding h 0 of the first frame, 1 which has ground-truth annotations, and the embeddings of N T ? 1 preceding frames. Similarly, we define the mask context as M t = {m * 0 , m t?N T +1 , ..., m t?1 }, where m * 0 is the provided ground-truth annotation for the first frame, and m t&gt;0 are the masks propagated by the algorithm detailed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Inference</head><p>We first compute the cosine similarity of embedding h t (i, j) w. r. t. all embeddings in context E, restricted to a spatio-temporal neighbourhood of size N T ? P ? P , spatially centred on location (i, j), which we denote as N P (i, j). Three coordinates (t * , l, n) for the temporal (first) and the spatial (second and third) dimensions specify the neighbour locations in N P (i, j). Next, we create a new nearest-neighbour set N (t) P (i, j) by selecting N K locations from N P (i, j) with the highest cosine similarity, and compute local attention in a single operation, denoted in <ref type="figure">Fig. 6</ref> as kNN-Softmax, as follows:</p><formula xml:id="formula_6">s (t) i,j (t * , l, n) = ? ? ? ? ? exp ?d (t) i,j (t * ,l,n)/? (t ,l ,n )?N (t) P (i,j) exp ?d (t) i,j (t ,l ,n )/? , if (t * , l, n) ? N (t) P (i, j) 0, otherwise,<label>(7)</label></formula><p>where d (t)</p><p>i,j (t * , l, n) is the cosine similarity between embeddings h t (i, j) and h t * (l, n) from E; and ? is the temperature hyperparameter set to 0.05, as in training (cf. Sec. 4). We compute the mask m t as a weighted sum of the mask predictions at locations N P (i, j) as</p><formula xml:id="formula_7">m t = (t * ,l,n)?N P (i,j) s (t) i,j (t * , l, n) m t * (l, n),<label>(8)</label></formula><p>where m t * (l, n) comes from the mask context M. Finally, we add m t and h t to the mask and embedding contexts, E and M, respectively, and remove the oldest entries (with exception of the first reference frames) to maintain their constant size of N T . We repeat this process for the remaining frames in the video clip. <ref type="figure">Fig. 6b</ref> outlines this algorithm using pseudo code. Note that we initialise M and E by simply replicating the masks and the embeddings of the first reference frame. <ref type="bibr" target="#b0">1</ref> In the case of YouTube-VOS, there may be multiple reference frames corresponding to objects appearing mid-sequence. Here, we handle the case of the first frame specifying all objects at once as an illustrative example.  <ref type="figure">Figure 6</ref>: Label propagation. We use the spatial correlation operator C(?, ?) to implement a local attention operation by computing cosine similarities between the embedding at location (i, j) at timestep t and the embeddings in context E considering only the spatial neighbourhood P ? P of the (i, j). Subsequently, we use local guided filtering operator F(?, ?) to propagate the masks from context M using the computed local attention s t i,j . Example (a) illustrates this process for P = 3 and context size N T = 3. Pseudo code (b) provides a general outline of the label propagation algorithm. We refer to the text for more details.</p><p>For consistency we use the label propagation implementation provided by Jabri et al. <ref type="bibr" target="#b16">[17]</ref>. The first operator C(?, ?) is a local spatial correlation operation commonly used in correlation layers of optical flow networks <ref type="bibr" target="#b48">[49]</ref>. The second operator F(?, ?) implementing Eq. <ref type="formula" target="#formula_7">(8)</ref> is a variant of local guided filtering <ref type="bibr" target="#b44">[45]</ref>, also used in pixel-adaptive convolutional networks <ref type="bibr" target="#b47">[48]</ref>. We set P = 25 and N K = 10 in our experiments with DAVIS-2017 <ref type="bibr" target="#b34">[35]</ref> and YouTube-VOS <ref type="bibr" target="#b41">[42]</ref>. We use a context size of N T = 20 for DAVIS-2017. For YouTube-VOS, where instances may appear in intermediate frames, we use a separate context for each object ID. We use the original resolution of both DAVIS-2017 and YouTube-VOS benchmarks, which is downscaled by a factor of 8 in our embedding by the feature extractor. We upsample the final object masks to the original resolution with bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative examples</head><p>We include videos demonstrating the qualitative results in the supplementary material. <ref type="bibr" target="#b1">2</ref> The segmentation results produced by our method show clear improvements over CRW <ref type="bibr" target="#b16">[17]</ref>, despite using only a fractional amount of time and data for training. Our model used for producing DAVIS-2017 examples was trained for 150K iterations on only 4.5K videos with a total duration of 5 hours, while the CRW [17] model was trained for 2M iterations on Kinetics-400 containing 300K videos spanning 833 hours. On the YouTube-VOS benchmark, we show the results from our model trained for 200K iterations on TrackingNet, which is still about 10 times smaller than Kinetics-400. Both methods tend to produce decent segmentation quality despite self-occlusions and complex transformations in the videos. By contrast, MAST tends to produce masks of poorer quality, especially when there is ambiguity in the appearance of the foreground object and the background. Nevertheless, both our approach and CRW may struggle in videos with large spatial displacements of the tracked object. We hypothesise that this may be in part due to the spatial bias introduced by the memory context (discussed in Appendix C), which considers only a spatially local neighbourhood for mask propagation. We also observe that our approach clearly surpasses a surprisingly strong baseline model with random weight initialisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E License note</head><p>The parts of the code we use from Jabri et al. <ref type="bibr" target="#b16">[17]</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview. Our framework (a) consists of a single CNN-based feature extractor that processes video sequences in the main branch. The second regularising branch takes a transformed version (random cropping and flipping) of the videos as input and produces pseudo labels? w. r. t. the anchors q extracted from the features of the main branch. The self-training loss L ST learns space-time embeddings implementing our Assumptions 1-3 and, implicitly, Assumption 4 on feature equivariance. The cross-view consistency loss L CV further facilitates the latter explicitly. To sample the anchors (b), we first randomly select one frame per video sequence in the current batch (Step 1), and then sample the features at random on a uniform grid (here, with size 3 ? 3) in Step 2 to obtain q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>T per video; (ii) the influence of the cross-view loss term via the trade-off hyperparameter ?; (iii) the framework's sensitivity to the size of the sampling grid of the anchors N and (iv) cross-view features M ; (v) increasing the temporal window of the video clips in training. We conduct all experiments by training the corresponding model on YouTube-VOS. Fig. 3 visualises the results in terms of the J &amp;F m score on DAVIS-2017 (val). Our baseline configuration, depicted as the centre node, corresponds to the default setting of B = 16, T = 5, ? = 0.1, N = 8, M = 4. Every edge of the same style and colour represents a change of selected hyperparameters, while keeping the default values for the rest. To put the significance of the differences into perspective, we also compute the standard deviation of the J &amp;F m score on DAVIS-2017 across 5 training runs, amounting to ?0.42.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Ablation study. We report J &amp;F m on DAVIS-2017 (val) by varying the hyperparameters of our baseline configuration (centre): B = 16 (no. of videos in a batch), T = 5 (no. frames per video), ? = 0.1 (cross-view tradeoff), N = 8 (anchor grid size), M = 4 (crossview grid size). See Sec. 4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>i 1 # 4 # x_hat : cropped and flipped x 5 # 8 # 11 L 12 L 23 # 27 v 30 # 37 #</head><label>1458111223273037</label><figDesc>Main training loop 2 # B : batch size ; T video length ; 3 for x in loader : T : similarity transform 6 x_hat , T = random_transform ( x ) 7 feature embeddings [ BTxKxHxW ] 9 k , k_hat = net ( x ) , net ( x_hat ) 10 ST = space_time_loss (k , k_hat ,T ,N ) CV = cross_view_loss (k , k_hat ,T ,T ,M ) 13 loss = L ST + ? L CV function 19 def affinity (x ,y ,? ) 20 z = einsum ( " nk , bkhw -&gt; bnhw " , x , y ) 21 return softmax ( z / ? ,1) (a) Main loop 22 def space_time_loss (k , k_hat , T , N ) : = affinity (q , k , ? ) 28 v_hat = affinity (q , k_hat , ? ) 29 compute pseudo labels 31 u_hat = ( v_hat * block_ones ) . argmax (1) 32 loss = cross_entropy (T ( v ) , u_hat ) 33 loss [:: T ]. zero_ () 34 return loss . mean () 35 36 def cross_view_loss (k , k_hat , T , T , M ) : subsampling features [ BMMxK ] 38 r = grid_sample ( T ( k [:: T ]) , M ) 39 r_hat = grid_sample ( k_hat [:: T ] , M ) 40 # affinity [ BMMxBMM ] 41 h = softmax ( mm (r , r_hat . t () ) / ? , 1) 42 return -log ( diag ( h ) ) . mean () (b) Loss functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Pseudo code implementation of our method. block_ones is a BT ? BN 2 block-diagonal matrix containing B blocks of all-ones matrices of size T ? N 2 . diag selects diagonal elements from a matrix. mm denotes matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5 # local spatial correlation 6 dt 7 # 9 # 11 # updating context 12 E</head><label>56791112</label><figDesc>E and M 2 # from the first frame 3 for t , frame in enumerate ( frames ) : 4 ht = net ( frame ) = C (E ,ht ) softmax b / w N K nearest neighbours 8 st = knn_softmax (dt ,N K ,? ) mask prediction for timestep t 10 mt = F (M,st ) ,M = add (E ,ht ) , add (M,mt ) (b) Pseudo code of label propagation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(the label propagation algorithm) are released under a MIT license. The datasets YouTube-VOS, OxUvA, TrackingNet, and Kinetics-400 are licensed under the Creative Commons Attribution 4.0 International License, while DAVIS-2017 is provided under the Creative Commons Attribution-NonCommercial 4.0 International License.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Video object segmentation quality on DAVIS-2017 validation in terms of the mask (J ) and boundary (F) accuracy (IoU). The subscript [?] r denotes the recall of the metric, while [?] m signifies the mean. ?2 denotes an output stride of 4 instead of 8, i. e. the spatial feature resolution is twice as large. # / t details the number of videos / the total dataset duration in hours.</figDesc><table><row><cell>Method</cell><cell cols="2">?2 Train Data</cell><cell># / t</cell><cell cols="5">Jm Jr Fm Fr J &amp;F m</cell></row><row><cell>Baseline (random initialisation)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.1</cell></row><row><cell>CorrFlow [18]</cell><cell>-</cell><cell></cell><cell></cell><cell cols="5">48.4 53.2 52.2 56.0 50.3</cell></row><row><cell>MAST [19]</cell><cell></cell><cell>OxUvA</cell><cell>366 / 14</cell><cell cols="5">61.2 73.2 66.3 78.3 63.7</cell></row><row><cell>Ours</cell><cell>-</cell><cell></cell><cell></cell><cell cols="5">63.4 76.1 67.2 79.7 65.3</cell></row><row><cell>MAST [19] Ours</cell><cell>-</cell><cell>YT-VOS</cell><cell>4.5K / 5</cell><cell cols="5">63.3 73.2 67.6 77.7 65.5 67.1 81.2 71.6 84.9 69.3</cell></row><row><cell>ContrastCorr [38] Ours</cell><cell cols="3">-TrackingNet 30K / 140 -</cell><cell cols="5">60.5 -65.5 -67.1 80.9 71.7 84.8 69.4 63.0</cell></row><row><cell>CRW [17] Ours</cell><cell cols="3">-Kinetics-400 300K / 833 -</cell><cell cols="5">64.8 76.1 70.2 82.1 67.6 66.7 81.4 70.7 84.1 68.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on YouTube-VOS 2018 (val).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Computational efficiency.</figDesc><table><row><cell>Method</cell><cell>#</cell><cell>?t</cell><cell>Mem</cell></row><row><cell>ContrastCorr [38]</cell><cell cols="3">0.4M 1d 44GB</cell></row><row><cell>Ours-TrackingNet</cell><cell cols="3">0.2M 1d 12GB</cell></row><row><cell>CRW [17]</cell><cell>2M</cell><cell cols="2">7d 22GB</cell></row><row><cell>Ours-Kinetics-400</cell><cell cols="3">0.3M 2d 12GB</cell></row><row><cell>MAST [19]</cell><cell>2M</cell><cell cols="2">-22GB</cell></row><row><cell cols="4">Ours-YouTube-VOS 0.1M 6h 12GB</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://youtu.be/BqVtZJSLOzg iii</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and disclosure of funding</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon2020 research and innovation programme (grant agreement No. 866008). This work has also been co-funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised augmentation consistency for adapting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15384" to="15394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Why do deep convolutional networks generalize so poorly to small image transformations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<idno>184:1-184:25</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Can temporal information help with contrastive self-supervised learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13046</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5320" to="5329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2020</title>
		<imprint>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07974</idno>
		<title level="m">TCLR: Temporal contrastive learning for video representation</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2020</title>
		<imprint>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2020</title>
		<imprint>
			<biblScope unit="page" from="5679" to="5690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SCOPS: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="869" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Space-time correspondence as a contrastive random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2020</title>
		<imprint>
			<biblScope unit="page" from="19545" to="19560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning for correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MAST: A memory-augmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6478" to="6487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint-task self-supervised learning for temporal correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2019</title>
		<imprint>
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PReMVOS: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11364</biblScope>
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7251" to="7259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6706" to="6716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TrackingNet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="310" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9225" to="9234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6024" to="6033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04621</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS*2020</title>
		<imprint>
			<biblScope unit="page" from="4489" to="4500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Self-supervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">RAFT: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long-term tracking in the wild: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="692" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748[cs.LG]</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tracking emerges by colorizing videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contrastive transformation for self-supervised correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10174" to="10182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12272" to="12281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning from flow equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10191" to="10200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">YouTube-VOS: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pixel-adaptive convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11166" to="11175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<editor>M. Meila and T. Zhang</editor>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
