<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Projective 3D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?remo</forename><surname>Felix</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lawin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fahad</roleName><forename type="first">Shahbaz</forename><surname>Bhat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Projective 3D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Point clouds</term>
					<term>semantic segmentation</term>
					<term>deep learning</term>
					<term>scanning arti- facts</term>
					<term>hard scape</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of 3D point clouds is a challenging problem with numerous real-world applications. While deep learning has revolutionized the field of image semantic segmentation, its impact on point cloud data has been limited so far. Recent attempts, based on 3D deep learning approaches (3D-CNNs), have achieved below-expected results. Such methods require voxelizations of the underlying point cloud data, leading to decreased spatial resolution and increased memory consumption. Additionally, 3D-CNNs greatly suffer from the limited availability of annotated datasets. In this paper, we propose an alternative framework that avoids the limitations of 3D-CNNs. Instead of directly solving the problem in 3D, we first project the point cloud onto a set of synthetic 2D-images. These images are then used as input to a 2D-CNN, designed for semantic segmentation. Finally, the obtained prediction scores are re-projected to the point cloud to obtain the segmentation results. We further investigate the impact of multiple modalities, such as color, depth and surface normals, in a multi-stream network architecture. Experiments are performed on the recent Semantic3D dataset. Our approach sets a new stateof-the-art by achieving a relative gain of 7.9%, compared to the previous best approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid development of 3D acquisition sensors, such as LIDARs and RGB-D cameras, has lead to an increased demand for automatic analysis of 3D point clouds. In particular, the ability to automatically categorize each point into a set of semantic labels, known as semantic point cloud segmentation, has numerous applications such as scene understanding and robotics. While the problem of semantic segmentation of 2D-images has gained a considerable amount of attention in recent years, semantic segmentation of point clouds has received little interest despite its significance. In this paper, we propose a framework for semantic segmentation of point clouds that greatly benefits from the recent developments in semantic image segmentation.</p><p>With the advent of deep learning, many tasks within computer vision have seen a rapid progress, including semantic segmentation of images. The key factors for this development are the introductions of large labeled datasets <ref type="bibr" target="#b1">[2]</ref> and GPU implementations of Convolutional Neural Networks (CNNs). However, CNNs have not yet been successfully applied for semantic segmentation of 3D point clouds due to several challenges.</p><p>In contrast to the regular grid-structure of image data, point clouds are in general sparse and unstructured. A common strategy is to resort to voxelization in order to directly apply CNNs in 3D. This introduces a radical increase in memory consumption and leads to a decrease in resolution. Additionally, labeled 3D data, which is crucial for training CNNs, is scarce due to difficulties in data annotation.</p><p>In this work, we investigate an alternative approach that avoids the aforementioned difficulties induced by 3D CNNs. As our first contribution, we propose a framework for 3D semantic segmentation that exploits the advantages of deep image segmentation approaches. The point cloud is first projected onto a set of synthetic images, which are then used as input to the deep network. The resulting pixel-wise segmentation scores are re-projected into the point cloud. The semantic label for each point is then obtained by fusing scores over the different views. As our second contribution, we investigate the impact of different input modalities, such as color, depth and surface normals, extracted from the point cloud. These modalities are fused in a multi-stream network architecture to obtain the final prediction scores.</p><p>Compared to semantic segmentation methods based on 3D CNNs <ref type="bibr" target="#b17">[17]</ref>, our approach has two major advantages. Firstly, our method benefits from the abundance of the already existing data sets for image segmentation and classification, such as ImageNet <ref type="bibr" target="#b1">[2]</ref> and ADE20K <ref type="bibr" target="#b29">[28]</ref>. This significantly reduces, or even eliminates the need of 3D data for training purposes. Secondly, by avoiding the large memory complexity induced by voxelization, our method achieves a higher spatial resolution which enables better segmentation quality.</p><p>We perform qualitative and quantitative experiments on the recently introduced Se-mantic3D dataset <ref type="bibr" target="#b6">[6]</ref>. We show that different modalities contain complementary information and their fusion significantly improves the final segmentation performance. Further, our approach sets a new state-of-the-art performance on the Semantic3D dataset, outperforming both classical machine learning methods and 3D-CNN based approaches. <ref type="figure" target="#fig_1">Figure 4</ref> shows an example segmentation result using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The task of semantic point cloud segmentation has received an increasing amount of attention due to the rapid development of sensors capable of capturing high-quality 3D data. RGB-D cameras, such as the Microsoft Kinect, have become popular for robotics and computer vision tasks. While RGB-D cameras are more suitable for indoors environments, terrestrial laser scanners capture large-scale point clouds for both indoors and outdoors applications. Both RGB-D cameras and modern laser scanners are capable of capturing color in association with the 3D information using calibrated RGB cameras. Besides visualization, this additional information is highly useful for automated analysis and processing of point clouds. While color is not a necessity for our approach, it alleviates the task of semantic segmentation and enables the use of large-scale image datasets.</p><p>Most previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b13">13]</ref> in 3D semantic segmentation apply a combination of (i) hand-crafted features, (ii) discriminative classifiers and (iii) spatial smoothness models. In this setting, the construction of discriminative 3D-features (i) is ar-guably the most important task. Popular alternatives include features based on the 3D structure tensor <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b0">1]</ref>, histogram-based descriptors <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b11">11]</ref> such as Spin Images <ref type="bibr" target="#b10">[10]</ref> and SHOT <ref type="bibr" target="#b22">[21]</ref>, and simple color features <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b11">11]</ref>. The classifiers (ii) are often based on maximum margin methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b0">1]</ref> or employ random forests <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b16">16]</ref>. To utilize spatial correlation between semantic labels (iii), many methods apply graphical models, such as the Conditional Random Field (CRF) <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">13]</ref>.</p><p>Recently, deep convolutional neural networks (CNNs) have been successfully applied for semantic segmentation of 2D images <ref type="bibr" target="#b15">[15]</ref>. Their main strength is the ability to learn high-level discriminative features, which eliminates the need of hand-designed representations. The rapid progress of deep CNNs for a variety of computer vision problems is generally attributed to the introduction of large-scale datasets, such as ImageNet <ref type="bibr" target="#b1">[2]</ref>, and improved performance for GPU computing.</p><p>Despite its success for image data, the application of CNNs to 3D point cloud data <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b28">27]</ref> have been severely hindered due to several important factors. Firstly, a point cloud does not have the neighborhood structure of an image. The data is instead sparse and scattered. As a consequence, CNN-based methods resort to voxelization strategies of the underlying point cloud data to enable 3D-convolutions to be performed (3D-CNNs). Secondly, voxelization have several disadvantages, including loss of spatial resolution and large memory requirements. 3D-CNNs are therefore restricted to small volumetric models or processing data in many smaller chunks, which limits the use of context. Thirdly, annotated 3D data is extremely limited, especially for the 3D semantic segmentation task. This greatly limits the power of CNNs for semantic segmentation of generic 3D point clouds.</p><p>In contrast, our approach avoids these short comings by projecting the point cloud into dense 2D image representations, thus removing the need for voxelizations. The 2D images can then be efficiently processed using 2D convolutions. Also, performing segmentation in image space allows us to leverage well developed 2D segmentation techniques as well as large amount of annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we present our method for point cloud segmentation. The input is an unstructured point cloud and the objective is to assign a semantic label to each point. In our method we render the point cloud from different views by projecting the points into synthetic images. We render color, depth and other attributes extracted from the point cloud. The images are then processed by a CNN for image-based semantic segmentation, providing a prediction scores for the predefined classes in every pixel. We make the final class selection from the aggregated prediction scores, using all images where the particular points are visible. An overview of the method is illustrated in <ref type="figure">Figure 1</ref>. A more detailed description is provided in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Render views</head><p>The objective of the point cloud rendering is to produce structured 2D-images that are used as input to a CNN-based semantic segmentation algorithm. A variety of information stemming from the point cloud can be projected onto the synthetic images. In <ref type="figure">Fig. 1</ref>: An overview of the proposed method. The input point cloud is projected into multiple virtual camera views, generating 2D color, depth and surface normal images. The images for each view are processed by a multi-stream CNN for semantic segmentation. The output prediction scores from all views are fused into a single prediction for each point, resulting in a 3D semantic segmentation of the point cloud.</p><p>this work we particularly investigate the use of depth, color, and normals. However, the approach can be trivially extended to other features such as HHA <ref type="bibr" target="#b5">[5]</ref> and other local information extracted from the point cloud. In order to map the semantic information back to the 3D points, we also need to keep track of the visibility of the projected points.</p><p>Our choice of rendering technique is a variant of point splatting <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b30">29]</ref>, where the points are projected with a spread function into the image plane. While other rendering techniques, such as surface reconstruction as in <ref type="bibr" target="#b12">[12]</ref>, require demanding preprocessing steps of the point cloud in 3D space, splatting could be completely processed in image space. This further enables efficient and easily parallelizable implementations, which is essential for large-scale or dense point clouds.</p><p>Splatting-based rendering is performed by first projecting each 3D-point x i of the point cloud into the image coordinates y i of a virtual camera. The projected points are stored along with their corresponding depth values z i and feature vectors c i . The latter can include, e.g., the RGB-color and normal vector of the point x i . The projection of a 3D-point is distributed by a Gaussian point spread function in the image plane,</p><formula xml:id="formula_0">w i,j = G(y i ? p j , ? 2 ) .<label>(1)</label></formula><p>Here, w i,j is the contributed weight of point x i to pixel j in the projected image. It is obtained by evaluating an isotropic Gaussian kernel G with scale ? 2 at the pixel location p j . In order to reduce computational complexity, the kernel is truncated at a distance r. However, point spread functions, which originate from different surfaces, may still intersect in the image plane. Thus, the visibility of the projected points needs to be determined to avoid contributions of occluded surfaces. Moreover, the sensor data may contain significant foreground noise, such as scanning artifacts, which complicates this task. The challenge is to exclude the contribution from the noise and the occluded surfaces in the rendering process. In traditional splatting <ref type="bibr" target="#b30">[29]</ref>, the resulting pixel value is obtained from the weighted average of the point spread functions in an accumulated fashion, using the weights w i,j . If the depth of a new point significantly differs from the current weighted average, the pixel depth is either re-initialized with the new value if the point is closer than a specific threshold, or discarded if it is further away <ref type="bibr" target="#b30">[29]</ref>. However, this implies that the resulting pixel value depends on both the threshold value and the order in which the points are projected. Furthermore, noise in the foreground will have significant impact on the resulting images, as it is always rendered.</p><p>Similar to the method proposed in <ref type="bibr" target="#b19">[19]</ref>, we perform mean-shift clustering <ref type="bibr" target="#b25">[24]</ref> of the projected points in each pixel with respect to the depth z i weighted with w i,j using a Gaussian kernel density estimator G(d, s 2 ), where s 2 denotes the kernel width. Starting from the depth value d 0 i = z i for each point i ? I j that contributes to the current pixel j, I j = {i : p j ? y i &lt; r}, the following expression is iterated until convergence</p><formula xml:id="formula_1">d n+1 i = i?Ij w i,j G(d n i ? z i , s 2 )z i i?Ij w i,j G(d n ? z i , s 2 )</formula><p>.</p><p>(</p><p>The iterative process determines a set of unique cluster centers {d k } K 1 from the converged iterates {d N i } i?Ij . The kernel density of cluster center d k is given by,</p><formula xml:id="formula_3">v k = i?Ij w i,j G(d k ? z i , s 2 ) i?Ij w i,j .<label>(3)</label></formula><p>We rank the clusters with respect to the kernel density estimates and the cluster centers,</p><formula xml:id="formula_4">s k = v k + D d k .<label>(4)</label></formula><p>Here, the weight D rewards clusters that are near the camera. It is set such that foreground noise and occluded points are not rendered. We chose the optimal cluster as k = arg max k s k and set the depth value of pixel j to the corresponding cluster center dk. The feature value is calculated as the weighted average, where the weight is determined by the proximity to the chosen cluster,</p><formula xml:id="formula_5">ck = i?Ij w i,j G(dk ? z i , s 2 )c i i?Ij w i,j G(dk ? z i , s 2 ) .<label>(5)</label></formula><p>Since the indices i ? I j of the contributing points i are stored, it is trivial to map the semantic segmentation scores produced by the CNN back to the point cloud itself. An example of the rendering output is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Multi Stream Image Segmentation</head><p>Following the current success of deep learning algorithms we deploy a CNN-based algorithm for performing semantic segmentation on the rendered images. We consider using multiple input modalities, which are combined using a multi-stream architecture <ref type="bibr" target="#b24">[23]</ref>. The predictions from the streams are fused in a sum layer, as proposed in <ref type="bibr" target="#b4">[4]</ref>. The full multi stream network can thus be trained end-to-end. However, note that our pipeline is agnostic to the applied image semantic segmentation approach. In our method, each stream is processed using a Fully Convolutional Network (FCN) <ref type="bibr" target="#b15">[15]</ref>. However, as previously mentioned, any CNN architecture can be employed. The FCN is based on the popular VGG16 network <ref type="bibr" target="#b23">[22]</ref>. The weights in each stream are initialized by pre-training on the ImageNet dataset <ref type="bibr" target="#b1">[2]</ref>. In this work, we investigate different combinations of input streams, namely color, depth, and surface normals. While <ref type="figure">Fig. 3</ref>: Illustration of the proposed multi-stream architecture for 2D semantic segmentation. Each input stream is processed by a Fully Convolutional Network <ref type="bibr" target="#b15">[15]</ref>. The prediction scores from each stream are summed to get the final prediction.</p><p>the RGB-stream naturally benefits from pre-training on ImageNet, this is also the case for the depth stream. Previous work <ref type="bibr" target="#b2">[3]</ref> has shown that a 3-channel jet colormap representation of the depth image better benefits from pre-training on RGB datasets, such as ImageNet. Finally, we also consider surface normals as input to a separate network stream. For this purpose, we deploy an efficient algorithm for approximate normals computation, which is based on direct differentiation of the depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Score fusion</head><p>The deep network outputs a prediction score for each class for every pixel in the image. The scores from each rendered view are mapped to the corresponding 3D points using the indices i ? I j as described in section 3.1. We fuse the scores by computing the sum over all projections. Finally, the points are assigned the labels corresponding to the largest sum of scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We conduct our experiments on the dataset Semantic3D <ref type="bibr" target="#b6">[6]</ref>, which provides a set of large scale 3D point clouds of outdoor environments. The point clouds were acquired by a laser scanner and include both urban and rural scenes. Colorization was performed using a cube map generated from a set of high-resolution camera images. In total, the dataset contains 30 separate scans and over 4 billion 3D-points. The points are labeled with 8 different semantic classes: man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hard scape, scanning artifacts, and cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>View selection In order to fully cover the point clouds in the rendered views, we collect images by rotating the camera 360 ? around a fix vertical axes. For each 360 ? rotation, we use 30 camera views at equally spaced angles. For each point cloud, we generate four such scans with different pitch angles and translations of the camera, resulting in a total of 120 camera views. To maintain a certain amount of contextual information, we remove images where where more than 10% of the pixels have a depth less than five meters. Furthermore, images with less than 5% coverage were discarded.</p><p>Network setup and training For the training we generated ground truth label images by selecting the most commonly occurring label in the optimal cluster from section 3.1. An example is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In addition to the 8 provided classes, we also included a 9th background class to label empty pixels, i.e pixels without any intersecting point spread functions. We generated training data from the training set provided by Semantic3D <ref type="bibr" target="#b6">[6]</ref>, consisting of 15 point clouds from different scenes. Our training data set consists of 3132 labeled images including color, jet visualization of the depth, and surface normals.</p><p>We investigate the proposed multi stream approach using color, depth and surface normals streams as input. In order to determine the contribution of each input stream we also evaluate network configurations with a single stream. Since some point clouds may not have color information we also investigate a multi stream approach without the color stream. All network configurations are listed in table 1. All network configurations were trained using the same training parameters. We trained for 45 epochs with a batch size of 16. The initial learning rate was set to 0.0001 and divided by two every tenth epoch. Following the recommendations from <ref type="bibr" target="#b14">[14]</ref>, we used a momentum of 0.99. The networks were trained using MatConvNet <ref type="bibr" target="#b26">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussions</head><p>We evaluated our method for the different network configurations on the reduced test set provided by Semantic3D. The test set consists of four point clouds, containing 80 million points in total. All points are assigned a class label j, which is compared to the ground truth label i. A confusion matrix C is constructed, were each entry c ij denotes the number of points with the ground truth label i that are assigned the label j. The quantitative measure provided by the benchmark <ref type="bibr" target="#b6">[6]</ref> is the intersection over union for each class i, given by</p><formula xml:id="formula_6">IoU i = c ii c ii + j =i c ij + k =i c kj .<label>(6)</label></formula><p>The over all accuracy is also provided and is given by</p><formula xml:id="formula_7">IoU = i c ii j jk c jk .<label>(7)</label></formula><p>The evaluation results are shown in table 2. The single-stream network with RGB and surface normals as input performs significantly better than the single-stream depth network. However, the three streams seem to provide complementary information, and give a significant gain in performance when used together. Our best multi-stream approach significantly improves over the previous state-of-the art method <ref type="bibr" target="#b8">[8]</ref>. Also our multi-stream approach without the color stream obtains results comparable to the previous state-of-the, showing that our method is applicable even if color information is absent. Interestingly, even our single-stream approaches with only RGB or surface normals as input achieves a remarkable gain compared to the 3D-CNN based VoxNet <ref type="bibr" target="#b6">[6]</ref>. <ref type="figure" target="#fig_1">Figure 4</ref> shows some qualitative results on the test set using our multi-stream RBG+D+N network.</p><p>Note that we are using a simple heuristic for generating camera views, and a basic segmentation network trained on limited data. Yet, we obtain very promising results. Replacing these blocks with better alternatives should improve the results even further. However, this is outside the scope of this paper. <ref type="table">Table 2</ref>: Benchmark results on the reduced test set in Semantic3D <ref type="bibr" target="#b6">[6]</ref>. IoU for categories (1) man-made terrain, (2) natural terrain, (3) high vegetation, (4) low vegetation, <ref type="bibr" target="#b5">(5)</ref> buildings, (6) hard scape, <ref type="bibr" target="#b7">(7)</ref> scanning artefacts, (8) cars.</p><p>Avg IoU OA IoU1 IoU2 IoU3 IoU4 IoU5 IoU6 IoU7 IoU8 TML-PCR <ref type="bibr" target="#b18">[18]</ref> 0.384 0.740 0.726 0.730 0.485 0.224 0.707 0.050 0.000 0.150 DeepNet <ref type="bibr" target="#b6">[6]</ref> 0.437 0.772 0.838 0.385 0.548 0.085 0.841 0.151 0.223 0.423 TLMC-MSR <ref type="bibr" target="#b8">[8]</ref> 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose an approach for semantic segmentation of 3D point clouds that avoids the limitations of 3D-CNNs. Our approach first projects the point cloud onto a set of synthetic 2D-images. The corresponding images are then used as input to a 2D-CNN for semantic segmentation. Consequently, the segmentation results are obtained by reprojecting the prediction scores to the point cloud. We further investigate the impact of multiple modalities in a multi-stream deep network architecture. Experiments are performed on the Semantic3D dataset. Our approach outperforms existing methods and sets a new state-of-the-art on this dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Example of rendering output. Left: color image. Right: label image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results. Top: input point clouds. Bottom: Segmentation output using our proposed RGB+D+N network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.542 0.862 0.898 0.745 0.537 0.268 0.888 0.189 0.364 0.447 Ours RGB 0.515 0.854 0.759 0.791 0.720 0.335 0.857 0.209 0.123 0.326 Ours D 0.262 0.662 0.281 0.468 0.395 0.179 0.763 0.006 0.001 0.000 Ours N 0.511 0.846 0.815 0.622 0.679 0.164 0.903 0.251 0.186 0.470 Ours RGB+D+N 0.585 0.889 0.856 0.832 0.742 0.324 0.897 0.185 0.251 0.592 Ours D+N 0.543 0.872 0.839 0.736 0.717 0.210 0.909 0.153 0.204 0.574</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network configurations with input streams in the left column</figDesc><table><row><cell></cell><cell cols="3">RGB D N RGB+D+N D+N</cell></row><row><cell>Color</cell><cell>X</cell><cell>X</cell><cell></cell></row><row><cell>Depth jet</cell><cell>X</cell><cell>X</cell><cell>X</cell></row><row><cell>Surface normals</cell><cell>X</cell><cell>X</cell><cell>X</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative learning of markov random fields for segmentation of 3d scan data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.213</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2016.213" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Se-mantic3d. net: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Annals -ISPRS Congress</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast semantic segmentation of 3d point clouds with strongly varying density. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient 3d scene labeling using fields of trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3064" to="3071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Screened poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d scene understanding by voxel-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d all the way: Semantic segmentation of urban scenes from start to end in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="4456" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mind the gap: modeling local and global context in (road) networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladick?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pushing the limits for view prediction in video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ogniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Forss?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Computer Vision Theory and Applications (VISAPP&apos;17)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Scitepress Digital Library</title>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SHOT: unique signatures of histograms for surface and texture description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Di Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="251" to="264" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer Vision: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3d point clouds using a dense CRF with learned parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation, ICRA 2015</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-30" />
			<biblScope unit="page" from="4867" to="4873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298801</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2015.7298801" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Surface splatting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
