<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Brain and Cognitive Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves highquality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Markerless motion capture of multi-person in a scene is important for many industry applications but still challenging and far from being solved. Although the literatures have reported single view 2D and 3D pose estimation methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b32">33]</ref>, they suffer from heavy occlusions and produce low-fidelity results. Comparably, multi-view cameras provide more than one views to alleviate occlusion, as well as stereo cues for accurate 3D triangulation, therefore are indispensable inputs for markerless motion capture of multi-person scenes. While remarkable advances have been made in * Equal contribution <ref type="figure">Figure 1</ref>. Our method enables multi-person motion capture system working at 30fps for 5 persons using 5 RGB cameras, while achieving high quality skeleton reconstruction results. many kinds of multi-camera motion capture systems for human <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24]</ref> or even animals <ref type="bibr" target="#b3">[4]</ref>, most of them fail to achieve the goals of realtime performance and high quality capture under extremely close interactions.</p><p>Given the 4D (2D spatial, 1D viewpoint and 1D temporal) multiview video input, the key to the success of realtime and high quality multi-person motion capture is how to leverage the rich data input, i.e., how to operate on the 4D data structure to achieve high accuracy while maintaining realtime performance. Essentially, based on the human body part features pre-detected in the separate 2D views using state-of-the-art CNN methods <ref type="bibr" target="#b10">[11]</ref>, three kinds of basic associations can be defined on this 4D structure. These include single image association (i.e., parsing) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref> to form human skeletons in a single image, cross-view association (i.e., matching) to establish correspondences among different views, and temporal association (i.e. tracking) to build correspondences between sequential frames.</p><p>Existing methods struggle to deal with all these association simultaneously and efficiently. They consider only parts of these associations, or simply operate them in a sequential manner, resulting in failure to be a high qual-ity and realtime method. For example, the state-of-theart methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39]</ref> share a similar high-level framework by first performing per-view person parsing, followed by cross-view person matching, and temporal tracking sequentially. They usually assume and rely on perfect perview person parsing results in the first stage. However, this can not be guaranteed in crowded or close interaction scenarios. Temporal extension <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> of the 3D pictorial structure (3DPS) model <ref type="bibr" target="#b5">[6]</ref> apply temporal tracking <ref type="bibr" target="#b22">[23]</ref>, followed with cross-view parsing using the very timeconsuming 3DPS structure optimization. The Panoptic Studio <ref type="bibr" target="#b23">[24]</ref> addresses these associations in a sequential manner, by first matching (generate node proposals), then tracking (generate trajectories), and finally assemble the 3D human instances. As it tracks over the whole sequence, it is impossible to achieve realtime performance.</p><p>In this paper, we formulate parsing, matching, and tracking in a unified graph optimization framework, called 4D association graph, to simultaneously and equally addressing 2D spatial, 1D viewpoint and 1D temporal information. By regarding the detected 2D skeleton joint candidates in the current frame and the 3D skeleton joints in the former frame as graph nodes, we construct edges by calculating confidence weights between nodes. Such calculation jointly takes advantage of feature confidences in each individual image, epipolar constraints and reconstructed skeletons in the temporal precedent frame. Compared with <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7]</ref> which adopt sequential processing strategy on image space, viewpoint, and time dimensions, our 4D graph formulation enables unified optimization on all these dimensions, thereby allowing better mutual benefit among them.</p><p>To realize realtime optimization on the 4D association graph, we further contribute an efficient method to solve the 4D association by separating the problem into a 4D limb parsing step and a skeleton assembling step. In the former step, we propose a heuristic searching algorithm to form 4D limb bundles and a modified minimum spanning tree algorithm to assemble the 4D limb bundles into skeletons. Both of these two steps are optimized based on an energy function designed to jointly consider the image feature, stereo and temporal cues, thus optimization quality is guaranteed while realtime efficiency is achieved. We demonstrate a realtime multi-person motion capture system using only 5 to 6 multiview video cameras, see <ref type="figure">Fig. 1</ref> and the supplemental video. Benefiting from this unified strategy, our system succeeds even in the close interaction scenarios (Video 02:55-03:30). Finally, we contribute a multiview multi-person close interacting motion dataset synchronized with markerbased motion capture system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly overview literature on multi-person skeleton estimation according to the dimension of input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Parsing</head><p>We restrict our single image parsing to the work that addresses multi-person pose estimation in 2D and 3D. As there are close interactions in the scene, they all need to consider skeleton joint or body part detection and their connection to form skeletons. Parsing methods can be typically categorized into two classes: bottom-up method and top-down method. In general, top-down methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28]</ref> demonstrate higher average precision benefiting from human instance information, and bottom-up methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38]</ref> tend to propose pixel-aligned low-level feature positions while assembling them is still a great challenge. Typically, a state-of-the-art bottom-up method, OpenPose <ref type="bibr" target="#b10">[11]</ref>, introduces part affinity field (PAF) to assist parsing low-level keypoints on limbs, obtaining realtime performance with high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-view Matching</head><p>Matching finds correspondences across views, no matter on high level features (human instances) or low-level features (keypoints). Previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref> implicitly solves matching and parsing using 3D pictorial structure model. However, such method is time-consuming due to large state space and iterative belief propagation. Joo et al. <ref type="bibr" target="#b23">[24]</ref> utilize detected features from dense multi-view images to vote for possible 3D joint positions, which does matching in another implicit way. Such voting method only works well with enough observation views. Most recent work <ref type="bibr" target="#b13">[14]</ref> matches per-view parsed human instances cross view with convex optimization method constrained by cycle-consistency. Though fast and robust, such method relies on appearance information to ensure good results, and could be affected by possible parsing error (e.g. false positive human instance and wrong joint estimation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Temporal Tracking</head><p>Tracking is one key step towards continuous and smooth motion capture, and helps solve current pose ambiguity according to history results. Tracking could be done either in 2D space or 3D space. Many works have addressed 2D tracking, known as pose-tracking tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref>. For 3D tracking, motion capture of multiple closely interacting persons <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref> has been proposed through joint 3D template tracking and multi-view body segmentation. Li et al. <ref type="bibr" target="#b28">[29]</ref> propose a spatio-temporal tracking for closely interacting persons from multi-view videos. However, these pure tracking algorithms are easy to fail because of temporal error accumulation. Elhayek et al. <ref type="bibr" target="#b14">[15]</ref> track 3D articulated model to 2D human appearance descriptor (Sum of Gaussian), achieving markerless motion capture for both indoor and outdoor scenes. However, it does not demonstrate multi-person case (more than 3 persons). Belagiannis et al. <ref type="bibr" target="#b7">[8]</ref> also utilize tracking information, but they derive human tracks in advance as prior to reduce state space, instead of solving tracking and matching simultaneously. Bridgeman et al. <ref type="bibr" target="#b9">[10]</ref> contribute a real time method, yet it adopt a sequential processing of image parsing, cross-view correction and temporal tracking, resulting in degraded accuracy compared with the state-of-the-art <ref type="bibr" target="#b13">[14]</ref>. In Panoptic Studio <ref type="bibr" target="#b23">[24]</ref>, after temporal tracking of 3D joint proposals on the whole sequence, optimization is started for human assembling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Our 4D association graph considers the information in two consecutive frames. We first use the off-the-shelf bottom-up human pose detector <ref type="bibr" target="#b10">[11]</ref> on each input view of the current frame to generate low-level human features on each view. Our 4D association graph takes as input multi-view human body part candidates (2D heatmaps position) and connection confidence (PAF <ref type="bibr" target="#b10">[11]</ref> score ranging between 0 and 1) between body parts (see <ref type="figure" target="#fig_0">Fig. 2</ref>(a)), together with the former reconstructed 3D skeletons. By regarding body parts and the 3D joints in the former frame as graph nodes, we construct edges with significant semantic meaning between nodes. Specifically, as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, there exist three kinds of edges: per-view parsing edges connecting adjacent body parts in each image view, crossview matching edges connecting the same body part across views, and temporal tracking edges connecting history 3D nodes and 2D candidates. The construction of these edges will be elaborated in Sect. 4.</p><p>Based on the input graph in <ref type="figure" target="#fig_0">Fig. 2</ref>(b), this 4D association problem can be described as a minimum-cost multi-cut problem, i.e., a 0-1 integer programming problem to select those edges that belong to the real skeletons and the physically real temporal and cross-view edges, see <ref type="figure" target="#fig_0">Fig. 2</ref>(c). Actually, our graph model is similar to the available single view association problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, except that it is more complex. As it is a NP-hard problem, we split it to 4D limb parsing (Sect. 5.1) and a skeleton assembling (Sect. 5.2) problems. Our proposed solving method can guarantee realtime performance while obtaining robust results. Here, it is worth mentioning that, our graph model and the solving method also work for special cases when there is no temporal edges, i.e., at the first frame of the whole sequence, or when new persons entering the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">4D Association Graph</head><p>For each image view c ? {1, 2, ..., N } at the current frame t, the convolutional pose machine (CPM) model <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11]</ref> is first applied to get the heatmaps of keypoints and their part affinity fields (PAFs). Denote D j (c) = {d m j (c) ? R 2 } as the candidate positions of the skeleton joints j ? {1, 2, ..., J}, with m as candidate index. Here, t is ignored by default as processing the current frame. Denote f mn ij (c) as PAF score connecting d m i (c) and d n j (c), where {ij} ? T is a limb on the skeleton topology tree T .</p><p>With both the candidate positions D j (c) and the skeleton joints reconstructed in former frame seen as graph nodes, we have three kinds of edges: per-view parsing edges E P connecting nodes in the same view, cross-view matching edges E V connecting nodes in different views geometrically, and temporal tracking edges E T connecting nodes temporally. The solving of this association graph is equivalent to determining bool variable z ? {0, 1} for each edge, where z = 1 means connected nodes are associated in the same human body, z = 0 otherwise. Note that z = 0 means the two nodes are linked with two different bodies, or are linked with a false position (a fake joint that is not on a real body). The connecting weight on edges is written as p(z) = p(z = 1). In the following, the weights of each edge is defined in the 4D association graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parsing Edges and Matching Edges</head><p>Without considering the temporal tracking edges introduced by the former reconstructed 3D skeletons, the parsing edges and the matching edges forms a 3D association graph G 3D . This case happens when processing the first frame of the whole sequence or when a new person is entering in the scene. The graph G 3D directly extends the original per-view multiple person parsing problem <ref type="bibr" target="#b10">[11]</ref> with cross view geometric matching constraints. With these geometric constraints, false limb connections in single view case may have good chance to be distinguished and corrected during joint 3D association.</p><p>Denote z mn ij (c 1 , c 2 ) as bool variable on edge connecting d m i (c 1 ) and d n j (c 2 ). Obviously, a feasible solution {z mn ij (c 1 , c 2 )} on G 3D must conforms to the following inequalities ?m,</p><formula xml:id="formula_0">n z mn ij (c, c) ? 1 ?c 2 = c 1 , m, n z mn ii (c 1 , c 2 ) ? 1<label>(1)</label></formula><p>Specifically, the top one forces that no two edges share a node, i.e., no two limbs of the same type (e.g., left forearm) share a part. The bottom one forces that no joint from one view connects to two joints of the same type from another view. Note also here c 1 and c 2 represent all possible combinations of view pairs. For the per-view parsing edge E P , we directly define the input edge weight as its PAF score:</p><formula xml:id="formula_1">p(z mn ij (c) = 1) = f mn ij (c)<label>(2)</label></formula><p>For cross-view matching edge E V , the weight is defined based on the epipolar distance, written as line-to-line distance in 3D space:</p><formula xml:id="formula_2">p(z mn ii (c 1 , c 2 )) = 1 ? 1 Z d m i (c 1 ) ? d n i (c 2 ) (3) d(c 1 ) ? d(c 2 ) = d(K ?1 c1d (c 1 ), K ?1 c2d (c 2 )) (4) whered = [d T , 1] T , K c is intrinsic matrix of view c, d(?, ?)</formula><p>means line-to-line distance between two rays emitting from the camera centers of view c 1 and c 2 . Z is an empirically defined normalization factor, which adjusts epipolar distance to range [0, 1]. Note that we only build edges for those cross-view nodes sharing the same joint index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Tracking Edges</head><p>Although solving G 3D at each time instant could provide good association in most cases, failures may happen for very crowded scene or severe occlusions. To improve skeleton reconstruction robustness, we take advantage of the temporal prior, i.e., the reconstructed skeletons at the former frame for regularization of the association problem, which forms the 4D association graph G 4D . We restrict the connecting edge between the former frame skeletons and the current frame joint features, by requiring the two nodes of the edge to be the same skeleton joint (can be on different persons). Denote z mk i (c) as the final optimized bool variable for edge connecting image joint feature d m i (c) and skeleton joint X k i . We define tracking edge connecting probability as</p><formula xml:id="formula_3">p(z mk i (c)) = 1 ? 1 T d (X k i , K ?1 c d m i (c))<label>(5)</label></formula><p>where d (X, d) indicates point-to-line distance between 3D point X and 3D line emitting from camera center to d, and T is normalization factor, ensuring p(z mk i (c)) to be in range </p><formula xml:id="formula_4">?i, c, m z mk i (c) ? 1, k z mk i (c) ? 1<label>(6)</label></formula><p>This constraint forces that each 3D joint at the last frame matches no more than one 2D joint on each view at the current frame, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Objective Function</head><p>Based on the predefined probabilities for the parsing edges, matching edges and tracking edges, our 4D association optimization can be formulated as an edge selection problem to maximize an objective function under conditions 1 and 6. Specifically, let q(z) = p(z) ? z denote the final energy of an edge, where z is a boolean variable, and then our objective function can then be written as the summation of energies of all the selected edges in E P , E M and E T :</p><formula xml:id="formula_5">E(Z) =w p q(z mn ij (c, c)) + w m q(z mn ii (c 1 , c 2 )) + w t q(z mk i (c))<label>(7)</label></formula><p>Note here would traverse all the possible edges, i.e., all feasible values of variables (i,j,m,n,k,c,c1,c2) by default. w p , w m and w t are empirically defined weighting factors for edges E P , E M and E T , respectively. With w t = 0, it degenerates to the objective function for solving association graph G 3D . Notice that, both G 3D and G 4D can be solved with the same procedure, as described in Sect. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Solving 4D Association</head><p>Solving the 4D Association graph means maximizing the objective function Eqn. 7 under constraints Eqn. 1 and Eqn. 6. Traversing the huge association space in brute force manner is infeasible for realtime systems. Instead, inspired by the realtime but high quality parsing method <ref type="bibr" target="#b10">[11]</ref> that assembles 2D human skeleton in a greedy manner, we propose a realtime 4D association solver. The key difference between our 4D association and the previous 2D association is that: the limb candidates scatter not only in a single image but in the whole space and time, and some limbs represent the same physical limbs. Therefore, we need to first associate those limbs that are likely to be the same limb bundle across views and times, before 4D skeletons assembling. Based on this idea, our realtime solution can be divided into two steps: 4D limb bundle association (Sect. 5.1), and 4D human skeleton association by the bundle Kruskal's algorithm (Sect. 5.2). It is worth noting that, both of these two steps rely on the objective function Eqn. 7 for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">4D Limb Bundle Parsing</head><p>To extract limb bundles across view and time, we first restrict G 4D on a limb {ij} (two adjacent types of joint) as G ij 4D . Since there are multiple persons in the scene, graph G ij 4D may contain multiple real limb bundles. In theory, each real limb bundle contains two joint cliques. For clarity, a clique means a graph where every two nodes are connected <ref type="bibr" target="#b41">[42]</ref>, see <ref type="figure" target="#fig_1">Fig. 3(a)</ref> for example. This implies that every two joints of the same type in the limb bundle must share a cross-view edge or a temporal edge. By further considering the parsing edges, a correct 4D limb bundle consists of two joint clique connected with parsing edges on each view. We call such limb bundle candidate as limb clique. <ref type="figure" target="#fig_1">Fig. 3(b)</ref> enumerates all the possible limb cliques of <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Consequently, our goal in this step is to search all possible limb cliques {G C |G C ? G ij 4D } for the real limb bundles. We measure each limb clique with E(Z G C ) based on the objective function Eqn. 7. However, directly maximizing E(Z G C ) would always encourage as many edges as possible to be selected in a clique, even false edges. Hence, we normalize E(Z G C ) with clique size |V C | of G C , and add a penalty term to balance the clique size and the average probability. Overall, the objective function for a limb clique is</p><formula xml:id="formula_6">E(G C ) = E(Z G C )/|V C | + w v ?(|V C |)<label>(8)</label></formula><p>where w v is balancing weight, and ? is a Welsch robust loss <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref> defined as</p><formula xml:id="formula_7">?(x) = 1 ? exp ? 1 2 (x/c) 2<label>(9)</label></formula><p>Here, c = (N ? 1)/2 is a parameter depending on the total number of views. <ref type="figure" target="#fig_2">Fig. 4</ref> illustrates the limb bundle parsing procedure. After selecting a limb clique and marking it as a limb bundle, we remove it from G ij 4D <ref type="figure" target="#fig_2">(Fig. 4(b)</ref>), together with all other edges connected with any joint in this clique <ref type="figure" target="#fig_2">(Fig. 4(c)</ref>). By doing this, our solution always conforms to feasibility inequalities <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b5">6)</ref>. This selection process is iterated until G ij 4D is empty ( <ref type="figure" target="#fig_2">Fig. 4(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">4D Skeleton Assembling</head><p>After generating all the 4D limb bundles, we need to assemble them into multiple 4D human skeletal structures. We first sort all the 4D limb bundles based on their scores, and build a priority queue to store them. In each iteration, we pop a 4D limb bundle from the queue with the maximum score (based on Eqn. 8), and merge it into the 4D skeletons. In this merging process, all the 2D joints (belongs to this bundle, from different views) should have a same labeled person ID. However, since a newly added limb bundle may share the same 4D joint as some limb bundles that are already assigned, conflicts would arise when these 2D joints have already been labeled with different person IDs on different views in the previous iterations, see <ref type="figure" target="#fig_3">Fig. 5(a)</ref>. To eliminate this conflict, we propose a simple yet effective way by splitting the newly added limb bundles to small limb bundles according to the persons whose joints are assigned to ( <ref type="figure" target="#fig_3">Fig. 5(b)</ref>). We then re-compute the objective function of each small bundle and push back to the prior queue for further assembling. If there is no conflict, we merge the bundle into the skeleton and label the 2D joints. We iterate popping and merging until the queue is empty <ref type="figure" target="#fig_3">(Fig. 5(c)</ref>).</p><p>We call the above method bundle Kruskal's algorithm. In the single view case, there would be no conflicts, and our method degenerates to traditional Kruskal's algorithm, which is a famous minimum spanning tree (MST) algorithm used in OpenPose <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Parametric Optimization</head><p>Based on 4D skeleton assembling results on the 2D view images, we can further optimize the full 3D body pose by embedding a parametric skeleton. We minimize the energy function</p><formula xml:id="formula_8">E(?) = w 2D E 2D + w shape E shape + w temp E temp (10)</formula><p>where E 2D is the data term aligning 2D projections on each view to the detected joints, E shape penalizes human shape prior (e.g. bone length and symmetry), and E temp is temporal smoothing term (w 2D , w shape and w temp are balancing weights, w temp = 0 if no temporal information exists). As this fitting process is a classic optimization step, please refer to <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29]</ref> for details. Temporally, we track each person and use the average bone lengths of the first five frames with high confidence (visible in more than 3 cameras) as the bone length prior for the person in the later frames. If the person is lost and re-appear, we simply regard him/her as a new person and re-calculate the bone lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>In <ref type="figure" target="#fig_4">Fig. 6</ref>, we demonstrate the results of our system. Using only geometry information from sparse view points, our method enables realtime and robust multi-person motion capture under severe occlusions ( <ref type="figure" target="#fig_4">Fig. 6(a)</ref>), challenging poses ( <ref type="figure" target="#fig_4">Fig. 6(b)</ref>) and subtle social interactions <ref type="figure" target="#fig_4">(Fig. 6(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation Details</head><p>The multi-view capture system consists of 5 synchronized industrial RGB cameras (with resolution 2048?2048) and a single PC with one 3.20 GHz CPU and one NVIDIA TITAN RTX GPU. Our system achieves 30 fps motion capture for 5 persons. Specifically, for each frame, the preprocessing step (including demosaicing, undistortion and resizing for multi-view inputs) takes less than 1 ms, the CNN inference step takes 22.9 ms in total for 5 images, the 4D association step takes 11 ms, and the parametric optimization step takes less than 4 ms. Moreover, we ping-pong the CNN inference and the 4D association for achieving realtime performance with affordable delay (60 ms). More details about the optimization parameters are provided in the supplementary material.</p><p>Note that the 4D association pipeline is fully implemented on CPU. Also, in the CNN inference step, the input RGB images are resized to 368 ? 368, and the CNNs for keypoints and PAFs are re-implemented using Ten-sorRT <ref type="bibr" target="#b39">[40]</ref> for further acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Dataset</head><p>We contribute a new evaluation dataset for multi-person 3D skeleton tracking with ground truth 3D skeletons captured by commercial motion capture system, OptiTrack <ref type="bibr" target="#b0">[1]</ref>. Compared with previous 3D human datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2]</ref>, our dataset is mainly focusing on the more challenging scenarios like close interactions and challenging motion. Our dataset contains 5 sequences with each around 20-second long capturing a 2-4 person scene using 6 cameras. Our actors all wear black marker-suit for ground truth skeletal motion capture. With ground truth 3D skeletons, our dataset enables more effective quantitative evaluations for both 2D parsing and 3D tracking algorithms. Note that besides evaluating our method using the proposed dataset, we also provide evaluation results using Shelf and Panoptic Studio dataset following previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Quantitative Comparison</head><p>We compare with state-of-the-art methods quantitatively using both the Shelf dataset and our testing dataset. The quantitative comparison on Shelf dataset is shown in Table. 1. Benefiting from our 4D association formulation, we achieve more accurate results than both temporal tracking methods based on 3DPS ( <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>) and appearancebased global optimization methods <ref type="bibr" target="#b13">[14]</ref>.</p><p>We also compare with <ref type="bibr" target="#b13">[14]</ref> on our testing dataset according to 'precision' (the ratio of correct joints in all estimated joints) and 'recall' (the ratio of correct joints in all ground truth joints). A joint is correct if its Euclidean distance to ground truth joint is less than a threshold (0.2m). As shown in <ref type="table">Table.</ref> 2, our method outperforms <ref type="bibr" target="#b13">[14]</ref> under both metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Qualitative Comparison</head><p>To further demonstrate the advantages of our bottomup system, we perform qualitative comparison with the state-of-the-art method <ref type="bibr" target="#b13">[14]</ref>, which utilizes top-down human pose detector <ref type="bibr" target="#b11">[12]</ref> to perform single view parsing. The qualitative results is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>, from which we can see that top-down method depends heavily on instance proposals, and may generate false positive human pose detection to deteriorate the cross-view matching performance (left case). Furthermore, per-view parsing would fail to infer correct human poses under severe occlusion, deteriorating pose reconstruction results (right). Instead, thanks to relatively pre- cise low-level features (e.g. keypoints) and robust 4D association algorithm, the joints are associated more accurately in our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Ablation Study</head><p>With/Without tracking. We first evaluate tracking edges in the 4D graph. By triangulating 2D bodies into 3D skeletons directly using G 3D , we eliminate the usage of tracking edges. The result is labeled as 'w/o tracking' in <ref type="table">Table.</ref> 3. Without using tracking edges, our method still exhibits competent result and out-performs state-of-the-art method <ref type="bibr" target="#b13">[14]</ref> (93.4% vs 91.1%). Moreover, our 4D association method is more robust in messy scenes ('Ours(final)' as shown in <ref type="table">Table.</ref> 3).</p><p>Compare with two-step pipeline. Traditional two-step pipeline would perform per-view parsing first, followed by cross-view matching. We implement a two-step pipeline for comparison, where we first parse human in each view similar to <ref type="bibr" target="#b10">[11]</ref>, and match them using our clique searching method with objective function defined on parsed body. Notice that no temporal information is used, and 3D poses are obtained by triangulation. Result is shown as 'two-step' in <ref type="table">Table.</ref> 3. As shown in <ref type="table">Table.</ref> 3, our per-frame G 3D solution 'w/o tracking' performs better than two-step pipeline, especially on actor 'A2'. To show our robustness to per-view parsing ambiguity, we use only 3 views to reconstruct 2 persons ( <ref type="figure" target="#fig_5">Fig. 7)</ref>. Wrong parsing result on one view would harm the inferred 3D pose, especially when very sparse views are available.  <ref type="table">Table 3</ref>. Ablation study on Shelf dataset. 'two-step' means first per-view parsing and then cross-view matching. 'w/o tracking' means we solve G3D in each frame. Both 'two-step' and 'w/o tracking' use triangulation to infer 3D poses. Numbers are percentage of correct parts(PCP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a realtime multi-person motion capture method with sparse view points. Build on top of the low-level detected features directly, we formulate parsing, matching and tracking problem simultaneously into a unified 4D graph association framework. The new 4D association formulation not only enables realtime motion capture performance, but also achieves state-of-the-art accuracy compared with other methods, especially for crowded and close interaction scenarios. Moreover, we contribute a new testing dataset for multi-person motion capture with ground truth 3D poses. Our system narrows the gap between laboratory markerless motion capture system and industrial applications in real world scenarios. Finally, our novel 4D graph formulation may stimulate future research in this topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Method overview. (a) We input body part positions and connection confidence of different views at time t, together with 3D person of last time. We use 3 views for example. (b) The 4D association graph. For clarity, we only highlight the association of the torso limb with three types of edges (parsing edges, matching edges and tracking edges) with different colors. (c) From the initial graph (b), our association method outputs the assembling results. (d) We optimize the assembled multiview 2D skeletons (c) to form 3D skeletons of current frame t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of limb cliques. (a) A sample 4D graph on limb {ij} denoted as G ij 4D . Two cliques are marked as red area and blue area. (b) Limb cliques of different sizes could be proposed from the 4D graph on limb. Joints of the same type (same color in the above figure) on a limb clique form a clique, and joints of different types on each view must share a green parsing edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of limb bundle parsing procedure. (a) Initial graph G ij 4D . A square/cube represents a limb (2D or 3D), and each grey dash line means an edge. (b) A best clique (limb bundle) detected from (a) is shown in blue. (c) Then, we remove both limbs and edges related to the best clique, and extract next best one. (d) Finally, all cliques are detected. We could extract cliques without temporal edges, like the orange one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Conflicts handling in our skeleton assembling step. (a) A limb bundle to be added. It contains 3 parsing edges on 3 views. In this case, each parsing edge contains a joint to be assembled (black node) and a joint already assembled (blue or red nodes) in previous iterations. Here conflict arises as blue and red belong to different person IDs. (b) We split original limb bundle into small bundles according to the existing person IDs. (c) A possible final assembling result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Results of our system. From top to bottom: input images, reprojection of 3D human, and 3D visualization respectively. (a) Our live captured data with fast motion (left), severe occlusion (middle) and crowded scene (right). 5 views used. (b) Our dataset with textureless clothing and rich motion. 6 views used. (c) Panoptic studio dataset with natural social interaction. 7 views used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Comparison with two-step pipeline. Top figures are association result, bottom figures are reprojection of 3D pose. Notice that, reprojection of 3D pose generated by two-step pipeline obviously deviates from correct position due to false parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative comparison with Dong[14] on Shelf (left figure) and our captured data (right figure), both with 5 cameras. For each case, we show association results and reprojection of 3D pose on two sample views. For 3D visualization, we show a side view rendering and a top view rendering for clear comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with<ref type="bibr" target="#b13">[14]</ref> using our testing dataset.</figDesc><table><row><cell>Shelf</cell><cell>A1</cell><cell>A2</cell><cell>A3</cell><cell>Avg</cell></row><row><cell>Belagiannis et al. [6]</cell><cell cols="4">66.1 65.0 83.2 71.4</cell></row><row><cell>?Belagiannis et al. [8]</cell><cell cols="4">75.0 67.0 86.0 76.0</cell></row><row><cell>Belagiannis et al. [7]</cell><cell cols="4">75.3 69.7 87.6 77.5</cell></row><row><cell cols="5">Ershadi-Nasab et al. [16] 93.3 75.9 94.8 88.0</cell></row><row><cell>Dong et al. [14]</cell><cell cols="4">97.2 79.5 96.5 91.1</cell></row><row><cell>*Dong et al. [14]</cell><cell cols="4">98.8 94.1 97.8 96.9</cell></row><row><cell>?# Tanke et al. [39]</cell><cell cols="4">99.8 90.0 98.0 96.0</cell></row><row><cell>?Ours(final)</cell><cell cols="4">99.0 96.2 97.6 97.6</cell></row><row><cell cols="5">Table 1. Quantitative comparison on Shelf dataset using percent-</cell></row><row><cell cols="5">age of correct parts (PCP) metric. '*' means method with appear-</cell></row><row><cell cols="5">ance information, ' ?' means method with temporal information,</cell></row><row><cell cols="5">'#' means accuracy without head. 'A1'-'A3' correspond to the re-</cell></row><row><cell cols="5">sults of three actors, respectively. The averaged result is in column</cell></row><row><cell>'Avg'.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our Dataset</cell><cell cols="3">Dong[14] Ours(final)</cell><cell></cell></row><row><cell cols="2">Precision(%) 71.0</cell><cell>88.5</cell><cell></cell><cell></cell></row><row><cell>Recall(%)</cell><cell>80.2</cell><cell>90.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Optitrack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mocap</surname></persName>
		</author>
		<ptr target="https://www.optitrack.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Utrecht multi-person motion (umpm) benchmark: a multiperson dataset with synchronized video and motion capture data for evaluation of articulated human motion and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvd</forename><surname>Aa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giezeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop HICV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Praneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seng Bum Michael</forename><surname>Benjamin R Eisenreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Hayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openmonkeystudio</surname></persName>
		</author>
		<title level="m">Automated markerless pose estimation in freely moving macaques. bioRxiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures revisited: Multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple human pose estimation with temporally consistent 3d pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-person 3d pose estimation and tracking in sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Bridgeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Techniques for nonlinear least squares and robust regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Marconiconvnetbased marker-less motion capture in outdoor and indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Edilson De Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Ershadi-Nasab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfan</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esmaeil</forename><surname>Sanaei</surname></persName>
		</author>
		<title level="m">Multiple human 3d pose estimation from multiview images. Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal Fua Jerome</forename><surname>Engin Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleuret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view body part recognition with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-scale structure-aware network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipeng</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shape and pose estimation for closely interacting persons using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianhong</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CGF</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Markerless motion capture of multiple characters using multiview image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Markerless motion capture of interacting characters using multi-view image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient online multi-person 2d pose tracking with recurrent spatio-temporal affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaadhav</forename><surname>Raaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end learning for graph decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Otmar Hilliges, and Siyu Tang</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative greedy matching for 3d human pose tracking from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Tanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient inference with tensorrt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Vanholder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Introduction to graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>fourth edition</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3d sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alin-Ionut</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
