<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Value-Decomposition Networks For Cooperative Multi-Agent Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">Sunehag</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">Lever</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gruslys</forename><surname>Audrunas</surname></persName>
							<email>audrunas@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Marian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Czarnecki</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><forename type="middle">Zambaldi</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">Jaderberg</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Lanctot</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Sonnerat</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Tuyls</surname></persName>
							<email>karltuyls@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><forename type="middle">Graepel</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind &amp; University of Liverpool</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Value-Decomposition Networks For Cooperative Multi-Agent Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent" problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the cooperative multi-agent reinforcement learning (MARL) problem <ref type="bibr" target="#b26">(Panait and Luke, 2005;</ref><ref type="bibr" target="#b3">Busoniu et al., 2008;</ref><ref type="bibr" target="#b36">Tuyls and Weiss, 2012)</ref>, in which a system of several learning agents must jointly optimize a single reward signal -the team reward -accumulated over time. Each agent has access to its own ("local") observations and is responsible for choosing actions from its own action set. Coordinated MARL problems emerge in applications such as coordinating self-driving vehicles and/or traffic signals in a transportation system, or optimizing the productivity of a factory comprised of many interacting components. More generally, with AI agents becoming more pervasive, they will have to learn to coordinate to achieve common goals.</p><p>Although in practice some applications may require local autonomy, in principle the cooperative MARL problem could be treated using a centralized approach, reducing the problem to single-agent reinforcement learning (RL) over the concatenated observations and combinatorial action space. We show that the centralized approach consistently fails on relatively simple cooperative MARL problems in practice. We present a simple experiment in which the centralised approach fails by learning inefficient policies with only one agent active and the other being "lazy". This happens when one agent learns a useful policy, but a second agent is discouraged from learning because its exploration would hinder the first agent and lead to worse team reward. <ref type="bibr">1</ref> An alternative approach is to train independent learners to optimize for the team reward. In general each agent is then faced with a non-stationary learning problem because the dynamics of its environment effectively changes as teammates change their behaviours through learning <ref type="bibr" target="#b17">(Laurent et al., 2011)</ref>. Furthermore, since from a single agent's perspective the environment is only partially observed, agents may receive spurious reward signals that originate from their teammates <ref type="bibr">' (unobserved)</ref> behaviour. Because of this inability to explain its own observed rewards naive independent RL is often unsuccessful: for example <ref type="bibr" target="#b4">Claus and Boutilier (1998)</ref> show that independent Q-learners cannot distinguish teammates' exploration from stochasticity in the environment, and fail to solve even an apparently trivial, 2-agent, stateless, 3 ? 3-action problem and the general Dec-POMDP problem is known to be intractable <ref type="bibr" target="#b2">(Bernstein et al., 2000;</ref><ref type="bibr" target="#b24">Oliehoek and Amato, 2016)</ref>. Though we here focus on 2 player coordination, we note that the problems with individual learners and centralized approaches just gets worse with more agents since then, most rewards do not relate to the individual agent and the action space grows exponentially for the fully centralized approach.</p><p>One approach to improving the performance of independent learners is to design individual reward functions, more directly related to individual agent observations. However, even in the single-agent case, reward shaping is difficult and only a small class of shaped reward functions are guaranteed to preserve optimality w.r.t. the true objective <ref type="bibr" target="#b23">(Ng et al., 1999;</ref><ref type="bibr" target="#b6">Devlin et al., 2014;</ref><ref type="bibr" target="#b7">Eck et al., 2016)</ref>. In this paper we aim for more general autonomous solutions, in which the decomposition of the team value function is learned.</p><p>We introduce a novel learned additive value-decomposition approach over individual agents. Implicitly, the value decomposition network aims to learn an optimal linear value decomposition from the team reward signal, by back-propagating the total Q gradient through deep neural networks representing the individual component value functions. This additive value decomposition is specifically motivated by avoiding the spurious reward signals that emerge in purely independent learners.The implicit value function learned by each agent depends only on local observations, and so is more easily learned. Our solution also ameliorates the coordination problem of independent learning highlighted in <ref type="bibr" target="#b4">Claus and Boutilier (1998)</ref> because it effectively learns in a centralised fashion at training time, while agents can be deployed individually.</p><p>Further, in the context of the introduced agent, we evaluate weight sharing, role information and information channels as additional enhancements that have recently been reported to improve sample complexity and memory requirements <ref type="bibr" target="#b11">(Hausknecht, 2016;</ref><ref type="bibr" target="#b8">Foerster et al., 2016;</ref><ref type="bibr" target="#b32">Sukhbaatar et al., 2016)</ref>. However, our main comparison is between three kinds of architecture; Value-Decomposition across individual agents, Independent Learners and Centralized approaches. We investigate and benchmark combinations of these techniques applied to a range of new interesting two-player coordination domains. We find that Value-Decomposition is a much better performing approach than centralization or fully independent learners, and that when combined with the additional techniques, results in an agent that consistently outperforms centralized and independent learners by a big margin. <ref type="bibr" target="#b31">Schneider et al. (1999)</ref> consider the optimization of the sum of individual reward functions, by optimizing local compositions of individual value functions learnt from them. <ref type="bibr" target="#b30">Russell and Zimdars (2003)</ref> sums the Q-functions of independent learning agents with individual rewards, before making the global action selection greedily to optimize for total reward. Our approach works with only a team reward, and learns the value-decomposition autonomously from experience, and it similarly differs from the approach with coordination graphs <ref type="bibr" target="#b9">(Guestrin et al., 2002)</ref> and the max-plus algorithm <ref type="bibr" target="#b16">(Kuyer et al., 2008;</ref><ref type="bibr" target="#b37">van der Pol and Oliehoek, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Other Related Work</head><p>Other work addressing team rewards in cooperative settings is based on difference rewards <ref type="bibr" target="#b35">(Tumer and Wolpert, 2004)</ref>, measuring the impact of an agent's action on the full system reward. This reward has nice properties (e.g. high learnability), but can be impractical as it requires knowledge about the system state <ref type="bibr" target="#b5">(Colby et al., 2016;</ref><ref type="bibr" target="#b0">Agogino and Tumer, 2008;</ref><ref type="bibr" target="#b27">Proper and Tumer, 2012)</ref>. Other approaches can be found in <ref type="bibr" target="#b6">Devlin et al. (2014);</ref><ref type="bibr" target="#b13">HolmesParker et al. (2016)</ref>; <ref type="bibr" target="#b1">Babes et al. (2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Reinforcement Learning</head><p>We recall some key concepts of the RL setting <ref type="bibr" target="#b33">(Sutton and Barto, 1998)</ref>, an agent-environment framework <ref type="bibr" target="#b29">(Russell and Norvig, 2010)</ref> in which an agent sequentially interacts with the environment over a sequence of timesteps, t = 1, 2, 3, . . ., by executing actions and receiving observations and rewards, and aims to maximize cumulative reward. This is typically modelled as a Markov decision process (MDP) (e.g. <ref type="bibr" target="#b28">Puterman, 1994)</ref> defined by a tuple S, A, T 1 , T , R comprising the state space S, action space A, a (possibly stochastic) reward function R : S ? A ? S ? R start state distribution T 1 ? P(S) and transition function T : S ? A ? P(S), where P(X ) denotes the set of probability distributions over the set X . We useR to denote the expected value of R. The agent's interactions give rise to a trajectory (S 1 ,</p><formula xml:id="formula_0">A 1 , R 1 , S 2 , ...) where S 1 ? T 1 , S t+1 ? T (?|S t , A t ) and R t = R(S t , A t , S t+1 )</formula><p>, and we denote random variables in upper-case, and their realizations in lower-case. At time t the agent observes o t ? O which is typically some function of the state s t , and when the state is not fully observed the system is called a partially observed Markov decision process (POMDP).</p><p>The agent's goal is to maximize expected cumulative discounted reward with a discount factor ?, R t := ? t=1 ? t?1 R t . The agent chooses actions according to a policy: a (stationary) policy is a function ? : S ? P(A) from states to probability distributions over A. An optimal policy is one which maximizes expected cumulative reward. In fully observed environments, stationary optimal policies exist. In partially observed environments, the policy usually incorporates past agent observations from the history h t = a 1 o 1 r 1 , ..., a t?1 o t?1 r t?1 (replacing s t ). A practical approach utilized here, is to parameterize policies using recurrent neural networks.</p><formula xml:id="formula_1">V ? (s) := E[ ? t=1 ? t?1 R(S t , A t , S t+1 )|S 1 = s; A t ? ?(?|S t )]</formula><p>is the value function and the actionvalue function is Q ? (s, a) := E S ?T (?|s,a) [R(S, a, S )+?V (S )] (generally, we denote the successor state of s by s ). The optimal value function is defined by V * (s) = sup ? V ? (s) and similarly Q * (s, a) = sup ? Q ? (s, a). For a given action-value function Q : S ? A ? R we define the (deterministic) greedy policy w.r.t. Q by ?(s) := arg max a?A Q(s, a) (ties broken arbitrarily). The greedy policy w.r.t. Q * is optimal (e.g. Szepesv?ri, 2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Q-Learning</head><p>One method for obtaining Q * is Q-learning which is based on the update Q i+1 (s t , a t ) = (1 ? ? t )Q i (s t , a t ) + ? t (r t + ? max a Q i (s t+1 , a)), where ? t ? (0, 1) is the learning rate. We employ the ?-greedy approach to action selection based on a value function, which means that with 1 ? ? probability we pick arg max a Q i (s, a) and with probability ? a random action. Our study focuses on deep architectures for the value function similar to those used by <ref type="bibr" target="#b21">Mnih et al. (2015)</ref>, and our approach incorporates the key techniques of target networks and experience replay employed there, making the update into a stochastic gradient step. Since we consider partially observed environments our Q-functions are defined over agent observation histories, Q(h t , a t ), and we incorporate a recurrent network similarly to <ref type="bibr" target="#b12">Hausknecht and Stone (2015)</ref>. To speed up learning we add the dueling architecture of <ref type="bibr" target="#b39">Wang et al. (2016)</ref> that represent Q using a value and an advantage function, including multi-step updates with a forward view eligibility trace (e.g. <ref type="bibr" target="#b10">Harb and Precup, 2016</ref>) over a certain number of steps. When training agents the recurrent network is updated with truncated backpropagation through time (BPTT) for this amount of steps. Although we concentrate on DQN-based agent architectures, our techniques are also applicable to policy gradient methods such as A3C <ref type="bibr" target="#b22">(Mnih et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Agent Reinforcement Learning</head><p>We consider problems where observations and actions are distributed across d agents, and are represented as d-dimensional tuples of primitive observations in O and actions in A. As is standard </p><formula xml:id="formula_2">a1 a2 Q1 Q2 o2 t o1 t o2 t-1 o2 t-2 o1 t-1 o1 t-2</formula><formula xml:id="formula_3">Q-values. a1 a2 Q Q1 Q2 o2 t o1 t o2 t-1 o2 t-2 o1 t-1 o1 t-2F</formula><p>igure 2: Value-decomposition individual architecture showing how local observations enter the networks of two agents over time (three steps shown), pass through the low-level linear layer to the recurrent layer, and then a dueling layer produces individual "values" that are summed to a joint Qfunction for training, while actions are produced independently from the individual outputs.</p><p>in MARL, the underlying environment is modeled as a Markov game where actions are chosen and executed simultaneously, and new observations are perceived simultaneously as a result of a transition to a new state <ref type="bibr" target="#b19">(Littman, 1994</ref><ref type="bibr" target="#b20">(Littman, , 2001</ref><ref type="bibr" target="#b14">Hu and Wellman, 2003;</ref><ref type="bibr" target="#b3">Busoniu et al., 2008)</ref>.</p><p>Although agents have individual observations and are responsible for individual actions, each agent only receives the joint reward, and we seek to optimize R t as defined above. This is consistent with the Dec-POMDP framework <ref type="bibr" target="#b25">(Oliehoek et al., 2008;</ref><ref type="bibr" target="#b24">Oliehoek and Amato, 2016</ref>).</p><p>If we denoteh := (h 1 , h 2 , ..., h d ) a tuple of agent histories, a joint policy is in general a map ? : H d ? P(A d ); we in particular consider policies where for any historyh, the distribution ?(h) has independent components in P(A). Hence, we write ? : H d ? P(A) d . The exception is when we use the most naive centralized agent with a combinatorial action space, aka joint action learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Deep-RL Architecture for Coop-MARL</head><p>Building on purely independent DQN-style agents (see <ref type="figure" target="#fig_0">Figure 1</ref>), we add enhancements to overcome the identified issues with the MARL problem. Our main contribution of value-decomposition is illustrated by the network in <ref type="figure">Figure 2</ref>.</p><p>The main assumption we make and exploit is that the joint action-value function for the system can be additively decomposed into value functions across agents,</p><formula xml:id="formula_4">Q((h 1 , h 2 , ..., h d ), (a 1 , a 2 , ..., a d )) ? d i=1Q i (h i , a i )</formula><p>where theQ i depends only on each agent's local observations. We learnQ i by backpropagating gradients from the Q-learning rule using the joint reward through the summation, i.e.Q i is learned implicitly rather than from any reward specific to agent i, and we do not impose constraints that theQ i are action-value functions for any specific reward. The value decomposition layer can be seen in the top-layer of <ref type="figure">Figure 2</ref>. One property of this approach is that, although learning requires some centralization, the learned agents can be deployed independently, since each agent acting greedily with respect to its local valueQ i is equivalent to a central arbiter choosing joint actions by maximizing the sum d i=1Q i .</p><p>For illustration of the idea consider the case with 2 agents (for simplicity of exposition) and where rewards decompose additively across agent observations 2 , r(s, a) = r 1 (o 1 , a 1 ) + r 2 (o 2 , a 2 ), where (o 1 , a 1 ) and (o 2 , a 2 ) are (observations, actions) of agents 1 and 2 respectively. This could be the case in team games for instance, when agents observe their own goals, but not necessarily those of teammates. In this case we have that</p><formula xml:id="formula_5">Q ? (s, a) = E[ ? t=1 ? t?1 r(s t , a t )|s 1 = s, a 1 = a; ?] = E[ ? t=1 ? t?1 r 1 (o 1 t , a 1 t )|s 1 = s, a 1 = a; ?] + E[ ? t=1 ? t?1 r 2 (o 2 t , a 2 t )|s 1 = s, a 1 = a; ?] =:Q ? 1 (s, a) +Q ? 2 (s, a) whereQ ? i (s, a) := E[ ? t=1 ? t?1 r 1 (o i t , a i t )|s 1 = s, a 1 = a; ?], i = 1, 2.</formula><p>The action-value function Q ? 1 (s, a) -agent 1's expected future return -could be expected to depend more strongly on observations and actions (o 1 , a 1 ) due to agent 1 than those due to agent 2. If (o 1 , a 1 ) is not sufficient to fully modelQ ? 1 (s, a) then agent 1 may store additional information from historical observations in its LSTM, or receive information from agent 2 in a communication channel, in which case we could expect the following approximation to be valid Q ? (s, a) =:Q ? 1 (s, a) +Q ? 2 (s, a) ?Q ? 1 (h 1 , a 1 ) +Q ? 2 (h 2 , a 2 ) Our architecture therefore encourages this decomposition into simpler functions, if possible. We see that natural decompositions of this type arise in practice (see Section 4.4).</p><p>One approach to reducing the number of learnable parameters, is to share certain network weights between agents. Weight sharing also gives rise to the concept of agent invariance, which is useful for avoiding the lazy agent problem. Definition 1 (Agent Invariance). If for any permutation (bijection) p : {1, ..., d} ? {1, ..., d}, ?(p(h)) = p(?(h)) we say that ? is agent invariant.</p><p>It is not always desirable to have agent invariance, when for example specialized roles are required to optimize a particular system. In such cases we provide each agent with role information, or an identifier. The role information is provided to the agent as a 1-hot encoding of their identity concatenated with every observation at the first layer. When agents share all network weights they are then only conditionally agent invariant, i.e. have identical policies only when conditioned on the same role. We also consider information channels between agent networks, i.e. differentiable connections between agent network modules. These architectures, with shared weights, satisfy agent invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We introduce a range of two-player domains, and experimentally evaluate the introduced valuedecomposition agents with different levels of enhancements, evaluating each addition in a logical sequence. We use two centralized agents as baselines, one of which is introduced here again relying on learned value-decomposition, as well as an individual agent learning directly from the joint reward signal. We perform this set of experiments on the same form of two dimensional maze environments used by <ref type="bibr" target="#b18">Leibo et al. (2017)</ref>, but with different tasks featuring more challenging coordination needs. Agents have a small 3 ? 5 ? 5 observation window, the first dimension being an RGB channel, the second and third are the maze dimensions, and each agent sees a box 2 squares either side and 4 squares forwards, see <ref type="figure" target="#fig_0">Figures 1 and 2</ref>. The simple graphics of our domains helps with running speed while, especially due to their multi-agent nature and severe partial observability and aliasing (very small observation window combined with map symmetries), they still pose a serious challenge and is comparable to the state-of-the-art in multi-agent reinforcement learning <ref type="bibr" target="#b18">(Leibo et al., 2017)</ref>, which exceeds what is common in this area <ref type="bibr" target="#b36">(Tuyls and Weiss, 2012</ref>  <ref type="table">Table 1</ref>: Agent architectures. V is value decomposition, S means shared weights and an invariant network, Id means role info was provided, L stands for lower-level communication, H for higher-level communication and C for centralization. These architectures were selected to show the advantages of the independent agent with value-decomposition and to study the benefits of additional enhancements added in a logical sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Agents</head><p>Our agent's learning algorithm is based on DQN <ref type="bibr" target="#b21">(Mnih et al., 2015)</ref> and includes its signature techniques of experience replay and target networks, enhanced with an LSTM value-network as in <ref type="bibr" target="#b12">Hausknecht and Stone (2015)</ref> (to alleviate severe partial observability), learning with truncated back-propagation through time, multi-step updates with forward view eligibility traces <ref type="bibr" target="#b10">(Harb and Precup, 2016)</ref> (which helps propagating learning back through longer sequences) and the dueling architecture <ref type="bibr" target="#b39">(Wang et al., 2016)</ref> (which speeds up learning by generalizing across the action space).</p><p>Since observations are from a local perspective, we do not benefit from convolutional networks, but use a fully connected linear layer to process the observations.</p><p>Our network architectures first process the input using a fully connected linear layer with 32 hidden units followed by a ReLU layer, and then an LSTM, with 32 hidden units followed by a ReLU layer, and finally a linear dueling layer, with 32 units. This produces a value function V (s) and advantage function A(s, a), which are combined to compute a Q-function Q(s, a) = V (s) + A(s, a) as described in <ref type="bibr" target="#b39">Wang et al. (2016)</ref>. Layers of 32 units are sufficiently expressive for these tasks with limited observation windows.</p><p>The architectures (see Appendix B for detailed diagrams) differ between approaches by what is input into each layer. For architectures without centralization or information channels, one observation of size 3 ? 5 ? 5 is fed to the first linear layer of 32 units, followed by the ReLU layer and the LSTM (see <ref type="figure" target="#fig_0">Figure 1)</ref>. For the other (information channels and centralized) agents, d such observations are fed separately to identical such linear layers and then concatenated into 64 dimensional vectors before passing though ReLUs to an LSTM.</p><p>For architectures with information channels we concatenate the outputs of certain layers with those of other agents. To preserve agent invariance, the agent's own previous output is always included first. For low-level communication, the signal's concatenation is after the first fully connected layer, while for high-level communication the concatenation takes place on the output of the LSTM layer. Note, that this has the implication that what starts as one agent's gradients are back-propagated through much of the other agents network, optimizing them to serve the purposes of all agents. Hence, representing in that sense, a higher degree of centralization than the lower-level sharing.</p><p>We have found a trajectory length of 8, determining both the length of the forward view and the length of the back propagation through time is sufficient for these domains. We use an eligibility trace parameter ? = 0.9. In particular, the individual agents learning directly from the joint reward without decomposition or information channels, has worse performance with lower ?. The Adam <ref type="bibr" target="#b15">(Kingma and Ba, 2014)</ref> learning rate scheme initialized with 0.0001 is uniformly used, and further fine-tuning this per agent (not domain) does not dramatically change the total performance. The agents that we evaluate are listed in the table above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Environments</head><p>We use 2D grid worlds with the same basic functioning as <ref type="bibr" target="#b18">Leibo et al. (2017)</ref>, but with different tasks we call Switch, Fetch and Checkers. We have observations of byte values of size 3 ? 5 ? 5 (RGB), which represent a window depending on the player's position and orientation by extending 4 squares ahead and 2 squares on each side. Hence, agents are very short-sighted. The actions are: step forward, step backward, step left, step right, rotate left, rotate right, use beam and stand still. The beam has no effect in our games, except for lighting up a row or column of squares straight ahead with yellow. Each player appears as a blue square in its own observation, and the other player, when in the observation window, is shown in red for Switch and Escape, and lighter blue for Fetch. We use three different maps shown in <ref type="figure" target="#fig_1">Figure 3</ref> for both Fetch and Switch and a different one for Checkers, also shown in <ref type="figure" target="#fig_1">Figure 3</ref> (bottom right). The tasks repeat as the agents succeed (either by full reset of the environment in Switch and Checkers or just by pickup being available again in Fetch), in training for 5,000 steps and 2,000 in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switch:</head><p>The task tests if two agents can effectively coordinate their use of available routes on two maps with narrow corridors. The task is challenging because of strong observation aliasing. The two agents appear on different ends of a map, and must reach a goal at the other end. If agents collide in a corridor, then one agent needs to leave the corridor to allow the other to pass. When both players have reached their goal the environment is reset. A point is scored whenever a player reaches a goal.</p><p>Fetch: The task tests if two agents can synchronize their behaviour, when picking up objects and returning them to a drop point. In the Fetch task both players start on the same side of the map and have pickup points on the opposite side. A player scores 3 points for the team for pick-up, and another 5 points for dropping off the item at the drop point near the starting position. Then the pickup is available to either player again. It is optimal for the agents to cycle such that when one player reaches the pickup point the other returns to base, to be ready to pick up again.</p><p>Checkers: The map contains apples and lemons. The first player is very sensitive and scores 10 for the team for an apple (green square) and ?10 for a lemon (orange square). The second, less sensitive player scores 1 for the team for an apple and ?1 for a lemon. There is a wall of lemons between the players and the apples. Apples and lemons disappear when collected, and the environment resets when all apples are eaten. It is important that the sensitive agent eats the apples while the less sensitive agent should leave them to its team mate but clear the way by eating obstructing lemons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare the eight approaches listed in <ref type="table">Table 1</ref>, on the seven tasks. Each is run ten times, with different random seeds determining spawn points in the environment, as well as initializations of the neural networks. We calculated curves of the average performance over 50,000 episodes (plots in Appendix A) for each approach on each task and we display the normalized area under the curve in <ref type="figure" target="#fig_2">Figure 4</ref>. <ref type="figure">Figure 5</ref> displays the normalized final performance averaged over runs and the last 1,000 episodes. Average performance across tasks is also shown for both ways of evaluation.</p><p>The very clear conclusion is that architectures based on value-decomposition perform much better, with any combination of other techniques or none, than the centralized approach and individual learners. The centralized agent with value-decomposition is better than the combinatorially centralized as well as individual learners while worse than the more individual agents with value-decomposition.</p><p>We particularly see the benefit of shared weights on the hard task of Fetch with one corridor. Without sharing, the individual value-decomposition agent suffers from the lazy agent problem. The agent with weight sharing and role information also perfectly learns the one corridor Fetch task. It performs  better than the agent just sharing weights on Switch, where coordination, in particular with one corridor, is easier with non-identical agents. Further, shared weights are problematic for the Checkers task because the magnitude of rewards (and hence the value function) from one agent is ten times higher than for the other agent.</p><p>Adding information channels does increase learning complexity because the input comes from more than one agent. However, the checkers task, designed for the purpose, shows that it can be very useful. Overall, the low-level channels where the agent's LSTM processes the combined observations of both agents turned out to learn faster in our experiments than the more centralized high level communication (after the LSTM). <ref type="figure">Figure 6</ref> shows the learned Q-decomposition for the value-decomposition network, using shared weights, in the game of Fetch. A video of the corresponding game can be seen at Video (2017). Spikes correspond to pick-up events (short spikes, 3 reward points), and return events (large spikes, 5 reward points). These are separated into events due to agent 1 (blue spikes) and agent 2 (red spikes). This disambiguation is for illustration purposes only: the environment gives a reward to the whole team for all of these events. The total Q-function is seen in yellow, clearly anticipating the team reward events, and dropping shortly afterwards. The component Q-functionsQ 1 andQ 2 for agents 1 and 2 are shown in green and purple. These have generally disambiguated the Q-function into rewarding events separately attributable to either player. The system has learned to autonomously decompose the joint Q-function into sensible components which, when combined, result in an effective Q-function. This would be difficult for independent learners since many rewards would not be observed by both players, see e.g. the situation at 15-16 seconds in the corresponding video available at Video (2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Learned Q-Decomposition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We study cooperative multi-agent reinforcement learning where only a single joint reward is provided to the agents. We found that the two naive approaches, individual agents learning directly from team reward, and fully centralized agents, provide unsatisfactory solutions as previous literature has found in simpler environments, while our value-decomposition networks do not suffer from the same problems and shows much better performance across a range of more complex tasks. Further, the approach can be nicely combined with weight sharing and information channels, leading to agents that consistently optimally solve our new benchmark challenges.</p><p>Value-decomposition networks are a step towards automatically decomposing complex learning problems into local, more readily learnable sub-problems. In future work we will investigate the scaling of value-decomposition with growing team sizes, which make individual learners with team reward even more confused (they mostly see rewards from other agents actions), and centralized learners even more impractical. We will also investigate decompositions based on non-linear value aggregation.</p><p>Appendix A: Plots </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Independent agents architecture showing how local observations enter the networks of two agents over time (three steps shown), pass through the low-level linear layer to the recurrent layer, and then a dueling layer produces individual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Maps for Fetch and Switch: open map (top left), map with 1 corridor (bottom left) and 2 corridors (top right). The green square is the goal for the agent to the left (blue). A similar goal is seen for the other agent (red) to the left but not displayed. The agents' observation windows are shown in the bottom left. Bottom right is the map for Checkers. Lemons are orange, apples green and agents red/blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Barplots showing normalized AUC for each agent and domain over 50000 episodes of training and the mean across domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Heatmap showing each agent's final performance, averaged over the last 5,000 episodes of 50,000 and across ten runs, normalized by the best architecture per task. The agents are ordered according to average over the domains, which can be seen in the right most column. Value-Decomposition architecture strongly outperform Individual Learners and Centralization The learned Q-decomposition in Fetch.The plot shows the total Q-function (yellow), the value of agent 1 (green), the value of agent 2 (purple), rewards from agent 1 (blue) events and agent 2 (red). Highlighted is a situation in which agent 2's Q-function spikes (purple line), anticipating reward for an imminent drop-off. The other agent's Q-function (green) remains relatively flat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :Figure 9 :</head><label>789</label><figDesc>Average reward with 90% confidence intervals for ten runs of the nine architectures on the Fetch domain with the open map Average reward with 90% confidence intervals for ten runs of the nine architectures on the Fetch domain with one corridor Average reward with 90% confidence intervals for ten runs of the nine architectures on the Fetch domain with two corridorsFigure 10: Average reward with 90% confidence intervals for ten runs of the nine architectures on the Switch domain with the open map Figure 11: Average reward with 90% confidence intervals for ten runs of the nine architectures on the Switch domain with one corridor Figure 12: Average reward with 90% confidence intervals for ten runs of the nine architectures on the Switch domain with two corridors Figure 17: High-level communication Architecture Figure 18: Low-level communication Architecture Figure 19: Independent Agents Architecture Figure 20: Combinatorially Centralized Architecture Figure 21: High+Low-level communication Architecture</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For example, imagine training a 2-player soccer team using RL with the number of goals serving as the team reward signal. Suppose one player has become a better scorer than the other. When the worse player takes a shot the outcome is on average much worse, and the weaker player learns to avoid taking shots<ref type="bibr" target="#b11">(Hausknecht, 2016)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Or, more generally, across agent histories.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Diagrams</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing and visualizing multiagent rewards in dynamic and stochastic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="320" to="338" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social reward shaping in the prisoner&apos;s dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>De Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2008)</title>
		<meeting><address><addrLine>Estoril, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1389" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The complexity of decentralized control of Markov Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;00: Proceedings of the 16th Conference in Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Stanford, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-07-03" />
			<biblScope unit="page" from="32" to="37" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Schutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions of Systems, Man, and Cybernetics Part C: Applications and Reviews</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98, IAAI 98</title>
		<meeting>the Fifteenth National Conference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98, IAAI 98<address><addrLine>Madison, Wisconsin, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="746" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local approximation of difference evaluation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duchow-Pressley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Joint Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the Fifteenth International Joint Conference on Autonomous Agents and Multiagent Systems<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Potential-based difference rewards for multiagent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yliniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Joint Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the Thirteenth International Joint Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Potential-based reward shaping for finite horizon online POMDP planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="445" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="2137" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coordinated reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<idno>1-55860-873-7</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=645531.757784" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Conference on Machine Learning, ICML &apos;02</title>
		<meeting>the Nineteenth International Conference on Machine Learning, ICML &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating recurrence and eligibility traces in deep Q-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Reinforcement Learning Workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cooperation and Communication in Multiagent Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep recurrent Q-learning for partially observable MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno>abs/1507.06527</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining reward shaping and hierarchies for scaling to large multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holmesparker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agogino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nash q-learning for general-sum stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1039" to="1069" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning for urban traffic control using coordination graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Antwerp, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-15" />
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The world of independent learners is not Markovian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Fort-Piat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Know.-Based Intell. Eng. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-agent Reinforcement Learning in Sequential Social Dilemmas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2017)</title>
		<meeting>the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2017)<address><addrLine>Sao Paulo, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Eleventh International Conference</title>
		<meeting><address><addrLine>New Brunswick, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
		<respStmt>
			<orgName>Rutgers University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Friend-or-foe q-learning in general-sum games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06-28" />
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML 1999<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Concise Introduction to Decentralized POMDPs. SpringerBriefs in Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal and approximate q-value functions for decentralized pomdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T J</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent learning: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Panait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="434" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling difference rewards for multiagent learning (extended abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Proper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Joint Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the Eleventh International Joint Conference on Autonomous Agents and Multiagent Systems<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Artificial Intelligence: A Modern Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>3 nd edition</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Q-decomposition for reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zimdars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="656" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML 1999<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="371" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiagent communication with backpropagation. CoRR, abs/1605.07736</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1605.07736" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithms for Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesv?ri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of collectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collectives and the Design of Complex Systems</title>
		<editor>K. Tumer and D. Wolpert</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multiagent learning: Basics, challenges, and prospects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>AI Magazine</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="41" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coordinated deep reinforcement learners for traffic light control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Video for the q-decomposition plot</title>
		<ptr target="https://youtu.be/aAH1eyUQsRo" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
