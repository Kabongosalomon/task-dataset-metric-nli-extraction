<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
							<email>zhengzhuo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
							<email>maailong007@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
							<email>zhongyanfei@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Change is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For high spatial resolution (HSR) remote sensing images, bitemporal supervised learning always dominates change detection using many pairwise labeled bitemporal images. However, it is very expensive and time-consuming to pairwise label large-scale bitemporal HSR remote sensing images. In this paper, we propose single-temporal supervised learning (STAR) for change detection from a new perspective of exploiting object changes in unpaired images as supervisory signals. STAR enables us to train a high-accuracy change detector only using unpaired labeled images and generalize to real-world bitemporal images. To evaluate the effectiveness of STAR, we design a simple yet effective change detector called ChangeStar, which can reuse any deep semantic segmentation architecture by the ChangeMixin module. The comprehensive experimental results show that ChangeStar outperforms the baseline with a large margin under single-temporal supervision and achieves superior performance under bitemporal supervision. Code is available at https://github. com/Z-Zheng/ChangeStar.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object change detection using multi-temporal high spatial resolution (HSR) remote sensing imagery is a meaningful but challenging fundamental task in remote sensing and earth vision, which can provide more accurate object change information of land surface for urban expansion, urban planning, environmental monitoring, and disaster assessment <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11]</ref>. This task takes bitemporal images as input and outputs pixel-wise object change.</p><p>The dominating change detection methods are based on deep convolutional neural networks (ConvNet) toward highaccuracy and reliable geospatial object change detection in complex application scenarios. Learning a ConvNet change detector needs a large number of pairwise labeled bitemporal images with bitemporal supervision, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref> * corresponding author.  (a). However, pairwise labeling large-scale and high-quality bitemporal HSR remote sensing images is very expensive and time-consuming because of the extensive coverage of remote sensing images. This significantly limits the realworld applications of the change detection technique.</p><p>We observed that the importance of pairwise labeled bitemporal images lies in that the change detector needs paired semantic information to define positive and negative samples for object change detection. These positive and negative samples are usually determined by whether the pixels at two different times have different semantics in the same geographical area. The semantics of bitemporal pixels controls the label assignment, while the positional consistency condition 1 is only used to guarantee independent and <ref type="bibr" target="#b0">1</ref> The bitemporal pixels should be at the same geographical position. arXiv:2108.07002v2 [cs.CV] 11 Aug 2022 identically distributed (i.i.d.) training and inference. It is conceivable that change is everywhere, especially between unpaired images, if we relax the positional consistency condition to define positive and negative samples.</p><p>In this paper, we propose a single-temporal supervised object change detection approach to bypass the problem of collecting paired labeled images by exploiting object change between unpaired images as supervisory signals, as shown in <ref type="figure" target="#fig_1">Fig. 1 (b)</ref>. This approach enables us to train a high-accuracy change detector using unpaired labeled images and generalize to real-world bitemporal images at the inference stage. Because it only needs single-temporal semantic segmentation labels to construct object changes as change detection labels, we refer to our approach as Single-Temporal supervised leARning (STAR).</p><p>Conditioned by the same geographical area, bitemporal supervised learning can avoid many out-of-distribution positive samples, whereas this is both an opportunity and a challenge for the STAR. These out-of-distribution samples make the change detector driven by STAR more potential to possess better generalization. Meanwhile, they also cause the overfitting problem to make the model learn biased representation. To alleviate this problem, we explore an inductive bias: temporal symmetry and leverage it to constraint the representation learning for the change detector.</p><p>To demonstrate the effectiveness of STAR algorithm, we design a simple yet unified change detector called ChangeStar, which follows the modular design and is made up of an arbitrary deep semantic segmentation model and the ChangeMixin module driven by STAR. The ChangeMixin module is designed to enable an arbitrary deep semantic segmentation model to detect object change. This allows ChangeStar to reuse excellent semantic segmentation architectures to assist in change detection without extra specific architecture design, which bridges the gap between semantic segmentation and change detection.</p><p>The main contributions of this paper are summarized as follows:</p><p>? To fundamentally alleviate the problem of collecting paired labeled images, we proposed single-temporal supervised learning (STAR) to enable object change detectors to learn from unpaired labeled images.</p><p>? To further stabilize the learning, we explore and leverage an inductive bias, temporal symmetry, to alleviate the overfitting problem caused by the absence of positional consistency condition in unpaired images.</p><p>? To reuse the modern semantic segmentation architectures, we proposed a simple yet effective multi-task architecture, called ChangeStar, for joint semantic segmentation and change detection. The core component of ChangeStar is the ChangeMixin, which enables offthe-shelf segmentation model to detect object change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Change Detection. Different from general remote sensing change detection <ref type="bibr" target="#b22">[23]</ref>, object change detection is an object-centric change detection, which is aimed at answering the question of whether the object of interest has been changed. By the type of change, object change detection can be divided into two categories: binary object change detection, i.e. building change detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3]</ref>, and semantic object change detection, i.e. building damage assessment <ref type="bibr" target="#b10">[11]</ref>, land cover change detection <ref type="bibr" target="#b23">[24]</ref>. Binary object change detection is a fundamental problem for object change detection. Thus, we focus on binary object change detection in this work.</p><p>Bitemporal Supervised Learning. So far the supervised object change detection methods are based on bitemporal supervised learning, which needs change labels from bitemporal remote sensing images of the same area. Although there are many change detection benchmark datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>, their scales are still limited for meeting deep learning model. Because the pairwise annotation is very expansive and time-consuming. Therefore, a more label-efficient learning algorithm for the change detector is necessary for real-world applications.</p><p>Deep ConvNet Change Detector. Towards HSR remote sensing geospatial object change detection, the dominant change detectors are based on deep ConvNet <ref type="bibr" target="#b16">[17]</ref>, especially fully convolutional siamese network (FC-Siam) <ref type="bibr" target="#b6">[7]</ref>. FC-Siam adopted a weight-shared encoder to extract temporal-wise deep features and then used a temporal feature difference decoder to detect object change from the perspective of encoder-decoder architecture. The further improvements mainly focus on three perspectives of the encoder, i.e. using pretrained deep network as the encoder <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, the decoder, i.e. RNN-based decoders <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>, spatial-temporal attention-based decoders <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, and the training strategy, i.e. deep supervision for multiple outputs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. It can be found that there are obvious redundant network architecture designs because these network architectures are motivated by the modern semantic segmentation models. Therefore, it is significantly important for the next generation change detector to reuse modern semantic segmentation architectures.</p><p>Object Segmentation. An intuitive yet effective singletemporal supervised object change detection method is the post-classification comparison (PCC), which can serve as a strong baseline with the help of the modern object segmentation model. However, this method only simply treats the change detection task as the semantic segmentation task and ignores the temporal information modeling, thus significantly decreasing the performance. The t 1 image must be coregistered with the t 2 image for the accurate supervision.</p><formula xml:id="formula_0">(a) t 1 image (b) t 2 image (c) t 1 ? t 2 label</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Rethinking Bitemporal Supervised Learning</head><p>Learning an object change detector with bitemporal supervision can be formulated as an optimization problem:</p><formula xml:id="formula_1">min ? L(F ? (X t1 , X t2 ), Y t1?t2 )<label>(1)</label></formula><p>where L indicates the objective function that minimizes the cost between the prediction obtained by the object change detector F ? on paired bitemporal images X t1 , X t2 ? R N ?C?H?W and change label Y t1?t2 ? R N ?H?W representing the change happened in the time period from t 1 to t 2 . For example, <ref type="figure" target="#fig_2">Fig. 2</ref> presents a training sample of bitemporal supervised object change detection. The core of bitemporal supervised learning is to train a change detector with labeled images at the same spatial position and different times, thus, the training stage is consistent with the inference stage. From Eq. 1, we can find that change label Y t1?t2 is the only source of supervisory signals. To obtain Y t1?t2 , paired semantic information is usually needed to define the positive and negative samples. However, paired semantic information is only related to the semantics of bitemporal pixels and is unrelated to their spatial positions. The same spatial position is only used to guarantee the consistency between training and inference. If we relax this condition, the original problem in Eq. 1 can be simplified as:</p><formula xml:id="formula_2">min ? L(F ? (X i , X j ), compare(Y i , Y j ))<label>(2)</label></formula><p>where X i , X j can be two unpaired images, and supervisory signals are more efficiently collected from semantic comparison between their semantic label Y i , Y j . The model learned by Eq. 2 is a superset of the model learned by Eq. 1, which is allowed to detect object change in any context, including multi-temporal remote sensing images of the same area. The original problem can be significantly simplified.  </p><formula xml:id="formula_3">(a) X t 1 (b) ?X t 1 (c) change label</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Single-Temporal Supervised Learning</head><p>The key idea of single-temporal supervised learning (STAR) is to learn a generalizable object change detector from arbitrary image pairs with only semantic labels via Eq. 2, as shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. To provide change supervisory signals with single-temporal data, pseudo bitemporal image pairs are first constructed. Leveraging pseudo bitemporal image pairs, bitemporal data in the original learning problem (Eq. 1) can be replaced with single-temporal data, thus the learning problem can be reformulated as:</p><formula xml:id="formula_4">min ? L(F ? (X t1 , ?X t1 ), Y t1 ? ?Y t1 )<label>(3)</label></formula><p>where pseudo bitemporal image pairs X t1 , ?X t1 with their change label Y t1 ? ?Y t1 provide single-temporal supervision. The superscript t 1 is only used to represent that the data is single-temporal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Constructing Pseudo Bitemporal Image Pair</head><p>To provide change supervisory signals with single-temporal data, we first construct pseudo bitemporal image pairs in a mini-batch and then assign labels to them during training. Random Permutation in Mini-batch. Given a mini-batch single-temporal images X t1 with its semantic labels Y t1 , X t1 can be seen as a sequence {X t1 1 , ..., X t1 n }. We use a random permutation ? ? S n of this sequence to generate a new sequence ?X t1 to replace the X t2 , where S n denotes the all permutations of indices {1, ..., n} except the permutations that cause any same element with the original sequence, and ?X t1 denotes the sequence {X t1 ?(1) , ..., X t1 ?(n) }. <ref type="figure" target="#fig_3">Fig. 3</ref> (a) and (b) present the original sequence of three images and the new sequence in case of a mini-batch of three images. Label Assignment. Different from manually pairwise dense labeling for bitemporal supervised learning, change labels are automatically generated by single-temporal semantic labels for STAR. Without loss of generality, we discuss the label assignment for binary object change for simplicity. The positive labels of object change are assigned to the pixel positions in which the object of interest only once appeared. If there are two object instances overlapped at pseudo bitemporal images, the pixel positions in the overlapping area are assigned as negative samples. Because the object change is only semantic-aware, not instance-aware. The rest of the pixel positions are assigned as negative samples. To implement this label assignment, logical exclusive OR (xor) operation is a natural choice to obtain change labels with semantic labels Y t1 . In this way, change labels Y t1?t2 in Eq. 1 can be replaced with Y t1 ? ?Y t1 , where ? denotes the xor operation, thus providing change supervisory signals with single-temporal data. <ref type="figure" target="#fig_3">Fig. 3</ref> (c) demonstrates the generated change labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-task Supervision</head><p>The overall objective function L is a multi-task objective function, which is used to sufficiently exploit singletemporal semantic labels for joint object segmentation and object change detection, as follows:</p><formula xml:id="formula_5">L = L seg + L change<label>(4)</label></formula><p>This work focus on the fundamental problem: binary object change, thus, there is only one type of object of interest. Therefore, we introduce the objective functions for binary classification, whereas it is straightforward to extend this to the multi-class case. Semantic Supervision. For binary object segmentation, we adopt binary cross-entropy loss L binary as L seg to provide semantic supervision, as follows:</p><formula xml:id="formula_6">L binary (p, y) = ?ylog(p) + (1 ? y)log(1 ? p)<label>(5)</label></formula><p>where y ? {0, 1} specifies the ground-truth class and p ? [0, 1] denotes predicted probability for positive class. Change Supervision by Temporal Symmetry. Temporal symmetry is a mathematical property of binary object change, which indicates that binary object change is undirected, i.e. Y t1?t2 = Y t2?t1 . Intuitively, the outputs of binary object change detector on the bitemporal image pair should follow this property. This means that the binary object change detector should not fit the temporal direction under the constraint of temporal symmetry. Motivated by this, we further propose symmetry loss for binary object change detection, which is formulated as follows:</p><formula xml:id="formula_7">L change = 1 2 [L binary (F ? (X t1 , ?X t1 ), Y t1 ? ?Y t1 ) +L binary (F ? (?X t1 , X t1 ), Y t1 ? ?Y t1 )]<label>(6)</label></formula><p>The symmetry loss features an inductive bias provided by temporal symmetry, which servers as a regularization term to alleviate the overfitting problem in binary object change detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture of ChangeStar</head><p>ChangeStar is a simple yet unified network composed of a deep semantic segmentation model and the ChangeMixin module. This design is inspired by reusing the modern semantic segmentation architecture because semantic segmentation and object change detection are both dense prediction tasks. To this end, we design the ChangeMixin module to enable any off-the-shelf deep semantic segmentation model to detect object change. <ref type="figure" target="#fig_6">Fig. 5</ref> presents the overall architecture of ChangeStar. Any Segmentation Model. The deep semantic segmentation model is used to extract a convolutional feature map for each image of the bitemporal inputs, respectively. The top block of a segmentation model is always a 3?3 conv layer with C filters, followed by an upsampling layer, where C is the number of classes and the upsampling scale is equal to the output stride of the specific segmentation model. The feature map for object segmentation is computed by the  whole segmentation model, while the feature map for object change detection is only computed by the ConvNet part of the segmentation model. ChangeMixin. The ChangeMixin module is composed of a temporal swap module (TSM) and a small FCN composed of N 3?3 conv layers, each with d c filters and each followed by BN and ReLU. Besides, a bilinear upsampling layer followed by a sigmoid activation is attached to output the binary predictions per pixel. The temporal swap module (Eq. 7) is responsible for temporal symmetry, providing an inductive bias in the network architecture, which takes bitemporal feature maps as input and then concatenates them along the channel axis in two different temporal permutations.</p><formula xml:id="formula_8">TSM(T 1 , T 2 ) = cat(T 1 , T 2 ), cat(T 2 , T 1 )<label>(7)</label></formula><p>where T 1 and T 2 denote bitemporal feature maps, respectively. During training, the small FCN is attached to each output of TSM and the weight of the small FCN is shared. During inference, the small FCN is only attached to the first output of TSM because we find that two outputs are temporal-symmetric in the converged model. We use N = 4 and d c = 16 for a better trade-off between speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experimental results on two HSR remote sensing building change detection datasets using the model trained on two HSR remote sensing building segmentation datasets with different domains, respectively, for a comprehensive analysis of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Training Datasets. Two HSR remote sensing building segmentation datasets were used to train segmentation models and object change detectors by single-temporal supervision.</p><p>? xView2 pre-disaster. We used a subset of the xView2 dataset <ref type="bibr" target="#b10">[11]</ref>, namely xView2 pre-disaster, which is made up of the pre-disaster images and their annotations from train split and tier3 split. The xView2 pre-disaster dataset consists of 9,168 HSR optical remote sensing images with a total of 316,114 building instances annotations in the context of the suddenonset natural disaster. The images were collected from the Maxar / DigitalGlobe Open Data Program 2 , and each image has a spatial size of 1024?1024 pixels. ? SpaceNet2. The public SpaceNet2 dataset <ref type="bibr" target="#b25">[26]</ref> consists of 10,590 HSR optical remote sensing images in the context of the urban scenarios, which were collected from DigitalGlobe's WorldView-3 satellite. This dataset also provides the annotation of 219,316 urban building instances. Each image has a spatial size of 650?650 pixels with a spatial resolution of 0.3 m.</p><p>In this study, we only used 3-bands pansharpened RGB images and their annotations.</p><p>Evaluation Datasets. Two large scale HSR remote sensing building change detection datasets were used to evaluate the performance of object change detection.</p><p>? WHU building change detection. This dataset <ref type="bibr" target="#b14">[15]</ref> consists of two aerial images obtained in 2012 and 2016 at same area of 20.5 km 2 , which contains 12,796 and 16,077 building instances respectively. Each image has a spatial size of 15354?32507 pixels with a spatial resolution of 0.2 m. There were a large number of rebuilt buildings and new constructions in this area because of a 6.3-magnitude earthquake in February 2011.</p><p>? LEVIR-CD. LEVIR-CD dataset <ref type="bibr" target="#b2">[3]</ref> consists of 637 HSR bitemporal remote sensing image pairs, which were collected from Google Earth platform. Each image has a spatial size of 1024?1024 pixels with a spatial resolution of 0.5 m. For annotation, this dataset provides a total of 31,333 change labels of building instances but without semantic labels. This dataset includes not only building appearing but also building disappearing for more general building changes. LEVIR-CD dataset is officially split into train, val, and test, three parts of which include 445,64, and 128 pairs, respectively. If not specified, the whole dataset (LEVIR-CD all ) is used for evaluation.</p><p>Implementation detail. Unless otherwise specified, all models were trained for 40k iterations with a poly learning rate policy, where the initial learning rate was set to 0.03 and multiplied by (1 ? step max step ) ? with ? = 0.9. We used SGD as the optimizer on single Titan RTX GPU with a minibatch of 16 images, weight decay of 0.0001 and momentum of 0.9. For training data augmentation, after horizontal and vertical flip, rotation of 90 ? k (k = 1, 2, 3) degree, and scale jitter, the images are then randomly cropped into 512?512 pixels for xView2 pre-disaster dataset and 256?256 pixels for SpaceNet2 dataset. Metrics. The binary object change detection belongs to pixel-wise binary classification task, therefore we adopt intersection over union (IoU) and F 1 score to evaluate the object change detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head><p>In the weakly-supervised setting that only singletemporal supervision is available, PCC series are reasonable baselines when using strong semantic segmentation models. Thus, we compare ChangeStar against PCCs with many representative segmentation models <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">30]</ref>. The results listed in <ref type="table" target="#tab_0">Table 1</ref> show that ChangeStar significantly outperforms PCC with different segmentation models in this challenging cross-domain evaluation. Notably, these improvements only come at the cost of much slight overhead, which confirms the significance of learning object change representation. Overall, training on the xView2 pre-disaster is obviously superior to training on SpaceNet2. We conjecture that richer background distribution of the xView2 pre-disaster can provide more diverse positive samples, which facilitates more generalized object change representation. Besides, the images of xView2 predisaster have multiple spatial resolution, while th images of SpaceNet2 have a fixed spatial resolution of 0.3 m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To delve into the proposed method, we conducted comprehensive experiments using ChangeStar based on FarSeg with ResNet-50 if not specified, since it is more robust than other variants of ChangeStar. Architecture of ChangeMixin. The ChangeMixin module is the most important component in ChangeStar, which introduces two hyper-parameters: N (number of conv layers) and d c (number of convolutional filters). The performance of ChangeStar with varying N are presented in <ref type="figure" target="#fig_8">Fig. 6 (a)</ref>. It can be found that over-deep convolutional subnetwork is harmful to the object change detection performance. ChangeStar performs better than the postclassification comparison when N &lt; 6 and achieves best performance at N = 4. The performance of ChangeStar with varying d c are presented in <ref type="figure" target="#fig_8">Fig. 6 (b)</ref>. As d c increases, the performance constantly decreases and worse than the post-classification comparison when d c ? 80. For a better trade-off between speed and accuracy, we use N = 4 and d c = 16 as the default setting.   Importance of Semantic Supervision. Semantic supervision not only enables ChangeStar to segment objects but also can facilitate object change representation learning. Table 2 (b)/(c) and (d)/(e) show that the introduction of semantic supervision is positive for object change detection. Quantitatively, semantic supervision improves the baseline by 0.57% IoU and 0.43% F 1 and improves the baseline with temporal symmetry by 1.61% IoU and 1.19% F 1 . This indicates that semantic representation provided by semantic supervision facilitates object change representation learning, and object change representation is stronger when possessing temporal symmetry. Importance of Temporal Symmetry. Temporal symmetry, as a mathematical property of binary object change, can provide a prior as regularization to learn more robust object change representation. <ref type="table" target="#tab_1">Table 2</ref> (a)/(d) and (c)/(e) shows that using temporal symmetry gives a 2.25% IoU and 1.69% F 1 gains over the baseline and gives a 3.29% IoU and 2.45% F 1 over the baseline with semantic supervision. This indicates that it is significantly important to guarantee temporal symmetry in binary object change detection for STAR. We can also find that temporal symmetry makes it better to learn object change representation from semantic representation. Label assignment. Here we discuss the impact of different label assignment strategies on accuracy. <ref type="table" target="#tab_2">Table 3</ref> presents that using or achieves 43.84% IoU, while using xor achieves 65.71% IoU. This is because these negative samples (i.e. overlapped region) are necessary to make the model learn to suppress false positives that occurred on objects that have not changed in the period from t 1 to t 2 , which can be satisfied by xor. However, or operation wrongly assigns their labels. ChangeStar using Bitemporal Sup. ChangeStar is a object change detection architecture driven by STAR as default, but it also can be driven by bitemporal supervision. We benchmark many variants of ChangeStar and the results are presented in <ref type="table" target="#tab_3">Table 4</ref>. We can find that atrous convo-   <ref type="bibr" target="#b30">[30]</ref>. Besides, the deeper backbone network brings more accuracy gains, which achieves agreement to other vision tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. These results suggest that ChangeStar is a simple yet effective object change detection architecture. Bitemporal Sup. vs. Single-Temporal Sup. Singletemporal supervision belongs to weak supervision for object change detection. To investigate the gap between bitemporal supervision and single-temporal supervision, we conducted comprehensive experiments to analyze their performance difference. The results are presented in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>We observe that there is 16?19% F 1 gap between PCC and bitemporal supervised methods. Our STAR can significantly bridge the gap to within 10% when using a large backbone. And it can be seen that the performance gap keeps getting smaller as the backbone network goes deeper. Error analysis. Comparing <ref type="figure" target="#fig_9">Fig. 7</ref> (e) with <ref type="figure" target="#fig_9">Fig. 7 (d)</ref> and (f), we can find that the error of PCC mainly lies in false positives due to various object appearance and object geometric offsets. This is because PCC only depends on semantic prediction to compare. To alleviate this problem, that bitemporal supervision directly learns how to compare from pairwise labeled data, while STAR learns how to compare from unpaired labeled data. From <ref type="figure" target="#fig_9">Fig. 7 (d</ref>)/(f), STAR is partly impacted by false positives due to the complete absence of the actual negative samples, e.g. the same object at different times. Nevertheless, STAR can still learn helpful object change representation to recognize many unseen negative examples successfully.</p><p>Does STAR really work? ChangeStar can simultaneously output bitemporal semantic predictions and the change prediction. The change prediction can also be obtained by semantic prediction comparison. We thus show their learning curves to explore their relationship, as shown in <ref type="figure">Fig. 8</ref>. We find that the semantic representation learning has a faster convergence speed than the object change representation learning in ChangeStar. In the early stage of training ((0, 40] epochs), semantic prediction comparison is superior to change prediction. This suggests that learning semantic representation is easier than learning object change representation. In the middle stage ((40, 60] epochs), change prediction achieves similar performance with semantic prediction comparison. After model convergence, change prediction achieves superior performance than semantic prediction comparison with a large margin. This observation suggests that STAR can bring extra contrastive information to assist object change representation learning rather than only benefit from semantic supervision.  <ref type="figure">Figure 8</ref>: Learning curves of IoU (%) and F 1 (%) on LEVIR-CD all using multi-task outputs from ChangeStar with FarSeg. The multi-task outputs include change mask from ChangeMixin and semantic masks from FarSeg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we present single-temporal supervised learning (STAR) to bypass the problem of collecting pairwise labeled images in conventional bitemporal supervised learning. STAR provides a new perspective of exploiting object changes in arbitrary image pairs as the supervisory signals. To demonstrate the effectiveness of STAR, we design a simple yet effective multi-task architecture, called ChangeStar, for joint semantic segmentation and object change detection, which can reuse any deep semantic segmentation architecture via the further proposed ChangeMixin module. The extensive experimental analysis shows its competitive performances in different domains with cheaper labels. We hope that STAR will serve as a solid baseline and help ease future research in weaklysupervised object change detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>areas (b) STAR: Single-Temporal supervised leARning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of conventional bitemporal supervised learning and the proposed single-temporal supervised learning for object change detection. By exploiting object changes in arbitrary image pairs as the supervisory signals, STAR makes it possible to learn a change detector from unpaired single-temporal images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Training sample of bitemporal supervised object change detection. (a) the image at time t 1 . (b) the image at time t 2 . (c) change label representing the change happened the time period from t 1 to t 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Pseudo bitemporal image pairs (a case of minibatch of three images) for single-temporal supervised learning. X t1 , ?X t1 are the original sequence and the new sequence generated by a random permutation ?. The change label is obtained by their semantic labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Overview of ChangeStar. The network architecture of ChangeStar is made up of an arbitrary deep semantic segmentation model and a ChangeMixin module. ChangeStar can be end-to-end trained by a segmentation loss and a symmetry loss with only single-temporal supervision. During training, weight sharing strategy is applied to the segmentation model and the ChangeMixin module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Network architecture of ChangeStar. The network architecture of ChangeStar is made up of a deep segmentation model and a ChangeMixin module. The ChangeMixin module contains a temporal swap module and many conv layers, each followed by BN and ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Object change detection results on LEVIR-CD all using different hyperparameter settings of the ChangeMixin Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Error analysis for ChangeStar with bitemporal supervision, PCC and ChangeStar with STAR. The basic segmentation model is FarSeg with ResNeXt-101 32x4d. The rendered colors represent true positives (TP), false positives (FP), and false negatives (FN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Change detection IoU (%) and F 1 (%) on WHU building change detection and LEVIR-CD datasets. The backbone network is ResNet-50 for all models. All methods were trained using only single-temporal images and their semantic segmentation labels.+16.71) 71.27 (+15.51) 65.21 (+9.02) 78.94 (+6.99) 37.63 (+10.03) 54.68 (+11.42) 25.86 (+18.77) 41.10 (+27.85) +26.56) 73.59 (+25.50) 65.71 (+10.62) 79.31 (+8.27) 39.02 (+11.33) 56.14 (+12.77) 30.42 (+22.45) 46.65 (+31.88)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Train on xView2 pre-disaster</cell><cell></cell><cell></cell><cell cols="2">Train on SpaceNet 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Segmentation Model</cell><cell></cell><cell>WHU</cell><cell cols="2">LEVIR-CD all</cell><cell cols="2">WHU</cell><cell cols="2">LEVIR-CD all</cell><cell cols="2">?Params (M) ?MAdds (B)</cell></row><row><cell></cell><cell></cell><cell>IoU (%)</cell><cell>F1 (%)</cell><cell>IoU (%)</cell><cell>F1 (%)</cell><cell>IoU (%)</cell><cell>F1 (%)</cell><cell>IoU (%)</cell><cell>F1 (%)</cell><cell></cell><cell></cell></row><row><cell>PCC</cell><cell>PSPNet [29]</cell><cell>37.46</cell><cell>54.51</cell><cell>55.87</cell><cell>71.69</cell><cell>21.39</cell><cell>35.25</cell><cell>10.19</cell><cell>18.50</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">ChangeStar (ours) + ChangeMixin</cell><cell cols="3">56.44 (+18.98) 72.15 (+17.64) 61.63 (+5.76)</cell><cell cols="2">76.26 (+4.57) 25.56 (+4.17)</cell><cell>40.72 (+5.47)</cell><cell>15.25 (+5.06)</cell><cell>26.47 (+7.97)</cell><cell>0.16</cell><cell>0.63</cell></row><row><cell>PCC</cell><cell>DeepLab v3 [5]</cell><cell>32.46</cell><cell>49.01</cell><cell>54.77</cell><cell>70.78</cell><cell>33.08</cell><cell>49.72</cell><cell>13.78</cell><cell>24.23</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">ChangeStar (ours) + ChangeMixin</cell><cell cols="3">56.85 (+24.39) 72.49 (+23.48) 60.94 (+6.17)</cell><cell cols="2">75.73 (+4.95) 35.57 (+2.49)</cell><cell>52.48 (+2.76)</cell><cell>15.92 (+2.14)</cell><cell>27.46 (+3.23)</cell><cell>0.08</cell><cell>0.33</cell></row><row><cell>PCC</cell><cell>DeepLab v3+ [6]</cell><cell>35.75</cell><cell>52.68</cell><cell>55.51</cell><cell>71.38</cell><cell>23.90</cell><cell>38.58</cell><cell>9.80</cell><cell>17.85</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">ChangeStar (ours) + ChangeMixin</cell><cell cols="3">52.01 (+16.26) 68.43 (+15.75) 57.96 (+2.45)</cell><cell cols="5">73.38 (+2.00) 38.42 (+15.42) 55.51 (+16.93) 22.22 (+12.42) 36.36 (+18.51)</cell><cell>0.08</cell><cell>0.33</cell></row><row><cell>PCC</cell><cell>Semantic FPN [16]</cell><cell>38.66</cell><cell>55.76</cell><cell>56.19</cell><cell>71.95</cell><cell>27.60</cell><cell>43.26</cell><cell>7.09</cell><cell>13.25</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">ChangeStar (ours) + ChangeMixin</cell><cell cols="9">55.37 (0.08</cell><cell>0.33</cell></row><row><cell>PCC</cell><cell>FarSeg [30]</cell><cell>31.66</cell><cell>48.09</cell><cell>55.09</cell><cell>71.04</cell><cell>27.69</cell><cell>43.37</cell><cell>7.97</cell><cell>14.77</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">ChangeStar (ours) + ChangeMixin</cell><cell cols="9">58.22 (0.08</cell><cell>0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Object change detection results on LEVIR-CD all for understanding the contribution of each component.</figDesc><table><row><cell>Method</cell><cell cols="2">STAR Semantic Sup. Temporal Sym. IoU (%) F1 (%)</cell></row><row><cell>(a) PCC</cell><cell>55.09</cell><cell>71.04</cell></row><row><cell>(b) Baseline</cell><cell>61.85</cell><cell>76.43</cell></row><row><cell>(c) Baseline w/ Semantic Sup.</cell><cell>62.42</cell><cell>76.86</cell></row><row><cell>(d) Baseline w/ Temporal Sym.</cell><cell>64.10</cell><cell>78.12</cell></row><row><cell>(e) ChangeStar</cell><cell>65.71</cell><cell>79.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The accuracy of different label assignment strategies.</figDesc><table><row><cell cols="3">Method IoU (%) F 1 (%)</cell></row><row><cell>or</cell><cell>43.84</cell><cell>60.96</cell></row><row><cell>xor</cell><cell>65.71</cell><cell>79.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Bitemporal supervised benchmark. All methods were trained on LEVIR-CD train and evaluated on LEVIR-CD test for fair comparison.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">IoU (%) F1 (%)</cell></row><row><cell>FCN + BAM [3]</cell><cell>ResNet-18</cell><cell>-</cell><cell>85.7</cell></row><row><cell>FCN + PAM [3]</cell><cell>ResNet-18</cell><cell>-</cell><cell>87.3</cell></row><row><cell>ChangeStar (PSPNet + ChangeMixin)</cell><cell>ResNet-18</cell><cell>78.08</cell><cell>87.69</cell></row><row><cell>ChangeStar (DeepLab v3 + ChangeMixin)</cell><cell>ResNet-18</cell><cell>77.95</cell><cell>87.61</cell></row><row><cell>ChangeStar (DeepLab v3+ + ChangeMixin)</cell><cell>ResNet-18</cell><cell>81.32</cell><cell>89.70</cell></row><row><cell cols="2">ChangeStar (Semantic FPN + ChangeMixin) ResNet-18</cell><cell>82.51</cell><cell>90.41</cell></row><row><cell>ChangeStar (FarSeg + ChangeMixin)</cell><cell>ResNet-18</cell><cell>82.31</cell><cell>90.29</cell></row><row><cell>ChangeStar (FarSeg + ChangeMixin)</cell><cell>ResNet-50</cell><cell>83.19</cell><cell>90.82</cell></row><row><cell>ChangeStar (FarSeg + ChangeMixin)</cell><cell cols="2">ResNeXt-101 32x4d 83.92</cell><cell>91.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Bitemporal supervision versus single-temporal supervision. All methods were evaluated on LEVIR-CD test for consistent comparison.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Training data</cell><cell cols="3">IoU (%) F1 (%) F1 gap (%)</cell></row><row><cell>Bitemporal Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ChangeStar (FarSeg + ChangeMixin) ResNet-18</cell><cell>LEVIR-CD train</cell><cell>82.31</cell><cell>90.29</cell><cell>-</cell></row><row><cell cols="2">ChangeStar (FarSeg + ChangeMixin) ResNet-50</cell><cell>LEVIR-CD train</cell><cell>83.19</cell><cell>90.82</cell><cell>-</cell></row><row><cell cols="3">ChangeStar (FarSeg + ChangeMixin) ResNeXt-101 32x4d LEVIR-CD train</cell><cell>83.92</cell><cell>91.25</cell><cell>-</cell></row><row><cell>Single-Temporal Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCC (FarSeg)</cell><cell>ResNet-18</cell><cell>xView2 pre-disaster</cell><cell>56.65</cell><cell>72.32</cell><cell>-17.97</cell></row><row><cell>PCC (FarSeg)</cell><cell>ResNet-50</cell><cell>xView2 pre-disaster</cell><cell>55.89</cell><cell>71.71</cell><cell>-19.11</cell></row><row><cell>PCC (FarSeg)</cell><cell cols="2">ResNeXt-101 32x4d xView2 pre-disaster</cell><cell>59.54</cell><cell>74.64</cell><cell>-16.61</cell></row><row><cell cols="2">ChangeStar (FarSeg + ChangeMixin) ResNet-18</cell><cell>xView2 pre-disaster</cell><cell>63.25</cell><cell>77.49</cell><cell>-12.08</cell></row><row><cell cols="2">ChangeStar (FarSeg + ChangeMixin) ResNet-50</cell><cell>xView2 pre-disaster</cell><cell>66.99</cell><cell>80.23</cell><cell>-10.58</cell></row><row><cell cols="3">ChangeStar (FarSeg + ChangeMixin) ResNeXt-101 32x4d xView2 pre-disaster</cell><cell>68.84</cell><cell>81.54</cell><cell>-9.71</cell></row><row><cell cols="6">lution based ChangeStars (PSPNet, DeepLab v3) achieves</cell></row><row><cell cols="6">compatible results with spatial-temporal attention based</cell></row><row><cell cols="6">methods (FCN + BAM and FCN + PAM). When introduc-</cell></row><row><cell cols="6">ing encoder-decoder architecture, ChangeStars (DeepLab</cell></row><row><cell cols="6">v3+, semantic FPN, FarSeg) achieves better performance by</cell></row><row><cell cols="6">a large margin. When further introducing FPN-family de-</cell></row><row><cell cols="6">coder, ChangeStars (semantic FPN, FarSeg) are superior to</cell></row><row><cell cols="6">other variants. We thus conclude that encoder-decoder and</cell></row><row><cell cols="6">FPN architectures are more friendly to object change detec-</cell></row><row><cell cols="5">tion, which may attribute to the multi-scale problem</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.digitalglobe.com/ecosystem/open-data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Change detection in optical aerial images by a multilayer conditional mixed markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam?s</forename><surname>Szir?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3416" to="3430" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constrained optical flow for aerial image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Marraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hichem</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="4176" to="4179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A spatial-temporal attentionbased method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Change detection in multisource vhr images via deep siamese convolutional multiple-layers recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongruixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2848" to="2864" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertr</forename><forename type="middle">Le</forename><surname>Rodrigo Caye Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Urban change detection for multispectral earth observation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertr</forename><forename type="middle">Le</forename><surname>Rodrigo Caye Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2115" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Caye</forename><surname>Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="page">102783</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Damage detection from aerial images via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aito</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riho</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Hikosaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hosfelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirav</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jigar</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><surname>Choset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gaston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09296</idno>
		<title level="m">A dataset for assessing building damage from satellite imagery</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Change detection from remotely sensed images: From pixel-based to object-based approaches. IS-PRS Journal of photogrammetry and remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masroor</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stanley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="91" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Change detection in remote sensing images using conditional adversarial networks. International Archives of the Photogrammetry, Remote Sensing &amp; Spatial Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Va Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A polsar change detection index based on neighborhood information for flood mapping. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahel</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahram</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meisam</forename><surname>Amani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Brisco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1854</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-toend change detection for high resolution satellite images using improved unet++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daifeng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1382</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Review article digital change detection techniques using remotely-sensed data. International journal of remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashbindu</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="989" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hi-ucd: A large-scale dataset for urban semantic change detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Shiqi Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03247</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Van Etten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><forename type="middle">M</forename><surname>Bacastow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01232</idno>
		<title level="m">Spacenet: A remote sensing dataset and challenge series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deodato</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchao</forename><surname>Liu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Separate segmentation of multi-temporal highresolution remote sensing images for object-based change detection in urban area</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
