<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Masked Image Reconstruction Network for Document-level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhang</surname></persName>
							<email>lzhang@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Cheng</surname></persName>
							<email>ydcheng@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Masked Image Reconstruction Network for Document-level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction aims to extract relations among entities within a document. Compared with its sentence-level counterpart, Documentlevel relation extraction requires inference over multiple sentences to extract complex relational triples. Previous research normally complete reasoning through information propagation on the mention-level or entity-level document-graphs, regardless of the correlations between the relationships. In this paper, we propose a novel Documentlevel Relation Extraction model based on a Masked Image Reconstruction network (DRE-MIR), which models inference as a masked image reconstruction problem to capture the correlations between relationships. Specifically, we first leverage an encoder module to get the features of entities and construct the entity-pair matrix based on the features. After that, we look on the entity-pair matrix as an image and then randomly mask it and restore it through an inference module to capture the correlations between the relationships. We evaluate our model on three public document-level relation extraction datasets, i.e. DocRED, CDR, and GDA. Experimental results demonstrate that our model achieves state-of-the-art performance on these three datasets and has excellent robustness against the noises during the inference process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to identify the semantic relations between entities from raw texts, which is of great importance to many real-world applications <ref type="bibr" target="#b2">[Qiu et al., 2019;</ref><ref type="bibr" target="#b11">Zhang et al., 2021c;</ref><ref type="bibr" target="#b11">Zhang et al., 2021b]</ref>. Previous researches focused on sentence-level RE, which predicts the relationship between entities in a single sentence <ref type="bibr" target="#b8">[Zeng et al., 2015;</ref><ref type="bibr" target="#b0">Baldini Soares et al., 2019]</ref>. However, large amounts of relationships are expressed by multiple sentences in real life <ref type="bibr" target="#b6">[Yao et al., 2019;</ref><ref type="bibr">Verga et al., 2018]</ref>. Therefore, many recent works have made efforts to extend sentence-level RE to document-level RE <ref type="bibr" target="#b6">[Yao et al., 2019;</ref><ref type="bibr" target="#b9">Zeng et al., 2020;</ref><ref type="bibr" target="#b11">Zhang et al., 2021a]</ref>. Compared with sentence-level RE where a sentence contains only one entity pair to be classified, document-level RE requires the model to classify the relations of multiple entity pairs simultaneously and the entities involved in a relationship may appear in different sentences. Besides, the documentlevel RE also poses a great challenge, i.e. relation inference. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is easy to identify the intra-sentence relations shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, such as (Altomonte, date of birth, 24 February 1694), (Altomonte, father, Martino Altomonte), and (Altomonte, country of citizenship, Austrian), owing to two related entities appear in the same sentence. However, it is more challenging for a model to predict the inter-sentential relations between Martino Altomonte and Austrian because the document does not explicitly express the relationship between them. This type of inter-sentential relations can only be identified through reasoning techniques. According to the statistics of the DocRED <ref type="bibr" target="#b6">[Yao et al., 2019]</ref> dataset which is a well-known document-level RE dataset, Most of the relation instances (61.1%) require reasoning to be identified in document-level RE, which indicates that reasoning is essential for the document-level RE. <ref type="figure">Figure 2</ref>: The overall architecture of our DRE-MIR model. Firstly, the Encoder encodes the input document to obtain the entities embedding (Es,Eo), and then we obtain the Entity-pair Matrix M through the linear layer. Secondly, we treat the entity-pair matrix M as an image, and then randomly mask it and restore it through an inference module. Through the Masked Image Reconstruction (MIR) task, our inference module can learn how to use the correlation between relationships to infer the masked relationship. Moreover, the MIR task contains two paths, i.e. the Original path and the Mask path. Finally, we utilize a classifier to predict the relationship of each entity pair. LR and LC represent reconstruction loss and classification loss, respectively.</p><p>To extract such complex inter-sentence relations, most current approaches constructed a document-level graph based on heuristics, structured attention, or dependency structures <ref type="bibr" target="#b9">[Zeng et al., 2020;</ref><ref type="bibr" target="#b2">Nan et al., 2020;</ref><ref type="bibr" target="#b1">Christopoulou et al., 2019;</ref>, and then perform inference with graph convolutional network (GCN) <ref type="bibr" target="#b2">Kipf and Welling, 2016]</ref> on the document-level graph. It should be noted that methods of this type complete reasoning through the information transfering between mentions or entities. Meanwhile, considering the transformer architecture can implicitly model long-distance dependencies and can be regarded as a tokenlevel fully connected graph, some studies  implicitly infers through the pre-trained model rather than via the document-level graphs.</p><p>However, these methods ignore the correlation between relationships. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we can easily infer the inter-sentence relation (Martino Altomonte, country of citizenship, Austrian) through the correlation between the relationships. Specifically, the model needs to firstly capture the correlation among (Altomonte, father, Martino Altomonte), (Altomonte, country of citizenship, Austrian), and (Martino Altomonte, country of citizenship, Austrian), and then use reasoning techniques to identify this complex inter-sentential relation as shown in <ref type="figure" target="#fig_0">Figure 1c</ref>.</p><p>To capture the interdependencies among the multiple relationships, DocuNet <ref type="bibr" target="#b11">[Zhang et al., 2021a]</ref> formulates the document-level RE as a semantic segmentation problem and uses a U-shaped segmentation module over the image-style feature map to capture global interdependencies among triples.</p><p>The DocuNet model has achieved the latest state-of-the-art performance, which shows that the correlation between relationships is essential for the document-level RE. However, capturing correlations between relations through convolutional neural networks is unintuitive and inefficient due to the intrinsic distinction between entity-pair matrices and image.</p><p>In this paper, we followed the DocuNet and model the document-level RE as a table filling problem. We first construct an entity-pair matrix, where each point represents the relevant feature of an entity pair. Then, the document-level RE model labels each point of the entity-pair matrix with the corresponding relationships class. Meanwhile, we also treat the entity-pair matrix as an image. To more effectively capture the interdependencies among the relations, we propose a novel Document-level Relation Extraction model based on a Masked Image Reconstruction network (DRE-MIR), which formulates the inference problem in document-level RE as a masked image reconstruction problem. As shown in <ref type="figure">Figure 2</ref>, we first randomly mask the entity-pair matrix, and then reconstruct the entity pair matrix through the inference model. Through this the Masked Image Reconstruction (MIR) task, our model can learn how to infer masked points with the help of correlations between relations. Moreover, to more efficiently and intuitively reconstruct the masked points in the entity-pair matrix, we propose an Inference Multi-head Self-Attention (I-MSA) module which can greatly improve the inference ability of the model. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the I-MSA contains four heads and each head corresponds to an inference mode including: A? * + * ?B =? A?B, A? * + B? * =? A?B, * ?A + B? * =? A?B, and * ?A + * ?B =? A?B.</p><p>Our contributions can be summarized as follows:</p><p>? To the best of our knowledge, our method is the first approach that treat the inference problem in documentlevel RE as an image reconstruction problem. ? We introduce the I-MSA to improve the model's ability to reconstruct the masked entity-pair matrix. ? Experimental results on three public document-level RE datasets shows that our Dense-CCNet model can achieve state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce in detail our DRE-MIR model. As shown in <ref type="figure">Figure 2</ref>, the DRE-MIR mainly consists of 3 modules, i.e. encoder module, inference module, and classifier module. We first describe the encoder module in Section 2.1, then introduce the core module, i.e. inference module, in Section 2.2 , finally we describe our classifier module and loss function in Section 2.3. <ref type="bibr" target="#b12">[Zhong and Chen, 2020]</ref> and  verified that marking entities in the input sentence by entity type can effectively improve the performance of sentence-level RE model. However, in document-level RE, each entity has multiple mentions and it is important to gather all the mention information for each entity. Therefore, we use the entity type and entity id to mark the mentions in the document, which not only can incorporate the entity type information earlier but also help to improve the aggregation of the mention information. Specifically, given document D = {w i } l j=1 containing l words, we first mark the mention in the document by inserting special symbols e t and e id at the start and end position of the mentions, where e t and e id respectively represent the entity type and entity id of the mention. Then we feed the adapted document to the pre-trained language model to obtain the context embedding of each word in the document:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder Module</head><formula xml:id="formula_0">H = [h 1 , ..., h l ] = Encoder([w 1 , ..., w l ]).</formula><p>(1)</p><p>Finally, we utilize the average of the embeddings of e t and e id to represent the mention.</p><p>For an entity e i with mentions {m i j } Ne i j , we follow <ref type="bibr" target="#b11">[Zhou et al., 2021b]</ref> and <ref type="bibr" target="#b11">[Zhang et al., 2021a]</ref>, and leverage logsumexp pooling <ref type="bibr">[Jia et al., 2019]</ref>, a smooth version of max pooling, to obtain the embedding h ei of entity e i :</p><formula xml:id="formula_1">h ei = log Ne i j=1 exp(h m i j ).<label>(2)</label></formula><p>In addition, we calculate an entity-pair-aware context representation c s,o for each entity pair (e s , e o ), which represents the contextual information in the document that the entity e s and the entity e o together pay attention to. The c s,o is formulated as:</p><formula xml:id="formula_2">c s,o = Ha s,o , a s,o = sof tmax(A s * A o ),<label>(3)</label></formula><p>where A s (A o ) refers to the attention score that entity e s (e o ) pays attention to each word in the document, H is the document embedding, and * refers to element-wise multiplication. Finally, we construct an entity-pair matrix M ?R Ne?Ne?d as follows:</p><formula xml:id="formula_3">M s,o = F F N ([u s , u o ]), u s = W s [h es , h doc , c s,o ], u o = W o [h eo , h doc , c s,o ],<label>(4)</label></formula><p>where N e represents the number of entities, F F N () refers to a feed-forward neural network, W o and W s are the learnable weight matrix, h doc is [CLS] token embedding which is used to represent the information of the entire document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference Module</head><p>After getting the entity-pair matrix, we treat it as an image. We obtain a masked image by randomly masking the pixels of the original image and reconstruct the masked image through an inference module, as shown in <ref type="figure">Figure 2</ref>. Through this the Masked Image Reconstruction (MIR) task, our inference module can learn how to infer the masked pixels from the unmasked pixels by the correlation between the relationships. Our inference module is a variant of Transformer's encoder, which replaces Multi-head Self-Attention (MSA) with Inference Multi-head Self-Attention (I-MSA), as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The I-MSA contains four heads and each head corresponds to an inference mode including: A? * + * ?B =? A?B, A? * + B? * =? A?B, * ?A + B? * =? A?B, and * ?A + * ?B =? A?B. For example, for head 1 in <ref type="figure" target="#fig_1">Figure 3</ref> which corresponds to A? * + * ?B =? A?B inference mode, we first concatenate the corresponding pixels in the A-th row and B-th column of the image,</p><formula xml:id="formula_4">{[M 1 A?1 , M 1 1?B ], ? ? ? , [M 1 A?N , M 1</formula><p>N ?B ]}, and perform dimensionality reduction through a linear layer,</p><formula xml:id="formula_5">{M 1 A?1?B , ? ? ? , M 1 A?N ?B }. Then, M 1 A,B performs an attention operation on {M 1 A?1?B , ? ? ? , M 1 A?N ?B ; M 1 A,B }.</formula><p>The whole process can be formulated as follows:</p><formula xml:id="formula_6">M 1 A,B = Attention(M 1 A,B W Q , M 1 inf W K , M 1 inf W V ), M 1 inf = {M 1 A?1?B , ? ? ? , M 1 A?N ?B ; M 1 A,B }, M 1 A? * ?B = Liner([M 1 A? * , M 1 * ?B ]), where, [ ] represents the concatenation operation, {} refers to a set, W Q , W K , W V are the learnable weight matrix.</formula><p>Inspired by <ref type="bibr" target="#b1">[Bao et al., 2021]</ref> and , we reconstruct the distribution of the pixels of the masked image on the label, p(r|M A,B ), instead of reconstructing the raw pixels, M A,B . The reason is that labels are more informationdense than pixels and p(r|M A,B ) is closer to our target task, relation classification. In addition, we reconstruct each pixel on the masked image including the masked pixel and the unmasked pixel, which is similar to the [He et al., 2021] method. In this way, the convergence of the model can be accelerated and better performance can be obtained. Specifically, the original image M o and the masked image M m are first sequentially input to the inference module and the classifier module to obtain the probability distributions p(r|M o ) and p(r|M m ). Then, we reconstruct the masked image by minimizing bidirectional KL-divergence between the two distributions of corresponding pixels in the original image and masked image. Finally, our reconstruction loss function L R is formulated as follows:</p><formula xml:id="formula_7">L R = 1 2 ? (D KL (p(r|M o ) p(r|M m )) + D KL (p(r|M m ) p(r|M o ))</formula><p>).</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classifier Module</head><p>Our classifier module is a single linear layer. The original image M o and the masked image M m are respectively input to the inference module to obtain the corresponding corrected image, M o and M m . Then, the relation probability of each entity pair is obtained through a linear layer:</p><formula xml:id="formula_8">P (r|M * ) = ?(W r M * ), M * = Inf erence(M * ),<label>(6)</label></formula><p>where * ? {o, m}, and W r is model parameters.</p><p>To alleviate the problem of unbalanced relationship distribution, we use adaptive-thresholding loss  as our classification loss function L C , which learns an adaptive threshold for each sample. Specifically, a T H class is introduced to separate positive classes and negative classes: positive classes would have higher probabilities than T H, and negative classes would have lower probabilities than T H. The adaptive-thresholding loss is formulated as follows:</p><formula xml:id="formula_9">L C = L 1 + L 2 , L 1 = ? r?P D log exp(logit r ) r ?{P D ,T H} exp(logit r ) , L 2 = ? log exp(logit T H ) r ?{N D ,T H} exp(logit r ) ,<label>(7)</label></formula><p>where P D and N D are the positive classes set and negative classes set respectively. The training objective is to minimize the loss function L, which is defined as follows: L = ?L R + ?L C ? and ? are hyperparameters and we simply set them to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct experiments on three document-level RE datasets to evaluate our DRE-MIR model. The statistics of the datasets could be found in Appendix A.</p><p>? DocRED <ref type="bibr" target="#b6">[Yao et al., 2019]</ref>  <ref type="bibr">et al., 2017]</ref> for the first 6% steps followed by a linear decay to 0. By default, we randomly mask 20% of points in the entity-pair matrix and set the number of layers in the inference module to 3. All hyper-parameters are tuned on the development set, some of which are listed in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on the DocRED Dataset</head><p>On the DocRED Dataset, we choose the following two types of models as the baseline:</p><p>?    <ref type="bibr">Xu et al., 2021b]</ref> , we report Intra-F 1 / Inter-F 1 scores in <ref type="table" target="#tab_1">Table 1</ref>, which only consider either intra-or inter-sentence relations respectively. Compared with Intra-F 1 , Inter-F 1 can better reflect the reasoning ability of the model. we can observe that our DRE-MIR model improved the Inter-F 1 score by 3.28 compared with the SIRE model. The improvement on Inter-F 1 demonstrates that our MIR task and Inference module can greatly improve the inference ability of the model. Moreover, the improvement on Inter-F 1 is greater than on intra-F 1 , which shows that the performance improvement of DRE-MIR is mainly contributed by the improvement of inter-sentence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on the Biomedical Datasets</head><p>On the two biomedical datasets, CDR and GDA, we compared our model with a large number of baseline models and recent state-of-the-art models including <ref type="bibr">BRAN [Verga et al., 2018</ref><ref type="bibr">], EoG [Christopoulou et al., 2019</ref>, <ref type="bibr">LSR [Nan et al., 2020]</ref>, DHG <ref type="bibr" target="#b11">[Zhang et al., 2020]</ref>, GLRE , <ref type="bibr">SciB-ERT [Beltagy et al., 2019]</ref>, ATLOP , and DocuNet <ref type="bibr" target="#b11">[Zhang et al., 2021a]</ref>.</p><p>Experiment results on two biomedical datasets are shown Layer-number  in <ref type="table" target="#tab_2">Table 2</ref>. Our DRE-MIR model achieves 76.9 F 1 on the CDR dataset, which slightly out performs the DocuNe model by 0.3 F 1 . There are three possible reasons: (1) The CDR contains only two types of relations, which indicates that the correlation between relations is weak.</p><p>(2) The CDR dataset contains very few annotated samples, making it difficult for our model to learn the underlying correlations.</p><p>(3) The samples in the CDR dataset contain few entities, which leads to a small entity pair matrix and weakens the effectiveness of our MIR task. Although the GDA dataset also has problems <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_2">(3)</ref>, it is a large-scale corpus and contains a large number of samples. Therefore our model achieves 86.4 F 1 score on the GDA dataset, which improves 1.1 F 1 compared with the DocuNe model. Since the MIR task is a pre-training task in the field of machine vision, more data is required to obtain better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We conducted an ablation experiment to validate the effectiveness of different components of our DRE-MIR model on the development set of the DocRED dataset. The results are listed in <ref type="table" target="#tab_3">Table 3</ref>. The w/o MIR removes the MIR task from our model and only contains the original path in the DRE-MIR model. The w/o MIR achieves an F1 score of 61.35, which outperforms the w/o Inference Model, our base model, by 0.99 F 1. This shows that our inference module has a certain inference ability even without the MIR task. However, the DRE-MIR model without the MIR task (w/o MIR) has a performance drop of 1.61 F 1, which proves that the MIR task can well improve the inference ability of our inference module.</p><p>The Only Mask path removes the original path from our model and only contains the Mask path. The Only Mask path is a variant of the DRE-MIR model, and its image reconstruction method is similar to <ref type="bibr" target="#b1">[Bao et al., 2021]</ref>. The Only Mask path leads to a drop of 0.76 F 1 in performance, which proves that the original image played a guiding role in the reconstruction process of the masked image to further improve the performance of the model.</p><p>As can be seen from w/o I-MSA, replacing the I-MSA with the MSA resulted in a huge performance drop of 6.74 F 1. This shows that our I-MSA can greatly improve the inference ability of the Transformer. We also introduce an experiment where only the masked pixels are reconstructed, i.e. Only reconsitution masked point, and observe a performance drop of 1.43 F 1. The possible reason is that the masked pixels may affect the unmasked pixels through our inference module, but reconstructing all the pixels can effectively alleviate this negative impact.</p><p>Overall, our model improves our base model by 2.5 F 1, which fully demonstrates that our inference module and MIR task can effectively improve the inference ability of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Analysis &amp; Discussion</head><p>In this section, we will further discuss and analyze our DRE-MIR model from four aspects: (1) the number of layers in the inference module, (2) the masking rate during training, (3) the inference performance, and (4) the performance to restore the masked entity-pair matrix. <ref type="table" target="#tab_5">Table 4</ref> shows the performance of the DRE-MIR model with different number of layers of inference modules. We observe that increasing the number of layers from 1 to 2 improves the model performance by 1.07 F 1 score. The possible reason is that increasing the number of layers can improve the multi-hop reasoning ability of the model. However, the performance of the model is slightly improved by 0.21 F 1 when the number of layers is increased from 2 to 3. Therefore, a two-layer inference module is sufficient for general cases. <ref type="figure" target="#fig_2">Figure 4</ref> shows that our model obtains the best performance when trained with a masking rate of 20%. However, our model still achieves a decent performance of 61.44/59.43 F 1 /IgnF 1 when setting the masking rate to 50%, which shows that our inference module has strong inference ability to restore the masked entity-pair matrix. This also implies that using a larger masking rate to increase the training difficulty should achieve better performance under large-scale corpora, which is similar to the conclusions drawn from pre-training tasks in machine vision, such as MAE <ref type="bibr">[He et al., 2021]</ref>.</p><p>To evaluate the inference ability of the models, we follow <ref type="bibr">[Xu et al., 2021b;</ref><ref type="bibr" target="#b9">Zeng et al., 2021]</ref> and report Infer-F 1 scores in table 5, which only considers relations that engaged in the relational reasoning process. We observe that our DRE-MIR model improves 2.71 Infer-F 1 compared with the GAIN model. Removing the inference module from our model results in a performance drop of 4.10 Infer-F 1 , which demonstrates that our inference module and the MIR task can improve the inference ability of the model. To evaluate the model's ability of restoring the masked  entity-pair matrix, we also randomly mask the entity-pair matrix during validating. We show the experimental results in <ref type="figure" target="#fig_3">Figure 5</ref>. Since we train our model with a masking rate of 20%, the performance drop of the model is very slight when the masking rate is less than 20%. Our model has only a slight performance drop of 1.84/1.88 F 1 /IgnF 1 with 50% masking rate, which shows that our model has excellent robustness. Even if the masking rate is increased to 80%, our model still achieves a score of 55.00/52.69 F 1 /IgnF 1 , which is better than the BERT-TS base  model. This shows that our model has strong restoring ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Since many relational facts in real applications can only be recognized across sentences, a lot of recent work gradually shift their attention to document-level RE. Due to graph neural network(GNN) can effectively model long-distance dependence and complete logical reasoning, Many methods based on document-graphs are widely used for document-level RE. Specifically, they first constructed a graph structure from the document, and then applied the GCN <ref type="bibr" target="#b2">[Kipf and Welling, 2016;</ref><ref type="bibr" target="#b1">Huang et al., 2017]</ref> to the graph to complete logical reasoning. The graph-based method was first introduced by [Quirk and <ref type="bibr">Poon, 2016]</ref> and has recently been extended by many works <ref type="bibr" target="#b1">[Christopoulou et al., 2019;</ref><ref type="bibr" target="#b2">Nan et al., 2020;</ref><ref type="bibr" target="#b9">Zeng et al., 2020;</ref><ref type="bibr" target="#b4">Wu et al., 2019]</ref>.  proposed the Graph Enhanced Dual Attention network (GEDA) model and used it to characterize the complex interaction between sentences and potential relation instances. <ref type="bibr" target="#b9">[Zeng et al., 2020]</ref> propose Graph Aggregation-and-Inference Network (GAIN) model. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document and then constructs an entitylevel graph (EG), finally uses the path reasoning mechanism to infer relations between entities on EG. [Nan et al., 2020] proposed a novel LSR model, which constructs a latent documentlevel graph and completes logical reasoning on the graph. In addition, due to the pre-trained language model based on the transformer architecture can implicitly model longdistance dependence and complete logical reasoning, some studies  directly apply pre-trained model without introducing document graphs.  proposed an ATLOP model that consists of two parts: adaptive thresholding and localized context pooling, to solve the multi-label and multientity problems. The SIRE <ref type="bibr" target="#b9">[Zeng et al., 2021]</ref> represents intraand inter-sentential relations in different ways, and design a new and straightforward form of logical reasoning. Recently, the state-of-the-ar model, DocuNet <ref type="bibr" target="#b11">[Zhang et al., 2021a]</ref>, formulates document-level RE as semantic segmentation task and capture global information among relational triples through the U-shaped segmentation module <ref type="bibr">[Ronneberger et al., 2015]</ref>.</p><p>Furthermore, our work is inspired by recent pre-training research in the field of machine vision, such as BIET <ref type="bibr" target="#b1">[Bao et al., 2021]</ref>, IBOT <ref type="bibr">MAE [He et al., 2021]</ref>. BEIT followed <ref type="bibr" target="#b1">BERT [Devlin et al., 2018]</ref> and proposed a masked image modeling (MIM) task and a tokenizer to pre-train vision Transformers. The tokenizer "tokenize" the image to discrete visual tokens, which is obtained by the latent codes of discrete VAE <ref type="bibr" target="#b2">[Ramesh et al., 2021]</ref>. The IBOT can perform the MIM task with an online tokenizer and formulates the MIM task as knowledge distillation (KD) distillation problem. The MAE develops an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, We first formulate the inference problem in document-level RE as a Masked Image Reconstruction (MIR) problem. Then, we propose an Inference Multi-head Self-Attention (I-MSA) module to restore masked images more efficiently. The MIR task and the I-MSA module greatly improve the inference ability of our model. Experiments on three public document-level RE datasets demonstrate that our DRE-MIR model achieved better results than the existing state-of-the-art model. In the future, we will try to use our model for other inter-sentence or document-level tasks, such as cross-sentence collective event detection. <ref type="table" target="#tab_1">Train  3053  500  23353  Dev  1000  500  5839  Test  1000  500  1000  Relations  97  2  2  Entities per</ref>    A Datasets <ref type="table" target="#tab_7">Table 5</ref> details the statistics of the three document-level relational extraction datasets, DocRED, CDR, and GDA. These statis-tics further demonstrate the complexity of entity structure in document-level relation extraction tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DocRED CDR GDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-parameters Setting</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example comes from the DocRED dataset, which shows that the use of correlation between relations (triple) to infer complex inter-sentence relations. (a) is a document, in which different colors represent different entities. (b) lists some intra-sentence relations, which can be easily identified. (c) shows an inter-sentence relations which require reasoning techniques to be identified. The arrows between (b) and (c) indicate the correlation among relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) The architecture of the Inference Multi-head Self-Attention (I-MSA), which is a variant of multi-head self-attention (MSA). The I-MSA has four types of heads and each head corresponds to one inference mode. (b) Inference module, which is a variant of Transformer's encoder by replacing MSA with I-MSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Results of different masking rates used in the training process on the development set of DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results of different masking rates used the validating process on the development set of DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>: DocRED is a large-scale human-annotated dataset for document-level RE, which constructed from Wikipedia and Wikidata. DocRED contains 96 types of relations, 132,275 entities, and 56,354 relationship triples in total. In DocRED, more than 40.7% of relational facts can only be extracted from multiple sentences, and 61.1% of relational triples require various reasoning skills. We follow the standard split of the dataset, 3,053 documents for training, 1,000 for development and, 1,000 for the test.</figDesc><table /><note>? CDR [Li et al., 2016]: The Chemical-Disease Reactions dataset (CDR) consists of 1,500 PubMed abstracts, which is equally divided into three sets for training, develop- ment, and testing. CDR is aimed to predict the binary interactions between Chemical and Disease concepts.? GDA [Wu et al., 2019]: The Gene-Disease Associations dataset (GDA) is a large-scale biomedical dataset, which is constructed from MEDLINE abstracts by method of distant supervision. GDA contains 29,192 documents as the training set and 1,000 as the test set. GDA is also a binary relation classification task that identifies Gene and Disease concepts interactions. We follow [Christopoulou et al., 2019] to divide the training set into two parts, 23,353 documents for training and 5,839 for validation.3.2 Experimental Settings Our model was implemented based on PyTorch and Hug- gingface's Trans-formers [Wolf et al., 2019]. We used cased BERT-base [Devlin et al., 2018] as the encoder on DocRED and SciBERT-base [Beltagy et al., 2019] on CDR and GDA. Our model is optimized with AdamW [Loshchilov and Hutter, 2017] with a linear warmup [Goyal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results (%) on the development and test set of the DocRED. The scores of all the baseline models come from ATLOP and SIRE<ref type="bibr" target="#b9">[Zeng et al., 2021]</ref>. And, the results on the test set are obtained by submitting to the official Codalab.</figDesc><table><row><cell>Graph-based Models: This type of method uses graph</cell></row><row><cell>convolutional networks (GCN) [Kipf and Welling, 2016;</cell></row><row><cell>Veli?kovi? et al., 2017; Schlichtkrull et al., 2018] to</cell></row><row><cell>complete inference on document-level graphs, including</cell></row><row><cell>GEDA [Li et al., 2020], LSR [Nan et al., 2020], GLRE</cell></row><row><cell>[Wang et al., 2020], GAIN [Zeng et al., 2020], and Het-</cell></row><row><cell>erGSAN [Xu et al., 2021b].</cell></row></table><note>? Transformer-based Models: These models directly use pre-trained language models for document-level RE with- out using graph structures, including BERT [Xu et al.,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results (%) on the biomedical datasets CDR and GDA. While, the SIRE represents intra-and inter-sentential relations in different ways, and design a new form of logical reasoning.We follow<ref type="bibr" target="#b6">[Yao et al., 2019]</ref> and use F 1 and IgnF 1 as evaluation metrics to evaluate the performance of a model, where IgnF 1 denotes the F 1 score excluding the relational facts that are shared by the training and dev/test sets. Comparing all baseline model, our DRE-MIR model outperforms the latest state-of-the-art models by 1.14/1.11 F 1 /IgnF 1 on the dev set and 1.29/1.1 F 1 /IgnF 1 on the test set. This demonstrates that our model has an excellent overall performance. Besides, comparing the graph-based state-of-the-art model, the DRE-MIR model outperforms the GAIN model by 1.74/1.83 F 1 /IgnF 1 on the dev set and 2.11/2.03 F 1 /IgnF 1 on the test set. This shows that our model has better reasoning ability than the previous graph-based models.</figDesc><table><row><cell>Model</cell><cell>IgnF 1</cell><cell>F 1</cell></row><row><cell>DRE-MIR</cell><cell>60.97</cell><cell>62.96</cell></row><row><cell>Only Mask path</cell><cell>60.13</cell><cell>62.20</cell></row><row><cell>Only reconsitution masked point</cell><cell>59.50</cell><cell>61.53</cell></row><row><cell>w/o MIR</cell><cell>59.31</cell><cell>61.35</cell></row><row><cell>w/o Inference Model</cell><cell>58.56</cell><cell>60.46</cell></row><row><cell>w/o I-MSA</cell><cell>54.67</cell><cell>56.22</cell></row></table><note>2021b], BERT-Two-Step [Wang et al., 2019], HIN-BERT [Tang et al., 2020], CorefBERT [Ye et al., 2020], and ATLOP-BERT [Zhou et al., 2021b]. In addition, we also consider the DocuNet [Zhang et al., 2021a] and SIRE [Zeng et al., 2021] in the comparison. The DocuNet formulates document-level RE as a semantic seg- mentation problem and get the latest SOTA results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the DRE-MIR model on the development set of the DocRED. w/o Inference Model and w/o MIR removes the</figDesc><table><row><cell>Inference Model and the MIR task from our mode, respectively; w/o</cell></row><row><cell>I-MSA replaces the Inference Multi-head Self-Attention (I-MSA)</cell></row><row><cell>with the multi-head self-attention (MSA); w/o Inference Model is</cell></row><row><cell>our base model.</cell></row></table><note>The same as [Nan</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of DRE-MIR with different numbers of layers in the inference module on the development set of DocRED.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Infer-F1 results of the DRE-MIR model on the development set of DocRED. P: Precision, R: Recall.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Summary of DocRED, CDR and GDA datasets.</figDesc><table><row><cell>Hyperparam</cell><cell>DocRED</cell><cell>CDR</cell><cell>GDA</cell></row><row><cell></cell><cell>BERT</cell><cell cols="2">SciBERT SciBERT</cell></row><row><cell>Batch size</cell><cell>8</cell><cell>16</cell><cell>16</cell></row><row><cell>Epoch</cell><cell>100</cell><cell>20</cell><cell>5</cell></row><row><cell>lr for encoder</cell><cell>2e-5</cell><cell>2e-5</cell><cell>2e-5</cell></row><row><cell>lr for other parts</cell><cell>1e-4</cell><cell>5e-5</cell><cell>5e-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hyper-parameters Setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc>details our hyper-parameters setting. All of our hyperparameters were tuned on the development set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>[baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edgeoriented graphs</title>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<idno>arXiv:1904.02347</idno>
	</analytic>
	<monogr>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<editor>Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll?r, and Ross Girshick</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of CVPR. Jia et al., 2019] Robin Jia, Cliff Wong, and Hoifung Poon. Document-level n-ary relation extraction with multiscale representation learning</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hin: Hierarchical inference network for documentlevel relation extraction. Advances in Knowledge Discovery and Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<idno>arXiv:1710.10903</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<editor>Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling</editor>
		<meeting><address><addrLine>Scott Gray, Chelsea Voss, Alec Radford, Mark; Emma Strubell</addrLine></address></meeting>
		<imprint>
			<publisher>Patrick Verga</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<idno type="arXiv">arXiv:2009.10359</idno>
		<idno>arXiv:1910.03771</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of RECOMB</title>
		<meeting>of RECOMB</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Huggingface&apos;s transformers: State-of-the-art natural language processing</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10249</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<editor>Xu et al., 2021b] Wang Xu, Kehai Chen, and Tiejun Zhao</editor>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Document-level relation extraction with reconstruction</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="arXiv">arXiv:2004.06870</idno>
		<title level="m">Coreferential reasoning learning for language representation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sire: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01709</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Double graph based reasoning for documentlevel relation extraction</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alicg: Finegrained and evolvable conceptual graph construction for semantic search at alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03618</idno>
		<idno>arXiv:2106.01686</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of SIGIR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global contextenhanced graph convolutional networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen ; Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen ; Wenxuan Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen ; Huiwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12812</idno>
		<idno>arXiv:2111.07832</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A frustratingly easy approach for entity and relation extraction. ibot: Image bert pre-training with online tokenizer</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
