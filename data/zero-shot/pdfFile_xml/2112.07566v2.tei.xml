<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VALSE : A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Letitia</forename><surname>Parcalabescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Cafagna</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Linguistics and Language Technology</orgName>
								<orgName type="institution">University of Malta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilitta</forename><surname>Muradjan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">Heidelberg University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Linguistics and Language Technology</orgName>
								<orgName type="institution">University of Malta</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VALSE : A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&amp;L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&amp;L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&amp;L models from a linguistic perspective, complementing the canonical taskcentred V&amp;L evaluations. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>General-purpose pretrained vision and language (V&amp;L) models have gained notable performance on many V&amp;L tasks <ref type="bibr">(Lu et al., 2019;</ref><ref type="bibr">Tan and Bansal, 2019;</ref><ref type="bibr">Li et al., 2019;</ref><ref type="bibr">Li et al., 2020a;</ref><ref type="bibr">Su et al., 2020)</ref>. As a result, V&amp;L research has changed its focus from task-specific architectures to fine-tuning large V&amp;L models.</p><p>Current benchmarks give a good perspective on model performance on a wide range of V&amp;L tasks <ref type="bibr" target="#b5">(Cao et al., 2020;</ref><ref type="bibr">Lourie et al., 2021;</ref><ref type="bibr">Li et al., 2021)</ref>, but the field is only starting to assess why models perform so well and whether models learn specific capabilities that span multiple V&amp;L tasks. Specifically, we lack detailed understanding of the extent to which such models are able to ground linguistic phenomena-from morphosyntax to semantics-in the visual modality <ref type="bibr" target="#b2">(Bernardi and Pezzelle, 2021)</ref>. For example, recent evidence suggests that models are insensitive to linguistic distinctions of verb-argument structure <ref type="bibr">(Hendricks and Nematzadeh, 2021)</ref> and word order <ref type="bibr" target="#b8">(Cirik et al., 2018;</ref><ref type="bibr" target="#b1">Akula et al., 2020)</ref>.</p><p>Our work addresses this gap with VALSE (Vision And Language Structured Evaluation), a benchmark for V&amp;L model evaluation comprising six tasks, or 'pieces', where each piece has the same structure: given a visual input, a model is asked to distinguish real captions from foils, where a foil is constructed from a caption by altering a word or phrase that realizes a specific linguistic phenomenon, e.g., semantic number of nouns, verb argument structure, or coreference. VALSE uses a resource-lean diagnostic setup that dispenses with large-scale annotation (e.g., of bounding boxes), and builds on existing high-quality image captioning and VQA data. VALSE is designed to leverage the existing prediction heads in pretrained (or finetuned) V&amp;L models; for that reason, our benchmark does not include any re-training and can be interpreted as a zero-shot evaluation. We build test data for each piece so as to safeguard against the possibility of models exploiting artefacts or statistical biases in the data, a well-known issue with highly parameterised neural models pretrained on large amounts of data <ref type="bibr">(Goyal et al., 2017;</ref><ref type="bibr">Madhyastha et al., 2018;</ref><ref type="bibr">Kafle et al., 2019)</ref>. With this in view, we propose novel methods to guard against the emergence of artefacts during foiling.</p><p>Our main contributions are: i) We introduce VALSE, a novel benchmark aimed at gauging the sensitivity of pre-trained V&amp;L models to foiled instances. ii) We cover a wide spectrum of basic linguistic phenomena affecting the linguistic and visual modalities: existence, plurality, counting, spatial relations, actions, and entity coreference. iii) We investigate novel strategies to build valid foils that include automatic and human valida-tion. We balance word frequency distributions between captions and foils, and test against pretrained models solving the benchmark unimodally by relying only on text. We employ masked language modeling (MLM) in foil creation and semantic inference for validating foils, and finally collect human annotations for the entire benchmark. iv) We establish initial experimental results for pretrained V&amp;L models of diverse architectures on VALSE. The overall weak performance of these models indicates that the time is ripe for a novel, reliable foiling dataset targeting the visual grounding capabilities of V&amp;L models through the lens of linguistic constructs. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related work</head><p>Pretrained V&amp;L models learn to combine vision and language through self-supervised multitask learning. Tasks include multimodal masked modeling-where words in the text and object labels or regions in the image are masked out, then predictedand image-sentence alignment, whereby a model learns to predict whether an image and a text correspond. Major architectures are single-and dualstream multimodal transformers: single-stream models concatenate word and image features, and encode the resulting sequence with a single transformer stack; dual-stream models use distinct transformer stacks to handle visual and textual inputs, and additional layers (e.g. co-attention) to fuse these into multimodal features.</p><p>Benchmarking V&amp;L models V&amp;L models <ref type="bibr">(Li et al., 2019;</ref><ref type="bibr">Lu et al., 2019;</ref><ref type="bibr">Tan and Bansal, 2019;</ref><ref type="bibr">Lu et al., 2020;</ref><ref type="bibr">Li et al., 2020b;</ref><ref type="bibr">Kim et al., 2021)</ref> are commonly evaluated on V&amp;L tasks such as VQA <ref type="bibr">(Goyal et al., 2017)</ref>, visual reasoning <ref type="bibr">(Suhr et al., 2019)</ref>, or image retrieval <ref type="bibr">(Lin et al., 2014;</ref><ref type="bibr">Plummer et al., 2015)</ref>. Given how well transformer-based models perform across unimodal and multimodal tasks, research efforts have recently started to address what makes them so effective, and to what extent they learn generalisable representations. Techniques to address these questions in unimodal and multimodal V&amp;L contexts include: adversarial examples <ref type="bibr">(Jia and Liang, 2017;</ref><ref type="bibr">Jia et al., 2019)</ref>; investigation of the impact of bias, be it linguistic <ref type="bibr">(Gururangan et al., 2018)</ref>, visual semantic <ref type="bibr" target="#b0">(Agarwal et al., 2020)</ref>, or socio-economic <ref type="bibr">(Garg et al., 2019)</ref>; and the use of linguistically-informed counterfactual and minimally-edited examples <ref type="bibr">(Levesque et al., 2012;</ref><ref type="bibr">Gardner et al., 2020)</ref>. A trend within the latter research line that is specific to V&amp;L models is vision-and-language foiling <ref type="bibr">(Shekhar et al., 2017b;</ref><ref type="bibr">Gokhale et al., 2020;</ref><ref type="bibr" target="#b3">Bitton et al., 2021;</ref><ref type="bibr">Parcalabescu et al., 2021;</ref><ref type="bibr">Rosenberg et al., 2021)</ref>, where the idea is to create counterfactual (i.e., foiled) and/or minimally edited examples by performing data augmentation on captions <ref type="bibr">(Shekhar et al., 2017b,a)</ref> or images <ref type="bibr">(Rosenberg et al., 2021)</ref>.</p><p>Since most V&amp;L models are pretrained on some version of the image-text alignment task, it is possible to test their ability to distinguish correct from foiled captions (in relation to an image) in a zeroshot setting. The construction of foils can serve many investigation purposes. With VALSE, we target the linguistic grounding capabilities of V&amp;L models, focusing on pervasive linguistic phenomena that span multiple tokens, described in ?3.1- ?3.6. At the same time, we ensure that our data is robust to perturbations and artefacts by i) controlling for word frequency biases between captions and foils, and ii) testing against unimodal collapse, a known issue of V&amp;L models <ref type="bibr">(Goyal et al., 2017;</ref><ref type="bibr">Madhyastha et al., 2018)</ref>, thereby preventing models from solving the task using a single input modality. The issue of neural models exploiting data artefacts is well-known <ref type="bibr">(Gururangan et al., 2018;</ref><ref type="bibr">Jia et al., 2019;</ref><ref type="bibr">Wang et al., 2020b;</ref><ref type="bibr">He et al., 2021)</ref> and methods have been proposed to uncover such effects, including gradient-based, adversarial perturbations or input reduction techniques (cf. <ref type="bibr">Wallace et al., 2020</ref>). Yet, these methods are still not fully understood <ref type="bibr">(He et al., 2021)</ref> and can be unreliable <ref type="bibr">(Wang et al., 2020b)</ref>.</p><p>Our work is related to <ref type="bibr">Gardner et al. (2020)</ref>, who construct task-specific contrast sets for NLU. However, our focus is on modelling linguistic phenomena instead of tasks, and we construct carefully curated, balanced, single foils from valid instances that we select from multiple multimodal datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constructing the VALSE benchmark</head><p>We resort to a musical analogy to describe VALSE: Vision And Language Structured Evaluation is composed of 6 pieces, each corresponding to a specific linguistic phenomenon (see <ref type="table">Table 1</ref> for an overview). Each piece consists of one or more instruments designed to evaluate a model's ability to ground that specific linguistic phenomenon. All instruments are built by applying foiling functions (FFs) specific to the linguistic phenomenon under study. FFs take a correct caption as input and change a specific part to produce a foiled caption (or foil). We design FFs such that the sentences they produce fail to describe the image, while still being grammatical and otherwise valid sentences.</p><p>Of course, a foiled caption may be less likely than the original caption from which it was produced, and such unwarranted biases can be easily picked up by overparameterised V&amp;L models. Moreover, an automatic FF may fail to produce a foil that contradicts the image, for example by altering the original caption to yield a near-synonymous one, or one that is entailed by the original caption. For phenomena that make it difficult to control these crucial properties of foils, we apply additional filters: i) some FFs make use of strong LMs to propose changes to captions, so that the generated foils are still high-probability sentences; ii) we use state-of-the-art natural language inference (NLI) methods to detect cases where there is an entailment between caption and foil, and filter out such foils from the dataset (see ?4 for discussion). As a final measure, we employ human annotators to validate all generated testing data in VALSE.</p><p>VALSE data is sourced from existing V&amp;L datasets. Below, we describe each piece and its instruments, and the corresponding task setup in VALSE. For each instrument, we follow the same procedure: i) we identify captions that contain instances of the targeted linguistic phenomenon; ii) we apply a FF that automatically replaces the expression with a variant that contradicts the original expression's visual content, thereby constructing one or more foils from each target instance in the original caption, as discussed in ?4; we then iii) subject the obtained foils to various filters, with the aim of distilling a subset of valid and reliable foils that cannot be easily tricked by a new generation of highly parameterised pretrained V&amp;L models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Existence</head><p>The existence piece has a single instrument and targets instances with existential quantifiers. Models need to differentiate between examples i) where there is no entity of a certain type or ii) where one or more of these entities are visible in an image. We use the Visual7W visual question answering dataset <ref type="bibr">(Zhu et al., 2016)</ref> and source its 'how many' examples, building a pool of those whose answers are numerals (0, 1, 2, etc.). We use templates to transform question and answer fields into a declarative statement that correctly describes what can be seen in the image, e.g. 'Q: How many animals are shown? A: 0' ? 'There are 0 animals shown'. We then transform these statements into an existential statement. In the example above, we replace the numeral by the word 'no' to create a correct caption ('There are no animals shown') and remove the numeral altogether to create a foil ('There are animals shown'). The existence piece has 505 imagecaption-foil tuples after manual validation, out of 534 candidates (cf. ?4), and captions/foils are balanced: 50% of the (correct) captions originally have answer 0, and the remaining have answer 1 or greater. Full details are provided in A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Plurality</head><p>The plurality piece has a single instrument, concerned with semantic number. It is intended to test whether a model is able to distinguish between noun phrases denoting a single entity in an image ('exactly one flower'), versus multiple entities ('some flowers'). The dataset consists of 851 validated instances out of 1000 generated candidates (cf. ?4), evenly divided between cases where the caption contains a plural NP, foiled by replacing it with a singular (pl2sg: 'some flowers' ? 'exactly one flower'), or conversely, the caption contains a singular which is foiled by replacing it with a plural (sg2pl). Foil candidates were generated from the COCO 2017 validation set <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>. Full details about the foil construction and our measures against introducing biases with quantifiers such as 'exactly one', are provided in A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Counting</head><p>The counting piece has three instruments: balanced, adversarial and small numbers. All instances are statements about the number of entities visible in an image. The model needs to differentiate between examples where the specific number of entities in the associated image is correct or incorrect, given the statement. Similarly to the existence piece, we use the Visual7W VQA dataset <ref type="bibr">(Zhu et al., 2016)</ref> and source its 'how many' examples whose answers are numerals (0, 1, 2, etc.). We use templates to transform question and answer fields into a declarative statement describing the image and create foils by replacing the numeral in the correct statement by another numeral.</p><p>All three instruments are designed to show whether models learn strategies that generalize beyond the training distribution, and to what extent a model exploits class frequency bias. 2 In counting balanced we cap the number of examples to a maximum per class and make sure correct and foil classes are balanced, so that models that exploit class frequency bias are penalized. In counting adversarial we ensure that all foils take class n ? {0, 1, 2, 3}, whereas all correct captions take class m ? {m | m ? 4}. Biased models are expected to favour more frequent classes. Since small numbers are naturally the most frequent, models that resort to such biases should perform poorly on this adversarial test set. Counting small numbers is a sanity check where all correct captions and foils have class n ? {0, 1, 2, 3}, and caption/foil classes are balanced. Since models likely have been exposed to many examples in this class set and all such classes are high-frequency, with this instrument we disentangle model performance from class exposure. Counting balanced, adversarial, and small numbers have 868 (1000), 691 (756), and 900 (1000) instances after (before) manual validation, respectively (cf. ?4). For details, see A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Spatial relations</head><p>The relations piece has a single instrument and focuses on the ability of models to distinguish between different spatial relations. Foils differ from the original caption only by the replacement of a spatial preposition. As with plurals, the data was sourced from the COCO 2017 validation split. To create foils, we first identified all preposition sequences in captions (e.g., 'in', 'out of'). Foils were created by masking the prepositions and using <ref type="bibr">SpanBERT (Joshi et al., 2020)</ref> to generate candidates of between 1-3 words in length. We keep SpanBERT candidates, which are spans whose lengths vary from 1 to 3, if they differ from the original preposition sequence, but exist in the dataset. There are 535 instances after manual validation out of 614 proposed instances (cf. ?4), and we ensure that prepositions are similarly distributed among captions and foils. Full details are provided in A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Actions</head><p>The actions piece has two instruments: i) action replacement and ii) actant swap. They test a V&amp;L model's capability to i) identify whether an action mentioned in the text matches the action seen in the image (e.g., 'a man shouts / smiles at a woman'), and ii) correctly identify the participants of an action and the roles they play (e.g., is it the man who is shouting or is it the woman, given the picture in <ref type="table">Table 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Coreference</head><p>The coreference piece aims to uncover whether V&amp;L models are able to perform pronominal coreference resolution. It encompasses cases where i) the pronoun has a noun (phrase) antecedent and pronoun and (noun) phrase are both grounded in the visual modality ('A woman is driving a motorcycle. Is she wearing a helmet?'), and cases where ii) the pronoun refers to a region in the image or even to the entire image ('Is this outside?'). We create foils based on VisDial v1.0 <ref type="bibr" target="#b9">(Das et al., 2017)</ref> with images from <ref type="bibr">MSCOCO (Lin et al., 2014)</ref>. VisDial captions and dialogues are Q&amp;A sequences. We select image descriptions of the form <ref type="bibr">[Caption. Question? Yes/No.]</ref> where the question contains at least one pronoun. When foiling, we exchange the answer from yes to no and viceversa (see <ref type="table">Table 1</ref>). We ensure a 50-50% balance between yes / no answers.</p><p>The coreference piece consists of two instruments: coreference standard originating from the VisDial train set and a small coreference clean set from the validation set, containing 708 (916) and 104 (141) examples after (before) manual validation, respectively (cf. ?4). 3 See A.6 for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reliable construction of valid foils</head><p>In VALSE, an instance consisting of an imagecaption-foil triple is considered valid if: the foil minimally differs from the original caption; the foil does not accurately describe the image; and independent judges agree that the caption, but not the foil, is an accurate description of the image. We consider a foiling method to be more reliable the more it ensures that a generated foil does not substantially differ from a human caption regarding distributional and plausibility bias, and cannot be easily solved unimodally.</p><p>In this section, we discuss automatic and manual means to reliably construct valid foils. In this context, two types of bias are especially worthy of note: distributional bias ( ?4.1) and plausibility bias ( ?4.2). In ?4.3 we discuss how we apply a natural language inference model to filter examples in our data pipeline, and ?4.4 show how we manually validate all examples in our benchmark. Random samples from the final version of each instrument are shown in Tab. 6-11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mitigating distributional bias</head><p>A first form of bias is related to distributional imbalance between captions and foils (e.g., certain words or phrases having a high probability only in foils). Previous foiling datasets exhibit such imbalance, enabling models to solve the task disregarding the image <ref type="bibr">(Madhyastha et al., 2019)</ref>. To mitigate this problem, for each phenomenon and throughout our data creation process, we ensure that the token frequency distributions in correct and foiled captions are approximately the same (cf. App. A and E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Countering plausibility bias</head><p>A second form of bias may arise from automatic procedures yielding foils that are implausible or unnatural, which can facilitate their detection. Often, VALSE pieces can be safely foiled by simple rules (e.g., switching from existence to non-existence, or from singular to plural or vice versa). However, with spatial relations and actions, a foil could be deemed unlikely given only the textual modality and independently of the image, e.g., 'a man stands under / on a chair'. Such plausibility biases may be detected by large language models that incorporate commonsense knowledge <ref type="bibr">(Petroni et al., 2019;</ref><ref type="bibr">Wang et al., 2020a)</ref>, and we expect future V&amp;L models to exhibit similar capabilities.</p><p>To ensure that foiled and correct captions are similarly plausible, we use language models such as BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> and <ref type="bibr">SpanBERT (Joshi et al., 2020)</ref> to suggest replacements in our foiling functions. Additionally, in the case of spatial relations and plurals, we also apply a grammaticality filter using GRUEN (Zhu and Bhat, 2020). GRUEN was originally proposed to automatically score generated sentences based on discourse-level and grammatical properties. We use only the grammaticality component of GRUEN, and retain only foil candidates with a grammaticality score ? 0.8.</p><p>Furthermore, we evaluate unimodal, languageonly models on VALSE to verify whether our benchmark could be solved by a multimodal model with strong linguistic capacities in unimodal collapse, whereby a model silently relies on a single modality within which biases are easier to exploit <ref type="bibr">(Goyal et al., 2017;</ref><ref type="bibr">Shekhar et al., 2019a)</ref>. By evaluating VALSE with unimodal models, we establish a baseline that V&amp;L models should exceed if we are to expect true multimodal integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Filtering foils with NL Inference</head><p>When constructing foils, we need to ensure that they fail to describe the image. To test this automatically, we apply natural language inference (NLI) with the following rationale: We consider an image and its caption as a premise and its entailed hypothesis, respectively (a similar rationale is applied in the visual entailment task; Xie et al., 2019). In addition, we consider the caption as premise and the foil as its hypothesis. If a NLI model predicts the foil to be entailed (E) by the caption, it cannot be a good foil since by transitivity it will give a truthful description of the image. By contrast, if the foil is predicted to contradict (C) or to be neutral (N) with respect to the caption, we take this as an indicator of a valid (C) or a plausible (N) foil. <ref type="bibr">4</ref> We use the NLI model ALBERT (Lan et al., 2020) finetuned on the task (see Appendix C for details). Filtering with NLI was initially applied to relations, plurals and actions, on the grounds that foils in these pieces may induce substantive changes to lexical content. 5 Following automatic labelling of caption-foil pairs, we manually validated a sample labelled as E, C or N. For relations (N = 30), labels were found to be near 100% accurate with only 2 (0.06%) errors overall. For plurals (N = 60, 50% sg2pl and 50% pl2sg), the error rate was also low, with 0 errors for C, 33% errors for E and 11% errors for N. Here, a number of entailment errors were due to odd formulations arising from the automatic foiling process, whereas no such oddities were observed for C. We therefore include only foils labelled C in the final relations and plurals pieces. For actions, the model labelled 4 See the following examples from action replacement: P: A mother scolds her son. H1: A mother encourages her son. (C; good foil); H2: A mother camps with her son. (N; needs image control); H3: A mother talks to her son. (E; not a suitable foil)</p><p>If the NLI prediction is N, we still need to check the image, since the description might happen to fit the image content. 5 By contrast, existence and counting foils involve a more straightforward swap (e.g., between numerical quantities); similarly, coreference foils simply involve the replacement of a positive with a negative answer. contradictions very accurately (0% error) but was erroneous up to 97.1% for E, meaning that a large number of valid foils would be spuriously excluded. To avoid reducing the dataset too much, we did not use NLI filtering for actions, but relied on human annotation as a final validity check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Manual evaluation of generated foils</head><p>As a final step, the data for each instrument was submitted to a manual validation. For each instance, annotators were shown the image, the caption and the foil. Caption and foil were numbered and displayed above each other to make differences more apparent, with differing elements highlighted in boldface <ref type="figure" target="#fig_1">(Fig. 2</ref>, App. E). Annotators were not informed which text was the caption and which was the foil, and captions appeared first (numbered 1) 50% of the time. The task was to determine which of the two texts accurately described what could be seen in the image. In each case, annotators had a forced choice between five options: a) the first, but not the second; b) the second, but not the first; c) both of them; d) neither of the two; and e) I cannot tell.</p><p>Each item was annotated by three individuals. The validation was conducted on Amazon Mechanical Turk with a fixed set of annotators who had qualified for the task. For details see App. E. For the final version of VALSE, we include instances which passed the following validation test: at least two out of three annotators identified the caption, but not the foil, as the text which accurately describes the image. Across all instruments, 87.7% of the instances satisfied this criterion (min 77.3%; max 94.6%), with 73.6% of instances overall having a unanimous (3/3) decision that the caption, but not the foil, was an accurate description. We consider these figures high, suggesting that the automatic construction and filtering procedures yield foils which are likely to be valid, in the sense discussed in ?4 above.</p><p>We compute inter-annotator agreement for each instrument (Tab. 5). On the valid subset, agreement is low to medium (Krippendorff's ?: min=0.23, max=0.64, mean=0.42, sd=0.12). We note that there is considerable variation in the number of annotations made by individuals, and ? is computed over 5 categories. Hence, this result cannot be straightforwardly interpreted as a ceiling of human performance for VALSE. However, ? is higher for pieces on which models also perform better (e.g. existence, Foil-It!; cf. ?5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Benchmarking with VALSE</head><p>We propose VALSE as a task-independent, zeroshot benchmark to assess the extent to which models learn to ground specific linguistic phenomena as a consequence of their pretraining (or fine-tuning). VALSE is built in the spirit of approaches such as Checklist <ref type="bibr">(Ribeiro et al., 2020)</ref>, including pairs consisting of captions and minimally edited foils.</p><p>The only requirement to evaluate a model on our benchmark is: i) to have a binary classification head to predict whether an image-sentence pair is foiled, or ii) to predict an image-sentence matching score between the image and the caption vs. the foil, returning the pair with the highest score. Systems reporting results on VALSE are expected to report any data used in model training prior to testing on VALSE, for comparability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Metrics</head><p>We employ five metrics 6 for evaluation: overall accuracy (acc) on all classes (foil and correct); precision (p c ) measuring how well models identify the correct examples; foil precision (p f ) measuring how well foiled cases are identified; pairwise ranking accuracy (acc r ), which measures whether the image-sentence alignment score is greater for a correct image-text pair than for its foiled pair; and area under the receiver operating characteristic curve (AUROC), which measures how well models distinguish correct vs. foiled examples across different prediction thresholds. acc r is more permissive than acc as it accepts model predictions if the score for a foil is lower than the caption's score. Our main metrics are AU-ROC and acc r . acc r gives results for a pair image, caption and image, foil . Both AUROC and acc r are well suited to evaluate minimally-edited pairs as neither uses a classification threshold. As for p c and p f , since these are competing metrics where naively increasing one can decrease the other, we report the smaller of the two as an indicator of how informed model predictions are. Since all instruments are implemented as a balanced binary classification, the random baseline is always 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">V&amp;L models</head><p>We benchmark five V&amp;L models on VALSE: CLIP (Radford et al., 2021), LXMERT (Tan and Bansal, <ref type="bibr">6</ref> All metrics are defined in Appendix B. 2019), <ref type="bibr">ViLBERT (Lu et al., 2019)</ref>, ViLBERT 12in-1 <ref type="bibr">(Lu et al., 2020), and</ref><ref type="bibr">VisualBERT (Li et al., 2019)</ref>. These models have different architectures and are pretrained on a variety of tasks with different training data. We also benchmark two unimodal text-only models, <ref type="bibr">GPT1 (Radford et al., 2018)</ref> and <ref type="bibr">GPT2 (Radford et al., 2019)</ref>. See Appendix D for details on all these models used in our evaluation.</p><p>Unimodal models GPT1 and GPT2 are autoregressive language models pretrained on English text. We test whether VALSE is solvable by these unimodal models by computing the perplexity of the correct and foiled caption and predicting the entry with the lowest perplexity. If the perplexity is higher for the foil, we take this as an indication that the foiled caption may suffer from plausibility bias or other linguistic biases (cf. ?4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments and Results</head><p>We test V&amp;L and unimodal models on VALSE in a zero-shot setting, and also evaluate on a number of correct captions and foils from the FOIL it! dataset (Shekhar et al., 2017b) (cf. App. A.7 for details). All results are listed in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Unimodal results For most instruments, unimodal results are close to random and hence do not signal strong linguistic or plausibility biases. One exception is the original FOIL it! dataset, in line with Madhyastha et al. (2019)'s findings. Also the spatial relations (77.2%), action replacement (66.8%) and actant swap (76.9%) instruments suggest plausibility biases in foils. Such biases are hard to avoid in automatic foil generation for actions due to the verb arguments' selectional restrictions, which are easily violated when flipping role fillers, or replacing the verb. Similar considerations hold for relations: though SpanBERT proposals are intended to aid selection of likely replacements for prepositions, plausibility issues arise with relatively rare argument-preposition combinations.</p><p>While these might be the first instruments in VALSE to be solved in the future, current V&amp;L models struggle to detect even blatant mismatches of actant swap, e.g., 'A ball throws a tennis player.' For VALSE, the unimodal scores will serve as a baseline for the pairwise accuracy of V&amp;L models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal results</head><p>The best zero-shot results are achieved by ViLBERT 12-in-1 with the highest scores across the board, followed by ViLBERT,  LXMERT, CLIP, 7 and finally VisualBERT. The latter obtains high p f but very low p c valuesreflected in the min(p c , p f ) scores-indicating that VisualBERT learned a heuristic that does not generalise (see Hendricks and Nematzadeh, 2021, for similar observations with other models). We hypothesise that this is due to the way image-sentence alignment is framed in VisualBERT's pretraining: the model expects an image and a correct sentence c 1 , and predicts whether a second sentence c 2 is a match. 8 During pretraining c 1 and c 2 are likely to differ in many ways, whereas in our setting, they are nearly identical. This may bias the model against predicting foils, which would raise the value p f . Instruments centered on individual objects like existence and the FOIL it! dataset are almost solved by ViLBERT 12-in-1, highlighting that models are capable of identifying named objects and their presence in images. However, none of the remaining pieces can be reliably solved in our adversarial foiling settings: i) distinguishing references to single vs. multiple objects or counting them in an 7 CLIP works in a contrastive fashion, therefore we report only accr (cf. Appendix D for details). 8 c1 is one of the 5 captions describing the relevant image in MSCOCO. During VisualBERT's pretraining, c2 can be an alternative caption out of these 5, or a randomly drawn caption which does not describe the image. The pretraining task is to determine if c2 correctly describes the image or not. image (plurality and counting); ii) correctly classifying a named spatial relation between objects in an image (relations); iii) distinguishing actions and identifying their participants, even if supported by preference biases (actions); or, iv) tracing multiple references to the same object in an image through the use of pronouns (coreference).</p><p>Correct vs. foil precision p c and p f show that V&amp;L models struggle to solve the phenomena in VALSE. When a model achieves high precision on correct captions p c this is often at the expense of very low precision on foiled captions p f (cf. ViL-BERT), or vice-versa (cf. VisualBERT). This suggests that such models are insensitive to VALSE's inputs: models that almost always predict a match will inflate p f at the expense of p c . min(p c , p f ) reveals that VisualBERT and ViLBERT perform poorly and below random baseline, and LXMERT close to or below it. ViLBERT 12-in-1 performs strongly on existence, well on counting, but struggles on plurality, spatial relations, coreference, and actions. These tendencies we see reflected in our main metrics, acc r and AUROC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We present the VALSE benchmark to help the community improve V&amp;L models by hard-testing their visual grounding capabilities through the lens of lin-guistic constructs. Our experiments show that V&amp;L models identify named objects and their presence in images well (as shown by the existence piece), but struggle to ground their interdependence and relationships in visual scenes when forced to respect linguistic indicators. We encourage the community to use VALSE for measuring progress towards V&amp;L models capable of true language grounding. Furthermore, VALSE could be used as an indirect assessment of datasets, as models could be evaluated before and after training or fine-tuning to see if a dataset helps models improve on any of the aspects tested by VALSE.</p><p>VALSE is designed as a living benchmark. As future work we plan to extend it to further linguistic phenomena, and to source data from diverse V&amp;L datasets to cover more linguistic variability and image distributions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Benchmark creation A.1 Existence</head><p>The existence piece has a single instrument and targets instances with existential quantifiers. Models need to differentiate between examples i) where there is no entity of a certain type or ii) where there is one or more of these entities visible in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sources</head><p>We use the Visual7W visual question answering dataset <ref type="bibr">(Zhu et al., 2016)</ref> to source examples, starting with the 'how many' questions in Visual7W and building a pool of those whose answers are numerals (e.g., 0, 1, 2, etc.). We use the templates from Parcalabescu et al. <ref type="formula">(2021)</ref> to transform question and answer fields into a declarative statement that correctly describes what can be seen in the image, e.g., 'Q: How many animals are shown? A: 0' ? 'There are 0 animals shown'.</p><p>Foiling method Let us use x = 'There are N animals shown' as a running example for a correct caption, where N is a number. If N &gt; 0, we simply remove N from the sentence, effectively creating the statement ?x or 'There are animals shown'. If N = 0, we replace N by 'no', creating the statement ??x or 'There are no animals shown'. If necessary, we fix singular-plural agreement. To create data with balanced correct and foil classes, we select 50% of our examples from those where the correct answer is originally 0, and the remaining 50% from those where the correct answer is any other number (e.g., 1, 2, etc.). To create foils, we then simply convert the statement from ?x to ??x, and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Plurality</head><p>The plurality piece has a single instrument, concerned with semantic number, that is, the distinction between single entities in an image ('exactly one flower') and multiple instances of the same type ('some flowers'). In this piece, foil candidates are created either by converting a singular NP and its coreferents to a plural, or vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sources</head><p>The data was sourced from the validation split of the COCO 2017 dataset <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>. Captions are only foiled if their length after tokenization with the pretrained BERT tokenizer 9 is of 80 tokens or less. This is done to minimise the risk that captions and foils need to be truncated of the two quantifiers for the other number. In the foregoing example, we end up with 'A small copper vase with some flowers / exactly one flower in it.' After generating all candidate foils, in both directions, we use the GRUEN pretrained model <ref type="bibr">(Zhu and Bhat, 2020)</ref> to score the foils for grammaticality. We only keep foils with a score ? 0.8, and run each foil-caption pair through the NLI model described in Section 4.3, keeping only pairs whose predicted label is contradiction, for an initial candidate set of 1000 cases (500 sg2pl and 500 pl2sg), of which 851 (85.1%) are considered valid following manual validation (see <ref type="bibr">?4.4)</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> shows the distribution of nouns in captions and foils, before and after the validation. Note that the validation process does not result in significant change to the distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Counting</head><p>The counting piece comes in three instruments: balanced, adversarial and small numbers. All three instruments include instances with statements about the number of entities visible in an image. The model needs to differentiate between examples where the specific number of entities in the associated image is correct or incorrect, given the statement.</p><p>All three instruments are designed to show whether models learn strategies that generalize beyond the training distribution, and to what extent a model exploits class frequency bias. 11 In counting balanced we cap the number of examples to a maximum per class and make sure correct/foil classes are balanced, so that models that exploit class frequency bias are penalized. In counting adversarial we make sure that all foils take class n ? {0, 1, 2, 3}, whereas all correct captions take class n ? {n | n ? 4}. Biased models are expected to favour more frequent classes and these correspond to smaller numbers, therefore models that resort to such biases should perform poorly on this adversarially built test. Instrument counting small numbers is a sanity check where all correct captions and foils have class n ? {0, 1, 2, 3}, and caption/foil classes are balanced. Models likely have been exposed to many examples in this class set, so with this instrument we assess model performance certain it does not suffer from (class) exposure bias. <ref type="bibr">11</ref> We take the original answer in Visual7W as the example class. E.g., in There are four zebras, the class is 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sources</head><p>We use the Visual7W visual question answering dataset <ref type="bibr">(Zhu et al., 2016)</ref> and source its 'how many' examples, building a pool of those whose answers are numerals (e.g., 0, 1, 2, etc.). We use the templates from <ref type="bibr">Parcalabescu et al. (2021)</ref> to transform question and answer fields into a declarative statement that correctly describes what can be seen in the image.</p><p>Foiling method We create foils by directly replacing the numeral in the correct caption by another numeral. When creating foils we make sure that the class distribution for correct and foiled captions are approximately the same, i.e., there are a similar number of correct and foiled examples in each class in each instrument. The only exception is the counting adversarial instrument, where the classes used in correct and foiled captions are disjoint, i.e., n ? {0, 1, 2, 3} and n ? {n | n ? 4}, respectively. See <ref type="figure" target="#fig_3">Figure 3</ref> for a visualisation of these distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Spatial relations</head><p>The relations piece has one instrument and focuses on the ability of models to distinguish between different spatial relations, as expressed by prepositions. Foils therefore consist of captions identical to the original except for the replacement of a spatial preposition.</p><p>Data sources Data was sourced from the COCO 2017 validation split <ref type="bibr" target="#b6">(Chen et al., 2015)</ref>. To generate foil candidates, we first extracted from the original COCO captions all the sequences consisting of one or more consecutive prepositions (e.g., 'on' or 'out of'). Foils are generated by detecting these preposition spans, and replacing them with another preposition span attested in the list.</p><p>Foiling method To generate foils, we mask the preposition span in an original caption, and use SpanBERT (Joshi et al., 2020), a pretraining method based on BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>. <ref type="bibr">12</ref> The advantage of SpanBERT over BERT is that in a masked language modelling context, with masks spanning more than a single word, SpanBERT predicts sequences and takes into account their joint probability, whereas BERT trained with standard Masked Language Modelling can only predict single tokens independently. With SpanBERT, we generate replacements of between 1 and 3 tokens in length, in each case retaining only the best prediction out of the top k which matches one of the preposition sequences in the pre-extracted list.</p><p>After all candidates are generated, we apply GRUEN (Zhu and Bhat, 2020) to score the foils for grammaticality, and further apply the NLI model descibed in Section 4.3 to label the entailment relationship between caption and foil pairs. From the resulting data, we sample as follows: i) we keep only caption-foil pairs labelled as contradiction, where the GRUEN grammaticality score is ? 0.8; ii) for every caption-foil pair sampled where p is replaced with q, we search for another caption-foil pair where q is replaced with p, if present. This strategy yields a roughly balanced dataset, where no single preposition or preposition sequence is over-represented in captions or foils.</p><p>These processes result in an initial set of 614 cases, of which 535 (87.1%) are selected following manual validation described in ?4.4. <ref type="figure" target="#fig_3">Figure 3</ref> shows proportions in captions and foils of the prepositions. E.g.: 'A cat plays with a pocket knife on / underneath a table.'</p><p>As with plurals, we implement procedures for foil candidate generation by extending the perturb functionality in Checklist <ref type="bibr">(Ribeiro et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Actions</head><p>The action piece consists of two instruments: i) action replacement and ii) actant swap. They are testing a V&amp;L model's capability of i) identifying whether an action mentioned in the textual modality matches the action seen in the image or not (e.g. 'a man shouts / smiles at a woman') and ii) correctly identifying the participants of an action and the roles they are playing in it (e.g., given the picture in <ref type="table">Table 1</ref>: is it the man or the woman who shouts?).</p><p>Data source For creating interesting foils with diverse actions, we focus on the SWiG dataset (Pratt et al., 2020) that comprises 504 action verbs annotated with semantic roles and their fillers, which are grounded in images of the imSitu dataset <ref type="bibr">(Yatskar et al., 2016)</ref>. We generate English captions for the images using SimpleNLG <ref type="bibr">(Gatt and Reiter, 2009)</ref>  <ref type="bibr">13</ref> . For generation we use the specified ac-tion verb, the realized FrameNet semantic roles and their annotated filler categories (see <ref type="table">Table 1</ref> for shout: AGENT: man, ADDRESSEE: woman), and generate short captions, with realization of two roles in active form. We apply various filters to ensure high quality of the generated captions using diverse metrics 14 and manual checks through AMT crowdsourcing.</p><p>Foiling method When creating the action replacement instrument, we need to make sure that the action replacement suits the context. We propose action replacements with BERT <ref type="bibr" target="#b10">(Devlin et al., 2019</ref>) that need to satisfy three conditions: 1) the proposed action verbs originate from the SWiG dataset -otherwise new verbs are introduced on the foil side only, which may induce biases; 2) the frequency distribution of action verbs on the caption and on the foil side is approximately the same (cf. <ref type="figure" target="#fig_4">Figure 4)</ref>; 3) we constrain the replacement verbs to be either antonyms of the original verb or at least not synonyms, hyponyms or hypernyms to the original, according to WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998)</ref> in order to avoid situations where replacements are almost synonymous to the original action. The actant swap instrument is based on the original image annotations, but swaps the two role fillers (e.g., 'A woman shouts at the man.' for the image in <ref type="table">Table 1</ref>). To avoid agreement mistakes, we generate these foils using the inverted role fillers as input.</p><p>We plot caption and foil word frequency distributions for action replacement in <ref type="figure" target="#fig_4">Figure 4</ref>. We do not plot statistics for the actant swap instrument since by construction it cannot suffer from distributional bias since caption and foil contain the same words up to a permutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Coreference</head><p>The coreference piece consists of two pieces: coreference standard and coreference clean. It aims to uncover whether V&amp;L models are able to perform pronoun coreference resolution. The coreference phenomenon encompasses both cases where i) the pronoun refers to a noun (phrase) and both the pronoun and the (noun) phrase are grounded in the visual modality (e.g. 'A woman is driving a motorcycle. Is she wearing a helmet?'), and cases where ii) the pronoun refers directly to a region in the image or even to the whole image (e.g. 'A man is sitting on a bench. Is this outside?').</p><p>Data source We source the data from VisDial v1.0 <ref type="bibr" target="#b9">(Das et al., 2017)</ref>, which contains images from <ref type="bibr">MSCOCO (Lin et al., 2014)</ref>, their captions and dialogues about the images in form of Q&amp;A sequences. To ensure that the coreference phenomenon is present in the [Caption. Question? Yes/No.] formulations, we check whether pronouns are present in the question. The list of pronouns and their frequencies in our train-val-test splits are represented in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The coreference standard instrument contains 916 data samples (708 are valid 15 ) from the Vis-Dial's training set. The data of coreference clean instrument consisting of 141 samples (104 are valid), originates from VisDial's validation set. With models that have been trained on VisDial, we would be in the situation where models are tested on their training data. Therefore we also have the coreference clean instrument based on the validation set of VisDial to test models safely. Unfortunately, we cannot use VisDial's test set because the required question-answers annotations necessary for foiling are withheld.</p><p>Foiling method When foiling, we take the image description of the form <ref type="bibr">[Caption. Question? Yes/No.]</ref> and exchange the answer: yes ?no and vice-versa (see example in <ref type="table">Table 1</ref>). This way, we keep the full textual description including pronoun and noun (phrase) intact, hence ensuring that the coreference phenomenon is present and valid in the foil too, and rely on the model to interpret affirmation and negation correctly. Note that we rely on the capability of models to correctly interpret negation also in the existence piece (cf. ?3.1).</p><p>Arguably, coreference is the most difficult phenomenon to foil in VALSE. Especially in cases where pronouns refer to a noun (phrase) (e.g., 'A woman is driving a motorcycle. Is she wearing a helmet? Yes.'), exchanging the pronoun with another pronoun would generate incoherent and unlikely sequences 16 (e.g., 'A woman is driving a mo- torcycle. Is he wearing a helmet?'), and exchanging it with a noun phrase would furthermore break the pronoun coreference phenomenon because there would be no pronoun anymore (e.g., 'A woman is driving a motorcycle. Is the man wearing a helmet?'). Therefore when foiling the coreference piece, we aim to keep the original description intact for ensuring the preservation of the coreference phenomenon. Hence we rely on the answers containing yes or no 17 and exchange affirmative to negative answers and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 FOIL it! data</head><p>We include an additional piece in VALSE consisting of 1000 randomly sampled entries from the FOIL it! dataset <ref type="bibr">(Shekhar et al., 2017b)</ref>. Each entry in FOIL it! consists of an <ref type="bibr">MSCOCO (Lin et al., 2014)</ref> image and a foiled caption where a noun phrase depicting an object visible in the image was replaced by a semantically related noun phrase. Since examples in the FOIL it! dataset are linked to MSCOCO, we use these links to retrieve one correct caption from the five captions available for the image, and create an image-caption-foil triple. From the original 1000 entries, 943 have been validated by our manual annotation procedure (in Appendix E). Please refer to <ref type="bibr">Shekhar et al. (2017b)</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation metrics</head><p>We evaluate pretrained V&amp;L models on VALSE using accuracy (acc), the overall accuracy on all classes; precision or positive predictive value (p c ), which measures the proportion of correctly identified correct captions; and foil precision or negative predictive value (p f ), which measures the proportion of correctly identified foiled examples; pairwise ranking accuracy acc r , computed using the image-sentence alignment score ? that the model assigns to correct and foiled image-text pairs; and area under the receiver operating characteristic curve (AUROC)-a classic metric used in machine learning classification problems-which in our case measures how well models distinguish correct vs. foiled examples across different prediction thresholds. The AUROC has a probabilistic interpretation and can be understood as the probability that a model will assign a higher score to a randomly chosen correct example relative to a randomly chosen foil.</p><p>With acc r , a prediction is considered successful, if given an image (i) paired with a correct (c) versus a foil (f ) text, the score of the positive/correct pair is greater than that of the foiled pair.</p><formula xml:id="formula_0">acc r = (i,c)?C f ?F s(i, c, f ) |C| + |F | , s(i, c, f ) = 1, if ?(i, f ) ? ?(i, c), 0, otherwise,</formula><p>where C is the set of correct image-caption pairs (i, c), and F is the set of foils for the pair (i, c).</p><p>The pairwise accuracy acc r is important for two reasons: First, it enables V&amp;L models to be evaluated on VALSE without a binary classification head for classifying image-sentence pairs as correct or foiled. For example, <ref type="bibr">CLIP (Radford et al., 2021)</ref> is a model that computes a score given an imagesentence pair. This score can be used to compare the scores of a correct image-sentence pair and the corresponding foiled pair. By contrast, a model like LXMERT (Tan and Bansal, 2019) has a binary image-sentence classification head and can predict a correct pair independently of the foiled pair (and vice-versa). Second, acc r enables the evaluation of unimodal models on VALSE, as motivated in ?4.2. In <ref type="table" target="#tab_6">Table 4</ref>, we show results for all models investigated according to all above-mentioned metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Filtering methods</head><p>NLI filtering For NLI filtering we make use of the HuggingFace (Wolf et al., 2020) implementation of ALBERT (xxlarge-v2) that was already finetuned on the concatenation of SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, <ref type="bibr">MultiNLI (Williams et al., 2018)</ref>, <ref type="bibr">FEVER-NLI (Nie et al., 2019)</ref> and ANLI datasets <ref type="bibr">(Nie et al., 2020)</ref>. The model is the best performing on the ANLI benchmark leaderboard 18 and it achieves 90% accuracy on MultiNLI devset. 18 github.com/facebookresearch/anli</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Vision &amp; Language and Unimodal Models</head><p>In  <ref type="bibr">21</ref> We use the image-sentence alignment head of the publicly available model checkpoints for  2019) are transformer-based autoregressive language models pretrained on English data through self-supervision. We test whether our benchmark is solvable by these unimodal models by computing the perplexity of the correct sentence and compare it to the perplexity of the foiled sentence. In case the computed perplexity is higher for the foil than for the correct sentence, we assume that the correctly detected foiled caption may possibly suffer from a plausibility bias (as described in section 4.2) or from other biases (e.g. a model's preference towards affirmative or negative sentences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Mechanical Turk Annotation and Evaluation</head><p>Setup The validation study was conducted on all the data for each instrument in VALSE, as well as for the FOIL it! data <ref type="bibr">(Shekhar et al., 2019b)</ref>. Each instance consisted of an image, a caption and a foiled version of the caption, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Annotators received the following general instructions:</p><p>You will see a series of images, each accompanied by two short texts. Your task is to judge which of the two texts accurately describes what can be seen in the image.</p><p>Each instance was accompanied by the caption and the foil, with the ordering balanced so that the caption appeared first 50% of the time. In each instance, the caption and foil were placed above each other, with the differing parts highlighted in bold. Annotators were asked to determine which of the two sentences accurately describes what can be seen in the image? In each case, they had to choose between five options: (a) the first, but not the second; (b) the second, but not the first; (c) both of them; (d) neither of the two; and (e) I cannot tell. We collected three annotations for each instance, from three independent workers.</p><p>Annotator selection We recruited annotators who had an approval rating of 90% or higher on Amazon Mechanical Turk. We ran an initial, preselection study with 10 batches of 100 instances each, in order to identify annotators who understood the instructions and performed the task adequately. The pre-selection batches were first manually annotated by the authors, and we identified 'good' annotators based on the criterion that they preferred the caption to the foil at least 70% of the time. Based on this, we selected a total of 63 annotators. Annotators were paid $0.05 per item (i.e. per HIT on Mechanical Turk).</p><p>Results <ref type="table" target="#tab_7">Table 5</ref> shows, for each instrument, the number of instances in total, as well as the proportion of instances which we consider valid, that is, those for which at least two out of three annotators chose the caption, but not the foil, as the text which accurately describes the image. We also show the number of instances for which annotators unanimously (3/3) chose the caption.</p><p>Annotator agreement As shown in <ref type="table" target="#tab_7">Table 5</ref>, the proportion of valid instances in each instrument was high, ranging from 73.8% to 94.6%, with most instruments having annotators choose the caption well over 80% of the time. The table also shows two inter-annotator agreement statistics, both computed using Krippendorff's ?: over all the data in a given instrument, and over the valid subset only. On the valid subset, agreement is higher, and ranges from 0.3 to 0.6 (mean = 0.42; sd=0.12). There is a significant positive correlation between the percentage of valid instances per instrument and the ? value (Spearman's ? = 0.75; p &lt; .05). The low to medium agreement suggested by the ? range is due to two factors: first, the statistic is computed over the entire pool of annotators, of whom there were significant diversions in the amount of annotations they computed (e.g. some workers annotated fewer than 5 HITs); furthermore, the agreement is computed over 5 categories (see above). Given these factors, the inter-annotator agreement results should be treated with caution, and are not straightforwardly interpretable as an index of human performance on VALSE -in particular, the validation task (with 5 categories) was framed differently from the benchmark (which is binary).</p><p>Bias check While measures were taken to control for distributional bias between captions and foils in the different pieces of VALSE (cf. ?4.1), it is possible that sub-sampling after manual validation could reintroduce such biases. To check that this is not the case, we compare the word frequency distributions between captions and foils in the original pieces, and the word frequency distribution of the manually validated set. We report the Jensen-Shannon divergence and the number of words that differ between caption and foil in <ref type="table" target="#tab_7">Table 5</ref>. The foil-caption word frequency distributions can be inspected in <ref type="figure" target="#fig_3">Figures 3 and 4</ref> where f is the normalized word frequency for foils, c the normalized word frequency for captions, m is the point-wise mean of f and c, and KL is the Kullback-Leibler divergence. As <ref type="table" target="#tab_7">Table 5</ref> shows, the JS-divergence between caption and foil distributions remains the same, or changes only marginally (compare columns JS-div and Js-div valid, where #Lexical Items indicates the number of lexical/phrasal categories in the relevant distributions). This indicates that no significant bias was introduced as a result of subsampling after manual validation.  There is a truck pictured.</p><p>There is no truck pictured.</p><p>There are no clouds in the sky. There are clouds in the sky.</p><p>There are no people riding on elephants.</p><p>There are people riding on elephants.</p><p>There is a kite. There is no kite. Two young men playing frisbee at night on a number of sports fields.</p><p>Exactly one row of motorcycles parked together on a grass yard area with a house in the background.</p><p>A number of rows of motorcycles parked together on a grass yard area with a house in the background.</p><p>Two men are looking inside of a single giant barbecue.</p><p>Two men are looking inside of a number of giant barbecues.</p><p>Some children are playing baseball outside in a field.</p><p>A single child is playing baseball outside in a field.</p><p>A number of people riding some motorbikes on the road.</p><p>A single person riding some motorbikes on the road. There is exactly 1 person snowboarding.</p><p>There are exactly 4 people snowboarding.</p><p>There are exactly 6 motorcycles in this photo altogether.</p><p>There are exactly 7 motorcycles in this photo altogether.</p><p>There are exactly 2 banana stalks.</p><p>There are exactly 4 banana stalks.</p><p>There are exactly 12 roman numerals on the clock.</p><p>There are exactly 9 roman numerals on the clock. A baby elephant is walking on a larger elephant.</p><p>Fruits and vegetables are being sold in a market.</p><p>Fruits and vegetables are being sold outside a market.</p><p>An airplane is letting off white smoke against a blue sky.</p><p>An airplane is letting in white smoke against a blue sky.</p><p>A cow stands on a sidewalk outside a building.</p><p>A cow stands on a sidewalk in a building.</p><p>Three giraffes banding down to drink water with trees in the background.</p><p>Three giraffes banding up to drink water with trees in the background.  A woman skips a jump rope. A woman releases a jump rope.</p><p>An old man coaches people. An old man bothers people.</p><p>The people unveil the prize. A prize unveils people.</p><p>A baby drools over clothing. A clothing drools over the baby.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Normalized pronoun frequencies in the coreference subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example of an instance from the validation study. The example is from the counting piece, adversarial instrument (see Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. The Jensen-Shannon (JS) divergence is defined as: JS(f c) = KL(f m) + KL(c m) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Word frequency distributions for captions and foils before and after the manual validation for existence, counting and relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Word frequency distributions for captions and foils before and after the manual validation for plurality, action replacement and FOIL it. The actant swap instrument is not visualised here: By construction, actant swap cannot suffer from distributional bias since caption and foil contain the same words up to a permutation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Performance of unimodal and multimodal models on the VALSE benchmark according to different metrics. We bold-face the best overall result per metric, and underscore all results below (or at) the random baseline. acc r is a pairwise ranking accuracy where a prediction is considered correct if p(caption, img) &gt; p(f oil, img). Precision p c and foil precision p f are competing metrics where na?vely increasing one can decrease the other: therefore looking at the smaller number among the two gives a good intuition of how informed is a model prediction. ?sns. Counting small numbers. adv. Counting adversarial. repl. Action replacement. ? Sp.rel. Spatial relations.</figDesc><table /><note>* Unimodal text-only models that do not use images as input. CLIP is only tested in pairwise ranking mode (fn. 6).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307-1323, Online. Association for Computational Linguistics. Sahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur Taly, Ed H. Chi, and Alex Beutel. 2019. Counterfactual fairness in text classification through robustness. -of-distribution generalization in visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 878-892, Online. Association for Computational Linguistics. Image and Vision Computing, 112:104194. Lisa Anne Hendricks and Aida Nematzadeh. 2021. Probing Image-Language Transformers for Verb Understanding. arXiv, 2106.09141. Jia, Aditi Raghunathan, Kerem G?ksel, and Percy Liang. 2019. Certified robustness to adversarial word substitutions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4129-4142, Hong Kong, China. Association for Computational Linguistics. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll?r, and C. Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision -ECCV 2014, pages 740-755, Cham. Springer International Publishing. Xiao Liu, and Dawn Song. 2020a. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967.</figDesc><table><row><cell cols="3">In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19, page 219-226, New York, NY, USA. Association for Computing Machinery. Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A re-alisation engine for practical applications. In Pro-ceedings of the 12th European Workshop on Natural Language Generation (ENLG 2009), pages 90-93, Athens, Greece. Association for Computational Lin-guistics. Gabriel Goh, Nick Cammarata  ?, Chelsea Voss  ?, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. 2021. Multi-modal neurons in artificial neural networks. Distill. Https://distill.pub/2021/multimodal-neurons. Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. 2020. MUTANT: A training Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Unicorn on rainbow: A uni-versal commonsense reasoning model on a new mul-titask benchmark. In Proceedings of the AAAI Con-ference on Artificial Intelligence, 15, pages 13480-13488. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visi-olinguistic representations for vision-and-language tasks. In Advances in Neural Information Process-ing Systems, pages 13-23. Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2020. 12-in-1: Multi-task vision and language representation learning. In The 2556-2565, Melbourne, Australia. Association for Computational Linguistics. Ravi Shekhar, Sandro Pezzelle, Aur?lie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017a. Vision and language integration: Moving be-yond objects. In IWCS 2017 -12th International Conference on Computational Semantics -Short papers. Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au-r?lie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017b. FOIL it! find one mis-match between image and language caption. In Pro-ceedings of the 55th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: Long Papers), pages 255-265, Vancouver, Canada. Asso-ciation for Computational Linguistics. Ravi Shekhar, Ece Takmaz, Raquel Fern?ndez, and Raffaella Bernardi. 2019a. Evaluating the represen-tational hub of language and vision models. In Pro-ceedings of the 13th International Conference on paradigm for outRobin Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Associa-tion for Computational Linguistics, 8:64-77. Kushal Kafle, Robik Shrestha, and Christopher Kanan. 2019. Challenges and prospects in vision and lan-guage research. Frontiers in Artificial Intelligence, 2:28. Alexander Miller. 2019. Language models as knowl-edge bases? In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. As-sociation for Computational Linguistics. Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641-2649. Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark D?az. 2021. On releasing annotator-level labels and information in datasets. Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi. 2020. Grounded situa-tion recognition. In Computer Vision -ECCV 2020 -pages 20-23, Online. Association for Computational Linguistics. Chenguang Wang, Junlin Wang, Jens Tuyls, Eric Wallace, and Sameer Singh. 2020b. Gradient-based analysis of NLP mod-els is manipulable. In Findings of the Associa-tion for Computational Linguistics: EMNLP 2020, pages 247-258, Online. Association for Computa-tional Linguistics. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguis-tics. IEEE/CVF Conference on Computer Vision and Pat-tern Recognition (CVPR). Pranava Madhyastha, Josiah Wang, and Lucia Specia. 2019. VIFIDEL: Evaluating the visual fidelity of image descriptions. In Proceedings of the 57th An-nual Meeting of the Association for Computational Linguistics, pages 6539-6550, Florence, Italy. Asso-ciation for Computational Linguistics. 16th European Conference, pages 314-332. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Computational Semantics -Long Papers, pages 211-Thomas Wolf, Julien Chaumond, Lysandre Debut, Vic-222, Gothenburg, Sweden. Association for Compu-tor Sanh, Clement Delangue, Anthony Moi, Pier-tational Linguistics. ric Cistac, Morgan Funtowicz, Joe Davison, Sam Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Ravi Shekhar, Aashish Venkatesh, Tim Baumg?rtner, Shleifer, et al. 2020. Transformers: State-of-the-Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Elia Bruni, Barbara Plank, Raffaella Bernardi, and art natural language processing. In Proceedings of et al. 2021. Learning transferable visual models Raquel Fern?ndez. 2019b. Beyond task success: A the 2020 Conference on Empirical Methods in Nat-from natural language supervision. arXiv preprint closer look at jointly learning to see, ask, and Guess-ural Language Processing: System Demonstrations, arXiv:2103.00020. What. In Proceedings of the 2019 Conference of pages 38-45.</cell></row><row><cell>Pranava Swaroop Madhyastha, Josiah Wang, and Lu-cia Specia. 2018. Defoiling foiled image captions. In Proceedings of the 2018 Conference of the North the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages 2578-2587, Minneapolis, Minnesota. Association</cell><cell cols="2">Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language under-standing by generative pre-training. Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019. Visual Entailment: A Novel Task for Fine-Grained Image Understanding. arXiv, 1901.06706.</cell></row><row><cell>American Chapter of the Association for Compu-tational Linguistics: Human Language Technolo-gies, Volume 2 (Short Papers), pages 433-438, New Orleans, Louisiana. Association for Computational Linguistics. for Computational Linguistics. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. Vl-bert: Pre-training of generic visual-linguistic representations. In International Conference on Learning Represen-</cell><cell cols="2">Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Mark Yatskar, Luke Zettlemoyer, and Ali Farhadi. Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, 2016. Situation recognition: Visual semantic role labeling for image understanding. In Proceedings of the IEEE Conference on Computer Vision and Pat-tern Recognition (CVPR).</cell></row><row><cell>Yixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and verification with neu-ral semantic matching networks. In Association for the Advancement of Artificial Intelligence (AAAI). Yixin Nie, Adina Williams, Emily Dinan, Mohit tations. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A corpus for reasoning about natural language grounded in pho-tographs. In Proceedings of the 57th Annual Meet-</cell><cell cols="2">and Sameer Singh. 2020. Beyond accuracy: Be-Wanzheng Zhu and Suma Bhat. 2020. GRUEN for havioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 4902-4912, Online. Association for Computational Lin-guistics. evaluating linguistic quality of generated text. In Findings of the Association for Computational Lin-guistics: EMNLP 2020, pages 94-108, Online. As-sociation for Computational Linguistics.</cell></row><row><cell>Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-versarial NLI: A new benchmark for natural lan-guage understanding. In Proceedings of the 58th An-nual Meeting of the Association for Computational Linguistics, pages 4885-4901, Online. Association ing of the Association for Computational Linguistics, pages 6418-6428, Florence, Italy. Association for Computational Linguistics. Hao Tan and Mohit Bansal. 2019. LXMERT: Learning</cell><cell cols="2">2021. Value: A multi-task benchmark for video-and-language understanding evaluation. arXiv preprint arXiv:2106.04632. Daniel Rosenberg, Itai Gat, Amir Feder, and Roi Re-ichart. 2021. Are VQA systems RAD? Measuring Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7W: Grounded Question Answer-ing in Images. In IEEE Conference on Computer robustness to augmented data with focused interven-tions. In Proceedings of the 59th Annual Meeting of Vision and Pattern Recognition.</cell></row><row><cell>for Computational Linguistics. cross-modality encoder representations from trans-</cell><cell cols="2">the Association for Computational Linguistics and</cell></row><row><cell>formers. In Proceedings of the 2019 Conference on</cell><cell cols="2">Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui the 11th International Joint Conference on Natu-</cell></row><row><cell>Letitia Parcalabescu, Albert Gatt, Anette Frank, and Empirical Methods in Natural Language Processing</cell><cell cols="2">Hsieh, and Kai-Wei Chang. 2019. Visualbert: A ral Language Processing (Volume 2: Short Papers),</cell></row><row><cell>Iacer Calixto. 2021. Seeing past words: Testing and the 9th International Joint Conference on Natu-</cell><cell cols="2">simple and performant baseline for vision and lan-pages 61-70, Online. Association for Computational</cell></row><row><cell>the cross-modal capabilities of pretrained v&amp;l mod-ral Language Processing (EMNLP-IJCNLP), pages</cell><cell>guage. In Arxiv. Linguistics.</cell><cell></cell></row><row><cell>els on counting tasks. In Proceedings of the 1st 5100-5111, Hong Kong, China. Association for</cell><cell></cell><cell></cell></row><row><cell>Workshop on Multimodal Semantic Representations Computational Linguistics.</cell><cell cols="2">Piyush Sharma, Nan Ding, Sebastian Goodman, and</cell></row><row><cell>(MMSR), pages 32-44, Groningen, Netherlands (On-</cell><cell>Radu Soricut. 2018.</cell><cell>Conceptual captions: A</cell></row><row><cell>line). Association for Computational Linguistics. Eric Wallace, Matt Gardner, and Sameer Singh. 2020.</cell><cell cols="2">cleaned, hypernymed, image alt-text dataset for au-</cell></row><row><cell>Interpreting predictions of NLP models. In Proceed-</cell><cell cols="2">tomatic image captioning. In Proceedings of the</cell></row><row><cell>Fabio Petroni, Tim Rockt?schel, Sebastian Riedel, ings of the 2020 Conference on Empirical Methods</cell><cell cols="2">56th Annual Meeting of the Association for Compu-</cell></row><row><cell>Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and in Natural Language Processing: Tutorial Abstracts,</cell><cell cols="2">tational Linguistics (Volume 1: Long Papers), pages</cell></row></table><note>Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image under- standing in visual question answering. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904-6913. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural lan- guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Associa- tion for Computational Linguistics. Feijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. 2021. Interpretable visual reasoning: A survey.Robin Jia and Percy Liang. 2017. Adversarial exam- ples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2021-2031, Copenhagen, Denmark. Association for Computational Linguistics.Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without con- volution or region supervision. arXiv preprint arXiv:2102.03334. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations. In International Con- ference on Learning Representations. Hector Levesque, Ernest Davis, and Leora Morgen- stern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Princi- ples of Knowledge Representation and Reasoning. Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020a. Unicoder-vl: A universal en- coder for vision and language by cross-modal pre- training. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty- Second Innovative Applications of Artificial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Sym- posium on Educational Advances in Artificial Intel- ligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 11336-11344. AAAI Press. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen- Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al.Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi- aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020b. Oscar: Object- semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121-137. Springer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>we summarise the five V&amp;L models used in our experiments, their architecture, pretraining tasks and data, and fine-tuning tasks (if any).</figDesc><table><row><cell>CLIP CLIP (Radford et al., 2021) is composed</cell></row><row><cell>of two transformer-based text and an image en-</cell></row><row><cell>coders. These are jointly trained on 400M image-</cell></row><row><cell>text pairs through contrastive learning for predict-</cell></row><row><cell>ing high scores for paired image-text examples and</cell></row><row><cell>low scores when image-text samples are not paired</cell></row><row><cell>in the dataset. CLIP has shown zero-shot capa-</cell></row><row><cell>bilities in e.g. object classification, OCR, activity</cell></row><row><cell>recognition (Radford et al., 2021). Goh et al. (2021)</cell></row><row><cell>have shown the existence of multimodal neurons</cell></row><row><cell>in CLIP, responding to the same topic regardless of</cell></row><row><cell>whether it is represented in an image, drawing or</cell></row><row><cell>handwritten text. We use CLIP's image-text align-</cell></row><row><cell>ment scores for benchmarking on VALSE: Given</cell></row><row><cell>an image, we compare whether CLIP 19 predicts</cell></row><row><cell>higher image-text similarity for the correct or for</cell></row><row><cell>the foiled caption.</cell></row><row><cell>LXMERT LXMERT (Tan and Bansal, 2019) is</cell></row><row><cell>a dual-stream transformer model combining V&amp;L</cell></row><row><cell>through cross-modal layers. It is pretrained on</cell></row><row><cell>MSCOCO (Lin et al., 2014) and on multiple VQA</cell></row><row><cell>datasets for (i) multimodal masked word and object</cell></row><row><cell>prediction, (ii) image-sentence alignment, i.e., de-</cell></row><row><cell>termining whether a text corresponds to an image</cell></row><row><cell>or not, and (iii) question-answering. For bench-</cell></row><row><cell>marking on VALSE, we use LXMERT's 20 image-</cell></row><row><cell>sentence alignment head.</cell></row><row><cell>ViLBERT and ViLBERT 12-in-1 ViLBERT</cell></row><row><cell>(Lu et al., 2019) is a BERT-based transformer archi-</cell></row><row><cell>tecture that combines V&amp;L on two separate streams</cell></row><row><cell>by co-attention layers. It is pretrained on Google</cell></row><row><cell>Conceptual Captions (Sharma et al., 2018) on (i)</cell></row><row><cell>multimodal masked word and object prediction;</cell></row><row><cell>and (ii) image-sentence alignment. ViLBERT 12-</cell></row><row><cell>in-1 (Lu et al., 2020) further finetuned a ViLBERT</cell></row><row><cell>model checkpoint on 12 different tasks including</cell></row><row><cell>VQA, image retrieval, phrase grounding and oth-</cell></row><row><cell>ers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of unimodal and multimodal models on the VALSE benchmark according to different metrics. We bold-face the best overall result per metric, and underscore all results below (or at) the random baseline. acc r is a pairwise ranking accuracy where a prediction is considered correct if p(caption, img) &gt; p(f oil, img). Precision p c and foil precision p f are competing metrics where na?vely increasing one can decrease the other: therefore looking at the smaller number among the two gives a good intuition of how informed is a model prediction. ?sns. Counting small numbers. adv. Counting adversarial. repl. Action replacement. ? Sp.rel. Spatial relations. Unimodal text-only models that do not use images as input. CLIP is only tested in pairwise ranking mode (fn. 6).</figDesc><table><row><cell>ViLBERT 22 and ViLBERT 12-in-1 23 .</cell></row><row><cell>VisualBERT VisualBERT (Li et al., 2019) is</cell></row><row><cell>also a BERT-based transformer. Its single-stream</cell></row><row><cell>architecture encodes image regions and linguis-</cell></row><row><cell>tic features via a transformer stack, using self-</cell></row><row><cell>attention to discover the alignments between the</cell></row><row><cell>two modalities. VisualBERT is pretrained on</cell></row><row><cell>MSCOCO captions (Chen et al., 2015) on two</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Manual validation results for each piece in VALSE, as well as for the Foil-it dataset. #Inst.: number of instances for linguistic phenomenon. #Valid (%): number (percent) of cases for which at least 2 out of 3 annotators chose the caption; #Unan. (%): number (percent) of cases for which all annotators chose the caption; #Lex.It.: number of phrases or lexical items in the vocabulary that differ between foils and captions; JS: Jensen-Shannon divergence between foil-caption distributions for all instances in the whole instrument; JS Val.: Jensen-Shannon divergence between foil-caption distribution for the valid subset of the instrument, after sub-sampling; ?: Krippendorff's ? coefficient computed over all the instances; ? valid: Krippendorff's ? coefficient computed over the Valid instances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Randomly selected data examples for existence.</figDesc><table><row><cell>piece</cell><cell>image</cell><cell>caption (blue)</cell><cell>foil (orange)</cell></row><row><cell></cell><cell></cell><cell>Two young men playing frisbee</cell><cell></cell></row><row><cell></cell><cell></cell><cell>at night on exactly one sports</cell><cell></cell></row><row><cell></cell><cell></cell><cell>field.</cell><cell></cell></row><row><cell>plurality</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Randomly selected data examples for plurality.</figDesc><table><row><cell>piece</cell><cell>image</cell><cell>caption (blue)</cell><cell>foil (orange)</cell></row><row><cell></cell><cell></cell><cell>There are exactly 8 horses.</cell><cell>There are exactly 5 horses.</cell></row><row><cell>counting</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Randomly selected data examples for counting.</figDesc><table><row><cell>piece</cell><cell>image</cell><cell>caption (blue)</cell><cell>foil (orange)</cell></row><row><cell></cell><cell></cell><cell>A baby elephant is walking un-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>der a larger elephant.</cell><cell></cell></row><row><cell>relations</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Randomly selected data examples for relations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Randomly selected data examples for actions.</figDesc><table><row><cell>piece</cell><cell>image</cell><cell>caption (blue)</cell><cell>foil (orange)</cell></row><row><cell></cell><cell></cell><cell>A close up of a hot dog with</cell><cell>A close up of a hot dog with</cell></row><row><cell></cell><cell></cell><cell>onions. Is it a big hot dog? Yes.</cell><cell>onions. Is it a big hot dog? No.</cell></row><row><cell>coreference</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>A skateboarding man is on a half</cell><cell>A skateboarding man is on a half</cell></row><row><cell></cell><cell></cell><cell>pipe. Does he wear a helmet?</cell><cell>pipe. Does he wear a helmet?</cell></row><row><cell></cell><cell></cell><cell>No.</cell><cell>Yes.</cell></row><row><cell></cell><cell></cell><cell>2 women who have painted on</cell><cell>2 women who have painted on</cell></row><row><cell></cell><cell></cell><cell>mustaches petting a horse. Are</cell><cell>mustaches petting a horse. Are</cell></row><row><cell></cell><cell></cell><cell>they wearing hats? No.</cell><cell>they wearing hats? Yes.</cell></row><row><cell></cell><cell></cell><cell>Yellow sunflowers are in a blue</cell><cell>Yellow sunflowers are in a blue</cell></row><row><cell></cell><cell></cell><cell>and white giraffe styled vase. Is</cell><cell>and white giraffe styled vase. Is</cell></row><row><cell></cell><cell></cell><cell>it inside? Yes.</cell><cell>it inside? No.</cell></row><row><cell></cell><cell></cell><cell>An adult giraffe and a child</cell><cell>An adult giraffe and a child</cell></row><row><cell></cell><cell></cell><cell>giraffe standing near a fence.</cell><cell>giraffe standing near a fence.</cell></row><row><cell></cell><cell></cell><cell>Does this look like zoo? Yes.</cell><cell>Does this look like zoo? No.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Randomly selected data examples for coreference.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We release our dataset containing all annotators' votes (Prabhakaran et al., 2021) at https://github.com/ Heidelberg-NLP/VALSE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We take the original answer in Visual7W as the example class: e.g., in 'There are 0 animals shown', the class is 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">VisDial annotations are not available for the test set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">We use the bert-large-cased pretrained tokenizer distributed as part of the transformers python library.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">NP chunking is performed using the Spacy v.3 pipeline for English using the en_core_web_md pretrained models. Coreference chains are detected using the pretrained English model for Coreferee (github.com/msg-systems/ coreferee). Pluralisation of head nouns is carried out using the inflect engine (github.com/jaraco/ inflect/).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">We useSpanBERT with the pretrained bert-large-cased model distributed as part of the transformers Python library.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">SimpleNLG is a surface realization engine that -given some content and crucial syntactic specifications -performs surface generation including morphological adjustments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">We use the GRUEN metric (Zhu and Bhat, 2020) that scores grammaticality, naturalness and coherence of generations and compute perplexity with GPT-2 to rank alternative outputs. We determined appropriate thresholds based on manual judgements of acceptability and chose the highest-ranked candidates. The final data quality is controlled by crowdsourced annotation with AMT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">The majority of manual annotators validated that the caption describes the image but the foil does not.16  Even more, the possibilities of exchanging pronouns with pronouns in grammatical ways are very limited: she -he but not she -they / her / their.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">If the answer is longer than just yes/no (e.g., 'Yes, she is') we shorten it to yes/no.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19">github.com/openai/CLIP 20 github.com/huggingface/transformers 21 github.com/facebookresearch/ vilbert-multi-task</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22">https://dl.fbaipublicfiles.com/ vilbert-multi-task/pretrained_model.bin 23 https://dl.fbaipublicfiles.com/ vilbert-multi-task/multi_task_model.bin tasks: (i) masked language modelling, and (ii) sentence-image prediction. The latter is framed as an extension of the next sentence prediction task used with BERT. Inputs consist of an image and a caption, with a second caption which has a 50% probability of being random. The goal is to determine if the second caption is also aligned to the image. In our experiments, we use the publicly available implementation of VisualBERT 24 .GPT-1 and GPT-2 -Unimodal models GPT1 (Radford et al., 2018) and GPT2 (Radford et al.,24  github.com/uclanlp/visualbert</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>to accommodate the input specifications of current pretrained V&amp;L models.</p><p>Foiling method Foiling is done in two directions: singular-to-plural (sg2pl) or plural-to-singular (pl2sg). Given a caption, NP chunking is applied to identify all non-pronominal NPs. In the sg2pl case, a foiled version of a caption containing a singular NP is created by pluralising the head noun. We automatically identify anaphoric expressions coreferring to the singular NP within the caption and pluralise them in the same way. For NPs which are subjects of copular VPs or VPs with an auxiliary requiring subject-verb number agreement (e.g. 'N is V'), we also pluralise the verb. Note that this procedure creates a potential foil for every singular NP in the caption; thus, more than one foil candidate can be created for each instance in the source dataset. 10 In the pl2sg case, the same procedure is carried out, but turning a plural NP, as well as its coreferents, into a singular. We generate all foil candidates using the Checklist framework <ref type="bibr">(Ribeiro et al., 2020)</ref>, within which we implement our procedures for data perturbation.</p><p>An important consideration, especially in the pl2sg case, is that singularising an NP in a foil can still be truth-preserving. Specifically, a caption with a plural NP, such as 'A small copper vase with some flowers in it', arguably still entails the version with the singular '(. . . ) a flower'. As a result, the singular version may still correctly be judged to match the image. One way around this problem is to insert a quantifier in the singular NP which makes it explicit that exactly one instance and no more is intended (e.g. 'exactly one flower'). This may however result in a biased dataset, with such singular quantifiers acting as signals for singular foils and enabling models to solve the task with no grounding in the visual information. We avoid this by adopting a uniform strategy for both sg2pl and pl2sg. We determine two singular quantifiers ('exactly one N' and 'a single N') and two plural quantifiers ('some N', 'a number of N'). When a foil candidate is generated, we alter the original NP by inserting one of the two quantifiers matching its semantic number, and generate a foil with one  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedika</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9690" to="9698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Words aren&apos;t enough, their order matters: On the robustness of grounding visual referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.586</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Linguistic issues behind visual question answering. Language and Linguistics Compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<idno type="DOI">10.1111/lnc3.12417</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic generation of contrast sets from scene graphs: Probing the compositional consistency of GQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="94" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Behind the scene: Revealing the secrets of pre-trained vision-andlanguage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07310</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll</surname></persName>
		</author>
		<title level="m">Microsoft COCO Captions : Data Collection and Evaluation Server. arXiv, 1504</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">00325</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visual referring expression recognition: What do systems actually learn?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="781" to="787" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Basmov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Gottumukkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<editor>Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou</editor>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
