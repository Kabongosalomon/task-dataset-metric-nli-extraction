<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
						</author>
						<title level="a" type="main">TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-Computer vision, Activity recognition !</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a novel visual representation learning which relies on a handful of adaptively learned tokens, and which is applicable to both image and video understanding tasks. Instead of relying on hand-designed splitting strategies to obtain visual tokens and processing a large number of densely sampled patches for attention, our approach learns to mine important tokens in visual data. This results in efficiently and effectively finding a few important visual tokens and enables modeling of pairwise attention between such tokens, over a longer temporal horizon for videos, or the spatial content in images. Our experiments demonstrate strong performance on several challenging benchmarks for both image and video recognition tasks. Importantly, due to our tokens being adaptive, we accomplish competitive results at significantly reduced compute amount. We obtain comparable results to the state-of-the-arts on ImageNet while being computationally more efficient. We also confirm the effectiveness of the approach on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD. The code is available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Images and videos provide an abundance of visual information. Image understanding is a long standing problem in computer vision, and despite incredible advances, obtaining the best visual representation for a variety of image understanding tasks is still an active area of research. Videos, in addition to addressing a similar image understanding task, require employing effective spatial-temporal processing of both RGB and time streams to capture long-range interactions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. An important aspect of this understanding is how to quickly learn which parts of the input video stream are important, both spatially and temporally, and to focus computational resources on them. But what basic processing mechanism are able to do so successfully for both images and videos?</p><p>Transformer models using multi-head self-attention <ref type="bibr" target="#b10">[11]</ref> have been very successful in both image and video recognition. Extending its original usage for text, the Vision Transformers (ViT) <ref type="bibr" target="#b11">[12]</ref> takes advantage of Transformers by treating an image as a sequence of patch tokens (e.g., 16x16 patches). At every layer, a ViT model recombines and processes patch tokens based on pairwise relations between the tokens, constructing a global representation of the entire image. The effectiveness of Transformers has also been shown in many computer vision tasks such as object detection <ref type="bibr" target="#b12">[13]</ref> and video classification <ref type="bibr" target="#b13">[14]</ref>.</p><p>The main challenge in many Vision Transformer architectures is that they often require too many tokens to obtain reasonable results. Even with 16x16 patch tokenization, for instance, a single 512x512 image corresponds to 1024 tokens. In the case of videos with multiple frames, this results in tens ? M. <ref type="bibr">Ryoo</ref> of thousands of tokens as each video 'tubelets' (e.g., 16x16x2 video segments) becomes a token. Further, such large number of tokens need to be processed at every layer, as the outputs from the previous layer become the tokens for the next layer. Considering that the Transformer computation (and memory) increases quadratically with the number of tokens, this can often make Transformers intractable for larger images and longer videos. This leads to the question: is it really necessary to process that many tokens at every layer?</p><p>In this paper, we show that adaptively generating a smaller number of tokens, rather than always relying on tokens formed by uniform splitting, enables Vision Transformers to run much faster and perform better. TokenLearner is a learnable module that takes an image-like tensor (i.e., input) and generates a small set of tokens. This module could be placed at various different locations within the model of interest, significantly reducing the number of tokens to be handled in all subsequent layers. The experiments demonstrate that having TokenLearner saves memory and computation by half or more without damaging classification performance. Furthermore, because of its ability to adapt to inputs, it often is capable of increasing the recognition accuracy while relying on less amount of computation.</p><p>We formulate TokenLearner using a straightforward spatial attention mechanism. The idea is to learn to adaptively compute important regions in the input image/video, and generate tokens out of such regions. We compute spatial attention maps highlighting regions-of-importance (using convolutional layers or MLPs), and they are applied to the input itself to weight each region differently (and discard unnecessary regions). The results are spatially pooled to generate the final learned tokens. This is in contrast to previous approaches which densely sampled tokens e.g., 16x16 or 32x32 for either images or videos <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>In our study, we find that very few tokens may be sufficient for a visual understanding task. More specifically, for images we show that one can significantly reduce the computational budget of the Vision Transformer, when learning 8-16 tokens as an intermediate representation (instead of keeping 200?500). We experimentally confirm that TokenLearner is able to reduce the number of total FLOPS, while maintaining or even increasing the classification accuracy. Similarly, for video recognition we show improved performance over the state-of-the art on three challenging datasets while only using 8-16 tokens per frame.</p><p>The approach is simple, efficient, and, as shown by the results, outperforms methods including both convolutional methods and previous space-time Transformer baselines without TokenLearner. We demonstrate that our models with TokenLearner performs comparably to previous Transformer models on ImageNet (and ImageNet ReaL) while meaningfully reducing the computation. In video understanding tasks, TokenLearner established new state-of-the-art numbers on multiple challenging video datasets.</p><p>This paper extends an earlier version <ref type="bibr" target="#b15">[16]</ref> published at a conference, by generalizing the TokenLearner for both image and video representation learning. Unlike the conference version which only focused on videos, in this manuscript, we add an extensive amount of new experiments confirming the benefits of TokenLearner on both image and video classifications. It also includes various detailed ablation experiments with further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TOKENLEARNER MODULES FOR ADAPTIVE TO-KENIZATION</head><p>In vision transformer architectures such as ViT <ref type="bibr" target="#b11">[12]</ref>, an input image is first tokenized by splitting it into small (e.g., 16x16) spatial patches, which are used as input to the model. Similarly, in recent video transformer architectures, such as ViViT <ref type="bibr" target="#b13">[14]</ref> and TimeSformer <ref type="bibr" target="#b14">[15]</ref>, the video is tokenized by cutting the video into 2d spatial or 3d spatio-temporal cubes on a regular grid.</p><p>Instead of processing fixed, tokenized inputs, our attention module learns the tokens that are to be used for the recognition task. We gain several important properties by doing so: (1) We enable the adaptive tokenization so that the tokens can be dynamically selected conditioned on the input. (2) This also effectively reduces the total number of tokens for the transformer, which is particularly beneficial considering that there are many tokens in videos and the computation is quadratic to the number of tokens. 3) Finally, we provide an ability for each subsequent layer to learn to rely on different space-time tokenizations, potentially allowing different layers to capture different aspects of the video. These dynamically and adaptively generated tokens can be used in standard transformer architectures such as ViT for images and ViViT for videos, or can be used within the specialized video architecture which we discuss further in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TokenLearner</head><p>Let X be an input tensor with a space-time shape: X ? R T ?H?W ?C where H ? W corresponds to the spatial dimension of the input, T is the temporal dimension (i.e., number of frames), and C is the number of channels. Let X t be a temporal slice of it, corresponding to the frame t: X t ? R H?W ?C . In the case of an image input, T = 1 and X = X t . Note that X could also be an intermediate representation within a network, and X t will be its slice in such case.</p><p>For every time frame t, we learn to generate a series of S tokens, Z t = [z i ] S i=1 , from the input frame X t . Specifically, we formulate a tokenizer function, z i = A i (X t ), which maps the input frame X t to a token vector z i : R H?W ?C ? R C . The idea is to learn our tokenizer function A i to adaptively select an informative combination of pixels (or spatial locations) in X t , and we have S number of such functions. This way, our tokens will not be fixed splits of the input tensor, but a set of adaptively changing spatial selections. Different tokens will be mined per frame, allowing us to model their space-time relations/interactions in case of videos. We also set S to be smaller than H ? W (e.g., S = 8 and H ? W = 32 ? 32), enabling the model to significantly reduce the computations needed for the layers following this module.</p><p>Here, our tokenizer z i = A i (X t ) is implemented with a spatial attention mechanism: i.e., the model learns to compute a weight map (of size H ? W ) conditioned on the input X t , and is multiplied with X t itself. More specifically, let ? i (X t ) be a function generating the spatial H ? W ? 1 weight map. Each token z i is generated by</p><formula xml:id="formula_0">z i = A i (X t ) = ?(X t A iw ) = ?(X t ?(? i (X t ))), (1)</formula><p>where is the Hadamard product (i.e., element-wise multiplication) and A iw ? R H?W ?C is an intermediate weight tensor computed with the function ? i (X t ) and the broadcasting function ?(?). Finally, spatial global average pooling ?(?) is applied on top of them to reduce the dimensionality to R C . The resulting tokens are gathered to form the output tensor:</p><formula xml:id="formula_1">Z t = [z i ] S i=1 ? R S?C .</formula><p>The overall process has a form of an element-wise spatial self-attention. In our initial version, {? i (?)} S i=1 are implemented together as a single or a series of convolutional layers (with the channel size S) followed by a sigmoid function. In our version 1.1, it is implemented with a single MLP layer (i.e., two dense layers with gelu in between). In case of an image, Z = Z t . In the case of a video, the tokens Z t from all the frames are collected to form the final output token tensor Z ? R ST ?C .</p><p>We specifically name our token learning module as "To-kenLeaner". <ref type="figure" target="#fig_0">Figure 1</ref> visually summarizes the TokenLearner module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Compute reduction in Transformers:</head><p>The learned tokens (i.e., the outputs of the TokenLearner Z) are provided to the subsequent layers for the visual representation learning, such as multi-head self-attention (MHSA) used in Vision Transformer and ViViT. With the TokenLearner, these subsequent layers only need to process a small number of tokens (e.g., 8 instead of 1024) and this significantly reduces the computations, as they are quadratic to the number of tokens. <ref type="figure" target="#fig_2">Figure 3</ref> (a) shows a basic architecture inserting the TokenLearner module within ViT. It could be added at any location within the network, and the relative compute of the Transformer layers after the TokenLearner become almost negligible due to the huge difference in the number of tokens. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input tensor</head><p>... <ref type="figure" target="#fig_0">Fig. 1</ref>: Visual illustration of the TokenLearner module, applied to a single image. TokenLearner learns to spatially attend over a subset of tensor pixels (i.e., from intermediate spatial representations), and generates a set of token vectors adaptive to the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned tokens</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TokenFuser</head><p>After the TokenLearner generates tokens and its subsequent Transformer layer (e.g., MHSA) processes them, the "To-kenFuser" could be used to further (1) fuse information across the tokens and (2) remap the representation back to its original spatial resolution. This enables the model to capture spatial (or spatio-temporal) 'patterns' formulated by the tokens, and recover the original input tensor shape when necessary.</p><p>First, given the token tensor Y ? R ST ?C from a Transformer layer, we apply a linear layer (i.e., a fully connected MLP layer) over the tokens, not channels. That is, we learn a linear function of R ST ? R ST where S is the number of our tokens mined per frame and T is temporal size of the input tensor, and apply it to every channel independently. That is, we update Y = (Y T M ) T where M is a learnable weight matrix with size ST ? ST . The result of such operation maintains the tensor size of ST ? C. We believe this also has a connection to the observations from the concurrent work, MLPMixer <ref type="bibr" target="#b16">[17]</ref>, that it is beneficial to have token-wise linear layers capturing patterns formed by tokens.</p><p>Next, the TokenFuser processes each temporal slice Y t ? R S?C individually, and remaps the token tensor of size S ?C back to H ? W ? C, by learning to combine the tokens for each spatial location in H ? W differently.</p><formula xml:id="formula_2">X j+1 t = B(Y t , X j t ) = B w Y t + X j t = ? i (X j t )Y t + X j t<label>(2)</label></formula><p>where X j t is the residual input to the previous TokenLearner module, Y t is the processed tokens in the TokenFuser module, and X j+1 t is the output. B w ? R HW ?S is an intermediate weight tensor computed with the function ? i (X t ). The function ? i (X t ) is implemented with a simple linear layer followed by a sigmoid function. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the overall process of the TokenFuser (the token-wise linear layer is omitted). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS WITH IMAGES</head><p>In order to validate the power of the TokenLearner module, we first try TokenLearner on image representation learning. We evaluate two different architectures: (a) simply inserting the TokenLearner within standard transformer models, and (b) using the TokenFuser in addition to the TokenLearner at multiple locations within the transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network architecture implementation</head><p>We use the Vision Transformer architecture <ref type="bibr" target="#b11">[12]</ref>, following its detailed settings and implementation <ref type="bibr" target="#b17">[18]</ref>. We use ViT-B/16 and ViT-L/16 as our backbone, while also applying the TokenLearner to ViT-B/32 (the same model but with an initial patch size of 32x32 in the beginning), ViT-S/32 (smaller version with 384 channels), and more. The ViT-S and ViT-B backbone models have 12 transformer layers, while ViT-L has 24. Following the exact setting of <ref type="bibr" target="#b11">[12]</ref>, we used the input resolution of 224x224, 384x384, or 512x512 depending on the dataset and the model (i.e., 196, 576, or 1024 tokens). Positional encodings identical to ViT are used. <ref type="figure" target="#fig_2">Figure 3</ref> (a) and (b) show two different architectures incorporating TokenLearner. (a) is formed by inserting TokenLearner in the middle of the network such as after the 6th transformer among 12, while (b) uses both TokenLearner and TokenFuser. In particular, our model (b) is formed by replacing conventional Transformer layers with a series of TokenLearner-Transformer-TokenFuser. Similar to (a), such replacement is done only for the layers after a certain layer. For instance, we keep six of the standard Transformer MHSA layers in the beginning, and replaces the remaining six layers with our TokenLearner-Transformer-TokenFuser modules repeated six times. We also modified some of our models to have more transformer layers (e.g., 21 instead of 12), and we specify this when we do so. Note that the computation increase caused by the transformer layers added after TokenLearner module is very small, as the number of tokens in these layers are few: 8 or 16.</p><p>We tried various number of tokens including S = 8, 16, 32, and use S = 8 and 16 as our default settings. That is, the TokenLearner is learning to abstract an image into 8 (or 16) tokens. The spatial attention function (?) in TokenLearner is implemented with four 3x3 conv. layers (with gelu in between), whose channel size is identical to the number of tokens (e.g., S = 8).</p><p>We adopt the training settings (e.g., learning rate, training epochs, etc.) of <ref type="bibr" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image classification datasets</head><p>ImageNet: We use the popular image benchmark, Ima-geNet <ref type="bibr" target="#b18">[19]</ref>. For our experiments, we use the standard ImageNet version which has 1000 categories and 1.1M images. We use the image resolution of 384x384 for S/16 and B/16 models, and use 512x512 for L/16 models. ImageNet ReaL <ref type="bibr" target="#b19">[20]</ref>, which is the dataset with Re-Assessed (ReaL) labels for ImageNet, was also used for the evaluation. JFT-300M. The JFT-300M dataset is an internal dataset collected for training image classification models, which was first introduced by <ref type="bibr" target="#b20">[21]</ref>. Images are harvested from the web and are filtered to maximize label precision. It contains 300M images and has been shown to be suitable for learning highcapacity models, such as transformers.</p><p>In this work, we use the JFT-300M dataset only for pre-training purposes, following the evaluation protocol, previously established for ViT <ref type="bibr" target="#b11">[12]</ref>. We use the image resolution of 224x224 for this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation: where should we have TokenLearner?</head><p>We first conducted an ablation to decide the best location to place the TokenLearner within the model. <ref type="figure">Figure 4</ref> shows the results. The experiment was done with our model (a), without TokenFuser. It is showing the few-shot classification accuracies on ImageNet with JFT pre-training, following the protocol of ViT <ref type="bibr" target="#b11">[12]</ref>. In addition, we show how the computation amount (FLOPS) changes per TokenLearner location. Basically, due to the large difference between the number of tokens with and without the TokenLearner (e.g., 8 with TokenLearner vs. 196 without), the computation of the transformers after the TokenLearner module becomes almost negligible compared to the transformers before the TokenLearner location. We found that inserting TokenLearner in the middle of the network (at 1/2) achieves almost identical accuracies, while cutting the computation by (almost) half. In addition, having the TokenLearner at the later layer (after 3/4 of the network) achieves even superior performance compared to not using the TokenLearner while performing faster, thanks to its adaptiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Following the protocol established in ViT <ref type="bibr" target="#b11">[12]</ref>, we evaluated the models with and without TokenLearner in terms of (i) fine-tuning accuracies and (ii) few-shot accuracies. For the fine-tuning accuracies, we pre-train the model with JFT and fine-tune it with the original ImageNet dataset using an image resolution of 384x384 (for ViT-S and ViT-B models) or 512x512 (for ViT-L models) as done in previous works. For the few-shot accuracies, we also follow the protocol of ViT <ref type="bibr" target="#b11">[12]</ref> where we do a regularized least-squares regression that maps the (frozen) representation of a subset of training images to {?1, 1} K target vectors. We report 5-shot ImageNet Top-1 accuracy, as well as the 5-shot accuracies averaged over multiple datasets: Caltech101, Caltech-UCSD Birds 2011, Cars196, CIFAR100, colorectal_histology, DTD, ImageNet, Oxford-IIIT Pet, and UC Merced Land Use Dataset. Few-shot accuracies are particularly interesting as it shows the generalization capability of the representation itself being learned. We note that 5-shot accuracy was also used to evaluate the representations learned by ViT <ref type="bibr" target="#b11">[12]</ref>. In the experiments in this subsection, we use the model (a) from <ref type="figure">Figure 4</ref> which is without TokenFuser. We inserted the TokenLearner module exactly at the mid-point of the network unless specified otherwise. The default number of tokens was 8 and we also use 16, as they showed the best accuracy-speed balance. <ref type="figure">Figure 5</ref> and <ref type="table" target="#tab_1">Table 1</ref> shows the ImageNet fine-tuning evaluation results, using smaller ViT-S and ViT-B models. We show accuracies of various versions of ViT and their TokenLearner versions, as specified in Section 3.1. We are able to observe that there is a substantial improvement in efficiency-accuracy trade-offs.</p><p>When directly applied to ViT (e.g., B/16), TokenLearner maintains the accuracy of ViT, while reducing the computation by almost a half. When more layers are used together with the TokenLearner, we are able to utilize the computation saved by the TokenLearner in the form of additional layers, and it performs superior while still having less FLOPS. The number of tokens the baseline ViT B/16 processes is 576, while the TokenLearner learns S = 8 tokens. As a result, as mentioned in the above subsection, the computation of the transformers after the TokenLearner module becomes almost negligible compared to the transformers before the TokenLearner location. <ref type="figure">Figure 6</ref> shows few-shot experiment results. For these experiments, an image resolution of 224x224 is used following <ref type="bibr" target="#b11">[12]</ref>. The baseline ViT therefore uses 196 tokens (as opposed to 576 used in ImageNet fine-tuning experiments). This makes the gap between the number of tokens used in TokenLearner and ViT smaller (compared to fine-tuning setting), increasing TokenLearner's relative accuracy. It is interesting to observe that the accuracies of TokenLearner do </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">TokenLearner on larger models.</head><p>We also evaluated our TokenLearner inserted into a 'large' model: ViT-L. In addition to the standard L/16 model that splits the scene into 16x16 patches, the same model with finer tokens were used including L/14, L/10, and L/8. Note that the model size stays the same, with 464M parameters, and only the inputs change for these models. As discussed above, the number of tokens become 8 or 16 after the TokenLearner layer, regardless the model. For these large models, the image resolution of 512x12 was used for ImageNet. <ref type="table" target="#tab_2">Table 2</ref> shows the results. It specifies how many tokens were used for each model as well as the location of the TokenLearner layer. "16-TL at 12" means the number of tokens were 16, and TokenLearner was inserted after the 12th Transformer layer (i.e., in the middle). Some of the models were set to use additional layers (e.g., '+11'), but the increase in FLOPS were negligible due to the number of tokens being small after the TokenLearner layer.</p><p>We are able to observe that, similar to our experiments with ViT-S and ViT-B, TokenLearner is able to save the amount of computations by half when inserted in the middle. Further, it is able to do so without sacrificing the accuracy, due to the adaptiveness in the token generation. When the saved compute was used in other forms (e.g., use of L/14   <ref type="table" target="#tab_3">Table 3</ref> compares our models with TokenLearner against the larger ViT models from <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b21">[22]</ref>. Despite using much smaller number of parameters, our TokenLearner models perform comparably to the huge and giant ViT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Making larger models much more efficient</head><p>We also confirmed the strategy of using TokenLearner at much earlier in the network, such as after the 2nd or 3rd attention layer. This makes the overall model even more computationally efficient than the smaller base ViT model (B/16), while performing better. <ref type="table" target="#tab_4">Table 4</ref> shows the results. We are able to observe that, for instance, the L/16 model with TokenLearner at the 3th attention layer gets a superior accuracy than ViT B/16, while its run time is around half of B/16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablations and Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">TokenFuser</head><p>First, we compare the TokenLearner models with and without the TokenFuser module. More specifically, we compared the model (a) and the model (b) from <ref type="figure">Figure 4</ref>, to confirm the effectiveness of the TokenFuser. <ref type="table" target="#tab_5">Table 5</ref> shows the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">TokenLearner vs. pooling</head><p>A straightforward alternative to the TokenLearner module is the use of spatial pooling to reduce the number of tokens. It can be done by spatially rearranging the tokens to have the  height and width, and then applying conventional spatial pooling. This is similar to the pooling-based MHSA module used in <ref type="bibr" target="#b22">[23]</ref>. <ref type="table" target="#tab_7">Table 6</ref> compares the TokenLearner against the spatial pooling. In all these experiments, ViT L/16 model was used. We are able to observe that there is a benefit in token 'learning'. The pooling-based token reduction does have computation similar to the TokenLearner, but it loses its accuracy compared to the base model. On the other hand, TokenLearner performs a bit better than the base model despite the low computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">TokenFuser alternatives</head><p>Here, we experimentally compare the proposed TokenFuser module with its alternatives. The role of the TokenFuser is to mix the output tokens from the Transformer layer and map it back to the original shape before the token reduction.</p><p>The most straightforward alternative would be to (1) use the masks from the TokenLearner module to 'unpool' the output tokens. The idea is to multiply each output token with the corresponding spatial map computed during the previous TokenLearner module, and sum all of them to recover the original input tensor shape. Alternatively, (2) we can use one more transformer layer to increase the number of tokens back to the original number of tokens, similar to the 're-projection' used in <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_5">Figure 7</ref> shows the results with B/16. The unpooling strategy performed worse. The reprojection strategy performed comparably to the TokenFuser, but required more FLOPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TOKENLEARNER FOR VIDEOS</head><p>In this section, we illustrate how TokenLearner works for video representations. The TokenLearner and TokenFuser modules introduced in Section 2 are directly applicable for video representation learning. The only difference between the TokenLearner used for images and it used for videos is that TokenLearner generates multiple Z t for videos and they need to be stacked to form Z. Once Z is generated, any  <ref type="figure">Figure 4</ref> (a). The model with TokenFuser uses the architecture of <ref type="figure">Figure 4 (b</ref>  TokenLearner first learns to generate a set of token vectors, the Transformer (e.g., MHSA) models their space-time relations, and TokenFuser combines them. S is the number of tokens we learn per frame, and T is the number of frames. C is the number of channels, which we made to be identical across the modules for efficiency. Note that this combination can serve as a 'module' itself, and one may stack such module multiple times within the network. TokenFuser could be dropped depending on the architecture. standard Transformer layers could be used to parse them jointly. <ref type="figure">Figure 8</ref> provides an overview of the combined framework for videos. TokenLearner first extracts S number of tokens per frame, resulting in a total of ST tokens where T is the number of frames. Once TokenLearner generates these adaptively learned tokens, they are provided to the subsequent Transformer layer to capture the global spacetime patterns. Finally (and optionally depending on the architecture), TokenFuser applies a linear layer over the token axis and then remaps the tensor shape back, as discussed in Section 2.2. Following Eq. 2, TokenFuser is applied for per-frame representation Y t . This results in a lightweight approach, which brings forth an efficient video representation by capturing long-range visual patterns.</p><p>What we show for the video representation learning is a combination of TokenLearner, Transformer, and TokenFuser modules repeated multiple times, as described in <ref type="figure" target="#fig_2">Figure 3</ref>    <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>ViViT has been one of the state-of-the-art approaches for the Kinetics datasets <ref type="bibr" target="#b0">[1]</ref>, and the idea is to confirm whether TokenLearner could directly be added to such general video representation models and outperform them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets and training</head><p>For the experiments in this section, we use the Kinetics datasets, which are video classification datasets with relatively short video clips (?10 seconds). We train and evaluate on both Kinetics-400 and Kinetics-600 datasets, which have about 240k and 390k training samples. We follow the standard settings used in previous papers and report accuracy on the validation set <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Following ViViT <ref type="bibr" target="#b13">[14]</ref>, we first pretrain models on JFT to obtain initial weights. We directly use the models from Section 3. The weights of the initial convolutional layers to handle image patches (e.g., 16x16) are processed to handle 16x16x2 video patches by following the 3D initialization strategies of ViViT, and the weights of the Transformer and the TokenLearner layers were directly adopted for the initialization. Next, the model was finetuned on the Kinetics data.</p><p>Similar to ViViT, we train the model for 30 epochs with the base learning rate of 0.05 with the Momentum optimizer. Basically, all the settings in our Kinetics experiments follow the setting of ViViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We evaluate various versions of the ViT-L models incorporating the TokenLearner module. As mentioned above, all of the models are pre-trained on JFT and finetuned on Kinetics. In addition to the standard L/16 models + TokenLearner, we use the L/14 and L/10 models introduced in <ref type="table" target="#tab_2">Tables 2 and 3</ref>. These models use additional layers compared to the standard ViT L/16, but as also described in the previous sections, the computation increase caused by them are minimal due to the number of tokens being much smaller, 8 or 16, in the added layers. We report both their classification accuracies and the computation in FLOPS. <ref type="table" target="#tab_8">Table 7</ref> compares the accuracies of the base ViViT models against our ViViT + TokenLearner models on Kinetics-400. These models are directly comparable as they follow the exact same setting and the pre-train dataset. "TokenLearner 16at12" means that we have the TokenLearner layer learning 16 tokens, after the 12th Transformer layer. We are able to observe that the use of TokenLearner enables a better classification accuracy while also reducing the compute. <ref type="table" target="#tab_9">Table 8</ref> compares the TokenLearner accuracy against the prior models. Note that these approaches follow slightly different settings and pretrain datasets (e.g., the use of ImageNet-21K instead of JFT like ours). We believe the accuracy of 85.4 is the highest that has been reported so far, and we believe it is meaningful. <ref type="table" target="#tab_10">Table 9</ref> compares the results on Kinetics-600. Similar to our results on Kinetics-400, when TokenLearner was first released, it extended the state-of-the-arts while also being computationally efficient. In this experiment, we follow the Bottleneck Transformer <ref type="bibr" target="#b27">[28]</ref> network style, while taking advantage of X3D <ref type="bibr" target="#b26">[27]</ref> as the backbone. This is motivated by the successful usage of X3D on Charades. Charades has longer duration videos (average of 30 seconds) with long actions (average action length of 15 seconds). This requires the model to understand longer term temporal information by considering multiple temporal tokens, and TokenLearner allows efficient computation of them over many frames. Specifically, we modified X3D to be more computationally efficient by (1) replacing its 3D XYT convolutional layers with a pair of 2D conv. layer and 1D conv. layer, and (2) removing Squeeze-and-Excitation layers <ref type="bibr" target="#b28">[29]</ref> and swish activations. Our backbone could be viewed as X(2+1)D. We use the channel sizes and the number of layers identical to X3D-M, which is an efficient model.</p><p>Based on such X(2+1)D architecture, and following the Bottleneck Transformer concept, we replace the space-time convolution layers in the last block with our transformers. <ref type="figure">Figure 9</ref> illustrates the residual module architecture, which is repeated multiple times in the block. We have tried different versions, and our final model is built by replacing 1D temporal convolution with our TokenLearner while keeping the 2D 3 ? 3 convolution layer in the X(2+1)D modules. The spatial attention function (i.e., ?(?)) in TokenLearner is implemented with a single conv2d layer.</p><p>Here, we used a Vector Transformer instead of MHSA as our Transformer layer, which could be also viewed as the MHSA with the number of heads being identical to the number of channels. We provide more details in Appendix.</p><p>We use 224?224?64 videos for training and 256?256? 64 videos for testing. After the 3rd residual block, the input tensor has the shape of 8 ? 8 ? 64, and this becomes the input to the TokenLearner. For an efficient implementation the intermediate channel size of TokenLearner was set identical to the output channel size, d = 432. Notice that 64 frames were used to best capture longer-term temporal information. S = 8 number of tokens were used.  <ref type="figure">Fig. 9</ref>: Our network module following the bottleneck transformer, with X(2+1)D backbone. It is an inverted bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Datasets</head><p>Charades dataset: The Charades dataset <ref type="bibr" target="#b29">[30]</ref> is a dataset collected by assigning activity tasks which people in various environments are acting out, for example by performing a sequence of actions which involve interaction with objects. For example, sitting on the couch and reading a book, closing the book, standing up and speaking on the phone. It comprises 8000 training and 1686 validation videos with an average duration of 30 seconds. It has 157 activity classes. This dataset is very challenging as it is a multi-class, multilabel video dataset, that is, more than one activity can occur at the same time, and it includes fine grained motions or interactions with small objects in real-world environments. We follow the standard evaluation protocols, reporting the mean Average Precision (mAP) % (v1 classification setting of the dataset). We used the frame rate of 6 fps and 12 fps to obtain the training/testing videos (12 fps worked better). The dataset has a Non-Commercial Use license.</p><p>AViD dataset: The Anonymized Videos from Diverse countries (AViD) dataset <ref type="bibr" target="#b30">[31]</ref> is a unique dataset which is representative of the world's population video content generation. It is collected from videos uploaded from multiple countries across six continents and demonstrates higher diversity compared to other video datasets such as Kinetics in its concepts, actions and visual representations. For example a 'greeting' in certain countries involves a handshake, in some a kiss, but in others a slight bow. The dataset is explicitly designed to contain less bias, encourage diversity, while respecting privacy and licenses. The AViD dataset contains 887 classes and 450k videos (410k training 40k testing) and is of comparable size to Kinetics-400 and Kinetics-600 datasets with 400 and 600 classes respectively, 10: Performance on the Charades multi-label classification task. 12 fps setting. Performance is measured using the Mean Average Precision (mAP) since more than one ground truth action is possible. Methods with RGB and optical flow input modalities are listed.  also containing variable duration videos 3 ? 15s. We report classification accuracy over the 887 classes. All the videos in this dataset have the Creative Commons License. <ref type="table" target="#tab_1">Table 10</ref> we compare the proposed TokenLearner to the state-of-the-art methods. Our approach outperforms these, including recent work of which is most aligned to ours. The mAP of 66.3% on Charades classification establishes the new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Charades dataset results: In</head><p>Anonymized Videos from Diverse countries (AViD) results: <ref type="table" target="#tab_1">Table 11</ref> shows the results on the AViD dataset. As seen, our approach outperforms prior work on this challenging dataset too. We also compared ours to the reimplementation of TimeSformer module <ref type="bibr" target="#b14">[15]</ref> applied to the same backbone as ours. This uses disjoint spatial and temporal transformer modules, which was also tested in <ref type="bibr" target="#b13">[14]</ref>. We are able to observe that we establish the new state-of-thearts on this dataset, while also being more computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Comparison against different tokenizations</head><p>Here, we compare the model with TokenLearner against different space-time transformer tokenization strategies. More specifically, we compare the use of TokenLearner + Vector Transformer + TokenFuser against the tokenization in the full joint space-time transformer module (advocated in <ref type="bibr" target="#b13">[14]</ref> and also mentioned in <ref type="bibr" target="#b14">[15]</ref>). The full joint space-time transformer module is a transformer layer on space-time tokens similar to ours, but it relies only on the hand-designed tokenization. Compared to TokenLearner which generates S ?T number of tokens, the full joint space-time transformer uses H ? W ? T number of tokens. In our bottleneck implementation, it uses ?8 times more tokens (i.e., 8*64 vs. 8*8*64). For the joint space-time transformer modules, the standard multi-head self-attention (MHSA) with 8 heads is used. <ref type="table" target="#tab_1">Table 12</ref> shows the results. Interestingly, despite the heavier computation of the full joint space-time transformer, it performed slightly worse to the TokenLearner modules. We believe this shows the advantage of the 'adaptiveness' of the tokens in the TokenLearner and shows that the standard transformers might be suffering from the tokens irrelevant to the actions serving as noise or distractors.</p><p>We also report the amount of computation and the number of parameters of each module in these models. This depends on the input size and the hyper parameter setting, and our measurement is based on the input size (i.e., T ? H ? W ? C) of 8 ? 8 ? 64 ? 492. Note that this is the measurement of modules, not the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Comparison between multiple space-time layer combinations</head><p>As also suggested in previous literature, it is a common strategy for video representations to pair a layer focusing on spatial information with a layer focusing on temporal information (e.g., R(2+1)D <ref type="bibr" target="#b24">[25]</ref> and TimeSformer <ref type="bibr" target="#b14">[15]</ref>). <ref type="table" target="#tab_1">Table 13</ref> shows the results of this ablation. For spatial and temporal transformer implementations, the standard multi-head selfattention was used, as was done in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The result shows that the proposed TokenLearner is more accurate than other popular combinations. The modules based on TokenLearner also effectively only uses a fraction of the Tokens per frame (i.e., 8) as opposed to other methods which use 16 ? 16 or 32 ? 32 tokens.</p><p>One of the main benefits of the TokenLearner (in addition to the adaptive tokenization of the input and that we explicitly fuse the tokens to capture their spatio-temporal patterns) is that, unlike the disjoint space/time transformers used in this ablation study, it is a joint space-time transformer. Simultaneously, it still manages its computation to be much more tractable (as shown in <ref type="table" target="#tab_1">Table 13</ref>): A naive full version of the space-time transformer would require consideration of 8 ? 8 ? 64 = 4096 tokens in our case, building and multiply the attention tensor of size 4096 ? 4096. On the other hand, the TokenLearner learns to consider 8 ? 64 = 512 tokens jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">More TokenLearner alternatives</head><p>We also compared our spatial attention-based token learning with alternative approaches: (1) using a fixed grid to split  <ref type="bibr" target="#b13">[14]</ref>, applied to our backbone. They use the X(2+1)D backbone, tested on Charades with the 6 fps setting, Charades 12 fps setting, and AViD dataset. GFLOPs and # params are of each module (with 64 frame inputs), not the entire network.  each frame into the same number of tokens (i.e., 8 tokens), (2) the approach of directly generating tokens using a fully connected layer, and (3) the approach of spatially average pooling the entire frame pixels and using fully connected layers to generate multiple tokens per frame. In the second approach, we directly model z i = A i (x) as a dense layer, producing T ? S ? C tensor based on the T ? H ? W ? C input. The third approach is similar, except that we apply spatial global average pooling per frame and then use MLP to generate tokens. The fixed split tokenization method (1) provided us the accuracy of 58.8 on Charades, as opposed to 59.6 of ours. The direct token generation method (2) provided the accuracy of 56.6 on Charades, failing to obtain better tokens. Pooling and generation method (3) gave us the accuracy of 58.6. These results suggest the importance of spatial attention for the token learning, our TokenLearner. The same vector transformer and TokenFuser (from Section 2) were used for this ablation. <ref type="figure" target="#fig_0">Figure 10</ref> shows visualizations of the tokens being learned with our approach. We show the spatial attention maps (i.e., ? i (x)) from the first TokenLearner module, as the inputs to the higher-level TokenLearner becomes more mixed spatially and temporally. We are able to observe that they tend to focus more on human regions, and that they change over time responding to the changes in the visual input. Among the S = 8 tokens per frame we learn, we visualize 4 of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualizations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Visual understanding is a long-standing problem in computer vision. Image recognition tasks including object classification and detection have been extensively studied in many different directions. Most of today's methods focus on learning to represent spatial appearance in images. It is a challenging task which spans many years of computer vision research <ref type="bibr" target="#b39">[40]</ref>.</p><p>Video understanding has an even more challenging task for extracting both the spatial and the temporal information in a video <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In order to adequately capture both motion and appearance information in videos, full 3D space-time convolutional layers as well as (2+1)D convolutional layers have been used <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b46">[47]</ref>. More advanced network designs have also been extremely popular in video CNNs particularly two-stream ones <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> and, recently, architecture searched ones <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>.</p><p>Attention-based architectures, e.g., the Transformer <ref type="bibr" target="#b10">[11]</ref> have shown remarkable success in both Natural Language processing and Computer Vision. The Vision Transformer <ref type="bibr" target="#b11">[12]</ref> demonstrated how the NLP-specific Transformer architecture can elegantly work for images, and image recognition tasks. This is done by subdividing the input image into non-overlapping patches on a regular grid and feeding them as token embeddings to the Trasnformer, where O(N 2 ) tokens are used or order of 256 or 1024. A plethora of approaches have followed this strategy <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, with some of the approaches proposing multi-scale visual transformer versions <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Some methods focus on optimizations of these models and layers, and they also have been successful <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>.</p><p>Applying attention-based architectures to videos presents a definite challenge as the model needs to learn dependencies across both the spatial and temporal domains. <ref type="bibr" target="#b61">[62]</ref> relied on the region proposal network to use the detected human and object candidates as tokens, showing that it could be combined with CNNs. A couple of recent works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, in the spirit of the Vision Transformer, subdivided the video into token in a 3D grid to capture the video input. This leads to O(N 3 ) increase in the number of tokens required for learning (typically ? 25k tokens for 96-frame model). Attention-based architectures have also been used in the context of video generation <ref type="bibr" target="#b62">[63]</ref>. Several architectures have demonstrated attention-based architectures for handling multiple inputs of various modalities <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>.</p><p>Our work, in contrast to both related work in image and video recognition, learns the tokens from data which results in a significantly fewer tokens, and more efficient approach. We see that even 8x times fewer tokens (e.g., 512 vs 4096), when learned, are able to capture successfully the information needed for video representation learning. Importantly, our proposed TokenLearner, is applicable to both video and image recognition tasks achieving better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>We have presented TokenLearner, a novel approach for visual representation learning, which adaptively tokenizes the inputs. The goal is to learn to extract important tokens in images and video frames for the recognition tasks at hand. Our approach is more efficient, than contemporary work, by finding few important space-time tokens which can model visual representations of images and videos. We observe improved accuracies across image classification and challenging video understanding tasks, and outperformed prior approaches in many datasets. One of the remaining challenges is in learning full spatio-temporal tokens. The current TokenLearner focuses on finding spatial tokens over a sequence of frames, and it could be extended to mine tokens over space-time volumes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank Dmitry Kalashnikov, Andy Zeng, and Robotics at Google NYC team members for valuable discussions on attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A VECTOR TRANSFORMER: PAIRWISE VECTOR ATTEN-TION</head><p>Here, we summarize the details of the Vector Transformer used in the Bottleneck Transformer experiments.</p><p>Once TokenLearner generates adaptively learned tokens, a vector attention between key-query pairs could be computed. This can be thought as a version of multi-head selfattention in which the number of heads is the same as channels, allowing us to learn a different attention matrix for each channel. It captures in an efficient way pairwise spacetime relations per channel, particularly benefiting tokens with rich channel information.</p><p>Given Z, a set of tokens reflecting different space-time aspects of a video, the Transformer models space-time interactions between them. In particular, we follow the formulation of <ref type="bibr" target="#b58">[59]</ref>, which enables a vector-version of the Transformer, although it is also possible to incorporate other attention mechanisms.</p><p>For every token z i , the output of the Transformer y i is computed by considering all possible z j as:</p><formula xml:id="formula_3">y i = zj ?Z ?(f q (z i ) f k (z j )) f v (z j )<label>(3)</label></formula><p>where i and j are the indexes of the tokens in Z whose size is |Z| = ST . f q , f k , and f v are the linear layers projecting the tokens. ? is an extra projection layer to match the channel dimensionality followed by a softmax function over j. When the channel sizes of the projections are identical, ? is simplified as a single softmax layer identical to the standard transformer. In the original transformer notation, the query matrix Q corresponds to our {f q (z i )} i , and the key matrix K corresponds to our {f k (z j )} j . Instead of computing the dot product between Q and K as QK T to generate the attention 'matrix', this vector formulation computes an attention 'tensor' {?(f q (z i ) f k (z j ))} (i,j) preserving the channel information. It has shape ST ? ST ? d where d is the intermediate channel size. The computed attention tensor is multiplied with the value matrix {f v (z j )} j to get the final transformer outputs.</p><p>Notice that this vector transformer is a global representation, and the temporal range of the information it is able to capture entirely depends on what tokens we provide to it. With our learnable adaptive tokens, we have the capability to cover a larger number of frames and focus on the temporal structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B IMAGE AND VIDEO CLASSIFICATION TRAINING DE-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Image classification</head><p>We follow the exact training protocols and the hyper parameters of <ref type="bibr" target="#b11">[12]</ref> for our image classification experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Video classification -ViViT</head><p>We follow the exact training protocols and the hyper parameters of <ref type="bibr" target="#b13">[14]</ref>. We use the same code (the Scenic library <ref type="bibr" target="#b17">[18]</ref>) and the hardware for the training as well as for the evaluation.</p><p>We train the Kinetics model for 30 epochs with the base learning rate of 0.05 with the Momentum optimizer. Basically, all the settings in our Kinetics experiments follow the setting of ViViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Video classification -Bottleneck Transformer</head><p>We provide the training details as below. For the training/testing splits of the datasets, we followed their standard settings.</p><p>We use the cosine-decay learning rate which was popularly used in many video CNN model trainings. The base learning rate of 0.8 per TPU core (which is equivalent to a single GPU) is used for the Charades dataset (multi-label action classification) and the base rate of 0.025 per TPU was used for the AViD dataset (video classification). The training was done for 100k iterations with the batch size of 4 per TPU core (i.e., 4*64=256 was our batch size) in the Charades experiments. The batch size of 8 per TPU core was used for AViD. 100k iterations correspond to roughly 125 epoches in AViD. Label smoothing of 0.2 was used for the AViD training. No label smoothing was used for the Charades. In Charades, the training was done by temporally cropping a long Charades videos (e.g., 30 seconds) into 64 frame segments. The evaluation was done similarly with 64 frame segments by merging their output responses.</p><p>The training time of a single model was around ?16 hours with 32 TPU v3. This was bottlenecked by the data pipeline, and the actual computation is less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C COMPARING MORE MODELS ON FEW-SHOT LEARN-</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ING</head><p>Here, we report few-shot learning accuracy of more models with TokenLearner, extending <ref type="figure">Figure 6</ref>. Specifically, we additionally show L/16 (and L/14) models with TokenLearner  at various locations, including inserting it at the 2nd, 3rd, 6th, and 12th attention layers. We are able to observe the strategy of adding TokenLearner early in the network allows us to save the computation while obtain superior image classification accuracy to the base models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D ADDITIONAL ABLATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Different components</head><p>Using the setting of the Bottleneck Transformer experiments, we did an ablation to evaluate components of our approach and their combinations. We conducted ablations removing/adding different components of our model. In addition to Vector Transformer described in the above subsection, we also tried an ablation of replacing it with the multi-head self-attention. <ref type="table" target="#tab_1">Table 14</ref> shows the results, demonstrating the benefits each of the elements bring to the approach. For this experiment, we used the module composed of Conv2D + transformer (within the bottleneck), which we found to perform the best from the other ablations. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>HxWxC ? 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visual illustration of the TokenFuser module, applied to each image frame individually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Our models following the ViT architecture. (a) with TokenLearner and (b) with both TokenLearner and Token-Fuser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>ImageNet 5-shot accuracy (left) and FLOPS (right) per different TokenLearner location within the model. '0' means that the TokenLearner is at the very beginning of the model (before any transformer), '0.5' means the middle of the model, and 'Base' means that there is no token learning. Visualization of ImageNet fine-tuning accuracies of the baseline ViT models vs. TokenLearner. X-axis is GFLOPs, which measures the amount of computation required.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>32 Fig. 6 :</head><label>326</label><figDesc>shot linear top-1 accuracy TL-B/16 (21) ViT-B/16 TL-B/16 TL-B/32 (20) ViT-B/32 TL-S/32 (22) shot linear top-1 accuracy TL-B/16 (21) ViT-B/16 TL-B/16 TL-B/32 (20) ViT-B/32 TL-S/32 (22) ViT-S/32 TL-S/Few-shot classification experiments. It shows 5-shot classification accuracies on ImageNet (left) and average of multiple datasets listed in Sec. 3.4 (right). 'TL' stands for TokenLearner. Check Appendix for results with more models. not drop (e.g., TokenLearner-B/16 vs. ViT-B/16), despite the difference in the number of tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Ablations with TokenFuser alternatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>? 2 Fig. 8 :</head><label>28</label><figDesc>An illustration of TokenLearner, Transformer, and TokenFuser combined for video representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(b). The TokenFuser part is dropped if we are using the model architecture (a), and only the Transformer layers are repeated multiple times after the TokenLearner module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Visualization of the spatial attention maps for the tokenizations. Attention maps for four among a total of eight learned tokens are shown. results in both domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Few-shot classification experiments. It shows 5-shot classification accuracies on ImageNet (left) and average of multiple datasets listed in Sec.3.4 (right). 'TL' stands for TokenLearner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova are with Google Research.</figDesc><table /><note>E-mail: {mryoo, ajpiergi, aarnab, dehghani, anelia}@google.com? M. Ryoo is also with Stony Brook University. Manuscript received Feb 15, 2022.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>ImageNet fine-tuning Top1 accuracies and FLOPS. The numbers in the parenthesis are the number of transformer layers. 16-TokenLearner is with 16 tokens instead of 8.</figDesc><table><row><cell>Method</cell><cell cols="2">GFLOPS Accuracy</cell></row><row><cell>ViT S/32</cell><cell>3.4</cell><cell>77.87</cell></row><row><cell>ViT B/32</cell><cell>19.8</cell><cell>80.69</cell></row><row><cell>ViT B/16</cell><cell>55.6</cell><cell>84.73</cell></row><row><cell>TokenLearner S/32</cell><cell>1.9</cell><cell>76.13</cell></row><row><cell>TokenLearner B/16</cell><cell>28.7</cell><cell>83.65</cell></row><row><cell>TokenLearner S/32 (22)</cell><cell>3.3</cell><cell>79.42</cell></row><row><cell>TokenLearner B/32 (20)</cell><cell>11.5</cell><cell>82.74</cell></row><row><cell>TokenLearner B/16 (21)</cell><cell>47.1</cell><cell>85.21</cell></row><row><cell cols="2">16-TokenLearner B/16 (21) 47.7</cell><cell>85.45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>TokenLearner with ViT L/16 and L/14. 512x512 input images used.</figDesc><table><row><cell>Base</cell><cell cols="4"># layers TokenLearner GFLOPS ImageNet Top1</cell></row><row><cell cols="2">ViT L/16 24</cell><cell>-</cell><cell>363.1</cell><cell>87.35</cell></row><row><cell cols="2">ViT L/16 24</cell><cell>16-TL at 12</cell><cell>178.1</cell><cell>87.68</cell></row><row><cell cols="2">ViT L/16 24+11</cell><cell>16-TL at 12</cell><cell>186.8</cell><cell>87.47</cell></row><row><cell cols="2">ViT L/16 24+6</cell><cell>8-TL at 18</cell><cell>274.2</cell><cell>88.11</cell></row><row><cell cols="2">ViT L/14 24+11</cell><cell>16-TL at 18</cell><cell>361.6</cell><cell>88.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison to state-of-the-art ViT models. , it showed that it is able to meaningfully outperform the base model. The actual runtime of the base ViT L/16 vs. L/16 + TokenLearner was 1400 vs. 2000 images per second. It is not exactly linear to FLOPS due to the existence of other bottlenecks such as data loading.</figDesc><table><row><cell>Method</cell><cell cols="3"># params. ImageNet ImageNet ReaL</cell></row><row><cell>BiT-L</cell><cell>928M</cell><cell>87.54</cell><cell>90.54</cell></row><row><cell>ViT-H/14</cell><cell>654M</cell><cell>88.55</cell><cell>90.72</cell></row><row><cell>ViT-G/14</cell><cell>1843M</cell><cell>90.45</cell><cell>90.81</cell></row><row><cell cols="2">TL L/10 (24+11) 460M</cell><cell>88.5</cell><cell>90.75</cell></row><row><cell>TL L/8 (24+11)</cell><cell>460M</cell><cell>88.87</cell><cell>91.05</cell></row><row><cell>instead of L/16)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 :</head><label>4</label><figDesc>TokenLearner inserted earlier within ViT L/16. 384x384 input images used.</figDesc><table><row><cell>Base</cell><cell cols="4"># layers TokenLearner GFLOPS ImageNet Top1</cell></row><row><cell cols="2">ViT B/16 12</cell><cell>-</cell><cell>55.63</cell><cell>84.73</cell></row><row><cell cols="2">ViT L/16 24</cell><cell>16-TL at 2</cell><cell>20.91</cell><cell>83.89</cell></row><row><cell cols="2">ViT L/16 24</cell><cell>16-TL at 3</cell><cell>28.66</cell><cell>85.40</cell></row><row><cell cols="2">ViT L/16 24</cell><cell>16-TL at 6</cell><cell>51.92</cell><cell>86.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5 :</head><label>5</label><figDesc>Models with TokenLearner, with and without TokenFuser. The model without TokenFuser is described in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 6 :</head><label>6</label><figDesc>TokenLearner compared against pooling-based token reduction.</figDesc><table><row><cell>Details</cell><cell cols="2">ImageNet GFLOPS</cell></row><row><cell>Base ViT L/16</cell><cell>87.35</cell><cell>363.1</cell></row><row><cell>2x2 pool at 9 and 18</cell><cell>85.63</cell><cell>144.3</cell></row><row><cell cols="2">2x2 pool at 12 and 18 86.41</cell><cell>187.2</cell></row><row><cell>4x4 pool at 12</cell><cell>83.93</cell><cell>184.4</cell></row><row><cell>16-TL at 12</cell><cell>87.68</cell><cell>184.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 :</head><label>7</label><figDesc>Comparison of ViViT models with and without TokenLearner on Kinetics-400. GLOPS are per view. The difference in the number of parameters between the TokenLearner models (which are from Tables 2 and 5) comes from the different number of layers used after the TokenLearner module.</figDesc><table><row><cell>Method</cell><cell cols="4">Top-1 accuracy Top-5 accuracy # params. GFLOPS</cell></row><row><cell>ViViT-L/16 [14]</cell><cell>82.8</cell><cell>95.5</cell><cell>308M</cell><cell>1446</cell></row><row><cell>ViViT-L/16 320 [14]</cell><cell>83.5</cell><cell>95.5</cell><cell>308M</cell><cell>3992</cell></row><row><cell>ViViT-H/14 [14]</cell><cell>84.8</cell><cell>95.8</cell><cell>654M</cell><cell>3981</cell></row><row><cell>ViViT-L/16 (our run)</cell><cell>83.4</cell><cell>95.6</cell><cell>308M</cell><cell>1446</cell></row><row><cell>TokenLearner 16at12 + L/16</cell><cell>83.5</cell><cell>95.6</cell><cell>308M</cell><cell>766</cell></row><row><cell>TokenLearner 8at18 + L/16</cell><cell>84.5</cell><cell>96.1</cell><cell>383M</cell><cell>1105</cell></row><row><cell>TokenLearner 16at18+ L/14</cell><cell>84.7</cell><cell>96.1</cell><cell>447M</cell><cell>1621</cell></row><row><cell>TokenLearner 16at18+ L/10</cell><cell>85.4</cell><cell>96.3</cell><cell>450M</cell><cell>4076</cell></row><row><cell cols="2">5 EXPERIMENTS WITH VIDEOS: TOKENLEARNER</cell><cell></cell><cell></cell><cell></cell></row><row><cell>WITH VIDEO VISION TRANSFORMER</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>5.1 Network architecture implementation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ViViT [14] is a direct extension of ViT for videos, which uses</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">spatio-temporal patches from videos as its tokens. The size</cell><cell></cell><cell></cell><cell></cell></row></table><note>of the space-time patches are typically 16x16x2, which are given to the Transformer layers similar to ViT. ViViT and ViT share the architecture. For our experiments, we insert the TokenLearner module within the ViViT architecture, identically to how we inserted it within ViT in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 8 :</head><label>8</label><figDesc>ViViT + TokenLearner on Kinetics-400, compared to the previous models. Different approaches rely on different pre-training datasets, such as ImageNet-21K (for TimeSformer and Swin) and JFT (for ViViT and TokenLearner). The multiplication in GFLOPS correponds to the number of views used for the inference, such as 4x3 = 12.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 accuracy total GFLOPS</cell></row><row><cell>R(2+1)D [25]</cell><cell>73.9</cell><cell>304 ? 115</cell></row><row><cell>SlowFast 16x8, R101+NL [6]</cell><cell>79.8</cell><cell>234 ? 30</cell></row><row><cell>TimeSformer-L [15]</cell><cell>80.7</cell><cell>2380 ? 3</cell></row><row><cell>ViViT-L/16 [14]</cell><cell>82.8</cell><cell>1446 ? 12</cell></row><row><cell>Swin-L [26]</cell><cell>83.1</cell><cell>604 ? 12</cell></row><row><cell>Swin-L (384) [26]</cell><cell>84.6</cell><cell>2107 ? 12</cell></row><row><cell>Swin-L (384) [26]</cell><cell>84.9</cell><cell>2107 ? 50</cell></row><row><cell>TokenLearner 16at12 (L/16)</cell><cell>82.1</cell><cell>766 ? 6</cell></row><row><cell>TokenLearner 8at18 (L/16)</cell><cell>83.2</cell><cell>1105 ? 6</cell></row><row><cell>TokenLearner 16at12 (L/16)</cell><cell>83.5</cell><cell>766 ? 12</cell></row><row><cell>TokenLearner 8at18 (L/16)</cell><cell>84.5</cell><cell>1105 ? 12</cell></row><row><cell>TokenLearner 16at18 (L/14)</cell><cell>84.7</cell><cell>1621 ? 12</cell></row><row><cell>TokenLearner 16at18 (L/10)</cell><cell>85.4</cell><cell>4076 ? 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 9 :</head><label>9</label><figDesc>ViViT + TokenLearner on Kinetics-600. The multiplication in GFLOPS correponds to the number of views used for the inference, such as 4x3 = 12. TL stands for TokenLearner.</figDesc><table><row><cell>Method</cell><cell cols="3">Top-1 Top-5 total GFLOPS</cell></row><row><cell>SlowFast 16x8, R101+NL [6]</cell><cell>81.8</cell><cell>95.1</cell><cell>234 ? 30</cell></row><row><cell>X3D-XL [27]</cell><cell>81.9</cell><cell>95.5</cell><cell>48 ? 30</cell></row><row><cell>TimeSformer-HR [15]</cell><cell>82.4</cell><cell>96.0</cell><cell>1703 ? 3</cell></row><row><cell>ViViT-L/16 [14]</cell><cell>84.3</cell><cell>96.2</cell><cell>1446 ? 12</cell></row><row><cell>Swin-B [26]</cell><cell>84.0</cell><cell>96.5</cell><cell>282 ? 12</cell></row><row><cell>Swin-L (384) [26]</cell><cell>85.9</cell><cell>97.1</cell><cell>2107 ? 12</cell></row><row><cell>Swin-L (384) [26]</cell><cell>86.1</cell><cell>97.3</cell><cell>2107 ? 50</cell></row><row><cell>TL 16at12 (L/16)</cell><cell>84.4</cell><cell>96.0</cell><cell>766 ? 12</cell></row><row><cell>TL 8at18 (L/16)</cell><cell>86.0</cell><cell>97.0</cell><cell>1105 ? 12</cell></row><row><cell>TL 16at18 (L/10)</cell><cell>86.1</cell><cell>97.0</cell><cell>4076 ? 12</cell></row><row><cell>TL 16at18 w. Fuser (L/10)</cell><cell>86.3</cell><cell>97.0</cell><cell>4100 ? 12</cell></row><row><cell cols="4">6 EXPERIMENTS WITH VIDEOS: TOKENLEARNER</cell></row><row><cell cols="3">WITH BOTTLENECK TRANSFORMER</cell><cell></cell></row></table><note>6.1 Network architecture implementation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 11 :</head><label>11</label><figDesc></figDesc><table><row><cell cols="3">Performance on the Anonymized Videos from</cell></row><row><cell cols="3">Diverse countries (AViD) dataset. Performance in terms of</cell></row><row><cell cols="3">mean accuracy is shown in % averaged over 887 classes.</cell></row><row><cell cols="3">Previous approaches results are reported from [31], all based</cell></row><row><cell cols="3">on training from scratch with RGB-only inputs. X(2+1)D-M</cell></row><row><cell cols="3">baseline is with disjoint space+time Transformer (as in [15]).</cell></row><row><cell>Method</cell><cell cols="2">Accuracy Total GFLOPS</cell></row><row><cell>I3D [1]</cell><cell>46.5</cell><cell>108 ? N/A</cell></row><row><cell>(2+1)D ResNet-50</cell><cell>46.7</cell><cell>152 ? 115</cell></row><row><cell>3D ResNet-50</cell><cell>47.9</cell><cell>N/A</cell></row><row><cell>SlowFast-50 8x8 [6]</cell><cell>50.2</cell><cell>65.7 ? 30</cell></row><row><cell>SlowFast-101 16x4 [6]</cell><cell>50.8</cell><cell>213 ? 30</cell></row><row><cell cols="2">Backbone (X(2+1)D-M) 48.6</cell><cell>532 ? 1</cell></row><row><cell>X(2+1)D-M</cell><cell>50.6</cell><cell>493 ? 1</cell></row><row><cell>Ours</cell><cell>53.8</cell><cell>487 ? 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 12 :</head><label>12</label><figDesc>Comparison between TokenLearner and the joint space-time transformer modules similar to</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="4">: Comparison between different space-time trans-</cell></row><row><cell cols="4">former modules. They were all applied to the same back-</cell></row><row><cell cols="4">bone architecture (i.e., the Bottleneck Transformer-style with</cell></row><row><cell cols="4">X(2+1)D). The Charades-6fps is used in this experiment.</cell></row><row><cell cols="4">FLOPS are estimated with 64-frame settings, per module.</cell></row><row><cell>Module</cell><cell cols="3">Acc. (%) GFLOPs # params</cell></row><row><cell>Conv2D + Conv1D</cell><cell>56.6</cell><cell>18.3</cell><cell>2.24M</cell></row><row><cell>Conv2D + MLPMixer [17]</cell><cell>57.0</cell><cell>13.8</cell><cell>2.06M</cell></row><row><cell>Conv2D + Temporal transformer</cell><cell>58.4</cell><cell>16.5</cell><cell>1.98M</cell></row><row><cell>Spatial + Temporal transformer</cell><cell>58.8</cell><cell>5.5</cell><cell>0.59M</cell></row><row><cell>Conv2D + Spatial + Temporal</cell><cell>58.0</cell><cell>19.2</cell><cell>2.27M</cell></row><row><cell>Ours (TokenLearner)</cell><cell>58.8</cell><cell>3.4</cell><cell>0.81M</cell></row><row><cell>Ours (SpatialT + TokenLearner)</cell><cell>58.9</cell><cell>6.2</cell><cell>1.11M</cell></row><row><cell>Ours (Conv2D + TokenLearner)</cell><cell>59.6</cell><cell>17.2</cell><cell>2.49M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 14 :</head><label>14</label><figDesc>Comparing different components of our Token-Learner. On Charades dataset (6fps).</figDesc><table><row><cell>Module</cell><cell>Accuracy (%)</cell></row><row><cell>Standard transformer (MHSA)</cell><cell>58.4</cell></row><row><cell>Vector transformer (VectT)</cell><cell>58.1</cell></row><row><cell>Prior-only-attention + broadcasting</cell><cell>58.6</cell></row><row><cell>Vector transformer (VectT) + broadcasting</cell><cell>58.9</cell></row><row><cell>Vector transformer (VectT) + TokenFuser</cell><cell>59.0</cell></row><row><cell>TokenLearner + MHSA + TokenFuser</cell><cell>59.0</cell></row><row><cell>TokenLearner + VectT + TokenFuser</cell><cell>59.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Michael S. Ryoo is a SUNY Empire Innovation Associate Professor in the Department of Computer Science at Stony Brook University, and is also a staff research scientist at Robotics at Google. He previously was an assistant professor at Indiana University Bloomington, and was a staff researcher within the Robotics Section of NASA's Jet Propulsion Laboratory (JPL). Dr. Ryoo received his Ph.D. from the University of Texas at Austin and B.S. from Korea Advanced Institute of Science and Technology (KAIST).AJ Piergiovanni is a research scientist at Google. He has a PhD in computer science from Indiana University and a BS from Rose-Hulman Institute of Technology. His research interests are in video understanding, building efficient models and learning from vision and language.</figDesc><table /><note>Anurag Arnab is a research scientist at Google. Previously, he com- pleted his PhD at the University of Oxford.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mostafa</head><p>Dehghani is a research scientist at Google Brain. Previously, he completed his PhD at the University of Amsterdam.</p><p>Anelia Angelova is a research scientist in the area of computer vision. She leads the Vision and Language team in Brain Research at Google and was previously leading the Robot Vision team in Robotics at Google. Her research interests span many topics in computer vision: object recognition and detection, 3D scene understanding, self-supervised learning, video understanding, multi-modal learning, robotics perception, real-time algorithms and others. She has integrated her work in production systems, including X Robotics, Google Maps, Google Cloud, and Google's self-driving car (Waymo). Anelia received her MS and PhD degrees in Computer Science from California Institute of Technology.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition</title>
		<meeting>the ICCV Workshop on Action, Gesture, and Emotion Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03150</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">D3D: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08249</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tokenlearner: Adaptive space-time tokenization for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11403</idno>
		<title level="m">Scenic: A JAX library for computer vision research and beyond</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet?</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Visual transformers: Tokenbased image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03677</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">X3D: expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AViD dataset: Anonymized videos from diverse countries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05038</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action genome: Actions as composition of spatio-temporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Assem-bleNet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AssembleNet++: Assembling modality representations via attention connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kangaspunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for image classification: A comprehensive review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2352" to="2449" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Towards long-form video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Eco: efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Holistic large scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.114511</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tiny video networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied AI Letters Journal</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Vipnas: Efficient video pose estimation via neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Levit: A vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutationinvariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
