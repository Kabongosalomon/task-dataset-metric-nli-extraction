<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Spatial Dimensions of Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Sogang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong</forename><forename type="middle">Joon</forename><surname>Oh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Spatial Dimensions of Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The architectures based on the self-attention mechanism have achieved great success in the field of Natural Language Processing (NLP) <ref type="bibr" target="#b33">[34]</ref>. There have been attempts to utilize the self-attention mechanism in computer vision. Non-local networks <ref type="bibr" target="#b36">[37]</ref> and DETR <ref type="bibr" target="#b3">[4]</ref> are representative works, showing that the self-attention mechanism is also effective in video classification and object detection tasks, respectively. Recently, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>, a transformer architecture consisting of self-attention layers, has been proposed to compete with ResNet <ref type="bibr" target="#b12">[13]</ref>, and shows that it can achieve the best performance without convolution op-* Work done as a research scientist at NAVER AI Lab. eration on ImageNet <ref type="bibr" target="#b7">[8]</ref>. As a result, a new direction of network architectures based on self-attention mechanism, not convolution operation, has emerged in computer vision.</p><p>ViT is quite different from convolutional neural networks (CNN). Input images are divided into 16?16 patches and fed to the transformer network; except for the first embedding layer, there is no convolution operation in ViT, and the position interactions occur only through the self-attention layers. While CNNs have restricted spatial interactions, ViT allows all the positions in an image to interact through transformer layers. Although ViT is an innovative architecture and has proven its powerful image recognition ability, it follows the transformer architecture in NLP <ref type="bibr" target="#b33">[34]</ref> without any changes. Some essential design principles of CNNs, which have proved to be effective in the computer vision domain over the past decade, are not sufficiently reflected. We thus revisit the design principles of CNN architectures and investigate their efficacy when applied to ViT architectures.</p><p>CNNs start with a feature of large spatial sizes and a small channel size and gradually increase the channel size while decreasing the spatial size. This dimension conversion is indispensable due to the layer called spatial pooling. Modern CNN architectures, including AlexNet <ref type="bibr" target="#b20">[21]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, and EfficientNet <ref type="bibr" target="#b31">[32]</ref>, follow this design principle. The pooling layer is deeply related to the receptive field size of each layer. Some studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b4">5]</ref> show that the pooling layer contributes to the expressiveness and generalization performance of the network. However, unlike the CNNs, ViT does not use a pooling layer and uses the same spatial dimension for all layers.</p><p>First, we verify the advantages of dimensions configurations on CNNs. Our experiments show that ResNet-style dimensions improve the model capability and generalization performance of ResNet. To extend the advantages to ViT, we propose a Pooling-based Vision Transformer (PiT). PiT is a transformer architecture combined with a newly designed pooling layer. It enables the spatial size reduction in the ViT structure as in ResNet. We also investigate the benefits of PiT compared to ViT and confirm that ResNet-style dimension setting also improves the performance of ViT. Finally, to analyze the effect of PiT compared to ViT, we Pooling Pooling (c) PiT-S <ref type="figure">Figure 1</ref>. Schematic illustration of dimension configurations of networks. We visualize ResNet50 <ref type="bibr" target="#b12">[13]</ref>, Vision Transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>, and our Pooling-based Vision Transformer (PiT); (a) ResNet50 gradually downsamples the features from the input to the output; (b) ViT does not change the spatial dimensions; (c) PiT involves ResNet style spatial dimension into ViT.</p><p>analyze the attention matrix of transformer block with entropy and average distance measure. The analysis shows the attention patterns inside layers of ViT and PiT, and helps to understand the inner mechanism of ViT and PiT. We verify that PiT improves performances over ViT on various tasks. On ImageNet classification, PiT and outperforms ViT at various scales and training environments. Additionally, we have compared the performance of PiT with various convolutional architectures and have specified the scale at which the transformer architecture outperforms the CNN. We further measure the performance of PiT as a backbone for object detection. ViT-and PiT-based deformable DETR <ref type="bibr" target="#b43">[44]</ref> are trained on the COCO 2017 dataset <ref type="bibr" target="#b23">[24]</ref> and the result shows that PiT is even better than ViT as a backbone architecture for a task other than image classification. Finally, we verify the performance of PiT in various environments through the robustness benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Dimension configuration of CNN</head><p>Dimension conversion can be found in AlexNet <ref type="bibr" target="#b20">[21]</ref>, which is one of the earliest convolutional networks in computer vision. AlexNet uses three max-pooling layers. In the max-pooling layer, the spatial size of the feature is reduced by half, and the channel size is increased by the convolution after the max-pooling. VGGnet <ref type="bibr" target="#b29">[30]</ref> uses 5 spatial resolutions using 5 max-pooling. In the pooling layer, the spatial size is reduced by half and the channel size is doubled. GoogLeNet <ref type="bibr" target="#b30">[31]</ref> also used the pooling layer. ResNet <ref type="bibr" target="#b12">[13]</ref> performed spatial size reduction using the convolution layer of stride 2 instead of max pooling. It is an improvement in the spatial reduction method. The convolution layer of stride 2 is also used as a pooling method in recent architectures (EfficietNet <ref type="bibr" target="#b31">[32]</ref>, MobileNet <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b18">19]</ref>). Pyramid-Net <ref type="bibr" target="#b10">[11]</ref> pointed out that the channel increase occurs only in the pooling layer and proposed a method to gradually increase the channel size in layers other than the pooling layer. ReXNet <ref type="bibr" target="#b11">[12]</ref> reported that the channel configuration of the network has a significant influence on the network performance. In summary, most convolution networks use a dimension configuration with spatial reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Self-attention mechanism</head><p>Transformer architecture <ref type="bibr" target="#b33">[34]</ref> significantly increased the performance of the NLP task with the self-attention mechanism. Funnel Transformer <ref type="bibr" target="#b6">[7]</ref> improves the transformer architecture by reducing tokens by a pooling layer and skipconnection. However, because of the basic difference between the architecture of NLP and computer vision, the method of applying to pool is different from our method. Some studies are conducted to utilize the transformer architecture to the backbone network for computer vision tasks. Non-local network <ref type="bibr" target="#b36">[37]</ref> adds a few self-attention layers to CNN backbone, and it shows that the self-attention mechanism can be used in CNN. <ref type="bibr" target="#b27">[28]</ref> replaced 3 ? 3 convolution of ResNet to local self-attention layer. <ref type="bibr" target="#b35">[36]</ref> used an attention layer for each spatial axis. <ref type="bibr" target="#b1">[2]</ref> enables self-attention of the entire spatial map by reducing the computation of the attention mechanism. Most of these methods replace 3x3 convolution with self-attention or adds a few self-attention layers. Therefore, the basic structure of ResNet is inherited, that is, it has the convolution of stride 2 as ResNet, resulting in a network having a dimension configuration of ResNet.</p><p>Only the vision transformer uses a structure that uses the same spatial size in all layers. Although ViT did not follow the conventions of ResNet, it contains many valuable new components in the network architecture. In ViT, layer normalization is applied for each spatial token. Therefore, layer normalization of ViT is closer to positional normalization <ref type="bibr" target="#b21">[22]</ref> than a layer norm of convolutional neural network <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39]</ref>. Although it overlaps with the lambda network <ref type="bibr" target="#b1">[2]</ref>, it is not common to use global attention through all blocks of the network. The use of class tokens instead of global average pooling is also new, and it has been reported that separating tokens increases the efficiency of distillation <ref type="bibr" target="#b32">[33]</ref>. In addition, the layer configuration, the skipconnection position, and the normalization position of the Transformer are also different from ResNet. Therefore, our study gives a direction to the new architecture. <ref type="figure">Figure 2</ref>. Effects of the spatial dimensions in ResNet50 <ref type="bibr" target="#b12">[13]</ref>. We verify the effect of the spatial dimension with ResNet50. As shown in the figures, ResNet-style is better than ViT-style in the model capability, generalization performance, and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Revisiting spatial dimensions</head><p>In order to introduce dimension conversion to ViT, we investigate spatial dimensions in network architectures. First, we verify the benefits of dimension configuration in ResNet architecture. Although dimension conversion has been widely used for most convolutional architectures, its effectiveness is rarely verified. Based on the findings, we propose a Pooling-based Vision Transformer (PiT) that applies the ResNet-style dimension to ViT. We propose a new pooling layer for transformer architecture and design ViT with the new pooling layer (PiT). With PiT models, we verify whether the ResNet-style dimension brings advantages to ViT. In addition, we analyze the attention matrix of the self-attention block of ViT to investigate the effect of PiT in the transformer mechanism. Finally, we introduce PiT architectures corresponding to various scales of ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dimension setting of CNN</head><p>As shown in <ref type="figure">Figure 1</ref> (a), most convolutional architectures reduce the spatial dimension while increases the channel dimension. In ResNet50, a stem layer reduces the spatial size of an image to 56 ?56. After several layer blocks, Convolution layers with stride 2 reduce the spatial dimension by half and double the channel dimension. The spatial reduction using a convolution layer with stride 2 is a frequently used method in recent architectures <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>. We conduct an experiment to analyze the performance difference according to the presence or absence of the spatial reduction layer in a convolutional architecture. ResNet50, one of the most widely used networks in ImageNet, is used for architecture and is trained over 100 epochs without complex training techniques. For ResNet with ViT style dimension, we use the stem layer of ViT to reduce the feature to 14?14 spatial dimensions while reducing the spatial information loss in the stem layer. We also remove the spatial reduction layers of ResNet to maintain the initial feature dimensions for all layers like ViT. We measured the performance for several sizes by changing the channel size of ResNet.</p><p>First, we measured the relation between FLOPs and training loss of ResNet with ResNet-style or ViT-style dimension configuration. As shown in <ref type="figure">Figure 2</ref> (a), ResNet (ResNet-style) shows lower training loss over the same computation costs (FLOPs). It implies that ResNet-style dimensions increase the capability of architecture. Next, we analyzed the relation between training and validation accuracy, which represents the generalization performance of architecture. As shown in <ref type="figure">Figure 2</ref> (b), ResNet (ResNetstyle) achieves higher validation accuracy than ResNet (ViT-style). Therefore, ResNet-style dimension configuration is also helpful for generalization performance. In summary, ResNet-style dimension improves the model capability and generalization performance of the architecture and consequently brings a significant improvement in validation accuracy as shown in <ref type="figure">Figure 2</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pooling-based Vision Transformer (PiT)</head><p>Vision Transformer (ViT) performs network operations based on self-attention, not convolution operations. In the self-attention mechanism, the similarity between all locations is used for spatial interaction. <ref type="figure">Figure 1</ref> (b) shows the dimension structure of this ViT. Similar to the stem layer of CNN, ViT divides the image by patch at the first embedding layer and embedding it to tokens. Basically, the structure does not include a spatial reduction layer and keeps the same number of spatial tokens overall layer of the network. Although the self-attention operation is not limited by spatial distance, the size of the spatial area participating in attention is affected by the spatial size of the feature. Therefore, in order to adjust the dimension configuration like ResNet, a spatial reduction layer is also required in ViT.</p><p>To utilize the advantages of the dimension configuration to ViT, we propose a new architecture called Poolingbased Vision Transformer (PiT). First, we designed a pooling layer for ViT. Our pooling layer is shown in <ref type="figure" target="#fig_0">Figure 4</ref>. Since ViT handles neuron responses in the form of 2Dmatrix rather than 3D-tensor, the pooling layer should sep- <ref type="figure">Figure 3</ref>. Effects of the spatial dimensions in vision transformer (ViT) <ref type="bibr" target="#b8">[9]</ref>. We compare our Pooling-based Vision Transformer (PiT) with original ViT at various aspects. PiT outperforms ViT in capability, generalization performance, and model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pooling layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial tokens</head><p>Class token</p><formula xml:id="formula_0">( ??)? 1? ??? Reshape 2 ? ? 2 ?2 Depth-wise Convolution Reshape</formula><p>Spatial tokens</p><formula xml:id="formula_1">( 2 ? ? 2 )?2</formula><p>1?2 Fully-connected layer Class token arate spatial tokens and reshape them into 3D-tensor with spatial structure. After reshaping, spatial size reduction and channel increase are performed by depth-wise convolution. And, the responses are reshaped into a 2D matrix for the computation of transformer blocks. In ViT, there are parts that do not correspond to the spatial structure, such as a class token or distillation token <ref type="bibr" target="#b32">[33]</ref>. For these parts, the pooling layer uses an additional fully-connected layer to adjust the channel size to match the spatial tokens. Our pooling layer enables spatial reduction on ViT and is used for our PiT architecture as shown in <ref type="figure">Figure 1</ref> (c). PiT includes two pooling layers which make three spatial scales.</p><p>Using PiT architecture, we performed an experiment to verify the effect of PiT compared to ViT. The experiment setting is the same as the ResNet experiment. <ref type="figure">Figure 3</ref>   <ref type="figure">Figure 3</ref> (c). The phenomenon that ViT does not increase performance even when FLOPs increase in ImageNet is reported in ViT paper <ref type="bibr" target="#b8">[9]</ref>. In the training data of ImageNet scale, ViT shows poor generalization performance, and PiT alleviates this. So, we believe that the spatial reduction layer is also necessary for the generalization of ViT. Using the training trick is a way to improve the generalization performance of ViT in ImageNet. The combination of training tricks and PiT is covered in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention analysis</head><p>We analyze the transformer networks with measures on attention matrix <ref type="bibr" target="#b34">[35]</ref>. We denotes ? i,j as (i, j) component of attention matrix A ? R M ?N . Note that attention values after soft-max layer is used, i.e. i ? i,j = 1. The attention entropy is defined as</p><formula xml:id="formula_2">Entropy = ? 1 N N j i ? i,j log ? i,j .<label>(1)</label></formula><p>The entropy shows the spread and concentration degree of an attention interaction. A small entropy indicates a concentrated interaction, and a large entropy indicates a spread interaction. We also measure an attention distance,</p><formula xml:id="formula_3">Distance = 1 N N j i ? i,j p i ? p j 1 .<label>(2)</label></formula><p>p i represents relative spatial location of i-th token (x i /W, y i /H) for feature map F ? R H?W ?C . So, the attention distance shows a relative ratio compared to the overall feature size, which enables comparison between the different sizes of features. We analyze transformer-based models (ViT-S <ref type="bibr" target="#b32">[33]</ref> and PiT-S) and values are measured overall validation images and are averaged over all heads of each layer. Our analysis is only conducted for the spatial tokens rather than the class token following the previous study <ref type="bibr" target="#b34">[35]</ref>. We also skip the attention of the last transformer block since the spatial tokens of the last attention are independent of the network outputs.</p><p>The results are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. In ViT, the entropy and the distance increase as the layer become deeper. It implies that the interaction of ViT is concentrated to close tokens at the shallow layers and the interaction is spread in a wide range of tokens at the deep layers. The entropy and distance pattern of ViT is similar to the pattern of transformer in the language domain <ref type="bibr" target="#b34">[35]</ref>. PiT changes the patterns with the spatial dimension setting. At shallow layers (1-2 layers), large spatial size increases the entropy and distance. On the other hand, the entropy and distance are decreased at deep layers (9-11 layers) due to the small spatial size. In short, the pooling layer of PiT spreads the interaction in the shallow layers and concentrates the interaction in the deep layers. In contrast to discrete word inputs of the language domain, the vision domain uses image-patch inputs which require pre-processing operations such as filtering, contrast, and brightness calibration. In shallow layers, the spread interaction of PiT is close to the pre-processing than the concentrated interaction of ViT. Also, compared to language models, image recognition has relatively low output complexity. So, in deep layers, concentrated interaction might be enough. There are significant differences between the vision and the language domain, and we believe that the attention of PiT is suitable for image recognition backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Architecture design</head><p>The architectures proposed in ViT paper <ref type="bibr" target="#b8">[9]</ref> aimed at datasets larger than ImageNet. These architectures (ViT-Large, ViT-Huge) have an extremely large scale than general ImageNet networks, so it is not easy to compare them with other networks. So, following the previous study <ref type="bibr" target="#b32">[33]</ref> of Vision Transformer on ImageNet, we design the PiT at a scale similar to the small-scale ViT architectures (ViT-Base, ViT-Small, ViT-Tiny). In the DeiT paper <ref type="bibr" target="#b32">[33]</ref>, ViT-Small and ViT-Tiny are named DeiT-S and DeiT-Ti, but to avoid confusion due to the model name change, we use ViT for all models. Corresponding to the three scales of ViT (tiny, small, and base), we design four scales of PiT (tiny, extra small, small, and base). Detail architectures are described in <ref type="table">Table 1</ref>. For convenience, we abbreviate the model names: Tiny -Ti, eXtra Small -XS, Small -S, Base -B FLOPs and spatial size were measured based on 224?224 image. Since PiT uses a larger spatial size than ViT, we reduce the stride size of the embedding layer to 8, while patch-size is 16 as ViT. Two pooling layers are used for PiT, and the channel increase is implemented as increasing the number of heads of multi-head attention. We design PiT to have a similar depth to ViT, and adjust the channels and the heads to have  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We verified the performance of PiT through various experiments. First, we compared PiT at various scales with ViT in various training environments of ImageNet training. And, we extended the ImageNet comparison to architectures other than Transformer. In particular, we focus on the comparison of the performance of ResNet and PiT, and investigate whether PiT can beat ResNet. We also applied PiT to an object detector based on deformable DETR <ref type="bibr" target="#b43">[44]</ref>, and compared the performance as a backbone architecture for object detection. To analyze PiT in various views, we evaluated the performance of PiT on robustness benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet classification</head><p>We compared the performance of PiT models of <ref type="table">Table 1</ref> with corresponding ViT models. To clarify the computation time and size of the network, we measured FLOPs, the number of parameters, and GPU throughput (images/sec) of each network. The GPU throughput was measured on NVIDIA V100 single GPU with 128 batch-size. We trained the network using four representative training environments. The first is a vanilla setting that trains the network without complicated training techniques. The vanilla setting has the lowest performance due to the lack of techniques to help generalization performance and also used for the previous experiments in <ref type="figure">Figure 2, 3</ref>. The second is training with CutMix <ref type="bibr" target="#b40">[41]</ref> data augmentation. Although only data augmentation has changed, it shows significantly better performance than the vanilla setting. The third is the DeiT <ref type="bibr" target="#b32">[33]</ref> setting, which is a compilation of training techniques to train ViT on ImageNet-1k <ref type="bibr" target="#b7">[8]</ref>. DeiT setting includes various training techniques and parameter tuning, and we used the same training setting through the official open-source code. However, in the case of Repeated Augment <ref type="bibr" target="#b17">[18]</ref>, we confirmed that it had a negative effect in a small model, and it was used only for Base models. The last is a DeiT setting with knowledge distillation. The distillation setting is reported as the best performance setting in DeiT <ref type="bibr" target="#b32">[33]</ref> paper. The network uses an additional distillation token and is trained with distillation loss <ref type="bibr" target="#b16">[17]</ref> using RegNetY-16GF <ref type="bibr" target="#b26">[27]</ref> as a teacher network. We used AdamP <ref type="bibr" target="#b15">[16]</ref> optimizer for all settings, and the learning rate, weight decay, and warmup were set equal to DeiT <ref type="bibr" target="#b32">[33]</ref> paper. We train models over 100 epochs for Vanilla and CutMix settings, and 300 epochs for DeiT and Distill? settings.</p><p>The results are shown in <ref type="table">Table 2</ref>. Comparing the PiT and ViT of the same name, the PiT has fewer FLOPs and faster speed than ViT. Nevertheless, PiT shows higher performance than ViT. In the case of vanilla and CutMix settings, where a few training techniques are applied, the performance of PiT is superior to the performance of ViT. Even in the case of a DeiT and distill settings, PiT shows comparable or better performance to ViT. Therefore, PiT can be seen as a better architecture than ViT in terms of performance and computation. The generalization performance issue of ViT in <ref type="figure">Figure 3</ref> can also be observed in this experiment. Like ViT-S in the Vanilla setting and ViT-B in the Cut-Mix setting, ViT often shows no increase in performance even when the model size increases. On the other hand, the performance of PiT increases according to the model size in all training settings. it seems that the generalization performance problem of ViT is alleviated by the pooling layers.</p><p>We compared the performance of PiT with the convolutional networks. In the previous experiment, we performed the comparison in the same training setting using the similarity of architecture. However, when comparing various architectures, it is infeasible to unify with a setting that works well for all architectures. Therefore, we performed the comparison based on the best performance reported for each architecture. But, it was limited to the model trained using only ImageNet images. When the paper that proposed the architecture and the paper that reported the best performance was different, we cite both papers. When the architecture is different, the comparison of FLOPs often fails to reflect the actual throughput. Therefore, we re-measured the GPU throughput and number of params on a single V100 GPU and compared the top-1 accuracy for the performance index. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison result. In the case of the PiT-B scale, the transformer-based archi-  <ref type="bibr" target="#b26">[27]</ref> 83.6M 352 80.4% ResNeSt101 <ref type="bibr" target="#b42">[43]</ref> 48.3M 398 83.0% ViT-B <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref> 86.6M 303 81.8% PiT-B 73.8M 348 82.0% ViT-B? <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref> 86.6M 303 83.4% PiT-B? 73.8M 348 84.0%  <ref type="table">Table 5</ref>. COCO detection performance based on Deformable DETR <ref type="bibr" target="#b43">[44]</ref>. We evaluate the performance of PiT as a pretrained backbone for object detection.</p><p>training schemes: long training and fine-tune on large resolution. <ref type="table">Table 4</ref> shows the results. As shown in the previous study <ref type="bibr" target="#b32">[33]</ref>, the performance of ViT is significantly improved on the long training scheme (1000 epochs). So, we validate PiT on the long training scheme. As shown in <ref type="table">Table 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detection</head><p>We validate PiT through object detection on COCO dataset <ref type="bibr" target="#b23">[24]</ref> in Deformable-DETR <ref type="bibr" target="#b43">[44]</ref>. We train the detectors with different backbones including ResNet50, ViT-S, and our PiT-S. We follow the training setup of the original paper <ref type="bibr" target="#b43">[44]</ref>  test of all backbones. We use bounding box refinement and a two-stage scheme for the best performance <ref type="bibr" target="#b43">[44]</ref>. For multiscale features for ViT-S, we use features at the 2nd, 8th, and 12th layers following the position of pooling layers on PiT. All detectors are trained for 50 epochs and the learning rate is dropped by factor 1/10 at 40 epochs. <ref type="table">Table 5</ref> shows the measured AP score on val2017. The detector based on PiT-S outperforms the detector with ViT-S. It shows that the pooling layer of PiT is effective not only for ImageNet classification but also for pretrained backbone for object detection. We measured single image latency with a random noise image at resolution 600 ? 400 PiT based detector has lower latency than detector based on ResNet50 or ViT-S. Although PiT detector cannot beat the performance of the ResNet50 detector, PiT detector has better latency, and improvement over ViT-S is significant. Additional investigation on the training settings for PiT based detectors would improve the performance of the PiT detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness benchmarks</head><p>In this subsection, we investigate the effectiveness of the proposed architecture in terms of robustness against input changes. We presume that the existing ViT design concept, which keeps the spatial dimension from the input layer to the last layer, has two conceptual limitations: Lack of background robustness and sensitivity to the local discriminative visual features. We, therefore, presume that PiT, our new design choice with the pooling mechanism, performs better than ViT for the background robustness benchmarks and the local discriminative sensitivity benchmarks.</p><p>We employ four different robustness benchmarks. Occlusion benchmark measures the ImageNet validation accuracy where the center 112 ? 112 patch of the images is zero-ed out. This benchmark measures whether a model only focuses on a small discriminative visual feature or not. ImageNet-A <ref type="figure">(IN-A)</ref> is a dataset constructed by collecting the failure cases of ResNet50 from the web <ref type="bibr" target="#b14">[15]</ref> where the collected images contain unusual backgrounds or objects with very small size <ref type="bibr" target="#b22">[23]</ref>. From this benchmark, we can in-fer how a model is less sensitive to unusual backgrounds or object size changes. However, since IN-A is constructed by collecting images (queried by 200 ImageNet subclasses) where ResNet50 predicts a wrong label, this dataset can be biased towards ResNet50 features. We, therefore, employ background challenge (BGC) benchmark <ref type="bibr" target="#b39">[40]</ref> to explore the explicit background robustness. The BGC dataset consists of two parts, foregrounds, and backgrounds. This benchmark measures the model validation accuracy while keeping the foreground but adversarially changing the background from the other image. Since BGC dataset is built upon nine subclasses of ImageNet, the baseline random chance is 11.1%. Lastly, we tested adversarial attack robustness using the fast gradient sign method (FGSM) <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_5">Table 6</ref> shows the results. First, we observe that PiT shows better performances than ViT in all robustness benchmarks, despite they show comparable performances in the standard ImageNet benchmark (80.8 vs. 79.8). It supports that our dimension design makes the model less sensitive to the backgrounds and the local discriminative features. Also, we found that the performance drops for occluded samples by ResNet50 are much dramatic than PiT; 80.8 ? 74.6, 5% drops for PiT, 79.0 ? 67.1, 15% drops for ResNet50. This implies that ResNet50 focuses more on the local discriminative areas, by the nature of convolutional operations. Interestingly, in <ref type="table" target="#tab_5">Table 6</ref>, ResNet50 outperforms ViT variants in the background challenge dataset (32.7 vs. 21.0). This implies that the self-attention mechanism unintentionally attends more backgrounds comparing to ResNet design choice. Overcoming this potential drawback of vision transformers will be an interesting research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have shown that the design principle widely used in CNNs -the spatial dimensional transformation performed by pooling or convolution with strides, is not considered in transformer-based architectures such as ViT; ultimately affects the model performance. We have first studied with ResNet and found that the transformation in respect of the spatial dimension increases the computational efficiency and the generalization ability. To leverage the benefits in ViT, we propose a PiT that incorporates a pooling layer into Vit, and PiT shows that these advantages can be well harmonized to ViT through extensive experiments. Consequently, while significantly improving the performance of the ViT architecture, we have shown that the pooling layer by considering spatial interaction ratio is essential to a self-attention-based architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Pooling layer of PiT architecture. PiT uses the pooling layer based on depth-wise convolution to achieve channel multiplication and spatial reduction with small parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) represents the model capability of ViT and PiT. At the same computation cost, PiT has a lower train loss than ViT. Using the spatial reduction layers in ViT also improves the capability of architecture. The comparison between training accuracy and validation accuracy shows a significant difference. As shown inFigure 3(b), ViT does not improve validation accuracy even if training accuracy increases. On the other hand, in the case of PiT, validation accuracy increases as training accuracy increases. The big difference in generalization performance causes the performance difference between PiT and ViT as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Attention analysis. We investigate the attention matrix of the self-attention layer. Figure (a) shows the entropy and figure (b) shows the interaction distance. PiT increases the entropy and the distance in shallow layers and decreases in deep layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Architecture configuration. The table shows spatial sizes, number of blocks, number of heads, channel size, and FLOPs of ViT and PiT. The structure of PiT is designed to be as similar as possible to ViT and to have less GPU latency. ImageNet performance comparison with ViT. We compare the performances of ViT and PiT with some training techniques on ImageNet dataset. PiT shows better performance with low computation compared to ViT.</figDesc><table><row><cell>Network</cell><cell>Spatial size</cell><cell># of blocks</cell><cell># of heads</cell><cell>Channel size</cell><cell>FLOPs</cell></row><row><cell cols="2">ViT-Ti [33] 14 x 14</cell><cell>12</cell><cell>3</cell><cell>192</cell><cell>1.3B</cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>2</cell><cell>64</cell><cell></cell></row><row><cell>PiT-Ti</cell><cell>14 x 14</cell><cell>6</cell><cell>4</cell><cell>128</cell><cell>0.7B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>8</cell><cell>256</cell><cell></cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>2</cell><cell>96</cell><cell></cell></row><row><cell>PiT-XS</cell><cell>14 x 14</cell><cell>6</cell><cell>4</cell><cell>192</cell><cell>1.4B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>8</cell><cell>384</cell><cell></cell></row><row><cell cols="2">ViT-S [33] 14 x 14</cell><cell>12</cell><cell>6</cell><cell>384</cell><cell>4.6B</cell></row><row><cell></cell><cell>27 x 27</cell><cell>2</cell><cell>3</cell><cell>144</cell><cell></cell></row><row><cell>PiT-S</cell><cell>14 x 14</cell><cell>6</cell><cell>6</cell><cell>288</cell><cell>2.9B</cell></row><row><cell></cell><cell>7 x 7</cell><cell>4</cell><cell>12</cell><cell>576</cell><cell></cell></row><row><cell>ViT-B [9]</cell><cell>14 x 14</cell><cell>12</cell><cell>12</cell><cell>768</cell><cell>17.6B</cell></row><row><cell></cell><cell>31 x 31</cell><cell>3</cell><cell>4</cell><cell>256</cell><cell></cell></row><row><cell>PiT-B</cell><cell>16 x 16</cell><cell>6</cell><cell>8</cell><cell>512</cell><cell>12.5B</cell></row><row><cell></cell><cell>8 x 8</cell><cell>4</cell><cell>16</cell><cell>1024</cell><cell></cell></row></table><note>smaller FLOPs, parameter size, and GPU latency than those of ViT. We clarify that PiT is not designed with large-scale parameter search such as NAS [25, 3], so PiT can be further improved through a network architecture search.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>ImageNet performance.</figDesc><table><row><cell></cell><cell>We compare our PiT-(Ti, XS, S,</cell></row><row><cell cols="2">and B) models with the counterparts which have a similar number</cell></row><row><cell>of parameters.</cell><cell>? means a model trained with distillation [33].</cell></row><row><cell cols="2">tecture (ViT-B, PiT-B) outperforms the convolutional ar-</cell></row><row><cell cols="2">chitecture. Even in the PiT-S scale, PiT-S shows superior</cell></row><row><cell cols="2">performance than convolutional architecture (ResNet50) or</cell></row><row><cell cols="2">outperforms in throughput (EfficientNet-b3). However, in</cell></row><row><cell cols="2">the case of PiT-Ti, the performance of convolutional ar-</cell></row><row><cell cols="2">chitectures such as ResNet34 [13], MobileNetV3 [19], and</cell></row><row><cell cols="2">EfficientNet-b0 [32] outperforms ViT-Ti and PiT-Ti. Over-</cell></row><row><cell cols="2">all, the transformer architecture shows better performance</cell></row><row><cell cols="2">than the convolutional architecture at the scale of ResNet50</cell></row><row><cell cols="2">or higher, but it is weak at a small scale. Creating a light-</cell></row><row><cell cols="2">weight transformer architecture such as MobileNet is one of</cell></row><row><cell cols="2">the future works of ViT research.</cell></row><row><cell cols="2">Additionally, we conduct experiments on two extended</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>PiT has comparable performance with ViT, but, worse than ViT on throughput. It implies that PiT is designed for 224 ? 224 and the design is not compatible for the large resolution. However, we believe that PiT can outperform ViT with a new layer design for 384 ? 384.</figDesc><table><row><cell>,</cell></row><row><cell>PiT models show comparable performance with ViT mod-</cell></row><row><cell>els on the long training scheme. Although the performance</cell></row><row><cell>improvement is reduced than the Distill? setting, PiTs still</cell></row><row><cell>outperform ViT counterparts in throughput. Fine-tuning on</cell></row><row><cell>large resolution (384 ? 384) is a famous method to train a</cell></row><row><cell>large ViT model with small computation. In the large res-</cell></row><row><cell>olution setting,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>except for the image resolution. Since the original image resolution is too large for transformer-based backbones, we halve the image resolution for training and Standard Occ IN-A [15] BGC [40] FGSM [10] ImageNet robustness benchmarks.</figDesc><table><row><cell>PiT-S</cell><cell>80.8 74.6</cell><cell>21.7</cell><cell>21.0</cell><cell>29.5</cell></row><row><cell>ViT-S [33]</cell><cell>79.8 73.0</cell><cell>19.1</cell><cell>17.6</cell><cell>27.2</cell></row><row><cell>ResNet50 [13]</cell><cell>76.0 52.2</cell><cell>0.0</cell><cell>22.3</cell><cell>7.1</cell></row><row><cell cols="2">ResNet50  ? [38] 79.0 67.1</cell><cell>5.4</cell><cell>32.7</cell><cell>24.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">We compare three</cell></row><row><cell cols="5">comparable architectures, PiT-B, ViT-S, and ResNet50 on var-</cell></row><row><cell cols="5">ious ImageNet robustness benchmarks, including center occlu-</cell></row><row><cell cols="5">sion (Occ), ImageNet-A (IN-A), background challenge (BGC),</cell></row><row><cell cols="5">and fast sign gradient method (FGSM) attack. We evaluate two</cell></row><row><cell cols="5">ResNet50 models from the official PyTorch repository, and the</cell></row><row><cell cols="4">well-optimized implementation [38], denoted as  ?.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) Model capability (b) Generalization performance (c) Model performance</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We thank NAVER AI Lab members for valuable discussion and advice. NSML <ref type="bibr" target="#b19">[20]</ref> has been used for experiments. We thank the reviewers for the productive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inductive bias of deep convolutional networks through pooling geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03236</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5927" to="5935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking channel dimensions for efficient model design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07174</idno>
		<title level="m">Natural adversarial examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuwan</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">NSML: meet the mlaas platform with a real-world case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<idno>abs/1810.09957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Positional normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking natural adversarial examples for classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finetuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stand-alone selfattention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Analyzing the structure of attention in a transformer language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04284</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="108" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09994</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2340" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
