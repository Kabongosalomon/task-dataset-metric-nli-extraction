<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot Audio Source Separation through Query-based Learning from Weakly-labeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
							<email>knutchen@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
							<email>duxingjian.real@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
							<email>zhubilei@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
							<email>mazejun@bytedance.com</email>
							<affiliation key="aff1">
								<orgName type="department">Bytedance AI Lab</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
							<email>sdubnov@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California San Diego</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot Audio Source Separation through Query-based Learning from Weakly-labeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning techniques for separating audio into different sound sources face several challenges. Standard architectures require training separate models for different types of audio sources. Although some universal separators employ a single model to target multiple sources, they have difficulty generalizing to unseen sources. In this paper, we propose a threecomponent pipeline to train a universal audio source separator from a large, but weakly-labeled dataset: AudioSet. First, we propose a transformer-based sound event detection system for processing weakly-labeled training data. Second, we devise a query-based audio separation model that leverages this data for model training. Third, we design a latent embedding processor to encode queries that specify audio targets for separation, allowing for zero-shot generalization. Our approach uses a single model for source separation of multiple sound types, and relies solely on weakly-labeled data for training. In addition, the proposed audio separator can be used in a zero-shot setting, learning to separate types of audio sources that were never seen in training. To evaluate the separation performance, we test our model on MUSDB18, while training on the disjoint AudioSet. We further verify the zero-shot performance by conducting another experiment on audio source types that are held-out from training. The model achieves comparable Source-to-Distortion Ratio (SDR) performance to current supervised models in both cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Audio source separation is a core task in the field of audio processing using artificial intelligence. The goal is to separate one or more individual constituent sources from a single recording of a mixed audio piece. Audio source separation can be applied in various downstream tasks such as audio extraction, audio transcription, and music and speech enhancement. Although there are many successful backbone architectures (e.g. Wave-U-Net, TasNet, D3Net <ref type="bibr" target="#b34">(Stoller, Ewert, and Dixon 2018;</ref><ref type="bibr" target="#b27">Luo and Mesgarani 2018;</ref><ref type="bibr" target="#b37">Takahashi and Mitsufuji 2020)</ref>), fundamental challenges and questions remain: How can the models be made to better generalize to multiple, or even unseen, types of audio sources when super-vised training data is limited? Can large amounts of weaklylabeled data be used to increase generalization performance?</p><p>The first challenge is known as universal source separation, meaning that we only need a single model to separate as many sources as possible. Most models mentioned above require training a full set of model parameters for each target type of audio source. As a result, training these models is both time and memory intensive. There are several heuristic frameworks <ref type="bibr" target="#b31">(Samuel, Ganeshan, and Naradowsky 2020)</ref> that leverage meta-learning to bypass this problem, but they have difficulty generalizing to diverse types of audio sources. In other words, these frameworks succeeded in combining several source separators into one model, but the number of sources is still limited.</p><p>One approach to overcome this challenge is to train a model with an audio separation dataset that contains a very large variety of sound sources. The more sound sources a model can see, the better it will generalize. However, the scarcity of the supervised separation datasets makes this process challenging. Most separation datasets contain only a few source types. For example, MUSDB18 ) and DSD100 ) contain music tracks of only four source types (vocal, drum, bass, and other) with a total duration of 5-10 hours. MedleyDB <ref type="bibr" target="#b1">(Bittner et al. 2014</ref>) contains 82 instrument classes but with a total duration of only 3 hours. There exists some large-scale datasets such as AudioSet <ref type="bibr" target="#b13">(Gemmeke et al. 2017</ref>) and FUSS <ref type="bibr" target="#b44">(Wisdom et al. 2021</ref>), but they contain only weakly-labeled data. AudioSet, for example, contains 2.1 million 10-sec audio samples with 527 sound events. However, only 5% of recordings in Audioset have a localized event label . For the remaining 95% of recordings, the correct occurrence of each labeled sound event can be anywhere within the 10-sec sample. In order to leverage this large and diverse source of weakly-labeled data, we first need to localize the sound event in each audio sample, which is referred as an audio tagging task <ref type="bibr" target="#b10">(Fonseca et al. 2018)</ref>.</p><p>In this paper, as illustrated in <ref type="figure">Figure 1</ref>, we devise a pipeline 1 that comprises of three components: a transformerbased sound event detection system ST-SED for performing time-localization in weakly-labeled training data, a querybased U-Net source separator to be trained from this data,  <ref type="figure">Figure 1</ref>: The architecture of our proposed zero-shot separation system. and a latent source embedding processor that allows generalization to unseen types of audio sources. The ST-SED can localize the correct occurrences of sound events from weakly-labeled audio samples and encode them as latent source embeddings. The separator learns to separate out a target source from an audio mixture given a corresponding target source embedding query, which is produced by the embedding processor. Further, the embedding processor enables zero-shot generalization by forming queries for new audio source types that were unseen at training time. In the experiment, we find that our model can separate unseen types of audio sources, including musical instruments and held-out AudioSet's sound classes, effectively by achieving the SDR performance on par with existing state-of-the-art (SOTA) models. Our contributions are specified as follows:</p><p>? We propose a complete pipeline to leverage weaklylabeled audio data in training audio source separation systems. The results show that our utilization of these data is effective. ? We design a transformer-based sound event detection system ST-SED. It outperforms the SOTA for sound event detection in AudioSet, while achieving a strong localization performance on the weakly-labeled data. ? We employ a single latent source separator for multiple types of audio sources, which saves training time and reduces the number of parameters. Moreover, we experimentally demonstrate that our approach can support zero-shot generalization to unseen types of sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Sound Event Detection and Localization</head><p>The sound event detection task is to classify one or more target sound events in a given audio sample. The localization task, or the audio tagging, further requires the model to output the specific time-range of events on the audio timeline. Currently, the convolutional neural network (CNN) <ref type="bibr" target="#b20">(LeCun et al. 1999</ref>) is being widely used to detect sound events. The Pretrained Audio Neural Networks (PANN) <ref type="bibr" target="#b17">(Kong et al. 2020a</ref>) and the PSLA (Gong, Chung, and Glass 2021b) achieve the current CNN-based SOTA for the sound event detection, with their output featuremaps serving as an empirical probability map of events within the audio timeline. For the transformer-based structure, the latest audio spectrogram transformer (AST) (Gong, Chung, and Glass 2021a) re-purposes the visual transformer structure ViT <ref type="bibr" target="#b9">(Dosovitskiy et al. 2021)</ref> and DeiT <ref type="bibr" target="#b39">(Touvron et al. 2021)</ref> to use the transformer's class-token to predict the sound event. It achieves the best performance on the sound event detection task in AudioSet. However, it cannot directly localize the events because it outputs only a class-token instead of a featuremap. In this paper, we propose a transformer-based model ST-SED to detect and localize the sound event. Moreover, we use the ST-SED to process the weakly-labeled data that is sent downstream into the following separator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Universal Source Separation</head><p>Universal source separation attempts to employ a single model to separate different types of sources. Currently, the query-based model AQMSP <ref type="bibr" target="#b21">(Lee, Choi, and Lee 2019)</ref> and the meta-learning model MetaTasNet <ref type="bibr" target="#b31">(Samuel, Ganeshan, and Naradowsky 2020)</ref> can separate up to four sources in MUSDB18 dataset in the music source separation task. SuDoRM-RF (Tzinis, Wang, and Smaragdis 2020), the Uni-ConvTasNet <ref type="bibr" target="#b16">(Kavalerov et al. 2019)</ref>, the PANN-based separator <ref type="bibr" target="#b19">(Kong et al. 2020b)</ref>, and MSI-DIS  extend the universal source separation to speech separation, environmental source separation, speech enhancement and music separation and synthesis tasks. However, most existing models require a separation dataset with clean sources and mixtures to train, and only support a limited number of sources that are seen in the training set. An ideal universal source separator should separate as many sources as possible even if they are unseen or not clearly defined in the training. In this paper, based on the architecture from <ref type="bibr" target="#b19">(Kong et al. 2020b</ref>), we move further in this direction by proposing a pipeline that can use audio event samples for training a separator that generalizes to diverse and unseen sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology and Pipeline</head><p>In this section, we introduce three components of our source separation model. The sound event detection system is established to refine the weakly-labeled data before it is used by the separation model for training. A query-based source separator is designed to separate audio into different sources.</p><p>Then an embedding processor is proposed to connect the above two components and allows our model to perform separation on unseen types of audio sources.</p><p>(1,1) (1,2)</p><p>(1,t)  </p><formula xml:id="formula_0">(2,1) (2,2) (2,t) (f,1) (f,2) (f,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sound Event Detection System</head><p>In Audioset, each datum is a 10-sec audio sample with multiple sound events. The only accessible label is what sound events this sample contains (i.e., a multi-hot vector). However, we cannot get accurate start and end times for each sound event in a sample. This raises the problem of extracting a clip from a sample where one sound event most likely occurs (e.g., a 2-sec audio clip). As shown in the upper part of <ref type="figure">Figure 1</ref>, a pipeline is depicted by using a sound event detection (SED) system to process the weakly-labeled data. This system is designed to localize a 2-sec audio clip from a 10-sec sample, which will serve as an accurate sound event occurrence.</p><p>In this section, we will first briefly introduce an existing SOTA system: Pretrained Audio Neural Networks (PANN) (Left), which serves as the main model to compare in both sound event detection and localization experiments. Then we introduce our proposed system ST-SED (Middle) that leads to better performance than PANN.</p><p>Pretrained Audio Neural Networks As shown in the left of <ref type="figure" target="#fig_0">Figure 2</ref>, PANN contains VGG-like CNNs <ref type="bibr" target="#b33">(Simonyan and Zisserman 2015)</ref> to convert an audio mel-spectrogram into a (T, C) featuremap, where T is the number of time frames and C is the number of sound event classes. The model averages the featuremap over the time axis to obtain a final probability vector (1, C) and computes the binary cross-entropy loss between it and the groudtruth label.</p><p>Since CNNs can capture the information in each time window, the featuremap (T, C) is empircally regarded as a presence probability map of each sound event at each time frame. When determining the latent source embedding for the following pipeline, the penultimate layer's output (T, L) can be used to obtain its averaged vector (1, L) as the latent source embedding.</p><p>Swin Token-Semantic Transformer for SED The transformer structure <ref type="bibr" target="#b41">(Vaswani et al. 2017</ref>) and the tokensemantic module <ref type="bibr" target="#b12">(Gao et al. 2021</ref>) have been widely used in the image classification and segmentation task and achieve better performance. In this paper, we expect to bring similar improvements to the sound event detection and audio tag-ging task, which then will contribute also to the separation task. As mentioned in the related work, the audio spectrogram transformer (AST) cannot be applied to audio tagging. Therefore, we refer to swin-transformer  in order to propose a swin token-semantic transformer for sound event detection (ST-SED). In the middle of <ref type="figure" target="#fig_0">Figure 2</ref>, a mel-spectrogram is cut into different patch tokens with a patch-embed CNN and sent into the transformer in order. We make the time and frequency lengths of the patch equal as P ? P . Further, to better capture the relationship between frequency bins of the same time frame, we first split the mel-spectrogram into windows w 1 , w 2 , ..., w n and then split the patches in each window. The order of tokens Q follows time?frequency?window as:</p><formula xml:id="formula_1">Q = {q w1 1,1 , q w1 1,2 , ..., q w1 1,t , q w1 2,1 , q w1 2,2 , ..., q w1 2,t , ..., q w1 f,t , q w2 1,1 , q w2 1,2 , ..., q w2 1,t , q w2 2,1 , q w2 2,2 , ..., q w2 2,t , ..., q w2 f,t , q w3 1,1 , ..., q w3 f,t , q w4 1,1 , ..., q w4 f,t , ..., q wn f,t }</formula><p>Where t = T P , f = F P , n is the number of time windows, and q w k i,j denotes the patch in the position shown by <ref type="figure" target="#fig_0">Figure  2</ref>. The patch tokens pass through several network groups, each of which contains several transformer-encoder blocks. Between every two groups, we apply a patch-merge layer to reduce the number of tokens to construct a hierarchical representation. Each transformer-encoder block is a swintransformer block with the shifted window attention module ), a modified self-attention module to improve the training efficiency. As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the shape of the patch tokens is reduced by 8 times from</p><formula xml:id="formula_2">( T P ? F P , D) to ( T 8P ? F 8P , 8D</formula><p>) after 4 network groups. We reshape the final block's output to ( T 8P , F 8P , 8D). Then, we apply a token-semantic 2D-CNN <ref type="bibr" target="#b12">(Gao et al. 2021)</ref> with kernel size (3, F 8P ) and padding size (1, 0) to integrate all frequency bins, meanwhile map the channel size 8D into the sound event classes C. The output ( T 8P , C) is regarded as a featuremap within time frames in a certain resolution. Finally, we average the featuremap as the final vector (1, C) and compute the binary cross-entropy loss with the groundtruth label. Different from traditional visual transformers and AST, our proposed ST-SED does not use the  <ref type="figure">Figure 3</ref>: The mechanism to separate an audio into any given source. We collect N clean clips of the target event. Then we take the average of latent source embeddings as the query embedding e q . The separator receives the embedding then performs the separation on the given audio.</p><p>class-token but the averaged final vector from the tokensemantic layer to indicate the sound event. This makes the localization of sound events available in the output. In the practical scenario, we could use the featuremap ( T 8P , C) to localize sound events. And if we set 8D = L, the averaged vector (1, L) of the featuremap ( T 8P , L) can be used as the latent source embedding in line with PANN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query-based Source Separator</head><p>By SED systems, we can localize the most possible occurrence of a given sound event in an audio sample. Then, as shown in the <ref type="figure">Figure 1</ref>, suppose that we want to localize the sound event s 1 in the sample x 1 and another event s 2 in x 2 , we feed x 1 , x 2 into the SED system to obtain two featuremaps m 1 , m 2 . From m 1 , m 2 we can find the time frame t 1 , t 2 of the maximum probability on s 1 , s 2 , respectively. Finally, we could get two 2-sec clips c 1 , c 2 as the most possible occurrences of s 1 , s 2 by assigning t 1 , t 2 as center frames on two clips, respectively.</p><p>Subsequently, we resend two clips c 1 , c 2 into the SED system to obtain two source embeddings e 1 , e 2 . Each latent source embedding (1, L) is incorporated into the source separation model to specify which source needs to be separated. The incorporation mechanism will be introduced in detail in the following paragraphs.</p><p>After we collect c 1 , c 2 , e 1 , e 2 , we mix two clips as c = c 1 + c 2 with energy normalization. Then we send two training triplets (c, c 1 , e 1 ), (c, c 2 , e 2 ) into the separator f , respectively. We let the separator to learn the following regression:</p><formula xml:id="formula_3">f (c 1 + c 2 , e j ) ? c j , j ? {1, 2}.<label>(1)</label></formula><p>As shown in the right of <ref type="figure" target="#fig_0">Figure 2</ref>, we base on U-Net (Ronneberger, Fischer, and Brox 2015) to construct our source separator, which contains a stack of downsampling and upsampling CNNs. The mixture clip c is converted into the spectrogram by Short-time Fourier Transform (STFT). In each CNN block, the latent source embedding e j is in-corporated by two embedding layers producing two featuremaps and added into the audio featuremaps before passing through the next block. Therefore, the network will learn the relationship between the source embedding and the mixture, and adjust its weights to adapt to the separation of different sources. The output spectrogram of the final CNN block is converted into the separate waveform c by inverse STFT (iSTFT). Suppose that we have n training triplets {(c 1 , c 1 j , e 1 j ), (c 2 , c 2 j , e 2 j ), ..., (c n , c n j , e n j )}, we apply the Mean Absolute Error (MAE) to compute the loss between separate waveforms C = {c 1 , c 2 , ..., c n } and the target source clips C j = {c 1 j , c 2 j , ..., c n j }:</p><formula xml:id="formula_4">M AE(C j , C ) = 1 n n i=0 |c i j ? c i |<label>(2)</label></formula><p>Combining these two components together, we could utilize more datasets (i.e. containing sufficient audio samples but without separation data) in the source separation task. Indeed, it also indicates that we no longer require clean sources and mixtures for the source separation task <ref type="bibr" target="#b19">(Kong et al. 2020b</ref> if we succeed in using these datasets to achieve a good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Learning via Latent Source Embeddings</head><p>The third component, the embedding processor, serves as a communicator between the SED system and the source separator. As shown in <ref type="figure">Figure 1</ref>, during the training, the function of the latent source embedding processor is to obtain the latent source embedding e of given clips c from the SED system, and send the embedding into the separator. And in the inference stage, we enable the processor to utilize this model to separate more sources that are unseen or undefined in the training set.</p><p>Formally, suppose that we need to separate an audio x q according to a query source s q . In order to get the latent source embedding e q , we first need to collect N clean clips of this source {c q1 , c q2 , ..., c qN }. Then we feed them into the SED system to obtain the latent embeddings {e q1 , e q2 , ..., e qN }. The e q is obtained by taking the average of them:</p><formula xml:id="formula_5">e q = 1 N N i=1 e qi<label>(3)</label></formula><p>Then, we use e q as the query for the source s q and separate x q into the target track f (x q , e q ). A visualization of this process is depicted in <ref type="figure">Figure 3</ref>. The 527 classes of Audioset are ranged from ambient natural sounds to human activity sounds. Most of them are not clean sources as they contain other backgrounds and event sounds. After training our model in Audioset, we find that the model is able to achieve a good performance on separating unseen sources. According to <ref type="bibr" target="#b42">(Wang et al. 2019)</ref>, we declare that this follows a Class-Transductive Instance-Inductive (CTII) setting of zero-shot learning <ref type="bibr" target="#b42">(Wang et al. 2019)</ref> as we train the separation model by certain types of sources and use unseen queries to let the model separate unseen sources.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>There are two experimental stages for us to train a zero-shot audio source separator. First, we need to train a SED system as the first component. Then, we train an audio source separator as the second component based on the processed data from the SED system. In the following subsections, we will introduce the experiments in these two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sound Event Detection</head><p>Dataset and Training Details We choose AudioSet to train our sound event detection system ST-SED. It is a largescale collection of over 2 million 10-sec audio samples and labeled with sound events from a set of 527 labels. Following the same training pipeline with (Gong, Chung, and Glass 2021a), we use AudioSet's full-train set (2M samples) for training the ST-SED model and its evaluation set (22K samples) for evaluation. To further evaluate the localization performance, we use DESED test set <ref type="bibr" target="#b32">(Serizel et al. 2020)</ref>, which contains 692 10-sec audio samples with strong labels (time boundaries) of 2765 events in total. All labels in DESED are the subset (10 classes) of AudioSet's sound event classes. In that, we can directly map AudioSet's classes into DESED's classes. There is no overlap between AudioSet's full-train set and DESED test set. And there is no need to use DESED training set because AudioSet's fulltrain set contains more training data. For the pre-processing of audio, all samples are converted to mono as 1 channel by 32kHz sampling rate. To compute STFTs and mel-spectrograms, we use 1024 window size and 320 hop size. As a result, each frame is 320 32000 = 0.01 sec. The number of mel-frequency bins is F = 64. Each 10-sec sample constructs 1000 time frames and we pad them with 24 zero-frames (T = 1024). The shape of the output featuremap is (1024, 527) (C = 527). The patch size is 4 ? 4 and the time window is 256 frames in length. We propose two settings for the ST-SED with a latent dimension size L of 768 or 2048. We adopt the 768-d model to make use of the swin-transformer ImageNet-pretrained model for achieving a potential best result. And we adopt the 2048-d model in the following separation experiment because it shares the consistent latent dimension size with PANN's. We set 4 network groups in the ST-SED, containing 2,2,6, and 2 swintransformer blocks respectively.  We implement the ST-SED in PyTorch 2 , train it with a batch size of 128 and the AdamW optimizer (? 1 =0.9, ? 2 =0.999, eps=1e-8, decay=0.05) (Kingma and Ba 2015) in 8 NVIDIA Tesla V-100 GPUs in parallel. We adopt a warmup schedule by setting the learning rate as 0.05, 0.1, 0.2 in the first three epochs, then the learning rate is halved every ten epochs until it returns to 0.05.</p><p>AudioSet Results Following the standard evaluation pipeline, we use the mean average precision (mAP) to verify the classification performance on Audioset's evaluation set. In <ref type="table" target="#tab_3">Table 1</ref>, we compare the ST-SED with previous SO-TAs including the latest PANN, PSLA, and AST. Among all models, PSLA, AST, and our 768-d ST-SED apply the ImageNet-pretrained models. Specifically, PSLA uses the pretrained EfficientNet (Tan and Le 2019); AST uses the pretrained DeiT; and 768-d ST-SED uses the pretrained swin-transformer in Swin-T/C24 setting 3 . We also provide the mAP result of the 768-d ST-SED without pretraining for comparison. For the 2048-d ST-SED, we train it from zero because there is no pretrained model. For the AST, we compare our model with its single model's report instead of the ensemble one to ensure the fairness of the experiment. All ST-SEDs are converged around 30-40 epochs in about 20 hours' training.</p><p>From <ref type="table" target="#tab_3">Table 1</ref>, we find that the 768-d pretrained ST-SED achieves a new mAP SOTA as 0.467 in Audioset. Moreover, our 768-d and 2048-d ST-SEDs without pretraining can also achieve the pre-SOTA mAP as 0.458 and 0.459, while the AST without pretraining could only achieve a low mAP as 0.368. This indicates that the ST-SED is not limited to the pretraining parameters of the computer vision model, and can be used more flexibly in audio tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DESED Results</head><p>We conduct an experiment on DESED test set to evaluate the localization performance of PANN and the 2048-d ST-SED. We do not include AST and PSLA since AST does not directly support the event localization and the PSLA's code is not published. We use the eventbased F1-score on each class as the evaluation metric, implemented by a Python library psds eval 4 .</p><p>The F1-scores on all 10 classes in DESED by two models are shown in <ref type="table" target="#tab_7">Table 3</ref>. We find that the 2048-d ST-SED achieves better F1-scores on 8 classes and a better average F1-score than PANN. A large increment is on the Frying class as increasing the F1-score by 40.92. However, we  also notice that the F1-scores on Speech class and Cleaner class are dropped when using ST-SED, indicating that there are still some improvements for a better localization performance. From the above experiments, we can conclude that the ST-SED achieves the best sound event detection results and the superior results on localization performance in AudioSet and DESED. These results are sufficient for us to use the 2048-d ST-SED model to conduct the following separation experiments. It is better to evaluate the ST-SED on datasets. Due to the page limit, we leave these as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Source Separation</head><p>Dataset and Training Details We train our audio separator in AudioSet full-train set, validate it in Audioset evaluation set, and evaluate it in MUSDB18 test set as following the 6th community-based Signal Separation Evaluation Campaign (SiSEC 2018). MUSDB18 contains 150 songs with a total duration of 3.5 hours in different genres. Each song provides a mixture track and four original stems: vocal, drum, bass, and other. All SOTAs are trained with MUSDB18 training set (100 songs) and evaluated in its test set (50 songs). Different from these SOTAs, we train our model only with Audioset full-train set other than MUSDB and directly evaluate it in MUSDB18 test set.</p><p>Since Audioset is not a natural separation dataset (i.e., no mixture data), to construct the training set and the validation set, during each training step, we sample two classes from 527 classes and randomly take each sample x 1 , x 2 from two classes in the full-train set. We implement a balanced sampler that all classes will be sampled equally during the whole training. During the validation stage, we follow the same sampling paradigm to construct 5096 audio pairs from Audioset evaluation set and fix these pairs. By setting a fixed random seed, all models will face the same training data and the validation data.</p><p>For the model design, our SED system has two choices: PANN or ST-SED. And the separator we apply comprises 6 encoder blocks and 6 decoder blocks. In encoder blocks, the numbers of channels are namely <ref type="bibr">32, 64, 128, 256, 512, 1024.</ref> In decoder blocks, they are reversed (i.e., from 1024 to 32). There is a final convolution kernel that converts 32 channels into the output audio channel. Batch normalization <ref type="bibr" target="#b15">(Ioffe and Szegedy 2015)</ref> and ReLU non-linearity <ref type="bibr" target="#b0">(Agarap 2018)</ref> are used in each block. The final output is a spectrogram, which can be converted into the final separate audio c by iSTFT. Similarly, we implement our separator in PyTorch and train it with the Adam optimizer (? 1 =0.9, ? 2 =0.999, eps=1e-8, decay=0), the learning rate 0.001 and the batch size of 64 in 8 NVIDIA Tesla V-100 GPUs in parallel.  Evaluation Metrics We use source-to-distortion ratio (SDR) as the metric to evaluate our separator. For the validation set, we compute three SDR metrics between the prediction and the groundtruth in different separation targets:</p><p>? mixture-SDR's target: f (c 1 + c 2 , e j ) ? c j ? clean-SDR's target: f (c j , e j ) ? c j ? silence-SDR's target: f (c ?j , e j ) ? 0</p><p>Where the symbol ?j denotes any clip which does not share the same class with the j-th clip. In our setting, ?1 = 2 and ?2 = 1. The clean SDR is to verify if the model can maintain the clean source given the self latent source embedding.  embedding from ST-SED is better than that from PANN.</p><p>In the training and validation stage, we get each latent source embedding directly from each 2-sec clip according to the pipeline in <ref type="figure">Figure 1</ref>. After picking the best model in the validation set, we follow <ref type="figure">Figure 3</ref> to get the query source embeddings in MUSDB18. Specifically, we collect all separate tracks in the highlight version of MUSDB10 training set (30 secs in each song, 100 songs in total) and take the average of their embeddings on each source as four queries: vocal, drum, bass, and other. <ref type="table" target="#tab_5">Table 2</ref> shows the SDRs of two models in the validation set. We could clearly figure out that when using the 2048-d latent source embedding, PANN achieves better performance in increasing three types of SDR by 2-4 dB than that of 527-d model. A potential reason is that the extra capacity of the 2048-d embedding space helped the model better capture the feature of the sound comparing to the 527-d probability embedding. In that, the model can receive more discriminative embeddings and perform a more accurate separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Separation Results</head><p>Then we pick the best models of 527-d PANN-SEP, 2048d PANN-SEP, 2048-d ST-SED-SEP and evaluate them in MUSDB18. As shown in <ref type="table" target="#tab_9">Table 4</ref>, there are three categories of models: (1) Standard Model: these models can only separate one source, in that they need to train 4 models to separate each source in MUSDB18. (2) Query-based Model: these models can separate four sources in one model. Both models in (1) and (2) require the training data in MUSDB training set and cannot generalize to separate other sources. And (3) Zero-shot Model: our proposed models can separate four sources in one model without any MUSDB18 training data. Additionally, they can even separate more sources. Specifically, for our proposed 2048-d ST-SED model, we repeat the training three times with different random seeds.</p><p>From <ref type="table" target="#tab_9">Table 4</ref> our proposed model 2048-d ST-SED-SEP outperforms PANN-SEP models in all <ref type="bibr">SDRs (6.15,</ref><ref type="bibr">5.44,</ref><ref type="bibr">3.80,</ref><ref type="bibr">3.05)</ref>. The SDRs in vocal, drum, and bass are compatible with standard and query-based SOTAs. However, we observe a relatively low SDR in the "other" source. One possible reason is that we compute a wrong "other" embedding for separation by averaging over all source embedding of "other" in MUSDB18 training set. But this "other" embedding might not be a general embedding because "other" denotes different instruments and timbres in different tracks. Another observation lies in the relatively large standard deviations of all four instruments. One possible reason is that the separation quality is related to the random combination of training data, and different orders may cause differences on some specific types of sounds. One further improving idea is to increase the numbers of combinations (e.g., three instead of two). These sub-topics can be further researched in the future.</p><p>In summary, the most novel and surprising observation is that our proposed audio separator succeeds in separating 4 sources in MUSDB18 test set without any of its training data but only Audioset. The model performs as a zero-shot separator by using any latent source embedding collected from accessible data, to separator any source it faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-Shot Verification</head><p>In this section, we conduct another experiment to let the model separate sources that are held-out from training. We first select 10 sound event classes in Audioset. Then during the training, we remove all data of these 10 classes. The model only learns how to separate clips mixed by the left 517 classes. During the evaluation, we construct 1000 (100?10) mixture samples in Audioset evaluation set whose constituents only belong to these 10 classes. Then we calculate the mixture SDR, the clean SDR, and the silence SDR of them. <ref type="table" target="#tab_11">Table 5</ref> shows the results by the 2048-d ST-SED model. We can find that the model can still separate the held-out sources well by achieving the average mixture SDR, clean SDR, and silence SDR as 8.52 dB, 14.23 dB, and 13.59 dB. The detailed SDR distribution of these 1000 samples is depicted in the open source repository. The intrinsic reason for this good performance is that the SED system captures many features of 517 sound classes in its latent space. And it generalizes to regions of the embedding space it never saw during training, which the unseen 10 classes lie in. Finally, the separator utilizes these features in the embedding to separate the target source. The zero-shot setting of our model is essentially built by a solid feature extraction mechanism and a latent source separator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>In this paper, we propose a zero-shot audio source separator that can utilize weakly-labeled data to train, target different sources to separate, and support more unseen sources. We train our model in Audioset while evaluating it in MUSDB18 test set. The experimental results show that our model outperforms the query-based SOTAs, meanwhile achieves a compatible result with standard supervised models. We further verify our model in a complete zero-shot setting to prove its generalization ability. With our model, more weakly-labeled audio data can be trained for the source separation problem. And more sources can be separated via one model. In future work, since audio embeddings have been widely used in other audio tasks such as music recommendation <ref type="bibr">music generation (2019;</ref><ref type="bibr" target="#b2">2021;</ref>, we expect to use these audio embeddings as source queries to see if they can capture different audio features and lead to better separation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The network architecture of SED systems and the source separator. Left: PANN (Kong et al. 2020a); Middle: our proposed ST-SED; Right: the U-Net-based source separator. All CNNs are named as [2D-kernel size ? channel size].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.. . ......</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>event</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>#1</cell><cell></cell><cell></cell><cell cols="2">#2</cell><cell></cell><cell></cell><cell>#3</cell><cell>#4</cell></row><row><cell></cell><cell>class</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>average presence map</cell><cell>latent embedding</cell><cell>...</cell><cell cols="2">patch tokens</cell><cell></cell><cell>linear</cell><cell></cell><cell cols="2">swin transformer</cell><cell>patch merging</cell><cell cols="2">swin transformer</cell><cell>patch merging</cell><cell>swin transformer</cell><cell>patch merging</cell><cell>swin transformer</cell><cell>reshape</cell></row><row><cell>interpolate linear</cell><cell>avg-pool</cell><cell>...</cell><cell cols="2">patch embed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>... ...</cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell cols="2">(1,1) (1,2)</cell><cell>...</cell><cell>(1,t)</cell><cell></cell><cell>(1,1) (1,2)</cell><cell></cell><cell>(1,t)</cell><cell></cell><cell>avg-pool</cell><cell>token-semantic CNN</cell><cell>embedding layer</cell></row><row><cell></cell><cell></cell><cell>...</cell><cell></cell><cell cols="2">(2,1) (2,2)</cell><cell>...</cell><cell>(2,t)</cell><cell>......</cell><cell>(2,1) (2,2)</cell><cell></cell><cell>(2,t)</cell><cell></cell><cell>average</cell></row><row><cell></cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>...</cell><cell>.. .</cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>...</cell><cell>t)</cell><cell cols="2">(f,1) (f,2)</cell><cell>...</cell><cell>(f,t)</cell><cell></cell><cell>(f,1) (f,2)</cell><cell></cell><cell>(f,t)</cell><cell cols="2">latent embedding</cell><cell>event presence map event class</cell><cell>latent embedding</cell></row><row><cell>log-mel spectrogram</cell><cell></cell><cell cols="7">log-mel spectrogram</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>stft-spec</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>The mAP results in Audioset evaluation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>The SDR performance of different models with different source embeddings in the validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The F1-score results on each class of two models in DESED test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>?.22 5.44 ?.32 3.80 ?.23 3.05 ?.20</figDesc><table><row><cell cols="3">Standard State-of-the-art Model</cell><cell></cell><cell></cell></row><row><cell>Metric -Median SDR</cell><cell>vocal</cell><cell>drum</cell><cell>bass</cell><cell>other</cell></row><row><cell>WaveNet (2019)</cell><cell>3.25</cell><cell>4.22</cell><cell>3.21</cell><cell>2.25</cell></row><row><cell>WK (2014)</cell><cell>3.76</cell><cell>4.00</cell><cell>2.94</cell><cell>2.43</cell></row><row><cell>RGT1 (2018)</cell><cell>3.85</cell><cell>3.44</cell><cell>2.70</cell><cell>2.63</cell></row><row><cell>Spec-U-Net (2018)</cell><cell>5.74</cell><cell>4.66</cell><cell>3.67</cell><cell>3.40</cell></row><row><cell>UHL2 (2017)</cell><cell>5.93</cell><cell>5.92</cell><cell>5.03</cell><cell>4.19</cell></row><row><cell>MMDenseLSTM (2018)</cell><cell>6.60</cell><cell>6.41</cell><cell>5.16</cell><cell>4.15</cell></row><row><cell>Open Unmix (2019)</cell><cell>6.32</cell><cell>5.73</cell><cell>5.23</cell><cell>4.02</cell></row><row><cell>Demucs (2019)</cell><cell>6.21</cell><cell>6.50</cell><cell>6.21</cell><cell>3.80</cell></row><row><cell cols="4">Query-based Model w/. MUSDB18 Training</cell><cell></cell></row><row><cell>Metric -Median SDR</cell><cell>vocal</cell><cell>drum</cell><cell>bass</cell><cell>other</cell></row><row><cell>AQMSP-Mean (2019)</cell><cell>4.90</cell><cell>4.34</cell><cell>3.09</cell><cell>3.16</cell></row><row><cell>Meta-TasNet (2020)</cell><cell>6.40</cell><cell>5.91</cell><cell>5.58</cell><cell>4.19</cell></row><row><cell cols="4">Zero-shot Model w/o. MUSDB18 Training</cell><cell></cell></row><row><cell>Metric -Median SDR</cell><cell>vocal</cell><cell>drum</cell><cell>bass</cell><cell>other</cell></row><row><cell>527-d PANN-SEP</cell><cell>4.16</cell><cell>0.95</cell><cell>-0.86</cell><cell>-2.65</cell></row><row><cell>2048-d PANN-SEP</cell><cell>6.06</cell><cell>5.00</cell><cell>3.38</cell><cell>2.86</cell></row><row><cell>2048-d ST-SED-SEP</cell><cell>6.15</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>The SDR performance in MUSDB18 test set. All models are categorized into three slots.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>The silence SDR is to verify if the model can separate nothing if there is no target source in the given audio. These help us understand if the model can be generalized to more general separation scenarios only by using the mixture training. For the testing, we only compute the mixture SDR between each stem and each original song in MUSDB18 test set. Each song is divided into 1-sec clips. The song's SDR is the median SDR over all clips. And the final SDR is the median SDR over all songs.</figDesc><table><row><cell>Class</cell><cell cols="3">Conversation Whispering Clapping</cell><cell>Cat</cell><cell cols="7">Orchestra Aircraft Medium Engine Pour Scratch Creak Average</cell></row><row><cell>Mixture-SDR</cell><cell>9.08</cell><cell>8.04</cell><cell>9.67</cell><cell>9.49</cell><cell>9.18</cell><cell>8.47</cell><cell>8.31</cell><cell>7.92</cell><cell>8.42</cell><cell>6.56</cell><cell>8.52</cell></row><row><cell>Clean-SDR</cell><cell>17.44</cell><cell>10.50</cell><cell>17.78</cell><cell>15.01</cell><cell>10.06</cell><cell>13.09</cell><cell>14.85</cell><cell>14.28</cell><cell>15.52</cell><cell>13.79</cell><cell>14.23</cell></row><row><cell>Silence-SDR</cell><cell>14.05</cell><cell>13.86</cell><cell>14.45</cell><cell>17.63</cell><cell>12.08</cell><cell>11.97</cell><cell>11.56</cell><cell>12.76</cell><cell>13.95</cell><cell>13.61</cell><cell>13.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">The Choice of Source Embeddings We choose three</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">source embeddings for our separator: (1) the 527-d pres-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ence probability vector from PANN, referring to (Kong</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">et al. 2020b); (2) the 2048-d latent embedding from PANN's</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">penultimate layer; and (3) the 2048-d latent embedding from</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">ST-SED. This helps to verify if the latent source embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">can perform a better representation for separation, and if the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>The SDR performance of the 2048-d ST-SED-SEP in the zero-shot verification experiment.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://pytorch.org/ 3 https://github.com/microsoft/Swin-Transformer 4 https://github.com/audioanalytic/psds eval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep Learning using Rectified Linear Units (ReLU)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cannam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Society for Music Information Retrieval Conference</title>
		<meeting>the 15th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Controllable Monophonic Music Generation Via Latent Variable Disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Master Thesis Archive</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Audio Embeddings with User Listening Data for Content-Based Music Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="3015" to="3019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Society for Music Information Retrieval Conference</title>
		<meeting>the 21th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous Melody Generation via Disentangled Short-Term Representations and Structural Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Conference on Semantic Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multilayer Music Representation and Processing, MMRP 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Demucs: Deep Extractor for Music Sources with extra unlabeled data remixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01174</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MusPy: A Toolkit for Symbolic Music Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Society for Music Information Retrieval Conference</title>
		<meeting>the 21th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR 2021. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09902</idno>
		<title level="m">General-purpose Tagging of Freesound Audio with AudioSet Labels: Task Description, Dataset, and Baseline</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Deep Residual Network for Large-Scale Acoustic Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Interspeech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2568" to="2572" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Premier International Computer Vision Event, ICCV 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Annual Conference of the International Speech Communication Association, Interspeech 2021</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
	<note>IEEE. Gong, Y</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Benefit of Temporally-Strong Labels in Audio Event Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="366" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R. ; D P</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="175" to="179" />
		</imprint>
	</monogr>
	<note>3rd International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09971</idno>
		<title level="m">Speech enhancement with weakly labelled data from AudioSet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Source Separation with Weakly Labelled Data: an Approach to Computational Auditory Scene Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="101" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object Recognition with Gradient-Based Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1681</biblScope>
			<biblScope unit="page">319</biblScope>
		</imprint>
	</monogr>
	<note>Contour and Grouping in Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audio Query-based Music Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019</title>
		<meeting>the 20th International Society for Music Information Retrieval Conference, ISMIR 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="878" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified model for zero-shot music source separation, transcription and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Society for Music Information Retrieval Conference</title>
		<meeting>the 22nd International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="381" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Denoising Auto-Encoder with Recurrent Skip Connections and Residual Regression for Music Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Machine Learning and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="773" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The 2016 Signal Separation Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rivet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fontecave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation -12th International Conference, LVA/ICA 2015</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-End Music Source Separation: Is it Possible in the Waveform Domain?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Llu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Interspeech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4619" to="4623" />
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving Single-Network Single-Channel Separation of Musical Audio with Convolutional Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Latent Variable Analysis and Signal Separation -14th International Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10891</biblScope>
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
	<note>LVA/</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-Learning Extractors for Music Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="816" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sound Event Detection in Synthetic Domestic Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ewert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="334" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open-Unmix -A Reference Implementation for Music Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>St?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mmdenselstm: An Efficient Combination of Convolutional and Recurrent Neural Networks for Audio Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Workshop on Acoustic Signal Enhancement</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="106" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">D3Net: Densely connected multidilated DenseNet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving music source separation based on deep neural networks through data augmentation and network blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Porcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Giron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enenkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
	<note>International Conference on Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Survey of Zero-Shot Learning: Settings, Methods, and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural networks for single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Global Conference on Signal and Information Processing</title>
		<meeting><address><addrLine>GlobalSIP</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="577" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What&apos;s all the Fuss about Free Universal Sound Separation Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="186" to="190" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
