<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A NOVEL 1D STATE SPACE FOR EFFICIENT MUSIC RHYTHMIC ANALYSIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mojtaba</forename><surname>Heydari</surname></persName>
							<email>mheydari@ur.rochester.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mccallum</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Ehmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Pandora Media, Inc</orgName>
								<address>
									<settlement>Oakland</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A NOVEL 1D STATE SPACE FOR EFFICIENT MUSIC RHYTHMIC ANALYSIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-State space</term>
					<term>semi-Markov process</term>
					<term>jump- back reward</term>
					<term>inference optimization</term>
					<term>music time structure analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring music time structures has a broad range of applications in music production, processing and analysis. Scholars have proposed various methods to analyze different aspects of time structures, such as beat, downbeat, tempo and meter. Many state-of-the-art (SOFA) methods, however, are computationally expensive. This makes them inapplicable in realworld industrial settings where the scale of the music collections can be millions. This paper proposes a new state space and a semi-Markov model for music time structure analysis. The proposed approach turns the commonly used 2D state spaces into a 1D model through a jump-back reward strategy. It reduces the state spaces size drastically. We then utilize the proposed method for causal, joint beat, downbeat, tempo, and meter tracking, and compare it against several previous methods. The proposed method delivers similar performance with the SOFA joint causal models with a much smaller state space and a more than 30 times speedup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Time is a fundamental concept in music. Automatic analysis of music time structures enables many applications in music generation, manipulation, and recommendation. As an example, such analysis is essential for tasks like music-score alignment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and music transcription <ref type="bibr" target="#b2">[3]</ref>. Several approaches have been proposed to extract music rhythmic parameters such as tempo, meter, beat, and downbeat. Although SOFA models for some of the mentioned tasks are promising, computational efficiency considerations make many of them not applicable in massive industry-scale settings. Furthermore, recent advancements of augmented and virtual reality and their interactive applications demands real-time processing, in which computational efficiency is a critical factor.</p><p>Many models attempt to estimate music rhythmic parameters separately e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> . However, due to the underlying ? Main work is accomplished as a research intern at Pandora Media Inc. This work is partially funded by National Science Foundation grant <ref type="bibr">No. 1846184.</ref> inter-dependencies among these parameters, joint approaches have demonstrated promising results as well <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Joint models often employ state spaces to model these interdependencies, and use some probabilistic models such as Bayesian models to infer the parameters of interest. As rhythmic parameters are intrinsically continuous variables, the state spaces are ideally continuous or discretized with a fine granularity. Hence, the inference can be computationally expensive when the state spaces are high dimensional.</p><p>To make the inference stages tractable, some works e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> used numeric approaches such as particle filtering (PF) to approximate the probability of the states in continuous state spaces. Another set of approaches e.g., <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> discretize the state space to deal with a limited number of hidden variables during the inference. Discretizing the state spaces makes it possible to use some efficient inference algorithms such as the forward algorithm to compute posterior probability of the hidden states.</p><p>In our previous work <ref type="bibr" target="#b7">[8]</ref>, we demonstrated that the combination of the ideas mentioned above can deliver the SOFA performance for causal and joint time structure analysis. It utilized a cascade of discrete state spaces and an enhanced PF inference strategy using a proposed information gate technique to make the inference faster.</p><p>In contrast to the above-mentioned Bayesian methods that assume the Markovian property, in this paper, we introduce a compact 1D state space and a semi-Markov jump-back reward technique where the transition model is time-variant. Such variance across time is likely to make the inference process more complex, in return, it makes the model much more efficient. Although semi-Markov processes are used before for some specific tasks, e.g., score-alignment <ref type="bibr" target="#b0">[1]</ref>, here we propose a generic model that can be utilized as an efficient alternative for the well-known bar pointer models to improve the efficiency on several music rhythmic analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">APPROACH</head><p>In this section, we describe the jump-back reward strategy. To elaborate it, we first describe the bar pointer model <ref type="bibr" target="#b13">[14]</ref> and its more efficient version <ref type="bibr" target="#b14">[15]</ref>. Then, we present the new model and demonstrate how it covers the same parameters i.e., tempo range and time resolution, with much fewer states. arXiv:2111.00704v2 [cs.SD] 20 Feb 2022</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bar pointer models</head><p>The bar pointer model for joint meter, tempo, and rhythmic pattern inference was proposed by Whiteley et al. <ref type="bibr" target="#b13">[14]</ref> to model the dynamics of a bar pointer that moves through a 2D state space throughout a piece of music. The two dimensions of the state space represent relative positions within a bar ? k ? {1, 2, ..., M } with M being the last position, and the tempo (in the unit of frames per bar) ? k ? {? min ,? min+1 , ...,? max }, in the k-th audio frame. This state space is discretized into even grids along both dimensions, assigning the equal number of positions per bar for different tempi, leading to different time resolutions for them.</p><p>Under the same framework, assuming a known and fixed meter, Krebs et al. <ref type="bibr" target="#b14">[15]</ref> proposed a different discretization strategy for the state space, where the number of positions within a bar M (T ) depends on the tempo T (in BPM) as:</p><formula xml:id="formula_0">M (T ) = Round( B ? 60 T ? ? ),<label>(1)</label></formula><p>where B is the number of beats per bar that is fixed and known, and ? is the frame length in seconds. By assigning fewer positions for higher tempi, this model ensures the same time resolution for different tempi. This makes the inference more efficient hence is called the efficient bar pointer model. Similarly, for beat tracking alone, a beat pointer model and its efficient counterpart can be derived, by replacing the horizontal axis with the relative positions within a beat.</p><p>When the meter is unknown, however, one limitation of the efficient bar pointer model is that the inference needs to be performed on multiple state spaces independently, making the total state space much larger. For instance, given the tempo axis ranging from 55 BPM to 215 BPM and the audio frame hop size of 20 ms, if a bar may contain 2 to 6 beats, then a total of 28,980 states will be needed counting all of the 5 independent state spaces. This problem is addressed by some works such as the BeatNet <ref type="bibr" target="#b7">[8]</ref>, using a cascade model of two separate state spaces for music beat-tempo tracking and downbeat-meter tracking, respectively. This allows to consider different bar lengths in a single state space, and shrinks the total state space size from 28,980 states down to 1,469 states for the example above. <ref type="figure" target="#fig_1">Fig. 1a</ref> and 1c demonstrate the state space of the efficient beat pointer model. and the downbeat state space to cover a bar length from 2 to 6 beats for the cascade model. The disadvantage of the cascade model is that the inference of downbeat and meter depends on the inference of beat and tempo. In other words, errors in beat and tempo tracking will be propagated to downbeat and meter tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Proposed 1-D state space</head><p>The main idea of the proposed approach is to modify the 2D bar pointer or beat pointer state space into a compact 1D space that covers the same tempo or meter range and time resolution but with much fewer states. This is achieved by re-defining the position dimension and replacing the tempo dimension with a jump-back strategy.</p><p>Specifically, taking the beat space as an example, instead of defining the position dimension as relative positions within a beat, the proposed 1D state space defines it as audio frame indices within a beat interval. The first index corresponds to the beat position, while the last corresponds to the last frame within the beat for the smallest possible tempo. As an audio frame arrives, a pointer hypothesis moves along this dimension one step to the right. It jumps back to the left end when it believes that a new beat arrives. As the tempo is unknown, there is uncertainty on when jump-back should happen. We hence use a probability distribution ?(? k ) to describe this uncertainty, where ? k is the position (i.e., the frame index within a beat) of the k-th audio frame. This is illustrated in <ref type="figure" target="#fig_1">Fig. 1b</ref>.</p><p>The key of this jump-back operation is that it circulates back some probability mass of the pointer's position distribution to the left end (i.e, the beat position), which is then gradually transported to the right as new frames arrive. When the music demonstrates regular beats, the jump-back probability will show a strong peak at the frame index corresponding to the beat interval, and the posterior distribution of the pointer position will be reinforced to show a significant peak that moves forward but circles back after each beat interval.  <ref type="figure" target="#fig_1">Fig. 1b</ref>. The number of states in <ref type="figure" target="#fig_1">Fig. 1b</ref> is equal to the number of frames within one beat interval for the smallest possible tempo. By taking the same example used in <ref type="figure" target="#fig_1">Fig. 1a</ref>, this number is 55. Therefore, the number of states is reduced from 1,449 in <ref type="figure" target="#fig_1">Fig. 1a</ref> to 55 in <ref type="figure" target="#fig_1">Fig. 1b</ref>. This 1D state space can also be constructed for the downbeat (bar) state space following the same logic. <ref type="figure" target="#fig_1">Fig. 1c</ref> shows the bar state space in the cascade approach <ref type="bibr" target="#b7">[8]</ref>, where the horizontal axis is relative positions within a bar at the granularity of a beat, and the vertical axis is the bar length. <ref type="figure" target="#fig_1">Fig. 1d</ref> reduces this space into 1D with only the position dimension; the bar length range and the position granularity are kept the same as those in <ref type="figure" target="#fig_1">Fig. 1c</ref>. It is clear that the number of states reduces from 20 in <ref type="figure" target="#fig_1">Fig. 1c</ref> to 6 in <ref type="figure" target="#fig_1">Fig. 1d</ref>.</p><p>In addition to assisting with computational cost reduction, fewer states may help increase the accuracy, given that the system should infer the correct states out of fewer hypotheses. Note that the new state space includes a vector of jumpback weights corresponding to the beat/bar positions, and the position that achieves the maximum one represents the local tempo/meter. The weights are updated based on the rewardpunishment mechanism discussed in the next session.  <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> beats. a) The efficient beat pointer state space <ref type="bibr" target="#b14">[15]</ref> for beat-tempo tracking. b) The proposed 1D state space with jump-backs for beat-tempo tracking. c) The cascade state space for downbeat-meter tracking <ref type="bibr" target="#b7">[8]</ref>. d) The proposed 1D state space with jump-backs for downbeat-meter tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Inference</head><p>This section describes an approach to incorporating the proposed state space into the HMM process for online music rhythmic analysis tasks in which the inference cost and speed are crucial factors. Given that the proposed state space is much smaller than previous ones, we compute the pointer's posterior probability exactly instead of approximating it using Monte Carlo PF <ref type="bibr" target="#b7">[8]</ref>. In the following, we describe the computational process for the beat state space. The process for the downbeat state space is the same, and is omitted here to save space.</p><p>Let ? k and y k denote the latent state and observation at frame k, respectively. Suppose the position posterior p(? k |y 1:k ) is already estimated, then a "predict-update" iterative procedure can be used to compute the next frame's position posterior p(? k+1 |y 1:k+1 ). First, a one-step-ahead prediction is computed as:</p><formula xml:id="formula_1">p(? k+1 |y 1:k ) = ? k p(? k+1 |? k )p(? k |y 1:k ),<label>(2)</label></formula><p>where p(? k+1 |? k ) is the state transition probability:</p><formula xml:id="formula_2">p(? k+1 |? k ) = ? ? ? ?(? k ) if ? k+1 = 1 1 ? ?(? k ) if ? k+1 = ? k + 1 0 otherwise ,<label>(3)</label></formula><p>where ?(? k ) is the normalized jump-back probability for the pointer to jump from position ? k back to the beat state. The pointer can also move one step to the right with the probability of 1 ? ?(? k ), but no other transition is allowed.</p><p>Given the tempo range [T min , T max ], the jump-back would only happen between states M (T max ) and M (T min ), where the former is the least possible number of frames within a beat and the latter is the maximum possible number of frames. In other words, ?(? k ) = 0 for ? k ? {1, 2, ..., M (T max ) ? 1}. Also, we set ?(M (T min )) = 1 to guarantee that the probability mass never exceeds the last possible state. For the rest of the states i.e., ? k?1 ? {M (T max ), M (T max ) + 1, ..., M (T min ) ? 1}, jump-back probabilities require updating.</p><p>Then the update step absorbs the newly observed audio frame as follows:</p><formula xml:id="formula_3">p(? k+1 |y 1:k+1 ) = 1 Z k+1 p(y k+1 |? k+1 )p(? k+1 |y 1:k ),<label>(4)</label></formula><p>where Z k+1 is the normalization constant, and the observation likelihood p(y k+1 |? k+1 ) is defined as:</p><formula xml:id="formula_4">p(y k+1 |? k+1 ) ? b k+1 if ? k+1 = 1 and b k+1 ? T otherwise ,<label>(5)</label></formula><p>where the first branch is for the beat state, i.e., the pointer position ? k is at the left end of <ref type="figure" target="#fig_1">Fig. 1b</ref>. We also use a threshold T to omit frames that have too low beat activation b k , which is computed from audio features using a neural network <ref type="bibr" target="#b7">[8]</ref> or other models. The second branch is for the other states, and a small constant is assigned as the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Jump-back reward strategy</head><p>In this section, we introduce a method to update the jumpback probability vector. It is noted that jump back technique should not be confused with back tracking operation that is included in some dynamic programming offline inferences such as that of Ellis <ref type="bibr" target="#b17">[18]</ref>. The transition model used in the prediction stage returns a portion of the position probability to the beat state. For the update step, when the beat activation is larger than the threshold, it increases the position probability of the beat state, and decreases that of the other states. For ? k+1 ? {M (T max ) : M (T min ) ? 1}, updating the jump-back probabilities is accomplished through an iterative equation with a forgetting factor ? at each frame using an update signal denoted by ?(? k+1 ):</p><formula xml:id="formula_5">?(? k+1 ) = ??(? k ) + (1 ? ?)?(? k+1 ), where (6) ?(? k+1 ) = ? ? ? ? ? ? ? ? ? ? ? p(? k+1 |y 1:k ) ? p(? k+1 |y 1:k+1 ) if b k+1 ? T ?(p(? k |y 1:k ) ? p(? k+1 |y 1:k )) if b k+1 &lt; T and ? k+1 = 1 0 otherwise .<label>(7)</label></formula><p>The first branch corresponds to the situation where the (k+1)th frame is likely a beat. It computes the amount of probability changes in the update step, which are probability decreases for non-beat states. This probability change would be informative in detecting the main loop (tempo), since the higher contrast between before and after update stage indicates that from which state the majority of the probability mass (frame phase) loops back. The second branch corresponds to the situation where the k + 1-th frame is not likely a beat. It is the negative amount of probability mass that is jumped back to the beat state in the prediction step. This wrong jump back would have been avoided in an ideal case, and its negative value serves as a punishment for the wrong jumps. The source code and video demos of the implementation are available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EVALUATION</head><p>To demonstrate the capability of the 1D state space, we implemented a causal joint beat and downbeat tracking system using the proposed model. It is important to note that the model does not require tempo and meter prior knowledge and decodes them as well. The beat and downbeat activations are obtained from the pre-trained neural networks in our previous BeatNet work <ref type="bibr" target="#b7">[8]</ref>. Also, The states' probabilities and jump-back probabilities are initialized randomly. Following the SOFA online methods, we employ the GTZAN dataset to evaluate the performance of the proposed inference model. It is a comprehensive dataset including 1000 excerpts from 10 different music genres. Another reason that makes it more suitable for the evaluation is that it was entirely unseen during the training stage of all reported supervised models. <ref type="table" target="#tab_0">Table 1</ref> illustrates the beat and downbeat F-measure performance of several online methods. To demonstrate the efficiency of the proposed model, the processing time of each model is reported as well. The reported numbers are the average computational time for 30-second music excerpts of the GTZAN dataset. Note that we measure the speed of all methods on the same Windows machine with an AMD Ryzen 9 3900X CPU and 3.80 GHz clock. For DLB <ref type="bibr" target="#b4">[5]</ref> and Beat-Net <ref type="bibr" target="#b7">[8]</ref>, their paper-reported default number of particles is used, which is 1000 and 1750, respectively. Among all of the methods in <ref type="table" target="#tab_0">Table 1</ref>, the new model and BeatNet <ref type="bibr" target="#b7">[8]</ref> are joint beat and downbeat tracking approaches, and the rest are only beat tracking models. Also, except IBT <ref type="bibr" target="#b5">[6]</ref> and Aubio <ref type="bibr" target="#b18">[19]</ref>, all the other models are supervised, and they leverage deep neural networks to extract beat and/or downbeat activations. <ref type="table" target="#tab_0">Table 1</ref> demonstrates that the proposed model can deliver the same beat tracking F-measure performance as the BeatNet <ref type="bibr" target="#b7">[8]</ref>, which is the SOFA online approach. However, its downbeat performance is moderately lower than that of the BeatNet. The primary point is that using the proposed 1D state space and the jump-back reward technique leads to a more than 30x speedup than BeatNet, making it much more suitable for large scale industrial usages.</p><p>Another interesting observation is that Aubio <ref type="bibr" target="#b18">[19]</ref> is even faster than the proposed model. The reason is that it is a classical signal processing model and it does not use Bayesian temporal decoding at the inference stage. Its main drawback, however, is its performance which is the lowest and suffering from the common issues of the sliding window approaches <ref type="bibr" target="#b5">[6]</ref>. DLB <ref type="bibr" target="#b4">[5]</ref>, on the other hand, addresses the sliding window issues and delivers much better results, but it is the slowest model. Finally, comparing the proposed model with B?ck FF <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, which also performs exact inference instead of using a sampling approach, we see that the proposed method achieves a similar F-measure on beat tracking and a 7.5x speedup, even though the proposed model is a multi-task approach that also performs downbeat tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORKS</head><p>In this paper, we introduced a novel semi-Markov state space and a jump-back reward technique to reduce the computational cost of music rhythmic analysis tasks. This new state space is much smaller than the previous efficient space in the literature. We also implemented an online model to infer several music rhythmic parameters jointly. We showed that using the new state space along with the new inference process delivers the SOFA results for beat tracking and comparable results for downbeat tracking with a drastically faster speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1a illustrates</head><label></label><figDesc>the efficient beat pointer state space [15] that includes multiple rows corresponding to different tempi from M (T min ) = 55 (frames per beat interval) in the bottom row to M (T max ) = 14 (frames per beat interval) in the top one. The proposed state space, in contrast, is 1D as demonstrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Comparison of the state spaces for music time analysis for a tempo range of [55, 215] BPM, a frame hop size of 20 ms, and a bar length of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance and speed comparison of several online beat and downbeat tracking models on the GTZAN.</figDesc><table><row><cell>Method</cell><cell cols="3">F-Measure F-Measure Comp. Time</cell></row><row><cell></cell><cell>Beats</cell><cell>Downbeats</cell><cell>(Seconds)</cell></row><row><cell>Aubio [19]</cell><cell>57.09</cell><cell>-</cell><cell>0.1</cell></row><row><cell>BeatNet [8]</cell><cell>75.44</cell><cell>46.49</cell><cell>8.87</cell></row><row><cell>B?ck ACF [22]</cell><cell>64.63</cell><cell>-</cell><cell>7.01</cell></row><row><cell>B?ck FF [20]</cell><cell>74.18</cell><cell>-</cell><cell>2.19</cell></row><row><cell>DLB [5]</cell><cell>73.77</cell><cell>-</cell><cell>21.30</cell></row><row><cell>IBT [6]</cell><cell>68.99</cell><cell>-</cell><cell>4.89</cell></row><row><cell>1D state space</cell><cell>76.48</cell><cell>42.57</cell><cell>0.29</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mjhydri/1D-StateSpace</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A coupled duration-focused architecture for real-time music-to-score alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="974" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A state space model for online polyphonic audio-score alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-local musical statistics as guides for audio-to-score piano transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoshii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">566</biblScope>
			<biblScope unit="page" from="262" to="280" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust downbeat tracking using an ensemble of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Don&apos;t Look Back: An online beat tracking method using rnn and enhanced particle filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">IBT: A real-time tempo and beat tracking system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Reis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 11th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint beat and downbeat tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 17th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BeatNet: CRNN and particle filtering for online joint beat downbeat and meter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heydari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cwitkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 22th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous beat and downbeat-tracking using a probabilistic framework: Theory and large-scale evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Particle filters for efficient meter tracking with dynamic bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Intl. Conf. on Music Information Retrieval (ISMIR)</title>
		<meeting>of the 16th Intl. Conf. on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beat tracking with particle filtering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hainsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macleod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Applications of Sienal Proc. to Audio and Acoust</title>
		<meeting>IEEE Workshop on Applications of Sienal . to Audio and Acoust</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monte carlo methods for tempo tracking and rhythm quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="45" to="81" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequential inference of rhythmic structure in musical audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Whiteley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian modelling of temporal structure in musical audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Whiteley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cemgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 7th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An efficient statespace model for joint tempo and meter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 16th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rhythmic pattern modeling for beat and downbeat tracking in musical audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 14th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="227" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meter inference in a culturally diverse music corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holzapfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivasamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 18th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beat tracking by dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic annotation of musical audio for interactive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Brossier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="58" to="102" />
			<pubPlace>London, UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Queen Marry University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multi-model approach to beat tracking considering heterogeneous music styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Intl. Conf. on Music Information Retrieval</title>
		<meeting>of the 15th Intl. Conf. on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="603" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ROBOD: a real-time online beat and offbeat drummer sebastian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>P?ll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balsyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE signal processing cup</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced beat tracking with context-aware neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>B?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 14th International Conference on Digital Audio Effects</title>
		<meeting>of the 14th International Conference on Digital Audio Effects</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="135" to="139" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
