<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Posterior Re-calibration for Imbalanced Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjiao</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
							<email>ycliu@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Glaser</surname></persName>
							<email>nglaser@gatech.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
							<email>yenchang.hsu@gatech.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Posterior Re-calibration for Imbalanced Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Networks can perform poorly when the training label distribution is heavily imbalanced, as well as when the testing data differs from the training distribution. In order to deal with shift in the testing label distribution, which imbalance causes, we motivate the problem from the perspective of an optimal Bayes classifier and derive a post-training prior rebalancing technique that can be solved through a KL-divergence based optimization. This method allows a flexible post-training hyper-parameter to be efficiently tuned on a validation set and effectively modify the classifier margin to deal with this imbalance. We further combine this method with existing likelihood shift methods, re-interpreting them from the same Bayesian perspective, and demonstrating that our method can deal with both problems in a unified way. The resulting algorithm can be conveniently used on probabilistic classification problems agnostic to underlying architectures. Our results on six different datasets and five different architectures show state of art accuracy, including on large-scale imbalanced datasets such as iNaturalist for classification and Synthia for semantic segmentation. Please see https://github.com/GT-RIPL/UNO-IC.git for implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Applications of deep learning algorithms in the real world have fueled new interest in research beyond well-constructed datasets where the classes are balanced and the training distribution faithfully reflects the true (testing) distribution.</p><p>However, it is unlikely that one can anticipate all possible scenarios and construct well-curated datasets consistently. Therefore it is important to study robust algorithms that can perform well with such imbalanced datasets and unseen situations during testing. The aforementioned problems can be categorized as distributional shift between the training and testing conditions, specifically:label prior shift and non-semantic likelihood shift.</p><p>In this paper, we focus on label prior shift, which can arise when i) the training class distribution does not match the true (test) class distribution due to the inherent difficulty in obtaining samples from certain classes, or ii) the distribution of classes does not reflect their relative importance. For example, the natural world is inherently imbalanced such that a collected dataset may exhibit a long-tailed distribution. Similarly, in semantic segmentation, the pixel percentage of pedestrains does not reflect their importance in prediction (e.g. for safety).</p><p>In order to deal with such shift in the testing label distribution, which imbalance causes, a number of methods have been developed such as class-balanced loss (CB) <ref type="bibr" target="#b0">[1]</ref> and Deferred Reweighting (DRW) <ref type="bibr" target="#b1">[2]</ref>, as well as methods that scale the margin in a fixed way as a function of the amount of data per class through a label-distribution-aware loss (LDAM) <ref type="bibr" target="#b1">[2]</ref> . However, such methods do not allow for a flexibly trade-off between precision and recall across the classes, and require re-training to target a testing label distribution.</p><p>In this paper, we instead motivate the problem from the perspective of an optimal Bayes classifier and derive a rebalanced posterior by introducing test priors and showing that it is the optimal Bayes classifier on the test distribution under ideal conditions. To compensate for imperfect learning in practice, an approximation can be solved through a KL-divergence based optimization by treating the rebalanced and original posteriors as approximations to a true posterior.</p><p>This method, which does not require re-training the original imbalanced classifiers, allows a flexible hyper-parameter to be efficiently tuned on a validation set and effectively modify the classifier margin to target a desired test label distribution. We further combine this method with existing methods dealing with non-semantic likelihood shift, which occurs when the representation of samples from a certain class changes due to changes in lighting, weather, sensor noise, modality, etc. We re-interpret these methods from the same Bayesian perspective, and demonstrate that our method can deal with both problems in a unified way.</p><p>We demonstrate our method on six datasets and five different neural network architectures, across two different imbalanced tasks: classification and semantic segmentation. Using a toy dataset that allows for visualizations of the classifier margins, we show that our method effectively shifts the decision boundary away from the minority class towards the over-represented class. We then demonstrate that our method can achieve state of the art results on imbalanced variants of CIFAR-10 and CIFAR-100, and can scale to larger datasets such as iNaturalist which has extreme imbalance. For semantic segmentation, which is an inherently imbalanced task, we show that our unified method can support both imbalance and likelihood shift in the form of unknown weather conditions encountered during testing (but not during training).</p><p>In summary, the main contributions of the paper are the following:</p><p>? We derive a principled imbalance calibration algorithm for models trained on imbalanced datasets. The method requires no re-training to target new test label distributions and can flexibly trade-off between precision and recall on under-represented classes via a single hyper-parameter.</p><p>? We introduce an efficient search algorithm for this hyper-parameter. Unlike cost sensitive learning, an optimal hyperparameter can be searched efficiently on a validation set posttraining.</p><p>? We test the algorithm on six datasets and five different architectures and outperforms state-of-the-art models on classification accuracy (recall) across all tasks and models while maintaining good precision.</p><p>? We further combine our method with non-semantic likelihood shift methods, re-motivate it from the Bayesian perspective, and show that we can tackle both problems in a unified way on a RGB-D semantic segmentation dataset with unseen weather conditions. We show significant improvement on mean accuracy while maintaining good mean IOU performance with both qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this paper, we focus on prior (label) distribution shift resulting from various degrees of imbalance in the training label distributions, and testing on a different distribution or emphasizing a different distribution in the testing metrics (e.g. class-averaged accuracy). We therefore summarize methods for dealing with such imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imbalance -Data Level Methods</head><p>The simplest methods for dealing with label imbalance during training are random under-sampling (RUS) which discards samples from the majority classes and random over-sampling (ROS) which re-samples from the minority classes <ref type="bibr" target="#b2">[3]</ref>. While ROS is infeasible when the data imbalance is extreme, RUS tends to overfit the minority classes <ref type="bibr" target="#b3">[4]</ref>. Synthetic generation <ref type="bibr" target="#b4">[5]</ref> or interpolation <ref type="bibr" target="#b5">[6]</ref> to increase the number of samples in the minority class are also used. However, these methods are sensitive to imperfections in the generated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Imbalance -Algorithm-Level Methods</head><p>The majority of work in this category modify the training procedure by introducing cost-sensitive losses. For example, median frequency balancing has been used in dense prediction tasks such as surface normal and depth prediction <ref type="bibr" target="#b6">[7]</ref> as well as semantic segmentation <ref type="bibr" target="#b7">[8]</ref>. DRW <ref type="bibr" target="#b1">[2]</ref> is a variant of the frequency balancing algorithm. It re-weights the loss function at a latter training stage not from the beginning. We specifically compare our method to three state-of-the-art cost sensitive losses LDAM <ref type="bibr" target="#b1">[2]</ref>, Class-Balanced Loss (CB) <ref type="bibr" target="#b0">[1]</ref>, and Focal Loss (FL) <ref type="bibr" target="#b8">[9]</ref>. LDAM derives a generalization error bound for the imbalanced training and proposes a margin-aware multi-class weighted cross entropy loss. CB motivates the concept of effective number of samples and derives a weighted cross entropy loss with approximation. FL is another weighted cross entropy loss with a sample-specific weight (1 ? p i ) ? , where p i is the model output probability for class i. A contemporary work Bilateral-Branch Network (BNN) <ref type="bibr" target="#b9">[10]</ref> proposes a two branch approach with one branch learning the original distribution and the the other learning a rebalanced distribution. Our method can be combined with algorithm-level methods to yield better performance.</p><p>Imbalance -Classifier Level Methods The most common method in this category is thresholding or post scaling. The process happens at the test phase and changes the output class probabilities. The simplest variant compensates for prior class probabilities, i.e. dividing the output for each class by its training set prior probabilities. This method can often improve the performance for imbalanced classification <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b11">[12]</ref>. The same technique under the name maximum likelihood <ref type="bibr" target="#b12">[13]</ref> is also used in semantic segmentation which is a naturally class-imbalanced task. While this decision rule was able to improve recall on minority classes, it deteriorates overall performance by introducing substantially more false detection. Our method belongs to this category and improves on previous works. For a more comprehensive review on dataset imbalance in deep learning, we refer readers to <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: A Unified Perspective on Label Prior and Non-Semantic Likelihood shift</head><p>Let P s (X, Y ) define a training (source) distribution and P t (X, Y ) define a test (target) distribution where X is the input and Y ? C = {1, ..K} is the corresponding label. In a multi-class classification task, the final decision on the target distribution is often made by following the Bayes decision rule:</p><formula xml:id="formula_0">y * = arg max y?C P t (y|x) = arg max y?C f t (x|y)P t (y) P t (x) = arg max y?C f t (x|y)P t (y)<label>(1)</label></formula><p>where f t (x|y) is the class conditional probability density function and P t (y) is the prior distribution.</p><p>Label prior shift refers to the prior (label) distribution changing between training and testing, i.e, P s (Y ) = P t (Y ); in this paper, we focus on the case where the training distributions have varying degrees of imbalance (e.g. long-tailed) and the testing distribution shifts (e.g. is uniformly distributed). Note, however, that the method we develop is not limited to this case. Note also that some metrics during testing implicitly emphasize a uniform distribution over labels, regardless of the actual label distribution in the testing dataset; see Appendix Section 6.3 for a proof for one popular class of metrics, namely class-averaged metrics.</p><p>Non-semantic likelihood shift refers to shift without introducing new semantic labels such as sensor degradations, changes in lighting, or presentation of the same categories in a different modality, i.e. f s (X|Y ) = f t (X|Y ).</p><p>In this paper, we focus on the label prior shift, commonly known as class imbalance. In sec. 3.2, to deal with prior label shift, we motivate the problem from the perspective of the optimal Bayes classifier and derive a prior rebalancing equation and a KL-divergence based optimization method.</p><p>In addition, we re-motivate temperature scaling <ref type="bibr" target="#b13">[14]</ref> for dealing with likelihood shift, starting with a similar Bayesian perspective, as a likelihood flattening process instead of as a confidence calibration technique. In the same section, we propose a unified algorithm to simultaneously handle both types of shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Imbalance Calibration (IC) with Prior Rebalancing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Theoretical Motivation</head><p>To motivate the framework for dealing with learning in an imbalanced setting, we start from the assumption that a discriminative model can learn the posterior distribution P s (Y |X) of the source dataset perfectly. The corresponding Bayes Classifier is defined as the following.</p><formula xml:id="formula_1">h s (x) = arg max y?C P s (Y = y|X = x)<label>(2)</label></formula><p>The classifier h s (x) is the optimal classifier on the training distribution, P s (X, Y ). Now let's assume that the test distribution has the same class conditional likelihood as the training set, i.e. f t (X|Y ) = f s (X|Y ) and differs only on the class priors P t (Y ) = P s (Y ). We show the following result:</p><formula xml:id="formula_2">Theorem 1 Given that h s (x) is the Bayes classifier on P s (X, Y ), h t (x) = arg max y?C P s (y|x)P t (y) P s (y) ,<label>(3)</label></formula><p>is the optimal Bayes classifier on</p><formula xml:id="formula_3">P t (X, Y ) where f t (X|Y ) = f s (X|Y ) and P t (Y ) = P s (Y ) and the Bayes risk is R(h t ) = P (h t (x) = y).</formula><p>This partition defines the decision region of a classifier for K classes.</p><formula xml:id="formula_4">1 ? R(h t ) = P (h t (x) = y) = K k=1 P t (y = k)P (h t (x) = k|y = k) (4) = K k=1 P t (k) ? k (ht) f t (x|k)dx = K k=1 P t (k) ? k (ht) f s (x|k)dx = K k=1 P t (k) ? k (ht) P s (k|x)f s (x) P s (k) dx = R D K k=1 I ? k (ht) (x) P s (k|x)P t (k) P s (k) f s (x) dx where I ? k (h) (x) = 1, ?x ? ? k (h)(x)</formula><p>and 0 otherwise. To avoid notational clutter, we abbreviate y = k as k. By choice of the decision rule h t (x) defined in Thm. 1, the function inside the integral is at its maximum and any other decision rules will result in higher risk. We note that the rebalancing technique in Thm. 1 is not new and has been studied before in <ref type="bibr" target="#b14">[15]</ref>. Our contribution is proving its optimality under the ideal setting with Bayes risk.</p><p>The theorem sheds light on the weakness of this approach: in reality, we do not have access to either P t (X, Y ) or P s (X, Y ). We merely have samples from those hidden distributions. In other words, we do not have perfect modeling nor learning of the generating distribution. We learn an approximation of the distribution according to relative entropy between a parametrized distribution P d (Y |X), denoting the learned discriminative posterior, and P s (Y |X). Therefore, the resulting classifier with proper substitution to Thm. 1,</p><formula xml:id="formula_5">h t (x) = arg max y?C P d (y|x)P t (y) P s (y) ,<label>(5)</label></formula><p>is not optimal and the corresponding partition ?(h s ) is not the optimal decision boundary on the test distribution. We will now show that we can obtain a better classifier thanh t (x) by exploring some observations.</p><p>We introduce a terminology rebalanced posterior, P r (y|x) to describe eq. 5 because the discriminative posterior P d (y|x) is rebalanced by P t (y)/P s (y). As we have discussed, the parametrization and learning of the training distribution are not perfect. It is unlikely that a direct application of theorem 1 leads to a good classifier on the test distribution. The weighting P t (y)/P s (y) in eq. 5 might be too large or too small for different setups. For example, in semantic segmentation, directly using eq. 5 results in excessive false positives for small classes as reported by <ref type="bibr" target="#b12">[13]</ref>. We also show in the experiment section that eq. 5 yields inferior performance on imbalanced classification datasets. <ref type="figure">Figure 1</ref>: Left: log-linear dependency on source prior ratio with ? ? 1. Right: log-linear dependency on source prior ratio with ? &gt; 1. x axis is the ratio P s (c i )/P s (c gt ) and y axis denotes the ratio raised to the power of ?. The amplification effect of the source prior ratio is reduced when ? ? 1 and increased when ? &gt; 1</p><p>Hypothesis 1 A better classifier can be found by finding a trade off between the discriminative classifier P d (y|x) and the derived rebalanced classifier P r (y|x).</p><p>Intuitively, if the weighting P t (y)/P s (y) is too large and P r (y|x) biases too much towards small classes, we might want to weigh in the original prediction P d (y|x) where no weighting is applied; if the weighting is too small, we can encourage a stronger weighting to move further away from P d (y|x). Based on this intuition, a natural distance metric for distributions is KL-divergence (or relative entropy). Therefore, we propose the following optimization problem based on minimizing the KL-divergence between the final calibrated posterior P f (y|x) and P d (y|x), P r (y|x).</p><formula xml:id="formula_6">P * f (y|x) = arg min P f (1 ? ?)KL(P f , P d (y|x)) + ?KL(P f , P r (y|x))<label>(6)</label></formula><p>where ? ? 0 is a hyperparameter. The optimization problem tries to find an interpolated distribution between P d (y|x) and P r (y|x) with a regularizing parameter ?. The information-theoretic justification for this formulation is that assuming P f represents the true distribution which we want to approximate, KL(P f , P d (y|x)) + KL(P f , P r (y|x)) captures the number of bits lost when encoding P f with both P d and P r . It is clear that when ? = 0 we recover the discriminative classifier and when ? = 1 we have the rebalanced classifier. More generally, there exists a closed form solution to this optimization problem <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_7">P * f (y|x) = 1 Z(x) P d (y|x) 1?? P r (y|x) ? ,<label>(7)</label></formula><p>where Z(x) is the normalization factor. This completes an extremely simple calibration method with only a single hyperparameter for class imbalance in classification. It can effectively adjust a classifier's decision boundary and outperform more sophisticated training methods on image classification tasks. Further more, unlike many existing class imbalance methods, we will show that the proposed method can adjust the trade-off between precision and recall. This is especially important for semantic segmentation which uses Intersection over Union (IOU) as criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Analysis of the Rebalancing Equation</head><p>We use the odd ratio form for analysis. Odd ratio has been used in logistic regression <ref type="bibr" target="#b16">[17]</ref>, where it is defined as the ratio of probability of success and probability of failure. Here odd ratio denotes ratio of the calibrated posteriors. Let c gt denote the ground truth class, c i denote one of the other classes. We rewrite Eq. 7 as the following ( detailed derivation is in appendix 6.1):</p><formula xml:id="formula_8">P * f (c gt |x) P * f (c i |x) = P d (c gt |x) P d (c i |x) P s (c i ) P s (c gt ) ? P t (c gt ) P t (c i ) ?<label>(8)</label></formula><p>For a ground truth class to be the selected (argmax) output, the odd ratio in eq. 8 needs to be greater than 1 ?i = gt. We first analyze the dependency on ? by setting the test priors, P t (y), equal for all classes.</p><p>It is clear from Eq.8 that if c gt is a minority class then the ratio of priors under ?, P s (c i )/P s (c gt ), denoted source prior ratio, is greater than 1 and when c gt is a large class it is smaller than 1.</p><p>Intuitively, this prior ratio amplifies the odd ratio for minority classes and downweights the odd ratio for large classes. However, as pointed out in <ref type="bibr" target="#b12">[13]</ref>, this scaling can be far too aggressive when ? = 1.</p><p>In other words, the prior ratio becomes too large when c gt is a minority class and too small when c gt is a large class.</p><p>As shown in the left part of <ref type="figure">Fig. 1</ref>, with ? ? 1 when the source prior ratio is greater than 1 in the case of c gt being a minority class, the growth is sublinear and when the source prior ratio is smaller than 1, the growth is superlinear. In the right section of <ref type="figure">fig. 1</ref>, we also see that the dependency is different when ? ? 1. When c gt is a minority class, the growth is superlinear and when c gt is a majority class, the growth is sublinear. In other words, the amplification effect of the source prior ratio is reduced when ? ? 1 and increased when ? ? 1. As we will show empirically, by finding a good ? on a validation set, we can strike a balance between recall and precision between large and small classes. The analysis validates our hypothesis that a better classifier can indeed be found through the trade-off of P d (y|x) and P r (y|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Search Algorithm and Time Complexity for ?</head><p>An efficient algorithm exists for searching ? based on an empirical observation of the dependency between accuracy and ?. Empirically, we found that performance metrics evaluated on a validation set vary with ? in a concave manner with a unique maximum. A modified binary search algorithm can find a maximum with a specified precision with O(log N ) time complexity where N is the number of ?s in the search range. Due to space constraint, please refer to the appendix for the complete algorithm 6.2. Please refer to the experiments in sec. 4.1 for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Confidence Calibration with Likelihood Flattening</head><p>In the previous section, we explored a probabilistic way to deal with prior shift. In this section, we similarly look at likelihood shift and cast a different perspective on an existing technique, temperature scaling <ref type="bibr" target="#b13">[14]</ref>, as a likelihood flattening process which implicitly increases the uncertainty in the likelihood P d (X|Y ). The reasoning is hypothetical because a discriminative model does not learn a likelihood explicitly. Temperature scaling has been used in uncertainty calibration <ref type="bibr" target="#b17">[18]</ref>. The basic temperature scaling takes the following form,</p><formula xml:id="formula_9">P(Y |x) = Softmax ([l 1 , ..., l Nc ] * ?) ,<label>(9)</label></formula><p>where [l 1 , ..., l Nc ] are the pre-softmax logits and ? is a scalar.</p><p>Graphically, smaller ? will result in a flatter softmax output and larger ? will result in a more peaked softmax distribution. In Eq. 1, we showed that y * = arg max y?C P t (y|x) = arg max y?C f t (x|y)P t (y). Considering the equal prior situation, then we can remove P t (y) from the previous equation. Now, flattening the softmax probability, P t (y|x), is equivalent to flattening the likelihood, f t (x|y). We formalize the discussion in the following hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis 2</head><p>In light of the decomposition in Eq. 1, temperature scaling in a softmax-activated discriminative model is numerically equivalent to flattening the class conditional likelihood f (x|y) ?y ? C implicitly and making the likelihood of belonging to any class more likely.</p><p>This behavior of temperature scaling is desirable especially in a multi-modal fusion setting. For example, if one modality is compromised, temperature scaling can flatten the output distribution from that modality to raise its uncertainty in the sense of entropy. More importantly, a flattened distribution has little power to affect the decision from other modalities.</p><p>UNO <ref type="bibr" target="#b18">[19]</ref> introduced an uncertainty-aware temperature scaling factor ?(x) which depends on a network's input x. The uncertainty-aware scalar correlates with uncertainty in an input image. Intuitively, if an input image is degraded, ?(x) as the temperature parameter will be a scalar less than 1 and thus "softens" the distribution and makes the model less confident in its prediction.</p><p>Following the hypothesis, we view the uncertainty-aware temperature scaling in UNO as a likelihood flattening method. Together with the prior rebalancing method from the previous section, we develop a unified multi-modal fusion algorithm Alg. 1 for prior and likelihood shift and apply it to multimodal tasks. We use noisy-or operation as the final fusion layer as in UNO <ref type="bibr" target="#b18">[19]</ref>. We demonstrate the performance of the algorithm on a RGB-D semantic segmentation task in Seq. 4.4.2. Please also see Appendix 6.6 for qualitative results.</p><p>Algorithm 1: UNO-IC: multi-modal fusion algorithm for prior and likelihood shift</p><formula xml:id="formula_10">Data: {x} ? Dtest Result: arg max y P f (Y |x) 1 Initialize: L m d (Y |X) ?m ? {1, ..., M } L m d is the pre-softmax logits of discriminative model m 2 for m = 1, .., M do 3 P m d (Y |x) = Softmax (L m d (Y |x) * ?m(x)) Likelihood Flattening 3.3 using ?m(x) in [19] 4 P m r (Y |x) = P m d (Y |x)P m r (Y ) P m s (Y )</formula><p>Prior Rebalancing 3.2 using Eq. 5 </p><formula xml:id="formula_11">5 P m f (Y |x) = 1 Z(x) P m d (Y |x) 1?? P m r (Y |x) ? Calibrated Posterior using Eq. 7 6 P f (Y |x) = 1 Z(x) 1 ? m 1 ? P m f (Y |x) Noisy-Or fusion</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments to test our imbalance calibration (IC) algorithm described in sec. 3.2 on six datasets with five different architectures. A summary of datasets and architectures used for each one is shown in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy Dataset</head><p>To visualize the effect of our imbalance calibration algorithm on decision boundaries, we generate two binary imbalanced datasets for classification, two moon and circle and train a 3-layer fully connected classifier with logistic loss on each. <ref type="figure" target="#fig_0">Fig. 2</ref> shows an overlay of the input data and decision boundary with different ?s and the accuracy vs. lambda plots for both datasets. The original decision boundary ? = 0.0 cuts through the minority class, and varying ? shifts the decision boundary away from the minority class towards the larger class. We can find a better classifier empirically by searching ? values on a validation set. The accuracy vs. lambda curves exhibit concavity as shown on the right side of <ref type="figure" target="#fig_0">Fig. 2</ref>. This observation motivates an efficient modified binary search algorithm for ? in Appendix Sec. 6.2. The same concavity is also observed for more complex tasks subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Imbalanced CIFAR Classification</head><p>We compare our methods with the state-of-the-art methods <ref type="bibr" target="#b0">[1]</ref> [2] <ref type="bibr" target="#b8">[9]</ref> [10] on CIFAR-10 and CIFAR-100 dataset with two different imbalance types: long-tailed imbalance <ref type="bibr" target="#b0">[1]</ref> and step imbalance <ref type="bibr" target="#b11">[12]</ref> with two different imbalance ratios for each imbalance type. Following <ref type="bibr" target="#b1">[2]</ref>, we present the validation error in <ref type="table" target="#tab_1">Table 2</ref>. We observe that Focal <ref type="bibr" target="#b8">[9]</ref> is not as effective against imbalance and this is also   observed in the same set of experiments in <ref type="bibr" target="#b1">[2]</ref>. BNN <ref type="bibr" target="#b9">[10]</ref> achieves comparable performance with more complicated training strategy and architecture. Even though LDAM <ref type="bibr" target="#b1">[2]</ref> improves the classification performance, DRW gives the most improvement. As a post-calibration method, our method can be combined with different learning losses and learning schedules to further improve performance. Our method combined with DRW and standard cross entropy loss, CE-DRW-IC, yields the best performance. The hyperparameter ? is searched on the validation set. Please refer to the Appendix Sec. 6.4 for the exact values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">iNaturalist Dataset Classification</head><p>iNaturalist2018 <ref type="bibr" target="#b20">[21]</ref> is a large classification dataset for species with 437,513 training images and 24,426 validation images. We randomly split the validation set into three subsets and use 3-fold cross validation for tuning and testing our IC method. We compare our method with Focal <ref type="bibr" target="#b8">[9]</ref>, LDAM <ref type="bibr" target="#b1">[2]</ref>, DRW <ref type="bibr" target="#b1">[2]</ref> and BNN <ref type="bibr" target="#b9">[10]</ref> in <ref type="table" target="#tab_2">Table 3</ref>. Again, IC methods yield the best performance. It outperforms BNN <ref type="bibr" target="#b9">[10]</ref> which has a much more complicated training strategy. The result demonstrates the effectiveness of our method for extremely large number of classes and severe imbalance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Synthia Multimodal Semantic Segmentation</head><p>For RGB-D semantic segmentation, we use the Synthia-Sequences dataset <ref type="bibr" target="#b24">[25]</ref> which is a photorealistic synthetic dataset of traffic scenarios under different weather, illumination, and season conditions. The hyperparameter ? is tuned on a validation split and the algorithm is tested on a test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">RGB Imbalance Experiment</head><p>Unlike previous classification problems which use only accuracy as the performance metric, semantic segmentation uses both mean intersection over union (IOU) and mean accuracy. Mean IOU considers the effect of false positive predictions while mean accuracy only considers true positives. The difference between the two metrics can be seen as the recall-precision trade-off. <ref type="table" target="#tab_3">Table 4</ref> shows the full evaluation against frequency weighted loss, Focal loss <ref type="bibr" target="#b8">[9]</ref> and LDAM <ref type="bibr" target="#b1">[2]</ref> on the in-distribution test splits. While the median and max frequency weighted losses achieve the best mean accuracy, their mean IOU performance drops considerably. LDAM <ref type="bibr" target="#b1">[2]</ref> resembles the performance of the frequency losses with high accuracy but low IOU because it also emphasises the minority classes by maintaining a fixed margin. Focal loss <ref type="bibr" target="#b8">[9]</ref>, on the other hand, resembles the performance of unweighted cross-entropy loss because it adopts a per-example weighting scheme and does not explicitly reweight a certain class. BNN <ref type="bibr" target="#b9">[10]</ref> does not generalize to segmentation due to its undersampling strategy for training a rebalancing branch. Overall, our method CE-IC and Focal-IC which use imbalance calibration on top of models trained with cross entropy and focal loss achieve significantly improved mean accuracy while maintaining a good mean IOU. Further analysis of the performance characteristics is in Appendix Sec. 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">RGBD Fusion Experiment</head><p>To demonstrate the proposed unified Alg. 1 for prior and likelihood shift, we conduct RGB-D multimodal semantic segmentation experiments on Synthia out-of-distribution splits which consist of seven weather conditions not seen during training. In this experiment, we compare to a simple baseline, SoftAve, which simply adds RGB and Depth predictions and averages the two, and UNO <ref type="bibr" target="#b18">[19]</ref> which is designed for unseen degradations. In <ref type="table" target="#tab_4">Table 5</ref>, we report both mean IOU and mean accuracy. While UNO achieves the best mean IOU, our method combined with UNO results in significant improvement in mean accuracy with negligible drop in mean IOU. This experiment demonstrates the effectiveness of our algorithm against extreme dataset imbalance and unseen degradations. We also show qualitative results in Appendix Sec. 6.6. Our method UNO-IC is able to identify small objects in the distance such as poles and pedestrians even under severe visual degradations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a unified perspective and algorithm to deal with label prior shift, and combine it with methods for non-semantic likelihood to tackle both types of shifts simultaneously. The major contribution is a novel imbalance calibration technique which is motivated from the concept of optimal Bayes classifier and optimization of KL divergence. To accompany the formula, we introduced a modified binary search algorithm for the hyperparameter (lambda) based on the empirical observation of a concave performance function as it is varied. Our method is also very computationally efficient because the hyperparameter is tuned during the validation stage whereas most SOTA methods require retraining. The final algorithm can be used in probabilistic classification tasks regardless of underlying architectures. We have shown the generality of the imbalanced calibration technique on six datasets and a number of different architectures, showing effectiveness of the unified algorithm against both extreme dataset imbalance and unseen degredations in multi-modal semantic segmentation.</p><p>6 Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Odd Ratio Derivation</head><p>In Sec. 3.2.2, we used odd ratio to analyze the behavior of the proposed imbalance calibration method.</p><p>Here we provide the full derivation of the odd ratio Eq. 8 from Eq. 7 and Eq. 7.</p><formula xml:id="formula_12">P f (c gt |x) P f (c i |x) = 1/Z P d (c gt |x) 1?? P r (c gt |x) ? ) 1/Z (P d (c i |x) 1?? P r (c i |x) ? )) (10) = P d (c gt |x) 1?? (P d (c gt |x)P t (c gt )/P s (c gt )) ? P d (c i |x) 1?? (P d (c i |x)P t (c i )/P s (c i )) ? = P d (c gt |x) P d (c i |x) P s (c i ) P s (c gt ) ? P t (c gt ) P t (c i ) ?</formula><p>where c gt denotes the ground truth class, c i denotes one of the other classes. P f , P d and P r denote the final calibrated posterior, the original discriminative posterior and the rebalanced posterior respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Search Algorithm for ?</head><p>Efficient algorithm exists for searching ? based on empirical observation of dependency on ?.</p><p>Empirically, we found that performance metrics evaluated on a validation set vary with ? in concave manner with a unique maximum. In other words, a function of ? first increases and then decreases. A modified binary search algorithm can find a maximum with a specified precision with O log(n) time complexity where n is the number of ?s in the search range. We outline the algorithm in a Pythonic pseudo code format implemented with recursion. To assist efficient search, we define three hyperparameters in this algorithm, Low,High and Prec which is the lower bound, upper bound and quantization precision of ? to be searched. Based on our observation,the parameters are set to 0.0, 2.0 and 0.1 respectively. Another notation introduced is metric() which denotes a metric function that takes ? as input and evaluates a model with the given ? on a validation set. metric() returns a scalar representing the performance of the model with the current ?. The full algorithm is presented in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Imbalance Due to Evaluation Metrics</head><p>We define two types of label prior shift: explicit and implicit. The shift occurs as a result of imbalance in the training data and they will lead to poor performance measured on certain metrics. We will discuss them in the general case of multi-class classification.</p><p>Explicit label prior shift refers to the prior (label) distribution changing between training and testing, i.e, P s (Y ) = P t (Y ) following the definition of prior shift in <ref type="bibr" target="#b25">[26]</ref>; in this paper, we focus on the case where the training distributions have varying degrees of imbalance (e.g. long-tailed) and the testing distribution shifts (e.g. is uniformly distributed, Y ? U(1/K)). To see that the imbalance causes label prior shift which in turn, causes performance drop, let's analyze the popular metric, accuracy used in image classification.</p><formula xml:id="formula_13">ACC(X, Y ) = 1 N K k=1 N i=1 I(h(x i ) = k, y i = k) = K k=1 N k N 1 N k N i=1 I(h(x i ) = k, y i = k) (11) = K k=1 P (y = k)acc(y = k) = E Y ?P (Y ) [acc(Y )]</formula><p>As shown in eq. 11, the accuracy metric is equal to the expectation of accuracy w.r.t to the distribution of labels, P (Y ). When P s (Y ) = P t (Y ) = U(1/K), training with imbalanced data maximizes accuracy w.r.t P s (Y ) where large classes dominate, while on test dataset the accuracy metric calculates the expectation of accuracy w.r.t a uniform distribution . Consequently, training with imbalanced data often biases towards large classes to maximize eq. 11 and often results in poor performance on uniform evaluation data.</p><p>Implicit label prior shift is unconventional because it happens when P s (Y ) = P t (Y ) = U(1/K) . This is the case for semantic segmentation, where classes such as pedestrians, poles, etc. occupy a much smaller portion of an image in both training and test data. However, people tend to assign equal importance to all classes regardless of their actual pixel percentage. This is reflected by the popularity of class average metrics such as mean accuracy and mean Intersection over Union (mIOU). In other words, label prior shift is implicitly caused by using class average metrics even though the test distribution is equal to training. Let's look at the another popular metric, mean accuracy, which is often used when the test data is not uniform (in contrast to the first case).</p><formula xml:id="formula_14">mACC(X, Y ) = 1 K K k=1 1 N k N i=1 I(h(x i ) = k, y i = k)<label>(12)</label></formula><formula xml:id="formula_15">= K k=1 1 K 1 N k N i=1 I(h(x i ) = k, y i = k) = K k=1 1 K acc(Y = k) = E Y ?U(1/K) [acc(Y )]</formula><p>As shown in eq. 12, mean accuracy is equal to the expectation of accuracy w.r.t a uniform label distribution. Mean accuracy is exactly the same as the accuracy metric on uniform test data in the first case. In other words, evaluating on non-uniform test data with mean accuracy is equivalent to evaluating on uniform test data with accuracy in expectation. Even though the training distribution is the same as the test distribution and both are non-uniform, using mean accuracy results in an implicit label prior shift.   <ref type="figure">Figure 3</ref>: Hyperparameter ? search on iNaturalist2018 (Left) and Synthia (Right) validation set for imbalance calibration. ? = 0.0 corresponds to the baseline performance. Note that in the main paper, we report 3-fold cross validated resutls for iNaturalist2018. Here we show the search on the entire validation set. ? = 0.4 is used as the parameter for subsequent experiments on Synthia fusion experiments in sec. 4.4.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Hyperparameter for Cifar Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Lambda Search on Validation Set for iNatrualist2018 and Synthia</head><p>The left side of <ref type="figure">Fig. 3</ref> shows the performance curve of top1 and top5 accuracy on iNatrualist2018 with different ?. The right side of <ref type="figure">Fig. 3</ref> shows the performance curve of both mean IOU and mean accuracy on a Synthia validation set with varying ? values. On the far right where ? = 1.0 we obtain the performance of the rebalanced posterior as in eq. 5 and on the left we recover the performance of the discriminative model. Intuitively, as we move towards the full rebalanced posterior by increasing ?, the algorithm gives more weight to minority classes and the mean accuracy continues to increase. However this comes at the cost of significant increase in false positives as the mean IOU drops. The trade-off between mean IOU and mean accuracy reflects the imperfect learning of the dataset discussed in sec. 3.2. By searching for a good balance, it is possible to improve mean accuracy while maintaining a good mean IOU. The performance curves in both experiments exhibit concavity as in the simpler 2D toy example 4.1. This further validates our proposed binary search algorithm in Appendix Sec. 6.2 which relies on the assumption of concavity. <ref type="figure">Fig. 4</ref> shows qualitative results for the Synthia fusion experiments in Sec. 4.4.2. Our method UNO-IC is able to capture small details such as pedestrians in the distance even under severe visual degradations. Because the rain condition is not encountered during training, the rgb channel failed to cope with the degradation. UNO <ref type="bibr" target="#b18">[19]</ref> dynamically shifts the fusion algorithms weight to the depth channel while our imbalance calibration method gives more weights to small objects. The qualitative and quatitative results in Sec 4.4.2 demonstrate the effectiveness of our unifed perspective 3.1 and Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Qualitative Results for Synthia RGBD fusion Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>Qualitative results for Synthia Rain. The rain condition is not seen during training. Our algorithm UNO-IC can capture fine details in segmentation even under severe viusal degradation not encounted during training. Our method combined with UNO demonstrates is effective against extreme dataset imbalance and unseen degradations in semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Top: Circle Bottom: Two moon. The red class is the marjority class. S ? increases, it effectively shifts the decision boundary away from the minority class towards the large class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of datasets and architectures: Imbalance Ratio is the ratio of class size between the largest and smallest class.</figDesc><table><row><cell></cell><cell cols="5">Dimension Num of Classes Max. Class Size Imbalance Type Imbalance Ratio</cell><cell>Arch.</cell></row><row><cell>Two Moon</cell><cell>R 2</cell><cell>2</cell><cell>? 2, 500</cell><cell>Step</cell><cell>9</cell><cell>3-layer FCN</cell></row><row><cell>Circle</cell><cell>R 2</cell><cell>2</cell><cell>? 2, 500</cell><cell>Step</cell><cell>9</cell><cell>3-layer FCN</cell></row><row><cell>Cifar10</cell><cell>R 32?32</cell><cell>10</cell><cell>5, 000</cell><cell>Step/Exp</cell><cell>100/10</cell><cell>Resnet32 [20]</cell></row><row><cell>Cifar100</cell><cell>R 32?32</cell><cell>100</cell><cell>5, 000</cell><cell>Step/Exp</cell><cell>100/10</cell><cell>Resnet32 [20]</cell></row><row><cell cols="2">iNaturalist18 [21] R 299?299</cell><cell>8, 142</cell><cell>1,000</cell><cell>Long-tail</cell><cell>500</cell><cell>IncepV3 [22]/Resnet50 [20]</cell></row><row><cell>Synthia [23]</cell><cell>R 768?384</cell><cell>14</cell><cell>1.9?10 9</cell><cell>Long-tail</cell><cell>1.3?10 4</cell><cell>DeepLab [24]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top1 validation error? * indicates the reported results from<ref type="bibr" target="#b9">[10]</ref>. Our model (-IC) achieves the best performance overall. CE is a baseline trained with unmodified cross-entropy loss. CB refers to class balanced loss in<ref type="bibr" target="#b0">[1]</ref>. Combining our method with techniques such as Deferred Reweighting (DRW)<ref type="bibr" target="#b1">[2]</ref> yields the best performance.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Imbalanced CIFAR-10</cell><cell></cell><cell cols="3">Imbalanced CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell><cell cols="2">long-tailed</cell><cell>step</cell><cell></cell><cell>AVE</cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>CE</cell><cell>28.48</cell><cell cols="8">13.47 34.01 14.54 62.16 43.21 61.28 45.33 37.70</cell></row><row><cell>Focal[9]</cell><cell>26.62</cell><cell cols="8">13.52 34.89 14.92 62.02 43.60 61.49 45.62 38.17</cell></row><row><cell>LDAM[2]</cell><cell>26.39</cell><cell cols="8">13.77 34.42 15.33 59.70 43.52 60.59 49.20 37.91</cell></row><row><cell>CB CE[1]</cell><cell>26.77</cell><cell cols="8">12.37 37.43 14.44 67.93 44.15 80.40 47.28 41.35</cell></row><row><cell>CB Focal[1]</cell><cell>25.93</cell><cell cols="3">13.06 36.07 14.8</cell><cell cols="5">68.08 45.17 78.14 46.84 41.40</cell></row><row><cell>CE-DRW[2]</cell><cell>23.78</cell><cell cols="8">11.85 29.05 12.79 58.64 41.53 55.70 40.65 34.25</cell></row><row><cell>Focal-DRW[2]</cell><cell>25.73</cell><cell cols="8">12.00 27.37 11.82 60.47 43.35 61.63 41.93 33.9</cell></row><row><cell>LDAM-DRW[2]</cell><cell>23.38</cell><cell cols="8">12.12 22.69 12.68 57.77 42.52 54.30 43.49 33.62</cell></row><row><cell>BNN[10]</cell><cell cols="2">20.18* 11.58*</cell><cell>?</cell><cell>?</cell><cell cols="2">57.44* 40.88</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>CE-IC (Ours)</cell><cell>20.14</cell><cell cols="8">11.37 25.61 11.34 59.08 41.91 53.48 40.64 32.95</cell></row><row><cell>Focal-IC (Ours)</cell><cell>19.84</cell><cell cols="8">11.99 25.68 10.99 58.35 42.01 52.73 40.70 32.79</cell></row><row><cell cols="2">Focal-DRW-IC (Ours) 21.63</cell><cell cols="8">11.80 21.63 11.48 59.43 43.35 56.89 41.93 33.52</cell></row><row><cell>CE-DRW-IC (Ours)</cell><cell>18.91</cell><cell cols="8">11.44 21.45 11.42 56.89 41.44 54.40 40.07 32.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Validation error? on iNaturalist2018. Our method (-IC) yields the best performance and is effective for extreme large number of classes and severe imbalance. We use 3-fold cross validation for tuning ? and testing. Subscript value represents the ? for that experiment. * indicates the reported results from<ref type="bibr" target="#b9">[10]</ref> </figDesc><table><row><cell></cell><cell>CE</cell><cell cols="6">CE-DRW[2] Focal[9] Focal-DRW[2] LDAM[2] LDAM-DRW[2] BNN[10]</cell><cell>CE-IC</cell><cell>CE-DRW-IC</cell></row><row><cell cols="2">InceptionV3 [22] 40.08</cell><cell>35.35</cell><cell>39.68</cell><cell>35.79</cell><cell>41.81</cell><cell>37.03</cell><cell>?</cell><cell cols="2">34.16 ? 0.03 34.22 ? 0.11</cell></row><row><cell>Resnet50 [20]</cell><cell>38.00</cell><cell>33.73</cell><cell>38.33</cell><cell>34.38</cell><cell>39.62</cell><cell>35.42*</cell><cell>33.71*</cell><cell cols="2">32.16 ? 0.41 32.06 ? 0.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test mean IOU, mean accuracy on Synthia in distribution splits (RGB only). Our method (IC) with cross entropy or Focal loss achieves significant improvement in mean accuracy while maintaining competitive mean IOU.</figDesc><table><row><cell>Ours)</cell></row></table><note>CE (Unweighted) CE (Median Freq.) CE(Max. Freq.) LDAM[2] Focal[9] CE-IC (Ours) Focal-IC (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test mean IOU and mean accuracy (mIOU | mACC) of the joint algorithm 1 on Synthia out-of-distribution splits.The listed conditions have not been encountered during training. 96|85.42 81.78|85.33 78.37|82.61 79.46|83.31 70.68|73.94 78.96|82.37 78.02|81.22 78.46|82.03 UNO[19] 82.05|85.49 81.85|85.41 78.37|82.68 79.37|83.30 74.28|77.61 79.51|83.04 78.41|81.68 79.12|82.74 IC ? = 0.4 81.33|92.73 80.87|93.68 78.07|90.49 79.01|91.57 72.92|79.04 78.76|89.16 78.86|88.80 78.55|87.92 UNO-IC ? = 0.4 81.14|93.04 80.65|93.91 77.77|90.88 78.69|92.10 74.74|83.23 78.53|90.05 78.33|89.91 78.55|90.45</figDesc><table><row><cell></cell><cell>mIOU|mACC</cell><cell>Fog</cell><cell>Fall</cell><cell>Winter</cell><cell>WinterNight</cell><cell>Rain</cell><cell>RainNight</cell><cell>SoftRain</cell><cell>AVE.</cell></row><row><cell>Baseline</cell><cell>SoftAve</cell><cell>81.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 documents</head><label>6</label><figDesc></figDesc><table /><note>the hyperparameters used for the Cifar experiments in table 2 in the main paper. The search for ? is very efficient because it dose not require training. The modified binary search algorithm in Appendix Sec. 6.2 can be used to find a ? in O(log N ) time.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Top1 validation error? for Imbalance Calibration. CE is a baseline trained with unmodified cross-entropy loss. CB refers to class balanced loss in<ref type="bibr" target="#b0">[1]</ref>.The subscript denotes the ? values for the experiments 40.64 1.3 32.95 Focal-IC (Ours) 19.84 1.3 11.99 1.1 25.68 0.8 10.99 1.0 58.35 1.1 42.01 1.0 52.73 1.2 40.70 1.0 32.79 CE-DRW-IC (Ours) 18.91 1.5 11.44 1.2 21.45 1.2 11.42 1.0 56.89 0.6 41.44 0.5 54.40 0.3 40.07 0.3 32.00 Focal-DRW-IC (Ours) 21.63 1.0 11.80 0.5 21.63 1.0 11.48 0.5 59.43 0.5 43.35 0.0 56.89 0.6 41.93 0.0 33.52</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">Imbalanced CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">Imbalanced CIFAR-100</cell><cell></cell></row><row><cell>Imbalance Type</cell><cell cols="2">long-tailed</cell><cell cols="2">step</cell><cell cols="2">long-tailed</cell><cell>step</cell><cell>AVE</cell></row><row><cell>Imbalance Ratio</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell><cell>100</cell><cell>10</cell></row><row><cell>CE-IC (Ours)</cell><cell>20.14 2.1</cell><cell>11.37 0.7</cell><cell>25.61 1.1</cell><cell>11.34 1.0</cell><cell>59.08 1.3</cell><cell>41.91 1.0</cell><cell>53.48 1.2</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1565" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experimental perspectives on learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amri</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adasyn: Adaptive synthetic sampling approach for imbalanced learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international joint conference on neural networks (IEEE world congress on computational intelligence)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1322" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02413</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural network classification and prior class probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Application of decision rules for handling class imbalance in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>H?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schlicht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Gottschalk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08394</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kullback-leibler view of linear and log-linear pools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali E Abbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Logistic regression: relating patient characteristics to outcomes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliana</forename><surname>Tolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="533" to="534" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Uno: Uncertaintyaware noisy-or multimodal fusion for unanticipated input degradation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjiao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05611</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unifying view on dataset shift in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roc?o</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alaiz-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
