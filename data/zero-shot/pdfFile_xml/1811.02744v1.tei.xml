<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-07">7 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
							<email>liuyushen@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<email>zwicker@cs.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">View Inter-Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-07">7 Nov 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Feature learning for 3D shapes is crucial for 3D shape analysis, including classification <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b5">Han et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018;</ref><ref type="bibr" target="#b6">Han et al. 2018</ref>), retrieval <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b5">Han et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018;</ref><ref type="bibr" target="#b6">Han et al. 2018)</ref>, correspondence <ref type="bibr" target="#b5">(Han et al. 2016;</ref><ref type="bibr" target="#b6">Han et al. 2018</ref>) and segmentation <ref type="bibr" target="#b9">(Qi et al. 2017a;</ref><ref type="bibr" target="#b10">Qi et al. 2017b</ref>). In recent years, supervised 3D feature learning has produced remarkable results under large scale 3D benchmarks by training deep neural networks with supervised information <ref type="bibr" target="#b9">(Qi et al. 2017a;</ref><ref type="bibr" target="#b10">Qi et al. 2017b)</ref>, such as class labels and point correspondences. However, obtaining supervised information requires intense manual labeling effort. Therefore, unsupervised 3D feature learning with deep neural networks is an important research challenge.</p><p>Several studies have addressed this challenge <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b5">Han et al. 2016;</ref><ref type="bibr" target="#b3">Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018;</ref><ref type="bibr" target="#b6">Han et al. 2018)</ref> by training deep learning models using "supervised" information mined from the unsupervised scenario. This mining procedure is usually implemented using different prediction strategies, such as the prediction of a shape from itself by minimizing reconstruction error or embedded energy, the prediction of a shape from its context given by views or local shape features, or the prediction of a shape from views and itself together. These methods use multiple views to provide a holistic context of 3D shapes, and they make a single global shape prediction based on all views.</p><p>In contrast, our approach called View Inter-Prediction GAN (VIP-GAN) learns to make multiple local view interpredictions among neighboring views. The view interprediction task is designed to mimic human perception of view-dependent patterns. That is, based on changes between neighbor views, humans can easily imagine the center view between, while the neighbor views can also be reversely imagined based on the center. As a key idea, our network architecture implements the shape representation as a shapespecific global memory whose contents are learned to support all local view inter-prediction tasks for each shape. Intuitively, the memory aggregates information over all view inter-prediction tasks, which leads to a view-independent shape representation. Our experimental results indicate that the obtained representation is highly discriminative and outperforms competing techniques on several standard shape classification benchmarks.</p><p>More specifically, VIP-GAN considers multiple views taken around a 3D shape in sequence as the context of the 3D shape, and it separates each view sequence into several overlapping sections of equal length. It then learns to predict the center view from its neighbors in each section, and the neighbors from the center. Crucially, VIP-GAN includes a memory shared by all view predictions of each shape. We show that the system uses this memory to improve its view prediction performance, in effect by learning a view independent shape representation. VIP-GAN employs an RNNbased generator with an encoder-decoder structure to implement the view inter-prediction strategy in different spaces. The encoder RNN captures the content information and spa-tial relationship of the neighbors to predict the center in 2D view space, while the decoder RNN predicts the neighbors in a low-level feature space according to the center predicted by the encoder. To further improve the prediction of the center, we train the generator jointly with a discriminator in an adversarial way. In summary, our significant contributions are as follows: i) We propose VIP-GAN as a novel deep learning model to perform unsupervised 3D global feature learning through view inter-prediction with adversarial training, which leads to state-of-the-art performance in shape classification and retrieval.</p><p>ii) VIP-GAN makes it possible to mine fine-grained "supervised" information within the multi-view context of 3D shapes by imitating human perception of viewdependent patterns, which facilitates effective unsupervised 3D global feature learning.</p><p>iii) We introduce a novel implicit aggregation technique for 3D global feature learning based on RNN, which enables VIP-GAN to aggregate knowledge learned from each view prediction across a view sequence effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Supervised <ref type="formula">3D</ref>  Sfikas, Theoharis, and Pratikakis 2017; <ref type="bibr" target="#b14">Sinha, Bai, and Ramani 2016;</ref><ref type="bibr" target="#b15">Su et al. 2015;</ref><ref type="bibr" target="#b8">Johns, Leutenegger, and Davison 2016;</ref><ref type="bibr" target="#b8">Kanezaki, Matsushita, and Nishida 2018)</ref>, which aims to capture the mapping between 3D raw representations and class labels. The mapping is captured by spotting the distribution patterns among voxels <ref type="bibr" target="#b15">(Wu et al. 2015)</ref>, points in cloud <ref type="bibr" target="#b9">(Qi et al. 2017a;</ref><ref type="bibr" target="#b10">Qi et al. 2017b)</ref>, vertices on mesh <ref type="bibr" target="#b6">(Han et al. 2018)</ref>, or view features taken from different shapes <ref type="bibr" target="#b13">Shi et al. 2015;</ref><ref type="bibr" target="#b12">Sfikas, Theoharis, and Pratikakis 2017;</ref><ref type="bibr" target="#b14">Sinha, Bai, and Ramani 2016;</ref><ref type="bibr" target="#b15">Su et al. 2015;</ref><ref type="bibr" target="#b8">Johns, Leutenegger, and Davison 2016;</ref><ref type="bibr" target="#b8">Kanezaki, Matsushita, and Nishida 2018)</ref>. Among these methods, multi-view based 3D feature learning methods perform the best, where pooling is widely used for view aggregation. Unsupervised 3D feature learning.</p><p>Although unsupervised 3D feature learning methods <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b5">Han et al. 2016;</ref><ref type="bibr" target="#b3">Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018;</ref><ref type="bibr" target="#b6">Han et al. 2018)</ref> are not always with high performance as supervised ones, their promising advantage of learning without labels still draws a lot of attention. To mine "supervised" information from unsupervised scenario, unsupervised feature learning methods usually train deep learning models by different prediction strategies, such as the prediction of a shape from itself by minimizing reconstruction error <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018)</ref> or embedded energy <ref type="bibr" target="#b5">(Han et al. 2016)</ref>, the prediction of a shape from context <ref type="bibr" target="#b6">(Han et al. 2018)</ref>, or the prediction of a shape from context and itself together <ref type="bibr" target="#b3">(Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016)</ref>.</p><p>These methods employ different kinds of 3D raw representations, such as voxels <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b15">Wu et al. 2016;</ref><ref type="bibr" target="#b3">Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016)</ref>, meshes <ref type="bibr" target="#b5">(Han et al. 2016;</ref><ref type="bibr" target="#b6">Han et al. 2018)</ref> or point clouds <ref type="bibr" target="#b17">(Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018)</ref>, and accordingly, different kinds of context, such as spatial context of virtual words <ref type="bibr" target="#b6">(Han et al. 2018)</ref> or views <ref type="bibr" target="#b3">(Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016)</ref>, are employed. With the ideas of auto-encoder <ref type="bibr" target="#b12">(Sharma, Grau, and Fritz 2016;</ref><ref type="bibr" target="#b3">Girdhar et al. 2016;</ref><ref type="bibr" target="#b11">Rezende et al. 2016;</ref><ref type="bibr" target="#b17">Yang et al. 2018;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018)</ref>, classification <ref type="bibr" target="#b6">(Han et al. 2018)</ref> or generative adversarial training <ref type="bibr" target="#b15">(Wu et al. 2016;</ref><ref type="bibr" target="#b0">Achlioptas et al. 2018)</ref>, these methods effectively learn discriminative 3D features. Different from these methods, VIP-GAN tries to learn 3D features by performing view inter-prediction to mine fine-grained "supervised" information within the multi-view context of 3D shapes, where context formed by multiple views is first explored for 3D global feature learning with adversarial training. View synthesis and unsupervised video feature learning. View synthesis aims to generate novel views according to existing views. Deep learning based view synthesis has been drawing more and more research interests <ref type="bibr" target="#b15">(Tatarchenko, Dosovitskiy, and Brox 2016;</ref><ref type="bibr" target="#b8">Lotter, Kreiman, and Cox 2017)</ref>.</p><p>First tries teach deep learning models to predict novel views according to input views and transformation parameters <ref type="bibr" target="#b15">(Tatarchenko, Dosovitskiy, and Brox 2016)</ref>. To generate views with more detail (i.e. texture) and less geometric distortions, external image sets or geometric constraints are further employed.</p><p>Similarly, to predict the future frames in a video, the information of multiple past frames is aggregated by RNN (Lotter, Kreiman, and Cox 2017). However, these methods mainly focus on the quality of generated views rather than the discriminability of learned features, where we find the view quality is not a sufficient condition for the feature discriminability in our experiments. In addition, the knowledge learned in each prediction cannot be aggregated by these methods to represent the global features. Therefore, these methods cannot be directly used for unsupervised 3D feature learning from view inter-prediction, which highlights our novelty by differentiating VIP-GAN apart from them.</p><p>VIP-GAN is also different from unsupervised video feature learning studies. Sequential views of 3D shapes are different from video frames because there is no firm starting position in view sequences. Each view could be the first view because of 3D shape rotation. This requires VIP-GAN to be invariant to the initial view position, that is, no matter which view of a 3D shape is the first, the learned feature of the shape should be the same. This is the main characteristic that makes VIP-GAN different from unsupervised video feature learning (At test stage, sensitive to the first frame of a video). Similarly, unsupervised image feature learning cannot aggregate multiple views and employ multiple view consistency as VIP-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIP-GAN</head><p>Overview. The framework of VIP-GAN is illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. Using multiple local view inter-predictions, VIP-GAN aims to learn a global representation or feature F of a 3D shape m from V views v i sequentially taken around m, where i ? [1, V ]. Note that F is learned for each shape as an F dimensional vector, effectively serving as a viewindependent memory that is used in all local view interpredictions for the shape. Hence F implicitly aggregates the knowledge learned from all sections s i across the V views. Learning F is performed via gradient descent together with the other parameters in VIP-GAN, where F is randomly initialized. We split the set of views into V sections of equal length, where a section s i is centered at each view v i . We denote the center view of the section as c, and its N neighbors as n j , where j ? [1, N ] (N = 2 in <ref type="figure" target="#fig_1">Fig. 1</ref>). In each section s i , VIP-GAN first predicts the center c in 2D space from the neighbors n j . Conversely, it also predicts n j in feature space from the predicted center c ? .  VIP-GAN consists of two main components, the generator G and discriminator D. The goal of the generator is to predict the center view in each section from its neighbors in image space, and the neighbors from the center in feature space. G consists of a VGG19 network, an encoder RNN E (in red), a decoder RNN R (in green) and a deconvolutional network U (in blue), where E and R are implemented by Gated Recurrent Units (GRUs). In addition, the discriminator D (in purple) is a convolutional network to distinguish whether a center view is real or not. G and D are jointly trained in an adversarial manner. Generator G. In each section s i of shape m, the first task of generator G is to collect a feature vector h i that will be used to generate the predicted center view c ? . For this purpose, the generator encodes the content within the neighbor views n j and the spatial relationship among them. We extract the content of each n j as a 4096 dimensional feature vector f j by the last fully connected layer of a VGG19 network, where the resolution of input n j is 224 ? 224. We further encode the f j with their spatial relationship using an encoder RNN E. We provide the global feature F of shape m, our learning target, at the first step of the encoder E serving as a knowledge container or memory that keeps incorporating the knowledge derived from each view prediction. Different from pooling, which is widely used as an explicit view aggregation, this implicit aggregation enables VIP-GAN to learn from more fine-grained information, such as the spatial relationship among the neighbors n j in each section s i , and the connection between knowledge derived from different sections s i across V views of m. Finally, at the last step of the encoder E for each section s i we obtain a 4096 dimensional feature h i as the hidden state, which we subsequently use to generate the predicted center c ? using a deconvolutional network U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Center</head><p>By reshaping the 4096 dimensional h i into 256 feature maps of size 4?4, the deconvolutional network U starts generating the predicted center c ? with a resolution of 64 ? 64 through four deconvolutional layers. The deconvolutional layers employ 256, 128, 64, and 3 kernels, respectively, and each kernel has size 3 ? 3 and a stride of 2. In each deconvolutional layer, we use a leaky ReLu with a leaky gradient of 0.2. We utilize the L-2 loss between the predicted center view U(s i ) = c ? and the ground truth center c to measure the center prediction performance of G, denoted as loss L U ,</p><formula xml:id="formula_0">L U = U(s i ) ? c 2 2 .</formula><p>(1)</p><p>The second task of generator G is to reversely predict the neighbors n j from the predicted center c ? in each section s i . Different from the center view prediction task, we evaluate the prediction in feature space here. The two prediction tasks in different spaces enable VIP-GAN to more fully understand the 3D shape m. To predict both the content information within each n j and the spatial relationship among n j from the predicted center c ? , we employ a decoder RNN R with h i as initialized hidden state that predicts the features f ? j of each neighbor view n j step by step. Similar to the encoder E, we provide the global feature F at the first step of R, which is regarded as a reference for the following neighbor feature predictions. Then, f ? j is produced at the j-th step of R using the feature f j?1 of its previous counterpart as input. We predict the features f ? j in the same order as we provide the corresponding f j to the encoder E. We measure the neighbor prediction performance of G using L-2 loss in feature space,</p><formula xml:id="formula_1">L R = 1 N N j=1 R(s i ) j ? f j 2 2 ,<label>(2)</label></formula><p>where R(s i ) j = f ? j is the output at the j-th step of R. In summary, the loss of G is formed by the loss L U of U and the loss L R of R. Discriminator D. In preliminary experiments, we found that the quality of predicted center views c ? is not a sufficient condition to obtain a highly discriminative global feature F . For example, a complex and powerful deconvolutional network could generate c ? with higher quality than our simple one introduced before, but we found that the learned feature F is much less discriminative. This phenomenon is caused by the large capacity of the more complex deconvolutional network to generate high quality view c ? from any feature h i . However, this may decrease the discriminability of the learned feature F . What we really want to achieve is that the quality of predicted views c ? is mainly due to the discriminability of the learned feature F , rather than the powerful learning ability of the deconvolutional network.</p><p>To resolve this issue, we employ discriminator D with adversarial training to facilitate our simple deconvolutional network U. Specifically, D is a CNN with five layers, including four convolutional layers and a one dimensional fully connected layer, where the resolution of input views is 64 ? 64. Each convolutional layer contains 64, 128, 256, 512 kernels respectively, and each kernel has size 5 ? 5 and a stride of 2, where we employ a leaky ReLu with a leaky gradient of 0.2. In the last layer of D, a sigmoid function provides the probability that the input is a real center view. Finally, the loss of D is the cross entropy of the probability produced from each s i , as defined as L D in Eq. 3, where D(U(s i )) is the probability that D thinks the predicted center c ? from s i by U is real,</p><formula xml:id="formula_2">L D = log D(c) + log(1 ? D(U(s i ))).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial training.</head><p>Adversarial training is based on Generative Adversarial Networks (GAN) <ref type="bibr" target="#b4">(Goodfellow et al. 2014)</ref>. The predicted center c ? from the generator G is passed to the discriminator D with the real center c, where D tries to learn how to distinguish whether a center is real or not. With adversarial training, the discriminator D is trained to maximize the probability when the center is real while minimizing it when the center is generated by generator G, as defined in Eq. 3. In contrast, the generator G has to be trained to fool the discriminator D. Therefore, in s i , the loss L D2U for G from D is defined to make the predicted center U(s i ) generated by U more real,</p><formula xml:id="formula_3">L D2U = log(1 ? D(U(s i ))).<label>(4)</label></formula><p>Finally, we define the loss function of VIP-GAN by combining the aforementioned losses as in Eq. 5, where the weights ? and ? are used to control the balance among them,</p><formula xml:id="formula_4">L = L U + ?L R + ?L D2U .<label>(5)</label></formula><p>Note that simultaneously with the other network parameters, we also optimize the learning target F by minimizing L using a standard gradient descent approach by iteratively up- </p><formula xml:id="formula_5">dating F by F ? F ? ? ? ?L/?F ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results and analysis</head><p>In this section, the performance of VIP-GAN is evaluated and analyzed. First we discuss the setup of parameters involved in VIP-GAN. These parameters are tuned to demonstrate how they affect the discriminability of learned Parameter setup. The balance weights ? and ? are important for the performance of VIP-GAN. In this experiment, we explore the effects of ? and ? on the performance of VIP-GAN under ModelNet10 in terms of average instance accuracy and average class accuracy, as shown in <ref type="table" target="#tab_2">Table 1</ref>. Initially, the dimension F of global feature F is 4096, the center c gets N = 4 neighbors, and the V = 12 views of all 3D shapes under ModelNet10 are employed to train VIP-GAN in known-test mode. ? and ? are set to 1 and 0.05, respectively, since they make the initial values of loss L U , L R and L D2U comparable to each other, where a normal distribution with mean of 0 and standard deviation of 0.02 is used to initialize the parameters involved in VIP-GAN. First, the effect of ? is explored by incrementally increasing ? from 1 to 3 and 5. With ? = 3, best performance of VIP-GAN is achieved up to 94.05%, and the results with ? = 3 are better than the results with ? = 5. Then, the effect of ? is explored based on ? = 3 by increasing ? to 0.1 and decreasing ? to 0.01. These degenerated results show that the adversarial loss should not be over-or under-weighted. Subsequently, we highlight the contribution of discriminator D and decoder R to deconvolutional network U by incrementally setting ? and ? to 0. By setting ? to 0, the results with "(3,0)" are better than the results with "(3,0.01)", but worse than the results with "(3,0.1)". This phenomenon implies that the under-weighted GAN loss is not helpful to increase the discriminability of learned features. We observe a similar phenomenon by comparing between "(0,0.01)" and "(0,0)". The comparison between "(3,0)" and "(0,0)" shows that the decoder R significantly increases the discriminability of learned features. In summary, these results show that   <ref type="figure">Figure 2</ref>: The predicted centers generated with different pairs of balance parameters (?, ?).</p><p>the decoder R and the discriminator D can both improve the performance of VIP-GAN. However, R contributes more than D to U, and ? is less sensitive than ?. Furthermore, as mentioned before, the quality of predicted center c ? is not a sufficient condition to obtain a highly discriminative global feature F . By replacing our simple U with a more complex one employed in (Dosovitskiy and Brox 2016), the quality of predicted centers becomes higher, as shown in the comparison between "(0,0)" and "(0,0)C" in <ref type="figure">Fig. 2</ref>. On the other hand, the discriminability of the learned global feature F dramatically decreases, as illustrated by the comparison between "(0,0)" and "(0,0)C" in <ref type="table" target="#tab_2">Table 1</ref>. The reason for this is that the more complex deconvolutional network in (Dosovitskiy and Brox 2016) is too deep to facilitate effective error back propagation to train a highly discriminative global feature. To keep the network in (Dosovitskiy and Brox 2016) unchanged, the predicted views are generated in the resolution of 256 ? 256 rather than 64 ? 64, where the 224 ? 224 ground truth views are padded with pixel values of 255 to enable the computation of loss L U . Finally, we also highlight the importance of R and D by merely using L R or L D2U to train, as shown by "R" and "D" in <ref type="table" target="#tab_3">Table 2</ref>. Compared with the importance of U as "(0,0)" in <ref type="table" target="#tab_2">Table 1</ref>, R plays the most important role in VIP-GAN. The predicted centers c ? generated by different ? and ? are demonstrated in <ref type="figure">Fig. 2</ref>, where the tags marking each column are consistent with the parameters in <ref type="table">Table.</ref> 1. According to the ground truth, the complex deconvolutional network ("(0,0)C") generates centers with higher quality than our simple ones ("(0,0)"). The comparison between "(0,0)" and "(3,0)" shows that the decoder R slightly degenerates the quality of predicted centers. In addition, the adversarial loss weighted by small ? can make the predicted centers sharper, but also produce distortions, as illustrated by the comparison between "(0,0)" and "(0,0.01)", and the comparison between "(3,0)" and "(3,0.01)". The adversarial loss weighted by big ? will make the loss L U subtle with big distortions, as shown by "(3,0.1)".</p><p>The effects of F , N and V are further explored in <ref type="table" target="#tab_3">Table 2</ref>. By gradually decreasing F from 4096 to 2048 and 1024, the results are degenerated from 94.05% to 92.51% and 92.29%. To conduct this experiment with the rest of VIP-GAN un-changed, one more 4096 dimensional fully connected layer is employed before F is inputted in G. Then, the number N of neighbors in each section s i is explored by respectively decreasing N to 2 and increasing N to 6, based on the N = 4 structure with our best results. Although these results are degenerated from our best results, they are still good. The degeneration is caused by that less neighbors could not provide enough discriminative information to learn while more neighbors would bring redundant discriminative information. Following this, we decrease V to 6 and 3 gradually, the results are also decreased due to the less information for learning, where N is adjusted to 2 when V is set to 3. Subsequently, we employ conditional GAN to replace the GAN structure in VIP-GAN, where the ground truth neighbors are regarded as the conditions of the center. The highlevel features f j of neighbors are concatenated with the extracted feature of the center after the last convolutional layer in discriminator D, which is further followed by an extra convolutional layer and the one dimensional fully connected layer. Although the results dramatically decreased as shown by "cGan", it is still better than merely using U as listed "(0,0)" in <ref type="table" target="#tab_2">Table 1</ref>. These results imply that GAN is better than conditional GAN for 3D global feature learning in VIP-GAN, while both the adversarial loss of GAN and conditional GAN are helpful to improve the discriminability of learned features. Moreover, we also try to train VIP-GAN by bidirectional view sequences, since human can perform the view inter-prediction from either left to right or right to left in a view sequence, as shown by the results listed as "BiDir". However, no further improvement is obtained from the doubled training samples. Classification. We compare VIP-GAN with the stateof-the-art methods in classification under ModelNet40 and ModelNet10. The parameters under ModelNet40 are the same ones with our best results under Mod-elNet10 in <ref type="table" target="#tab_3">Table 2</ref>. The compared methods include MVCNN <ref type="bibr" target="#b15">(Su et al. 2015)</ref>, <ref type="bibr">ORION (Sedaghat et al. 2017)</ref>, 3DDescriptorNet <ref type="bibr" target="#b16">(Xie et al. 2018</ref>), Pairwise <ref type="bibr" target="#b8">(Johns, Leutenegger, and Davison 2016)</ref>, <ref type="bibr">GIFT (Bai et al. 2017)</ref>, PANORAMA (Sfikas, Theoharis, and Pratikakis 2017), VRN <ref type="bibr" target="#b1">(Brock et al. 2016)</ref>, <ref type="bibr">Rotation-Net (Kanezaki, Matsushita, and Nishida 2018)</ref>, Point-  <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref>, and FNet <ref type="bibr" target="#b17">(Yang et al. 2018)</ref>.</p><p>VIP-GAN significantly outperforms all its unsupervised competitors under ModelNet40, and some of them under ModelNet10, as shown by "Our", which is also the best result compared to eight top ranked supervised methods. For fair comparison, the result of VRN <ref type="bibr" target="#b1">(Brock et al. 2016</ref>) is presented without ensemble learning, and the result of Ro-tationNet(Kanezaki, Matsushita, and Nishida 2018) is presented with views taken by the default camera system orientation that is identical to the others. In addition, we try to train VIP-GAN under ShapeNet55 in unknown-test mode. Hence, we fix the parameters to extract features under ModelNet40 and ModelNet10, as shown by "Our1(SN55)". Although the results of LGAN <ref type="bibr" target="#b0">(Achlioptas et al. 2018)</ref> and FNet <ref type="bibr" target="#b17">(Yang et al. 2018</ref>) are better than "Our1(SN55)" under ModelNet10, it is inconclusive whether they are better than ours. This is because these methods are trained under a version of ShapeNet55 that contains more than 57,000 3D shapes, including a number of 3D point clouds. However, VIP-GAN is trained only under the 51,679 3D shapes from ShapeNet55 that are available for public download.</p><p>Finally, we explore whether "Our" could be further improved by more training shapes from ShapeNet55 in knowntest mode, as shown by "Our2(+SN55)". However, with the existing parameters, only comparable results are obtained. Moreover, we evaluate VIP-GAN under ShapeNet55 in known-test mode using the same parameters with our best results under ModelNet10 in <ref type="table" target="#tab_3">Table 2</ref>, as shown in the rightmost column "Our" in <ref type="table" target="#tab_8">Table 6</ref>. Similar to "Our2(+SN55)", with the existing parameters, only comparable results are obtained by more training shapes from ModelNet40, as shown by "Our+". Our novel implicit view aggregation. The effect of our novel implicit view aggregation is first explored by visualization. In <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, we compare the training loss of our framework with a fixed, non-trainable F set to zero, and our trainable F . Our approach is able to learn the characteristics of each shape to make up the missing information in each prediction, which reduces the training loss. The two losses show that the generator is getting to the Nash equilibrium.</p><p>In <ref type="figure" target="#fig_3">Fig. 3(b)</ref>, we further evaluate the semantic meaning of  our features by manipulating them algebraically, and visualizing the result via nearest neighbor retrieval in ModelNet10, as shown on the right. The retrieved shapes exhibit characteristics similar to both input shapes, such as the surface of the bed in the first row, and the bedhead in the second row. Finally, we compare our implicit view aggregation with the widely used explicit view aggregation pooling under ModelNet10. Here, we use the output h i of the encoder E as the feature of each view, and obtain the global feature of the shape by pooling all the h i together with maxpooling and meanpooling, where each h i is obtained with trainable F and non-trainable all zero F . In <ref type="table" target="#tab_6">Table 5</ref>, with trainable or non-trainable F , our implicit view aggregation is always superior to the pooling. Without the support of trainable F , the pooled features are pushed to be more discriminative than the ones with trainable F to minimize the loss, which makes the pooling results better. However, it is still not good enough to keep the loss as low as ours shown in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. Retrieval. VIP-GAN is further evaluated in shape retrieval under ModelNet40, ModelNet10 and ShapeNet55, as shown in <ref type="table" target="#tab_5">Table 4</ref>, <ref type="table" target="#tab_8">Table 6</ref>   <ref type="bibr" target="#b7">(He et al. 2018)</ref>.</p><p>In these experiments, the 3D shapes in the test set are used as queries to retrieve the rest shapes in the same set, and mean Average Precision (mAP) is used as a metric. In addition, we employ global features involved in our classification results in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_8">Table 6</ref> for the retrieval experiments under the three benchmarks.</p><p>As shown in <ref type="table" target="#tab_5">Table 4</ref>, our results of "Our" outperform all the compared results under ModelNet40, and sightly lower than the best results of 91.12 by GIFT under ModelNet10. However, it is inconclusive whether GIFT outperforms VIP- GAN, since the dataset used by GIFT is formed by randomly selecting 100 shapes from each shape class, which is much simpler than the whole benchmark that we used. In addition, with trained by more shapes from ShapeNet55, the result of "Our2" under ModelNet10 is a little bit higher than the result of "Our". Their available PR curves under ModelNet40 and ModelNet10 are also compared in <ref type="figure" target="#fig_4">Fig. 4</ref>. In <ref type="table" target="#tab_8">Table 6</ref> and <ref type="table" target="#tab_9">Table 7</ref>, the results of "Our" outperform all the compared results under ShapeNet55. Besides Taco <ref type="bibr" target="#b2">(Cohen et al. 2018)</ref> in <ref type="table" target="#tab_8">Table 6</ref>, the compared results without reference are from SHREC2017 shape retrieval contest <ref type="bibr" target="#b12">(Savva et al. 2017)</ref> under ShapeNet55 with the same names, where micro-averaged and macro-averaged methods are employed to compute the metrics. Similar to "Our2" under ModelNet10, with trained by more shapes from Model-Net40, "Our+" is a little bit better than "Our". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We proposed VIP-GAN, an approach for unsupervised 3D global feature learning by view inter-prediction that is capable of learning from fine-grained "supervised" information within the multi-view context of 3D shapes. Inspired by human perception of view-dependent patterns, VIP-GAN successfully learns more discriminative golbal features than state-of-the-art view-based methods that regard the multiview context as a whole. With adversarial training, the global features can be learned more efficiently, which further improves their discriminability. In addition, our novel implicit aggregation enables VIP-GAN to learn within the multi-view context by effectively aggregating knowledge learned from multiple local view predictions across a view sequence. Our results show that VIP-GAN outperforms its unsupervised counterparts, as well as some top ranked supervised methods under large scale benchmarks in shape classification and retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>VIP-GAN is composed of generator G and discriminator D. The global feature F is learned in G by view inter-prediction through encoder E, decoder R and deconvolutional net U.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>features in shape classification under ModelNet10 (Wu et al. 2015). Then, VIP-GAN is compared with state-of-the-art methods in shape classification and retrieval under Model-Net10 (Wu et al. 2015), ModelNet40 (Wu et al. 2015) and ShapeNet55 (Savva et al. 2017). All classification is conducted by a linear SVM (with default parameters in scikitlearn toolkit) under the global features learned by VIP-GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a)The effectiveness of our novel implicit view aggregation is shown by the comparison between the loss with nonzero trainable F and the loss with zero non-trainable F . (b)The learned global features are visualized by feature manipulation in the embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The comparison of PR curves for retrieval under ModelNet40 and ModelNet10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where ? is the learning rate. Modes for testing. Typically, there are two modes of unsupervised learning of features F of 3D shapes for testing, which we call the known-test mode and the unknown-test mode. In known-test mode, the test shapes are given with the training shapes at the same time, such that the features of test shapes can be learned with the features of training shapes together. In unknown-test mode, VIP-GAN is first pre-trained under training shapes. At test time, we then iteratively learn the features of test shapes by minimizing Eq. 5 with fixed pre-trained parameters of U, R and D.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The effects of balance weights ? and ? on the performance of VIP-GAN under ModelNet10.</figDesc><table><row><cell>(?,?)</cell><cell cols="8">(1,0.05) (3,0.05) (5,0.05) (3,0.1) (3,0.01) (3,0) (0,0.01) (0,0) (0,0)C</cell></row><row><cell>Instance ACC</cell><cell>92.73</cell><cell>94.05</cell><cell>93.50</cell><cell>92.84</cell><cell>91.19</cell><cell>92.51</cell><cell>83.37</cell><cell>84.80 75.77</cell></row><row><cell>Class ACC</cell><cell>92.23</cell><cell>93.71</cell><cell>93.01</cell><cell>92.50</cell><cell>90.62</cell><cell>92.08</cell><cell>82.05</cell><cell>83.96 74.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The effects of parameters on VIP-GAN under ModelNet10 in terms of accuracy.</figDesc><table><row><cell>Parameters</cell><cell>R</cell><cell>D</cell><cell cols="4">F (1024) F (2048) F (4096) N (2) N (6) V (6) V (3) cGan BiDir</cell></row><row><cell cols="3">Instance ACC 90.53 47.80</cell><cell cols="2">92.29</cell><cell>92.51</cell><cell>94.05</cell><cell>93.17 93.50 92.62 92.51 89.10 93.83</cell></row><row><cell>Class ACC</cell><cell cols="2">89.88 44.49</cell><cell cols="2">91.73</cell><cell>92.03</cell><cell>93.71</cell><cell>92.91 93.08 92.32 92.22 88.34 93.45</cell></row><row><cell>Ground truth Complex U</cell><cell>Our U</cell><cell>U+R</cell><cell>U+D</cell><cell cols="2">U+R+D U+R+Big D</cell></row><row><cell>(0,0)C</cell><cell>(0,0)</cell><cell>(3,0)</cell><cell>(0,0.01)</cell><cell>(3,0.01)</cell><cell>(3,0.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">The comparison of classification accuracy under</cell></row><row><cell cols="2">ModelNet10 and ModelNet40.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">Supervised MN40 MN10</cell></row><row><cell>MVCNN</cell><cell>Yes</cell><cell>90.10</cell><cell>-</cell></row><row><cell>MVCNN-Multi</cell><cell>Yes</cell><cell>91.40</cell><cell>-</cell></row><row><cell>ORION</cell><cell>Yes</cell><cell>-</cell><cell>93.80</cell></row><row><cell>3DDescriptorNet</cell><cell>Yes</cell><cell>-</cell><cell>92.40</cell></row><row><cell>Pairwise</cell><cell>Yes</cell><cell>90.70</cell><cell>92.80</cell></row><row><cell>GIFT</cell><cell>Yes</cell><cell>89.50</cell><cell>91.50</cell></row><row><cell>PANORAMA</cell><cell>Yes</cell><cell>90.70</cell><cell>91.12</cell></row><row><cell>VoxNet</cell><cell>Yes</cell><cell>-</cell><cell>92.00</cell></row><row><cell>VRN</cell><cell>Yes</cell><cell>91.33</cell><cell>93.80</cell></row><row><cell>RotationNet</cell><cell>Yes</cell><cell>90.65</cell><cell>93.84</cell></row><row><cell>PointNet++</cell><cell>Yes</cell><cell>91.90</cell><cell>-</cell></row><row><cell>T-L</cell><cell>No</cell><cell>74.40</cell><cell>-</cell></row><row><cell>LFD</cell><cell>No</cell><cell>75.47</cell><cell>79.90</cell></row><row><cell>Vconv-DAE</cell><cell>No</cell><cell>75.50</cell><cell>80.50</cell></row><row><cell>3DGAN</cell><cell>No</cell><cell>83.30</cell><cell>91.00</cell></row><row><cell>LGAN</cell><cell>No</cell><cell>85.70</cell><cell>95.30</cell></row><row><cell>LGAN(MN40)</cell><cell>No</cell><cell>87.27</cell><cell>92.18</cell></row><row><cell>FNet</cell><cell>No</cell><cell>88.40</cell><cell>94.40</cell></row><row><cell>FNet(MN40)</cell><cell>No</cell><cell>84.36</cell><cell>91.85</cell></row><row><cell>Our</cell><cell>No</cell><cell>91.98</cell><cell>94.05</cell></row><row><cell>Our1(SN55)</cell><cell>No</cell><cell>90.19</cell><cell>92.18</cell></row><row><cell>Our2(+SN55)</cell><cell>No</cell><cell>91.25</cell><cell>92.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparison of retrieval in terms of mAP under ModelNet40 and ModelNet10.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">MN40 MN10</cell></row><row><cell></cell><cell>GeoImage</cell><cell>51.30</cell><cell>74.90</cell></row><row><cell></cell><cell>Pano</cell><cell>76.81</cell><cell>84.18</cell></row><row><cell></cell><cell>MVCNN</cell><cell>79.50</cell><cell>-</cell></row><row><cell></cell><cell>GIFT</cell><cell>81.94</cell><cell>91.12</cell></row><row><cell></cell><cell>RAMA</cell><cell>83.45</cell><cell>87.39</cell></row><row><cell></cell><cell>Trip</cell><cell>88.00</cell><cell>-</cell></row><row><cell></cell><cell>Our</cell><cell>89.23</cell><cell>90.69</cell></row><row><cell></cell><cell>Our1(SN55)</cell><cell>87.66</cell><cell>90.09</cell></row><row><cell></cell><cell cols="2">Our2(+SN55) 88.87</cell><cell>90.75</cell></row><row><cell>Net++</cell><cell>(Qi et al. 2017b),</cell><cell>T-L</cell><cell>(Girdhar et al. 2016),</cell></row><row><cell>LFD,</cell><cell>Vconv-DAE</cell><cell cols="2">(Sharma, Grau, and Fritz 2016),</cell></row><row><cell cols="3">3DGAN (Wu et al. 2016), LGAN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">: The effects of our novel implicit view aggregation</cell></row><row><cell cols="2">under ModelNet10.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ACC Non-trainable F</cell><cell></cell><cell>Trainable F</cell></row><row><cell></cell><cell cols="4">MaxP MeanP MaxP MeanP</cell><cell>Our</cell></row><row><cell>Ins</cell><cell>84.58</cell><cell>87.22</cell><cell>81.72</cell><cell cols="2">82.49 94.05</cell></row><row><cell>Cla</cell><cell>83.95</cell><cell>87.38</cell><cell>80.60</cell><cell cols="2">81.73 93.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Retrieval and classification comparison in terms of Micro-averaged metrics under ShapeNetCore55.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Micro</cell><cell></cell></row><row><cell>Methods</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell cols="2">mAP NDCG</cell></row><row><cell cols="5">Kanezaki 81.0 80.1 79.8 77.2</cell><cell>86.5</cell></row><row><cell>Zhou</cell><cell cols="4">78.6 77.3 76.7 72.2</cell><cell>82.7</cell></row><row><cell cols="5">Tatsuma 76.5 80.3 77.2 74.9</cell><cell>82.8</cell></row><row><cell>Furuya</cell><cell cols="4">81.8 68.9 71.2 66.3</cell><cell>76.2</cell></row><row><cell cols="5">Thermos 74.3 67.7 69.2 62.2</cell><cell>73.2</cell></row><row><cell>Deng</cell><cell cols="4">41.8 71.7 47.9 54.0</cell><cell>65.4</cell></row><row><cell>Li</cell><cell cols="4">53.5 25.6 28.2 19.9</cell><cell>33.0</cell></row><row><cell>Mk</cell><cell cols="4">79.3 21.1 25.3 19.2</cell><cell>27.7</cell></row><row><cell>Su</cell><cell cols="4">77.0 77.0 76.4 73.5</cell><cell>81.5</cell></row><row><cell>Bai</cell><cell cols="4">70.6 69.5 68.9 64.0</cell><cell>76.5</cell></row><row><cell>Taco</cell><cell cols="4">70.1 71.1 69.9 67.6</cell><cell>75.6</cell></row><row><cell>Our</cell><cell cols="4">60.0 80.3 61.2 83.5</cell><cell>89.4</cell></row><row><cell>Our+</cell><cell cols="4">60.0 80.3 61.2 83.6</cell><cell>89.5</cell></row><row><cell cols="2">Our accuracy</cell><cell></cell><cell></cell><cell>82.97</cell></row><row><cell cols="2">Our+ accuracy</cell><cell></cell><cell></cell><cell>82.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Retrieval comparison in terms of Macro-averaged metrics under ShapeNetCore55.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Macro</cell><cell></cell></row><row><cell>Methods</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell cols="2">mAP NDCG</cell></row><row><cell cols="5">Kanezaki 60.2 63.9 59.0 58.3</cell><cell>65.6</cell></row><row><cell>Zhou</cell><cell cols="4">59.2 65.4 58.1 57.5</cell><cell>65.7</cell></row><row><cell cols="5">Tatsuma 51.8 60.1 51.9 49.6</cell><cell>55.9</cell></row><row><cell>Furuya</cell><cell cols="4">61.8 53.3 50.5 47.7</cell><cell>56.3</cell></row><row><cell cols="5">Thermos 52.3 49.4 48.4 41.8</cell><cell>50.2</cell></row><row><cell>Deng</cell><cell cols="4">12.2 66.7 16.6 33.9</cell><cell>40.4</cell></row><row><cell>Li</cell><cell cols="4">21.9 40.9 19.7 25.5</cell><cell>37.7</cell></row><row><cell>Mk</cell><cell cols="4">59.8 28.3 25.8 23.2</cell><cell>33.7</cell></row><row><cell>Su</cell><cell cols="4">57.1 62.5 57.5 56.6</cell><cell>64.0</cell></row><row><cell>Bai</cell><cell cols="4">44.4 53.1 45.4 44.7</cell><cell>54.8</cell></row><row><cell>Our</cell><cell cols="4">18.9 81.2 24.0 69.2</cell><cell>83.7</cell></row><row><cell>Our+</cell><cell cols="4">18.8 81.3 24.0 69.9</cell><cell>84.0</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yu-Shen Liu is the corresponding author. This work was supported by National Key R&amp;D Program of China (2018YFB0505400), the National Natural Science Foundation of China (61472202), and Swiss National Science Foundation grant (169151). We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1257" to="1271" />
		</imprint>
	</monogr>
	<note>GIFT: Towards scalable 3D shape retrieval</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Brock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D deep learning workshop (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised 3D local feature learning by circle convolutional restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5331" to="5344" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep spatiality: Unsupervised learning of spatially-enhanced global and local 3D features by deep neural network with coupled softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3049" to="3063" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3D object retrieval</title>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leutenegger</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matsushita</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nishida ; Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3813" to="3822" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3D classification and segmentation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3D structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4997" to="5005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting the PANORAMA Representation for Convolutional Neural Network Classification and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thermos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sfikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratikakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
	<note>Eurographics Workshop on 3D Object Retrieval</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeppano: Deep panoramic representation for 3D shape recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2339" to="2343" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning 3D shape surfaces using geometry images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="223" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</title>
		<idno>Wu et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1912" />
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning descriptor networks for 3D shape synthesis and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
