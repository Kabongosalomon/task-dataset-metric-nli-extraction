<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiedong</forename><surname>Zhuang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhui</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-image retrieval</term>
					<term>geo-localization</term>
					<term>transformer</term>
					<term>drone</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-view geo-localization is a task of matching the same geographic image from different views, e.g., unmanned aerial vehicle (UAV) and satellite. The most difficult challenges are the position shift and the uncertainty of distance and scale. Existing methods are mainly aimed at digging for more comprehensive fine-grained information. However, it underestimates the importance of extracting robust feature representation and the impact of feature alignment. The CNN-based methods have achieved great success in cross-view geo-localization. However it still has some limitations, e.g., it can only extract part of the information in the neighborhood and some scale reduction operations will make some fine-grained information lost. In particular, we introduce a simple and efficient transformer-based structure called Feature Segmentation and Region Alignment (FSRA) to enhance the model's ability to understand contextual information as well as to understand the distribution of instances. Without using additional supervisory information, FSRA divides regions based on the heat distribution of the transformer's feature map, and then aligns multiple specific regions in different views one on one. Finally, FSRA integrates each region into a set of feature representations. The difference is that FSRA does not divide regions manually, but automatically based on the heat distribution of the feature map. So that specific instances can still be divided and aligned when there are significant shifts and scale changes in the image. In addition, a multiple sampling strategy is proposed to overcome the disparity in the number of satellite images and that of images from other sources. Experiments show that the proposed method has superior performance and achieves the state-of-the-art in both tasks of drone view target localization and drone navigation. Code will be released at https://github.com/Dmmm1997/FSRA</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>C ROSS-VIEW geo-localization aims to match an image from one perspective to the most similar image from another perspective that represents the same geographic target. Its essence can be understood as a retrieval task of images from two different sources. Cross-view geo-localization can be applied to many fields such as agriculture, aerial photography, autonomous vehicles, drone navigation, event detection, accurate delivery, and so on <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The predecessors did a lot of arduous work <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, Ming Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng are with the Unmanned System Application Technology Research Institute, China Jiliang University, Hangzhou 310018, China (email: s20010802003@cjlu.edu.cn; zjuhjh@126.com; p1901085206@cjlu.edu.cn; ehzheng@cjlu.edu.cn). Enhui Zheng is the Corresponding Author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs</head><p>Transformer-based CNN-based Drone Satellite <ref type="figure">Fig. 1</ref>. The images on the left column is the input image from drone-view and satellite-view. The images in the middle column is the heatmap of CNNbased state-of-the-art network LPN <ref type="bibr" target="#b11">[12]</ref>. The images on the right column is the heatmap of our Transformer-based strong baseline. mostly studying the matching of ground panoramic images and satellite images. However, the intervention of the droneview will further expand the application of cross-view geolocalization <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b10">[11]</ref>. The application of matching UAVs and satellite images can be roughly divided into the following two types: Drone view target localization and Drone navigation. For example, the image acquired by UAVs is used to match the satellite image of the same geographic location. Generally, the satellite image contains precise GPS coordinate information. Indirectly, UAVs can be located in real-time by adopting the geographical information from matched satellite images and the navigation of the drone can be realized without GPS equipment.</p><p>In recent years, Due to the rapid development of deep learning, significant progress has been made in cross-view geo-localization. By observing the CNN-based method, we found two potential problems. (I) Cross-view geo-localization needs to dig out the relevant information between contexts. Images from different domains have positional transformations such as rotation, scale, and offset. Therefore, fully understanding the semantic information of the global context is necessary. However, CNN-based methods mainly focus on small discriminative regions due to a Gaussian distribution of effective receptive fields <ref type="bibr" target="#b12">[13]</ref>. Given the limitations of the pure CNN-based methods <ref type="bibr" target="#b13">[14]</ref>, the attention modules have been Transformer-based strong baseline framework. Output [cls token] marked with * is served as the global feature f . Classif ierLayer contains linear layer, relu, batchnorm1d and dropout. ID Loss represents CrossEntropy loss without label-smooth. In addition, we provide a simplified Transformer Layer structure, the specific structure can be found in Vit <ref type="bibr" target="#b15">[16]</ref>.</p><p>introduced to explore long-range relationships <ref type="bibr" target="#b14">[15]</ref>. However, most of the methods embed the attention mechanism into the deep convolutional network, which enhances contextual connections to a certain extent. (II) Fine-grained information is very important for the task of retrieval. The down-sampling operations i.e., pooling and stride convolution of the CNNbased method can reduce the resolution of the image, while invisibly destroying the recognizable fine-grained information.</p><p>In view of this, Transformer as a strong context-sensitive information extractor will have a role to play in Cross-View Geo-Localization. In order to improve the visibility of model performance, we draw the heatmaps regarding Grad-CAM <ref type="bibr" target="#b16">[17]</ref>, as in <ref type="figure">Fig. 1</ref>. The heatmaps come from the output of the last attention layer of the Vit but excluding the patch of learnable embedding. However, the output of the Transformer has only 3 dimensions, and we reduce the dimension of patches to the original image dimension by the inverse method of flattening. Thus the results of Vit concerns are visualized. We compare the heatmaps between the state-of-the-art CNN-based method LPN <ref type="bibr" target="#b11">[12]</ref> and our transformer-based strong baseline. Compared to the CNNbased method, the Transformer-based method can more clearly identify salient features such as buildings and roads and ignore background information such as trees.</p><p>Observing that Transformer-based methods have the ability to distinguish instances and inspired by the part-based method <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. A new approach for Feature Segmentation and Region Alignment (FSRA) is proposed to achieve segmentation of specific instances (patch-level) and feature alignment of regions (region-level) for the purpose of extracting the corresponding parts and aligning features even when there are position deviations or scale changes between images. The proposed FSRA consists of two parts. The first one is Heatmap Segmentation Module (HSM): As shown in the light green part in the middle of <ref type="figure">Fig. 3</ref>, this module divides the feature map according to the heat distribution of the feature map, and splits the feature map into several blocks from 1 to n to achieve the segmentation of patch-level instances. The other part is the Heatmap Alignment Branch (HAB): According to the segmentation feature map of HSM, the parts corresponding to different viewpoints are cut out in turn to calculate the loss, which helps the network to learn the desired heat distribution rules. As shown in the light blue part of <ref type="figure">Fig. 3</ref>, where the left side is the image taken by the UAV and the right side is the satellite image, both of which are approached by HAB to close the distance of the corresponding blocks.</p><p>In addition, inspired by LCM <ref type="bibr" target="#b23">[24]</ref>, we realize that satellite images are highly scarce in the University-1652 <ref type="bibr" target="#b0">[1]</ref> datasets, and expanded images can effectively improve network learning capabilities. In view of that, we propose a multiple sampling strategy to expand satellite imagery. The proposed multiple sampling strategy will increase the training time, but will not cause any additional burden on inference. Experiments show Heatmap Alignment Branch Heatmap Segmentation Module 2 2 2 2 2 2 2 2 2 1 1 1 2 1 2 2 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 2 2 1 2 1 1 1 2 2 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 1 2 1 2 2 2 2 1 2 <ref type="figure">Fig. 3</ref>. The framework of proposed FSRA. The Heatmap Segmentation Module (light green) reorders the heatmap information and evenly divides it into regions according to the distribution of the heatmap to achieve the purpose of segmenting different characteristic content. Heatmap Alignment Branch (light blue) pools the features of each region to obtain feature vectors and performs classification supervision on each feature vector. In order to achieve end-to-end learning, TripletLoss is applied to each branch to narrow the distance of the same feature content. FSRA also retains the global branch (light purple) of the transformer-based strong baseline. that our multiple sampling strategy can effectively improve the accuracy of the model.</p><p>In short, the main contributions of this paper are as follows.</p><p>? We propose a transformer-based strong baseline for crossview geo-localization and achieve competitive performance with CNN-based frameworks. ? For the problem caused by position offset and uncertainty of distance and scale, we designed FSRA to implement patch-level segmentation and region-level alignment. ? We have carefully analyzed and improved some tricks to try to solve some problems in cross-view geolocalization. To resolve the problem of sample size imbalance under different perspectives in the University-1652, a multiple sampling strategy that increases accuracy with no pain was proposed. To further improve the performance of cross-view geo-localization, we exhaustively analyzed the impact of KLLoss <ref type="bibr" target="#b24">[25]</ref> and TripletLoss and made new improvements on TripletLoss. ? The final framework FSRA achieves state-of-the-art performance on both tasks of drone view target localization and drone navigation in the University-1652.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we briefly review related previous works, including Cross-View Geo-Localization and Transformer In Vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-View Geo-Localization</head><p>Cross-view geo-localization mainly focuses on two matching tasks: the matching of ground and satellite views and the matching of drone and satellite views. CVUSA <ref type="bibr" target="#b25">[26]</ref> and CVACT <ref type="bibr" target="#b26">[27]</ref> constructed a panoramic street-view image to match the satellite-view image which is a challenging task, with a change in perspective spanning around 90 degrees.</p><p>Recently, a large-scale benchmark called VIGOR <ref type="bibr" target="#b27">[28]</ref>, which beyond One-to-one Retrieval, was proposed to bridge the gap between the realistic setting and existing geo-localization datasets. University-1652 <ref type="bibr" target="#b0">[1]</ref> innovatively proposed two missions based on drone perspective: drone view target localization and drone navigation, which proposed the drone-view as a transition view, reducing the difficulty of cross-view geolocalization.</p><p>Efficient Loss Function. A popular pipeline for crossview is to design suitable loss functions to train a CNN backbone, which is used to extract features from images. The CrossEntropy loss <ref type="bibr" target="#b28">[29]</ref>, TripletLoss <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, and contrastive loss <ref type="bibr" target="#b31">[32]</ref> are most widely used in the task of retrieval. Zheng et al. <ref type="bibr" target="#b32">[33]</ref> applied instance loss and verification loss together to optimize the network, and achieve competitive results. Hu et al. <ref type="bibr" target="#b33">[34]</ref> proposed a weighted soft margin ranking loss, which not only speeds up the training convergence but also improves the retrieval accuracy. Luo et al. <ref type="bibr" target="#b34">[35]</ref> proposed a BNNeck to improve the coordination of ID loss and TripletLoss. Sun et al. <ref type="bibr" target="#b35">[36]</ref> proposed a unified perspective to optimize ID loss and TripletLoss.</p><p>Part-based Fine-grained Features. Focusing on the finegrained information of different parts helps the model learn more comprehensive features. In addition, by dividing and supervising the feature maps, the sub-salience features in the image will be fully excavated. Fine-grained regions can be manually generated by person but also can be automatically learned by supervised methods. And the part-based finegrained features have been proven reliable in the task of retrieval <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. LPN <ref type="bibr" target="#b11">[12]</ref> proposed the square-ring partition strategy to allow the network to pay attention to more fine-grained information at the edge and achieved a huge improvement. PCB <ref type="bibr" target="#b17">[18]</ref> applied a horizontal splitting method for human body parts to extract highlevel segmentation features. AlignedReID++ <ref type="bibr" target="#b21">[22]</ref> automatically aligned slice information without introducing additional supervision to solve pedestrian misalignment problems caused by occlusion, view variation, and attitude deviation. MGN <ref type="bibr" target="#b18">[19]</ref> designed a slicing network that combines multi-branch and characterization metric dual learning strategies to extract global coarse-grained and local fine-grained features. MSCAN <ref type="bibr" target="#b18">[19]</ref> proposed Spatial Transform Networks to learn the local features of various parts of the human body, and merge the local features and global features into the final feature representation. PL-Net <ref type="bibr" target="#b20">[21]</ref> introduces a part loss to realize automatic detection of various parts of the human body, thereby increasing the discrimination on unseen persons. Rodrigues et al. <ref type="bibr" target="#b41">[42]</ref> addressed the temporal gap between scenes by proposing a semantically driven data augmentation technique that gives Siamese networks the ability to hallucinate unseen objects, and then apply a multi-scale attentive embedding network to perform matching tasks. Our proposed FSRA is also one of the part-based methods which is inspired by the LPN, the difference is that we do not add additional supervision but achieve automatic region segmentation, which makes our FSRA have excellent robustness and resistance to position shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transformer In Vision</head><p>The attention mechanism <ref type="bibr" target="#b42">[43]</ref> of the transformer model was first proposed to solve problems in the field of Natural Language Processing. Subsequently, the strong visual performance of the transformer shown the superiority of its structure. Recently, Han et al. <ref type="bibr" target="#b43">[44]</ref> and Salman et al. <ref type="bibr" target="#b44">[45]</ref> investigated the application of the transformer in the field of computer vision.</p><p>Transformer In Various Field. Alexey et al. <ref type="bibr" target="#b15">[16]</ref> first applied the transformer model to the task of classification, and then the development of the transformer in vision was in full swing. The transformer has achieved competitive results in most mainstream visual fields, such as object detection, semantic segmentation, GAN, Super-Resolution, Reid, etc. DETR <ref type="bibr" target="#b45">[46]</ref> was the first object detection framework that successfully integrates the transformer as the central building block of the detection pipeline. SETR <ref type="bibr" target="#b46">[47]</ref> treated semantic segmentation as a sequence-to-sequence prediction task through a pure transformer. TransGAN <ref type="bibr" target="#b47">[48]</ref> built a generator and a discriminator based on two transformer structures. TTSR <ref type="bibr" target="#b48">[49]</ref> restored the texture information of the image super-resolution result based on the transformer. TransReID <ref type="bibr" target="#b49">[50]</ref> applied the transformer to the field of retrieval for the first time and achieved similar results with the CNN-based method. Yu et al. <ref type="bibr" target="#b50">[51]</ref> extend transformer model to Multimodal Transformer (MT) model for image captioning and significantly outperformed the previous state-of-the-art methods.</p><p>Combination Of CNN And Transformer. ConvTransformer <ref type="bibr" target="#b51">[52]</ref> mapped the input sequence to a feature map sequence using an encoder based on a multi-headed convolutional self-attentive layer, and then decoded the target synthetic frame from the feature map sequence using another deep network containing a multi-headed convolutional self-attentive layer. Conformer <ref type="bibr" target="#b52">[53]</ref> relied on Feature Coupling Unit (FCU) to interactively fuse local and global feature representations at different resolutions. Mobile-Former <ref type="bibr" target="#b53">[54]</ref> was a parallel design of MobileNet and Transformer with a bi-directional bridge which enabled bi-directional fusion of local and global features.</p><p>Transformer In Cross-View. In the cross-view domain, some novel and effective transformer structures have also been proposed to implement different downstream tasks.Chen et al. <ref type="bibr" target="#b54">[55]</ref> proposed a pair of cross-view transformers to transform the feature maps into the other view and introduce cross-view consistency loss on them. Yang et al. <ref type="bibr" target="#b55">[56]</ref> presented a novel framework that enables reconstructing a local map formed by road layout and vehicle occupancy in the bird's-eye view given a front-view monocular image only, and a cross-view transformation module was proposed to strengthen the view transformation and scene understanding. Tulder et al. <ref type="bibr" target="#b56">[57]</ref> presented a novel cross-view transformer method to transfer information between unregistered views at the level of spatial feature maps, which achieved remarkable results in field of Multi-view medical image analysis. Yang et al. <ref type="bibr" target="#b57">[58]</ref> proposed a simple yet effective self-cross attention mechanism to improve the quality of learned representations. Which improved the generalization ability and encourages representations to keep evolving as the network goes deeper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we will introduce the details of our proposed method, the complete network structure as shown in <ref type="figure">Fig.  3</ref>. Firstly, the structure of the vision transformer will be introduced in Section III-A. Secondly, we will introduce the details of the proposed FSRA in Section III-B. Then a multiple sampling strategy to improve accuracy without pain will be introduced in Section III-C. Finally, we will introduce other tricks we applied in Section III-D, including the specific process of our implementation of TripletLoss and mutual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformer-Based Strong Baseline</head><p>Following the general strong baseline for the University-1652 benchmark <ref type="bibr" target="#b0">[1]</ref>, we build a transformer-based strong baseline for cross-view geo-localization. Our baseline consists of two parts: feature extraction and classification supervised learning. As in <ref type="figure">Fig. 2</ref>. Given an input x ? R H?W ?C , where H, W , C represent its height, width, and channels. Then input will be divided into N fixed-size patches{x i p |i = 1, 2, ? ? ? , N } and flatten into a sequence. An extra learnable embedding token denoted as x cls is merged into spatial information to extract robust features through supervision learning. The output [cls token] as shown in <ref type="figure">Fig. 2</ref> is regarded as a global feature representation f . Position information is added to each patch through learnable position embedding. The input sequence can finally be expressed as follows.</p><formula xml:id="formula_0">Z 0 = [ x cls ; F(x 1 p ); F(x 2 p ); ? ? ? ; F(x N p )] + P<label>(1)</label></formula><p>Where Z 0 represents input sequence embeddings. F is linear projection mapping the patches to D dimensions. P ? R (N +1)?D is the position embeddings. L in <ref type="figure">Fig. 3</ref> represents the depth of the transformer layers. The transformer attention mechanism allows each layer of the transformer to have insight into the global context, which overcomes the limitation of the receptive field of the convolutional neural network. In addition, the down-sampling operation is no longer needed.</p><p>Position Embeddings. The image classification and the cross-view tasks are different in the resolution of the input, so the position embedding parameters can not directly be loaded from the pre-training weights on ImageNet. The parameters of position embedding are learnable.</p><p>Extra Learnable Embedding. The characteristic of the transformer structure is that it does not change the dimensions of the input data, and the output contains contextual information, which can represent global features. An Extra learnable parameter is added to the input to act as a global feature vector, and the parameters are also learnable.</p><p>Transformer Layers. Transformer Layers play the same role as the backbone to extract the contextual semantic relationship between each patch. Its structure has shown on the right side of <ref type="figure">Fig. 2</ref>, which takes all Patches containing Position Embedding as inputs, and finally outputs feature vectors of the same dimension as the original inputs after Multi-Head Attention.</p><p>Supervision Learning. Transformer-based strong baseline only regards classification results as supervision information and applies CrossEntropy loss without label-smooth as ID loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Concrete Implementation Of FSRA</head><p>Experiments in Effect of the Transformer in Cross-View show that the transformer-based strong baseline can achieve impressive performance in cross-view geo-localization. However, positional shift and Uncertainty of distance and scale are still major challenges to overcome. Although it is important to extract global features that are robust and contextually linked, much previous work has also shown that part-based methods are significantly more effective for image retrieval.</p><p>Aligning each part with features is a straightforward way to allow part-based methods to achieve end-to-end training. Based on that, we consider whether there is a reasonable and simple way for the model to learn the category to which each patch belongs, such as buildings, roads, and trees, so that we can segment and align them according to the category to which they belong. We suppose whether it is possible to cut out the characteristics of different categories according to the appearance of the heatmap and analyze the above problems as follows.</p><p>How to segment specific content. HSM was proposed to achieve the purpose of segmenting different instances such as buildings, roads, and trees. The overall idea is very simple. As in <ref type="figure">Fig. 4</ref>, we take n = 2 as an example and divide the heatmap into two categories based on the magnitude of the heat value, with the large heat value being the foreground and the small heat value representing the background. As shown in the thermodynamic diagram, it is easy to see that most Heatmap Alignment Inputs Heatmap Drone Satellite <ref type="figure">Fig. 4</ref>. The left column of the figure is the input images from the drone-view and the satellite-view in the same geographic location. In the middle is the heatmap corresponding to the output of the FSRA. The right is the regional distribution generated by the HSM, the red part can be understood as the building part (foreground), and the green part is the background.</p><p>of the building parts have larger thermal values, while the trees and background parts have smaller thermal values. The network pays different levels of attention to different parts, which would produce a certain regularity in the distribution of heatmap. HSM is inspired by that. We perform a uniform segmentation of the feature map according to the thermal distribution. As shown on the right side of <ref type="figure">Fig. 4</ref>, it is obvious that we have almost entirely distinguished the buildings from the other instances.</p><p>In the following, we will describe the detailed implementation steps of the segmentation. Firstly, we get all the outputs L ? R B?N ?S (where B stands for batch size, N stands for patch size, and S stands for the length of the feature vector corresponding to each patch) except for cls token through the forward propagation of the transformer, which can be represented as follows.</p><formula xml:id="formula_1">L = [F(x 1 p ); F(x 2 p ); ? ? ? ; F(x N p )]<label>(2)</label></formula><p>The thermal value of each patch can be represented as follows.</p><formula xml:id="formula_2">P c = 1 S S i=1 M i c = {1, 2, ? ? ? , N }<label>(3)</label></formula><p>where P c represents the heat value of the c th patch. M i represents the c th patch corresponds to the i th value of the feature vector. In short, we do an averaging operation for the feature vector of each patch to represent the thermal value of the patch. Then, we sort the value of P 1?N in descending order and divide patches equally according to the number of regions n. The number of patches corresponding to each region is as follows.</p><formula xml:id="formula_3">N i = { N n i = {1, 2, ? ? ? , n ? 1} N ? (n ? 1) ? N n i = n<label>(4)</label></formula><p>where N i represents the number of patches for the i th region, ? is the floor function. Finally, divide L into n parts <ref type="table">3 3 3 1 3 2 3 3 2 3 2 3 2 2 2 1  2 3 2 2 3 3 3 3 3 3 3 3 3 3 2 3  3 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3  3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3  3 3 3 3 3 3 2 3 3 3 2 2 2 2 3 3  3 3 3 3 3 3 2 2 2 3 2 3 3 2 3 3  1 1 3 3 2 1 1 2 2 1 1 1 1 3 1 2  3 2 2</ref>  <ref type="formula" target="#formula_0">2 2 2 2 2 1 1 1 1 1 1 1 1  2 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1  1 2 2 1 2 2 1 1 1 1 1 1 1 1 1 1  2 1 1 1 2 2 1 2 2 2 1 1 1 1 1 1  2 2 2 2 2 1 1 2 3 2 1 2 1 1 1 2  2 2 2 1 1 1 1 1 1 1 2 2 1 1 2 1  2 1 2 1 1 1 3 2 1 3 2 3 2 1 3 1  2 2 1 1 1 2 2 1 1 3 1 2 2 2 1 1  2 1 1 1 1 3 2 2 3 2 3 1 1 2 1 2   1 1 1 1 1 1 1 1 1 1   2 2 2 2 2 2 2 2 2 2   1 1 1 1 1 1 1 1 1</ref>   <ref type="table">1 2 2 3 2 3 3 3 3 3 3 3 3 2 3 3  1 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3  1 2 2 1 2 3 3 3 3 3 3 3 3 3 3 3  1 2 3 2 3 3 2 3 3 3 3 3 3 3 3 3  2 2 2 2 1 2 1 1 2 1 3 1 2 3 3 3  2 2 3 2 2 2 1 1 1 3 3 3 2 2 3 3  2 2 2 1 1 1 1 2 2 2 3 3 3 3 3 3</ref> 1 1 1 1 1 1 1 2 1 1 1 2 2 2 3 3 1 1 1 1 1 2 1 1 1 1 2 1 3 1 2 3 2 2 1 1 1 2 2 1 1 1 1 2 2 1 3 3 1 1 1 1 1 2 1 1 1 1 1 1 2 2 2 2 1 2 2 2 2 3 1 1 1 1 1 2 2 3 3 3 1 1 2 3 2 3 2 1 1 1 1 1 2 3 3 3 1 3 2 2 1 2 2 1 1 1 1 2 1 3 3 3 2 2 2 2 2 1 2 1 1 2 1 1 1 2 3 3 3 2 2 2 2 1 2 3 3 1 2 2 2 2 2 3 Feature Alignment <ref type="figure">Fig. 5</ref>. The left column is the input images from the drone and satellite views at the same geographic location, the middle column is the featuremaps generated by HAB with regions n = 3. In the right is the feature vector obtained by the average pooling of each region.</p><p>in order, and each part corresponds to a region and then we can tag each region as a category as shown in the right column of <ref type="figure">Fig. 4</ref>. Relying on HSM alone does not allow the model to move in the direction of focusing on what we want, so we need to develop an alignment supervision for this partitioning law to allow the model to distinguish between instances. The number of regions n is a hyperparameter. In the following ablation experiments, we found that n = 3 performed the best. The proposed HSM is located in the light green part of <ref type="figure">Fig.  3</ref>. It is worth mentioning that HSM is implemented based on patch-level. Alignment between specific content. HAB was proposed to achieve the effect of feature alignment. As in <ref type="figure">Fig. 5</ref>. After successfully segmenting the specific content, we divide all patches into n regions. <ref type="figure">Fig. 5</ref> takes n = 3 as an example. In essence, all patches are divided into 3 categories, and we use 1-3 to distinguish. The next step is to perform feature alignment based on the corresponding content in different regions. We respectively take out the part of buildings as f 1 , the part of roads as f 2 , and the part of trees as f 3 . Then a pooling operation is performed on f 1?3 to obtain the feature vector V i ? R B?N i ?S , i = {1, 2, 3} that characterizes each specific content. The visualization process can be seen on the right side of <ref type="figure">Fig. 5</ref>. The expression of V i is as follows.</p><formula xml:id="formula_4">V i = 1 N i N i j=1 f j i i = {1, 2, ? ? ? , n}<label>(5)</label></formula><p>where n stands for the number of regions (n is set to 3 in <ref type="figure">Fig. 5</ref>). f j i stands for the feature vector of the j th patch of the i th instance region. In short, V i is obtained by taking out all the patches in each region and taking the average pooling operation.</p><p>After the above steps, we obtain the vector expression of the corresponding feature content, and then we classify each feature content separately through a Classif ierLayer. In addition, to allow the model to establish more accurate matching relationships, we apply TripletLoss as in <ref type="figure">Fig. 3</ref> to all regions to narrow the distance between regions. The specific implementation will be explained in section III.D. The proposed HAB is located in the light blue part of <ref type="figure">Fig. 3</ref>.</p><p>It is worth noting that our HAB method is region-level feature alignment, and the division of regions is determined by HSM. The reason why HAB can achieve good performance is that it distinguishes the features of different instances, which is conducive to the model not only paying attention to the global salient features, but also paying attention to the details of the background, which will help the model extraction more comprehensive fine-grained features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A Multiple Sampling Strategy</head><p>There are some unstable factors during the training process based on the transformer model. For example, a model with the same settings is trained twice, the results obtained will have a large margin. However, We found that the main reason may be that there is only one image per category in the satellite-view, which results in only one image from other views at one time. This case will cause an imbalance between the satellite images and other images. Therefore, a multiple sampling strategy is proposed to alleviate the problem of sample imbalance.</p><p>We set a hyperparameter k, which represents the number of sampling. The specific implementation is as follows. Firstly, derive the image under the satellite perspective from the Unversity-1652, and enhance it to generate k augmented satellite images. Augmentation methods include random shifting, random padding, random cutting, random color enhancement, etc. At the same time, k images from other perspectives are randomly selected, which is the same category as the corresponding satellite perspective.</p><p>The detailed experiment on the number of sampling k was conducted in the part of the ablation study in Section IV, and the results of the experiment show that FSRA performed best when k = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Tricks On Cross-View</head><p>Mutual Learning Based on Cross-View. Cross-view geolocalization is a multi-input and multi-output task. Given this, we introduce a method of self-distillation. The specific implementation is as follows. Establish learning relationships between outputs from different domains to narrow the distance between similar instances. The calculation formula of KL divergence loss is shown below.</p><formula xml:id="formula_5">KLDiv(O 1 ||O 2 ) = N i=1 p(O i 1 ) ? log p(O i 1 ) q(O i 2 )<label>(6)</label></formula><p>p(x i ) = log( e xi j e xj )</p><p>q(x i ) = e xi j e xj</p><p>where O 1 represents the target output. O 2 represents the learning output from the model. The mutual learning loss function is expressed as follows. <ref type="table">3  4  5  6  7  8  1  2  3  4  5  6  7  8</ref> dist_an dist_ap ? <ref type="figure">Fig. 6</ref>. The number 1-8 indicates the category the image belongs to. The light green part represents 8 images from the views of drones or satellites, the light purple part represents 8 images from the views of satellites or drones, dist ap represents the distance between pictures of the same category, and dist an represents the distance between pictures of different categories, The red ? stands for that the distance is not calculated for images from the same views.</p><formula xml:id="formula_8">KLLoss = KLDiv(O d O s ) + KLDiv(O s O d ) (9) 1 2</formula><p>where O d stands for the output of the drone-view image after forwarding propagation. O s stands for the output of the satellite-view image after forwarding propagation.</p><p>We verify the effectiveness of KLLoss in the ablation study. Experiments show that when KLLoss is applied alone, the accuracy of the model is significantly improved, but when KLLoss is applied together with TripletLoss, the accuracy of the model is not improved significantly. This may be caused by the same optimization direction of TripletLoss and KLLoss.</p><p>TripletLoss based on Cross-View. Only using CrossEntropy loss can not make the model end-to-end. When testing the accuracy of the model, Euclidean distance is used to judge the similarity between samples. TripletLoss can act as a supervisor to narrow the distance between the same targets from different domains. The TripletLoss can be formulated as follows.</p><formula xml:id="formula_9">T L = d(a, p) ? d(a, n) + M + (10) d(a, x) = a ? x 2<label>(11)</label></formula><p>where ? + represents max(?,0) operation. ? 2 represents a 2-norm operation. M is the value of margin. We apply Euclidean distance in <ref type="bibr">Equation 11</ref> to measure the distance between vectors. In Equation 10, we compute the TripletLoss with M = 0.3 in all our experiments.</p><p>Unlike traditional TripletLoss, the task of cross-view is to match images from different domains, and it is not essential to be distinguished from images of the same perspective. Therefore, we only calculate TripletLoss for images between different views. As in <ref type="figure">Fig. 6</ref>, for example, we take out an image from the light green set (drone/satellite view) to calculate the TripletLoss with all images from the light purple set (satellite/drone view).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>We first introduce a large-scale cross-view geo-localization dataset in Section IV-A. Then Section IV-B describes the implementation details. We provide the comparison with stateof-the-art methods in Section IV-C, followed by the ablation study in Section IV-D.  <ref type="table" target="#tab_6">I  STATISTICS THE NUMBER OF IMAGES, BUILDINGS, AND UNIVERSITIES   FROM VIEW OF DRONE, SATELLITE, AND STREET IN THE TRAINING SET   AND TEST SET OF THE UNIVERSITY-1652 DATASET. AND STATISTICS THE  IMAGES NUMBER OF QUERY AND GALLERY IN THE TEST SET. THERE ARE   NO DUPLICATE UNIVERSITIES IN THE TRAINING SET AND TEST</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets And Evaluation Protocol</head><p>Our method is mainly used to solve UAV-related problems, including drone view target localization and drone navigation. We have done a lot of experiments based on the large-scale dataset, University-1652 <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table I</ref> shows the number of images from different views of the University-1652 dataset during training and testing. The column of classes indicates the number of buildings, and the column of university indicates the number of universities included in the sample. The entire dataset contains a total of 72 universities, and there is no intersection between the training set and the test set.</p><p>University-1652 is a multi-view multi-source benchmark for drone-based geo-localization, which contains images from three platforms, i.e., synthetic drones, satellites, and ground cameras. University-1652 is the first large-scale geolocalization dataset contained drone-view and enables two tasks, i.e., drone view target localization (Drone ? Satellite) and drone navigation (Satellite ? Drone). It aims to improve the accuracy of matching the images between drone-view and satellite-view. The dataset collected 1,652 buildings from 72 universities in the world. As in <ref type="table">Table I</ref>, the training set includes 701 buildings of 33 universities, and the testing set includes the 951 buildings of the 39 universities. The buildings in the training set and the test set have no overlap. There are 701 buildings with 50,195 images for training, which contains 37,854 drone-view images, 701 satellite-view images, and 11,640 street-view images.</p><p>For testing, In the drone view target localization task (Drone?Satellite), there are 37,855 drone-view images in the query set and 701 true-matched satellite-view images, and 250 satellite-view distractors in the gallery. There is only one true-matched satellite-view image under this setting. In the drone navigation task (Satellite ? Drone), there are 701 satellite-view query images, and 37,855 true-matched droneview images, and 13,500 drone-view distractors in the gallery. There are about 54 true-matched drone-view images that can be matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In data processing. We apply a multiple sampling strategy. Considering that there is only one satellite image for each category, image augmentation is applied to extend the satellite set for alleviating the imbalance of images in different domains.</p><p>In network structure and training strategy. We adopt a small size Vision Transformer (Vit-S) pretrained on ImageNet as our backbone. We have adopted the FSRA structure which regions the output of the transformer by HSM, and aligns the feature map by HAB. In terms of parameter initialization, we adopt kaiming initialization <ref type="bibr" target="#b58">[59]</ref> for the classifier module. In training, we resize the input image to the size of 256?256 and perform image augmentation, e.g., random padding, random cropping, and random flipping. For the optimizer, we adopt stochastic gradient descent (SGD) with momentum 0.9 and weight decay 0.0005 with a mini-batch of 8. For the setting of the initial learning rate, the backbone parameter is set to 0.003, and the rest of the learnable parameters are set to 0.01. The learning rate of all parameters are decayed by 0.1 in the epoch of 70 and 110 respectively, the model is trained for 120 epochs in total.</p><p>In the loss function. We use the CrossEntropy loss as the classification loss function and adopt TripletLoss with a margin of 0.3 to narrow the distance of the same target from different domains. Besides, KL divergence loss is introduced to narrow the distance of the classification vectors.</p><p>During the test. We utilize the Euclidean distance to calculate the similarity between query images and candidate images in the gallery set. Our model is based on the framework of Pytorch, and all experiments are performed on Nvidia GTX 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison With Existing Methods</head><p>On the University-1652 <ref type="bibr" target="#b0">[1]</ref> dataset, we employ the proposed FSRA to compare with existing competitive methods. As shown in Table II, in the task of Drone ? Satellite, the proposed HAB achieved 82.25% Recall@1 and 84.82% AP; In the task of Satellite ? Drone, FSRA has achieved 88.45% Recall@1 and 83.37% AP. All our experiments only use drone and satellite views for training. The performance has surpassed state-of-the-art method e.g., LPN by a large margin of about 6% AP improvement. When we adopt different sampling strategies, the experimental results of our method have been further improved. When we use 3? sampling, the value of Recall@1 rises from 82.25% to 84.51% and the value of AP rises from 84.82% to 86.71% in the drone view target localization task (Drone?Satellite). The value of Recall@1 rises from 87.87% to 88.45% and the value of AP is from 81.53% to 83.37% in the drone navigation task (Satellite?Drone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>To verify the effectiveness of our method, we design several ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orignal Image</head><p>ResNet Heatmap Transformer Heatmap <ref type="figure">Fig. 7</ref>. The left is the original image, the middle is the heatmap of the last layer of ResNet-50, and the right is the heatmap of the last layer of Vit-S.</p><p>Effect of the Transformer in Cross-View. We bring the transformer network structure into the field of cross-view and compare the performance of Transformer-based and ResNetbased networks. As shown in <ref type="table">Table III</ref>. The Vit-S network with a single branch outperforms ResNet-50 by 9.31% and outperforms ResNet-101 by 6.3%, and the inference time is only 1.21? of ResNet-50, which is faster than ResNet-101. Besides, we also compared the accuracy and speed of Vit-B with other backbones. We found that deepening the transformer can not bring a significant improvement. The transformer's attention mechanism has its limitations, and the impact of its model size on performance depends on the size of the data volume. University-1652 is a 10,000-level data set, which is not suitable for large-scale transformer networks. Therefore, we use Vit-S as the backbone for other ablation experiments. Through the comparison of baseline between CNN-based method and Transformer-based method, we found that there is a large margin between the CNN-based method and the Transformer-based method. Since we checked the heatmap based on ResNet-50 and Vit-S respectively. As in <ref type="figure">Fig.  7</ref>. The attention mechanism allows the network to focus on global information, while the CNN-based approach will only focus on notable information but ignore the peripheral features. In addition, the heatmaps generated by the Transformer-based method can segment buildings, roads, and trees, which pave the way for our method.</p><p>Effect of the number of regions. The number of regions is an important indicator in our network. By default, we deploy n = 3. The model only applies the global branch of Vit when n = 0. When n = 1, the HAB deploys global average pooling of the feature vectors and concats them with the global branch of Vit. We make an experiment to verify the influence of the number of regions on the accuracy of Recall@1 and AP, as in <ref type="figure" target="#fig_1">Fig. 8</ref>. When the number of regions n = 3, all indicators are the best. We believe that when n = 3, the proposed FSRA divides the images of the University-1652 <ref type="bibr" target="#b0">[1]</ref> dataset into three categories: buildings, roads, and trees. And the features between different domains can be well segmented and aligned. When the number of regions n = 2, the proposed FSRA divides the image into two categories: architecture and background, which also achieves good performance.</p><p>Robustness of FSRA to position shifting. In order to verify the robustness of FSRA against position shifting, two different shifting methods are proposed for testing: BlackP ad(BP ) and F lipP ad(F P ). BlackP ad fills the black block with width P on the left side of the image and cuts out the image  <ref type="table" target="#tab_6">II  COMPARISION WITH STATE-OF-THE-ART RESULTS WHICH HAVE REPORTED IN UNIVERSITY-1652. M REPRESENTS THE MARGIN OF TRIPLETLOSS, K  REPRESENTS THE NUMBER OF SAMPLING, S REPRESENTS THE SIZE OF INPUT IMAGES AND VIT-S REPRESENTS THE SMALL-SCALE VISION   TRANSFORMER</ref>  with width P on the right side. F lipP ad flips the part with width P on the left side of the image and cuts out the image with width P on the right. As in <ref type="figure" target="#fig_2">Fig. 9</ref>. In order to verify the anti-offset of FSRA, we compare the proposed FSRA with the state-of-the-art method LPN. As shown in <ref type="figure">Fig. 10</ref>, when the padding size increases, the accuracy of FSRA decreases much slower than LPN. Besides, the accuracy of BlackP ad decreases more slowly than F lipP ad, which may be caused by the fact that F P increases the confusion information at the edge and causes the uneven content distribution. As shown in <ref type="table" target="#tab_6">Table IV</ref>, when BP = 60, The AP of LPN was reduced by 29.08%, while the AP of our FSRA was reduced by <ref type="bibr" target="#b18">19</ref>.01%, which is about 10 points less than that of LPN. When F P = 60, The AP of LPN was reduced by 31.29%, while the AP of our FSRA was reduced by 26.81%, which is about 4.5 points less than that of LPN.. The advantage of FSRA over part-based like LPN in resisting position shift mainly lies in the fact that FSRA does not artificially design regions, but allows the model to learn  <ref type="figure">Fig. 10</ref>. Just like the two padding methods shown in <ref type="figure" target="#fig_2">Fig. 9</ref>, we explore the impact of the number of the black pad and the flip pad on AP and Recall@1. The vertical axis represents the magnitude of the decrease in accuracy.</p><p>to a set of division rules by itself, and this segmentation is patch-level. Therefore, when the input image has a large position offset, the network can still distinguish which parts are buildings and which parts are trees. In contrast, artificially designed segmentation no longer makes sense when significant offsets occur, but is often effective in the absence of offsets and anomalies. This idea can also be applied to the field of ReID. For example, During object detection, there might be incomplete cuts of the human body, or the cut image contains a lot of background. In this case, our FSRA can still be recognized effectively by automatic segmentation.</p><p>The impact of sampling on accuracy. Adequate sampling has a great influence on the fitting of the network. Unbalanced or insufficient data will lead to unstable model training, and the final results will be unsatisfactory. In the University-1652 dataset, one satellite image corresponds to 27 drone-view images. Previously, single-fold sampling was done by taking one from a specific category in each iteration, i.e., one of the 27 drone-view images and only one of the satellite-view.The multiple sampling approach can optimize two aspects of the problem: 1) the sample imbalance problem of different viewpoint images. This problem has essentially been raised in LCM <ref type="bibr" target="#b23">[24]</ref> (the authors achieved the best using equalmultiplicity sampling of UAV and satellite images).</p><p>2) The number and proportion of positive and negative samples for TripletLoss. When we change the sampling multiplicity, the number of positive samples in a single batch of TripletLoss will change, which has an impact on the metric learning. To verify that our approach is not due to the effect of TripletLoss, we conducted experiments using the FSRA with region n = 3 and no TripletLoss. As shown in <ref type="figure" target="#fig_3">Fig. 11</ref>(a) and (b), the trend is up and then down in both AP and R@1 indicators, and the overall optimum is reached at k = 3. In addition, we train the model with the addition of TripletLoss, as shown in <ref type="figure" target="#fig_3">Fig. 11(c) and (d)</ref>, which also shows the same trend of rising then falling and optimal at k = 3. The parameter k can be interpreted as a hyperparameter, and k = 3 is a more effective value in Unversity-1652. k affects the training time, but has no effect on the inference phase. We believe that the reason why k is too large for model training is overfitting on one hand, and on the other hand, as k increases, the proportion of similar samples in a single batch will increase, and the model will learn fewer inter-class differences in a single batch.We conjecture that batchsize has an impact on the choice of k values in the multiple sampling strategy, which we discuss in Appendix B.</p><p>Effect of the input image size. Image with small size will compress the fine-grained information and damage the complete features of the original image. Large-scale images can often achieve higher accuracy because they maintain the original fine-grained information. In contrast, large-scale  Effect of the drone distance to the geographic target. The scale of the satellite-view image in University-1652 is fixed, while the scale of the drone-view image changes dynamically with the distance of the drone to the geographic target. According to the distance between the drone and the target building, we divide the University-1652 dataset into three parts: Long, Middle, and Short. We verify the effect of the proposed FSRA under three different levels of distance, as shown in <ref type="table" target="#tab_6">Table VI</ref>. The proposed FSRA does not have a big margin at a different level of distance. It has the lowest accuracy at Long distances and the highest accuracy at Middle. Compared with the current state-of-the-art network e.g., LPN, which has a margin of 20% Recall@1 and 17% AP between Long and Middle distance, the proposed FSRA has better scale adaptive capabilities.</p><p>Effect of some other tricks. For the task of matching the drone-view and the satellite-view images, we adopt three tricks of KLLoss, TripletLoss with margin=0.3, and multiple sampling for the FSRA to improve the performance. As shown in <ref type="table" target="#tab_6">Table VII</ref>. Only using KLLoss increases by 0.84%/1.06% AP on the task of Drone?Satellite / Satellite?Drone. Only using TripletLoss increases by 1.52%/1.66% AP. When we use KLLoss and TripletLoss at the same time, the accuracy of AP is not improved much. Thus, we did not use KLLoss but TripletLoss in our model. We guess that TripletLoss and KL-Loss are consistent in the same direction of network fitting. In addition, we deploy the sampling strategy as a trick. Based on TripletLoss with margin=0.3. When the number of sampling reaches 2?, the AP of FSRA increases by 1.51%/1.17% on the task of Drone?Satellite / Satellite?Drone. When the number of sampling reaches 3?, the AP increases by 1.86%/1.85%. The performance improvement obtained by multiple sampling is due to the expansion of the data which can strengthen the fitting of the network and balance the resources from different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualization Of Qualitative Result</head><p>For the two basic tasks of the University-1652 dataset: drone view target localization and Drone Navigation, we visualize some retrieved results in <ref type="figure" target="#fig_4">Fig. 12</ref>. We observe that FSRA can adapt to retrieving the available images from the gallery set in both drone view target localization and drone navigation tasks. In the task of drone view target localization, we randomly take out three drone-view images from the test dataset. For each drone-view image, we take out the top five similar images from the gallery set, and the FSRA obtains completely correct results as in <ref type="figure" target="#fig_4">Fig. 12 (I)</ref>. In the Drone Navigation task, we randomly take out three Satellite-view images from the test dataset. For each Satellite-view image, we also take out the top 5 similar images in the gallery, because there is only one satellite image for each category. The proposed FSRA still achieved completely correct results as in <ref type="figure" target="#fig_4">Fig. 12 (II)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLSION</head><p>In this paper, we apply the structure of the Transformer to the field of cross-view geo-localization. The context information contained in the attention mechanism can distinguish more fine-grained features, and explore some associated information. Our experiments prove that the transformer-based FSRA can obtain state-of-the-art performance in the benchmark of the University-1652. In addition, some modules are proposed to improve model performance. HSM is proposed to implement patch-level semantic segmentation, and HAB is proposed to achieve region-level feature alignment. Although experiments shows that the proposed FSRA has strong robustness to feature misalignment and position shifts, there are still many parts that can be further improved. e.g., the structure of Vit can be modified to achieve more amazing performance. the backbone based on Vit-S has an increase in inference time compared to Resnet-50, which will be considered a shortcoming of this method. Besides, we also adopted a multiple sampling strategy to fit the model to a better state. This strategy can achieve a stunning rise, but the disadvantage is that it increases the training time. Finally, some other tricks such as mutual learning and TripletLoss are applied to make the FSRA stronger. In the field of current geo-localization based on the perspective of drones. It is very necessary to construct a dense geographic dataset that the model can learn more distinctive and fine-grained features to achieve precise positioning. In the future, we will propose a new intensive UAV cross-view geo-localization dataset to meet the requirements of practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The research is supported by the National Natural Science Foundation of China (Project 51605462). The drone-view can be used as an intermediate view between the satellite and ground view because there is a 90 degree deviation between the ground-view image and the satellite-view image, and the occlusion between objects, the drastic differences in viewpoints and even the temporal gap between scenes make it very challenging. We try to use the drone-view to match the ground-view and thus indirectly reduce the difficulty of the matching between ground and satellite. For fairness, we apply the same learning strategy to the different models (all using only classification loss). The experimental results are shown in <ref type="table" target="#tab_6">Table VIII</ref>. The proposed FSRA has improved somewhat compared with University-1652 and LPN, but still remains in single digits. Matching single-view ground images with UAV images is a huge challenge, mainly because there is a mismatch of shooting angles between ground-view images and UAV-view images, which in turn leads to large differences in the included content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B DOES BATCHSIZE HAVE EFFECTS ON THE CHOICE OF K?</head><p>In order to verify the effect of batchsize on the choice of the optimal k value, we increased the batchsize from 8 to 16 and conducted experiments for k varying from 1 to 8 (to avoid the effect of TripletLoss positive and negative sample ratios on the experiments, only classification loss was used in the experiments). as shown in <ref type="figure" target="#fig_5">Fig. 13</ref>, when the batchsize increases to 16, the optimal hyperparameter k should be chosen to be around 5 (k = 3 reaches the optimum for batchsize=8). This also verifies our statement in The impact of sampling on accuracy that the proportion of samples of the same class in a batch affects the effectiveness of model training. Our proposed multiple sampling strategy can be used not only to expand the severely underrepresented satellite images in University-1652, but also to change the distribution of samples in the batch, so our proposed multiple sampling strategy needs to select the best k value according to the actual batchsize. We conclude that k = 3 is optimal when batchsize=8 and k = 5 is optimal when batchsize=16. It should also be noted that increasing the value of k increases the model training time exponentially, but does not have any effect on the inference process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2. Transformer-based strong baseline framework. Output [cls token] marked with * is served as the global feature f . Classif ierLayer contains linear layer, relu, batchnorm1d and dropout. ID Loss represents CrossEntropy loss without label-smooth. In addition, we provide a simplified Transformer Layer structure, the specific structure can be found in Vit [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 8 .</head><label>8</label><figDesc>Compare the effects of the number of regions n on the task of drone view target localization and the task of drone navigation. The red line represents the task of drone view target localization (Drone ? Satellite), and the blue line represents the task of drone navigation (Satellite ? Drone). Our experiments are all based on TripletLoss (M=0.3). (a) Show the effect of the number of regions n on the accuracy of Recall@1. (b) Show the effect of the number of regions n on the accuracy of AP. We find that R@1 and AP achieve the best performance when n=3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 9 .</head><label>9</label><figDesc>The image on the left is the original drone image, and the middle image is the image with a width of 20 expanded with black on the left side of the image and cropped to the same width on the right side of the image. The image on the right is obtained by mirroring and expanding a 20-pixel wide portion of the left side of the image and cutting off an equal pixel width on the right side of the image. The red dotted line is the dividing line of Padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 11 .</head><label>11</label><figDesc>We conducted experiments on the impact of the number of sampling on AP and Recall@1 in tasks of Drone?Satellite and Satellite?Drone. k stands for the number of the sampling. When the number of sampling is 3, the accuracy of AP and R@1 in both tasks reaches the best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 12 .</head><label>12</label><figDesc>Qualitative image retrieval results. (I) Top-5 retrieval results of drone view target localization on University-1652. (II) Top-5 retrieval results of drone navigation on University-1652. The yellow box indicates the truematched image, and the blue box indicates the false-matched image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 13 .</head><label>13</label><figDesc>The effect of the variation of numbers of sampling k on the final training results of the model when batchsize=16 is demonstrated, where the results of the R@1 evaluation metric are shown on the left and the results of the AP evaluation metric are shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>NETWORK.. TABLE III COMPARISON OF RESNET AND VISION TRANSFORMER. INFERENCE TIME IS MEASURED COMPARED TO RESNET-50, AND OTHER BACKBONES ARE EVALUATED RELATIVE TO THE BASELINE OF RESNET-50. ALL RESULTS ARE PERFORMED ON THE SAME DEVICE FOR A FAIR COMPARISON. VIT-S/16 IS REGARDED AS THE BASELINE MODEL AND ABBREVIATED AS BASELINE IN THE REST OF THIS PAPER. VIT-B/16 IS THE STANDARD MODEL PROPOSED IN THE ORIGINAL PAPER [16] TWO CASES OF BLACK PAD AND FLIP PAD, THE PROPOSED FSRA AND STATE-OF-THE-ART METHOD LPN CORRESPOND TO THE AP ACCURACY VALUES OF DIFFERENT PAD SIZES AND THE SPEED OF DECLINE.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell>Backbone</cell><cell>Drone?Satellite R@1 AP</cell><cell>Satellite?Drone R@1 AP</cell></row><row><cell></cell><cell></cell><cell cols="3">Contrastive Loss [32]</cell><cell></cell><cell>VGG16</cell><cell>52.39</cell><cell>57.44</cell><cell>63.91</cell><cell>52.24</cell></row><row><cell cols="6">Weighted Soft Margin TripletLoss [34]</cell><cell>VGG16</cell><cell>53.21</cell><cell>58.03</cell><cell>65.62</cell><cell>54.47</cell></row><row><cell></cell><cell cols="4">TripletLoss (M = 0.3) [30]</cell><cell></cell><cell>ResNet-50</cell><cell>55.18</cell><cell>59.97</cell><cell>63.62</cell><cell>53.85</cell></row><row><cell cols="6">Instance Loss + GeM Pooling [60]</cell><cell>ResNet-50</cell><cell>65.32</cell><cell>69.61</cell><cell>79.03</cell><cell>65.35</cell></row><row><cell></cell><cell></cell><cell cols="3">Instance Loss [1]</cell><cell></cell><cell>ResNet-50</cell><cell>58.23</cell><cell>62.91</cell><cell>74.47</cell><cell>59.45</cell></row><row><cell></cell><cell cols="4">LCM (ResNet-50) [24]</cell><cell></cell><cell>ResNet-50</cell><cell>66.65</cell><cell>70.82</cell><cell>79.89</cell><cell>65.38</cell></row><row><cell></cell><cell></cell><cell cols="2">LPN [12]</cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell>75.93</cell><cell>79.14</cell><cell>86.45</cell><cell>74.79</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (k=1)</cell><cell></cell><cell></cell><cell>Vit-S</cell><cell>82.25</cell><cell>84.82</cell><cell>87.87</cell><cell>81.53</cell></row><row><cell></cell><cell></cell><cell cols="2">Ours (k=3)</cell><cell></cell><cell></cell><cell>Vit-S</cell><cell>84.51</cell><cell>86.71</cell><cell>88.45</cell><cell>83.37</cell></row><row><cell></cell><cell></cell><cell cols="3">Ours (k=1, s=512)</cell><cell></cell><cell>Vit-S</cell><cell>85.50</cell><cell>87.53</cell><cell>89.73</cell><cell>84.94</cell></row><row><cell cols="2">Backbone</cell><cell cols="2">Inference Time</cell><cell cols="2">Drone?Satellite</cell><cell>Satellite?Drone</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>R@1</cell><cell>AP</cell><cell>R@1</cell><cell>AP</cell></row><row><cell cols="2">ResNet-50</cell><cell>1x</cell><cell></cell><cell>60.93</cell><cell>65.31</cell><cell>75.61</cell><cell>61.69</cell></row><row><cell cols="2">ResNet-101</cell><cell cols="2">1.48x</cell><cell>65.33</cell><cell>68.32</cell><cell>79.44</cell><cell>65.43</cell></row><row><cell cols="2">Vit-S/16</cell><cell cols="2">1.21x</cell><cell>71.04</cell><cell>74.62</cell><cell>83.31</cell><cell>72.08</cell></row><row><cell cols="2">Vit-B/16</cell><cell cols="2">1.79x</cell><cell>73.32</cell><cell>76.88</cell><cell>84.74</cell><cell>74.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell>IN THE Pad</cell><cell></cell><cell cols="2">Black Pad AP (%)</cell><cell></cell><cell cols="2">Flip Pad AP (%)</cell></row><row><cell>Pixel</cell><cell></cell><cell>FSRA</cell><cell>LPN</cell><cell></cell><cell>FSRA</cell><cell>LPN</cell></row><row><cell>0</cell><cell cols="2">84.77 ?0</cell><cell cols="2">81.17 ?0</cell><cell>84.77 ?0</cell><cell>81.17 ?0</cell></row><row><cell>10</cell><cell cols="2">84.13?0.64</cell><cell cols="2">80.79?0.38</cell><cell>84.19?0.58</cell><cell>80.07?1.10</cell></row><row><cell>20</cell><cell cols="2">82.7?2.07</cell><cell cols="2">78.29?2.88</cell><cell>82.26?2.51</cell><cell>77.18?3.99</cell></row><row><cell>30</cell><cell cols="2">80.03?4.74</cell><cell cols="2">74.01?7.16</cell><cell>78.46?6.31</cell><cell>72.67?8.50</cell></row><row><cell>40</cell><cell cols="2">76.41?8.36</cell><cell cols="2">68.06?13.08</cell><cell>73.13?11.64</cell><cell>65.83?15.34</cell></row><row><cell>50</cell><cell cols="2">71.6?13.17</cell><cell cols="2">60.61?20.56</cell><cell>66.07?18.70</cell><cell>58.17?23.00</cell></row><row><cell>60</cell><cell cols="2">65.76 ?19.01</cell><cell cols="2">52.09 ?29.08</cell><cell cols="2">57.96 ?26.81</cell><cell>49.88 ?31.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY ON THE IMPACT OF DIFFERENT INPUT SIZES ON UNIVERSITY-1652. THE EXPERIMENTAL RESULTS ARE BASED ON THENUMBER OF SAMPLING K=1, TRIPLE LOSS WITH MARGIN=0.3. According to different input sizes, the experimental results are shown inTable V. In both tasks, i.e., Drone?Satellite and Satellite?Drone, we observe that the performance gradually improves when the input image size increases from 224 to 512, and the AP has a big improvement when the image input size is changed from 256 to 320. We hope that when the hardware resources are limited, this ablation experiment can play a reference role in selecting the appropriate input image size.</figDesc><table><row><cell>Image Size</cell><cell cols="2">Drone ?Satellite</cell><cell cols="2">Drone ?Satellite</cell></row><row><cell></cell><cell>R@1</cell><cell>AP</cell><cell>R@1</cell><cell>AP</cell></row><row><cell>224</cell><cell>80.81</cell><cell>83.65</cell><cell>87.73</cell><cell>80.02</cell></row><row><cell>256</cell><cell>82.25</cell><cell>84.82</cell><cell>87.87</cell><cell>81.53</cell></row><row><cell>320</cell><cell>84.08</cell><cell>86.38</cell><cell>87.87</cell><cell>82.63</cell></row><row><cell>384</cell><cell>84.82</cell><cell>87.03</cell><cell>87.59</cell><cell>83.37</cell></row><row><cell>512</cell><cell>85.5</cell><cell>87.53</cell><cell>89.73</cell><cell>84.94</cell></row><row><cell cols="5">images often require larger memory resources and longer</cell></row><row><cell cols="5">inference time during training and testing. To balance the input</cell></row><row><cell cols="5">image size with memory usage, we conduct experiments on</cell></row><row><cell cols="4">FSRA with the number of regions n = 3.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI ABLATION</head><label>VI</label><figDesc>STUDY TO VERIFY THE ROBUSTNESS OF THE PROPOSED FSRA AT DIFFERENT DISTANCES BETWEEN DRONES AND TARGET IN UNIVERSITY-1652.</figDesc><table><row><cell>Distance</cell><cell cols="2">Drone ? Satellite</cell></row><row><cell></cell><cell>R@1</cell><cell>AP</cell></row><row><cell>ALL</cell><cell>82.25</cell><cell>84.82</cell></row><row><cell>Long</cell><cell>79.71</cell><cell>82.69</cell></row><row><cell>Middle</cell><cell>84.05</cell><cell>86.36</cell></row><row><cell>Short</cell><cell>82.87</cell><cell>85.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>STUDIES TO VERIFY THE EFFECTS OF SOME OTHER TRICKS, INCLUDING KLLOSS, TRIPLETLOSS, AND THE NUMBER OF SAMPLING IN UNIVERSITY-1652.D?S MEANS THE TASK OF DRONE?SATELLITE, AND S?D MEANS THE TASK OF SATELLITE?DRONE.</figDesc><table><row><cell>KLLoss</cell><cell>TripletLoss (M=0.3)</cell><cell>Sampling Rate</cell><cell>D?S</cell><cell>AP (%)</cell><cell>S?D</cell></row><row><cell></cell><cell></cell><cell>1?</cell><cell>83.30</cell><cell></cell><cell>79.87</cell></row><row><cell></cell><cell></cell><cell>1?</cell><cell>84.14</cell><cell></cell><cell>80.93</cell></row><row><cell></cell><cell></cell><cell>1?</cell><cell>84.82</cell><cell></cell><cell>81.53</cell></row><row><cell></cell><cell></cell><cell>1?</cell><cell>84.85</cell><cell></cell><cell>81.52</cell></row><row><cell></cell><cell></cell><cell>2?</cell><cell>86.36</cell><cell></cell><cell>82.69</cell></row><row><cell></cell><cell></cell><cell>3?</cell><cell>86.71</cell><cell></cell><cell>83.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII THE</head><label>VIII</label><figDesc>MODELS WERE TRAINED ON GROUND, DRONE AND SATELLITE VIEWS, AND THE ACCURACY OF MATCHING BETWEEN GROUND AND DRONE VIEWS WAS TESTED. WHERE G REFERS TO GROUND-VIEW AND D REFERS TO DRONE-VIEW.</figDesc><table><row><cell>Model</cell><cell>Direction</cell><cell cols="2">R@1 R@Top1%</cell><cell>AP</cell></row><row><cell>University-1652</cell><cell>G?D</cell><cell>0.85</cell><cell>20.36</cell><cell>0.71</cell></row><row><cell></cell><cell>D?G</cell><cell>0.99</cell><cell>15.07</cell><cell>1.11</cell></row><row><cell>LPN</cell><cell>G?D</cell><cell>0.85</cell><cell>20.47</cell><cell>0.94</cell></row><row><cell></cell><cell>D?G</cell><cell>1.70</cell><cell>17.41</cell><cell>1.70</cell></row><row><cell>FSRA(ours)</cell><cell>G?D</cell><cell>1.94</cell><cell>31.91</cell><cell>1.67</cell></row><row><cell></cell><cell>D?G</cell><cell>2.75</cell><cell>24.92</cell><cell>2.63</cell></row><row><cell></cell><cell cols="2">APPENDIX A</cell><cell></cell><cell></cell></row><row><cell cols="4">DOES IT WORK FOR GROUND VIEW?</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">University-1652: A multi-view multisource benchmark for drone-based geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413896</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413896" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM international conference on Multimedia</title>
		<meeting>the 28th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Drones for deliveries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raithatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Runcie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
		<respStmt>
			<orgName>Sutardja Center for Entrepreneurship &amp; Technology, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Building information modeling and classification by visual learning at a city scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cetiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taciroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Law</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06391</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vision meets drones: A challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accurate object localization in remote sensing images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2645610</idno>
		<ptr target="https://doi.org/10.1109/TGRS.2016.2645610" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2486" to="2498" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2711011</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2017.2711011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299135</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7299135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5007" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-view image matching for geo-localization in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.216</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.216" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3608" to="3616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298790</idno>
		<ptr target="https://doi.org/10.1109/cvpr.2015.7298790" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2858817</idno>
		<ptr target="https://doi.org/10.1109/TGRS.2018.2858817" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uav-satellite view synthesis for cross-view geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2021.3121987</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2021.3121987" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Each part matters: Local patterns facilitate cross-view geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2021.3061265</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2021.3061265" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_30</idno>
		<idno>1007/978-3-030-01225-0 30</idno>
		<ptr target="https://doi.org/10" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1145%2F3240508.3240552" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Multimedia</title>
		<meeting>the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep contextaware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2019.2891888</idno>
		<ptr target="https://doi.org/10.1109/TIP.2019.2891888" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2860" to="2871" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alignedreid++: Dynamically matching local information for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.05.028</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2019.05.028" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pedestrian alignment network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2018.2873599</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2018.2873599" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3037" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A practical cross-view image matching method between uav and satellite for uav-based geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs13010047</idno>
		<ptr target="https://doi.org/10.3390/rs13010047" />
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting groundlevel scene layout from aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bessinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="867" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lending orientation to neural networks for crossview geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5624" to="5633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vigor: Cross-view image geo-localization beyond one-to-one retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.00364</idno>
		<ptr target="https://doi.org/10.1109/cvpr46437.2021.00364" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159171</idno>
		<ptr target="https://doi.org/10.1145/3159171" />
	</analytic>
	<monogr>
		<title level="m">Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2700762</idno>
		<ptr target="https://doi.org/10.1109/TIP.2017.2700762" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical temporal modeling with mutual distance matching for video based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2020.2988034</idno>
		<ptr target="https://doi.org/10.1109/TCSVT.2020.2988034" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="503" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383184</idno>
		<ptr target="https://doi.org/10.1145/3383184" />
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cvm-net: Cross-view matching network for image-based ground-to-aerial geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7258" to="7267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lending orientation to neural networks for crossview geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5624" to="5633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stripe-based and attributeaware network: A two-branch deep model for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement Science and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">95401</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dissecting person re-identification from the viewpoint of viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalizable person re-identification by domain-invariant mapping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8295" to="8302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are these from the same place? seeing the unseen in cross-view image geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tani</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv48630.2021.00380</idno>
		<ptr target="https://doi.org/10.1109/wacv48630.2021.00380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3753" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4905" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A survey on visual transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.13140/RG.2.2.14420.53124</idno>
		<idno type="arXiv">arXiv:2102.04378</idno>
		<ptr target="https://doi.org/10.13140/RG.2.2.14420.53124" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal transformer with multiview visual representation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tcsvt.2019.2947482</idno>
		<ptr target="https://doi.org/10.1109/tcsvt.2019.2947482" />
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4467" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Convtransformer: A convolutional transformer network for video frame synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10185</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Conformer: Local features coupling global representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03889</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/uemcon47517.2019.8993089</idno>
		<idno type="arXiv">arXiv:2108.05895</idno>
		<ptr target="https://doi.org/10.1109/uemcon47517.2019.8993089" />
		<title level="m">Mobile-former: Bridging mobilenet and transformer</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Every view counts: Cross-view consistency in 3d object detection with hybrid-cylindricalspherical voxelization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Projecting your view attentively: Monocular road scene layout estimation via crossview transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr46437.2021.01528</idno>
		<ptr target="https://doi.org/10.1109/cvpr46437.2021.01528" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">545</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-view analysis of unregistered medical images using cross-view transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_10</idno>
		<idno type="arXiv">arXiv:2103.11390</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-410" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cross-view geo-localization with evolving transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00842</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2846566</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2846566" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
