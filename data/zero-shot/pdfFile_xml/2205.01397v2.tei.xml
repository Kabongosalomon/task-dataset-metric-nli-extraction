<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
						</author>
						<title level="a" type="main">Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastively trained language-image models such as CLIP, ALIGN, and BASIC have demonstrated unprecedented robustness to multiple challenging natural distribution shifts. Since these language-image models differ from previous training approaches in several ways, an important question is what causes the large robustness gains. We answer this question via a systematic experimental investigation. Concretely, we study five different possible causes for the robustness gains: (i) the training set size, (ii) the training distribution, (iii) language supervision at training time, (iv) language supervision at test time, and (v) the contrastive loss function. Our experiments show that the more diverse training distribution is the main cause for the robustness gains, with the other factors contributing little to no robustness. Beyond our experimental results, we also introduce ImageNet-Captions, a version of ImageNet with original text annotations from Flickr, to enable further controlled experiments of language-image training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large pre-trained language-image models such as CLIP <ref type="bibr" target="#b26">[27]</ref>, ALIGN <ref type="bibr" target="#b20">[21]</ref>, and BASIC <ref type="bibr" target="#b25">[26]</ref> have recently demonstrated unprecedented robustness on a variety of natural distribution shifts. In contrast to prior models that are trained on images with class annotations, CLIP and relatives 1 are directly trained on images and their corresponding unstructured text from the web. The resulting models achieve large robustness even on challenging distribution shifts such as ImageNetV2 <ref type="bibr" target="#b27">[28]</ref> and ObjectNet <ref type="bibr" target="#b1">[2]</ref>. No prior algorithmic techniques had enhanced robustness on these datasets even after multiple years of intensive research in reliable machine learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>. As CLIP also improves robustness on a wide range of other distribution shifts, an important question emerges: What causes CLIP's unprecedented robustness?</p><p>The fact that language-image models were the first to achieve large robustness gains suggests that multimodal learning on language and image data may be key to more robust image representations. However, pinpointing the exact cause of CLIP's robustness is complicated by the fact that CLIP relied on several changes to the common supervised training paradigm for image classification models. For instance, the CLIP models with highest accuracy follow the vision transformer (ViT) architecture <ref type="bibr" target="#b13">[14]</ref>. Radford et al. <ref type="bibr" target="#b26">[27]</ref> already investigated model architecture and size, showing that these factors do not affect the robustness of their CLIP models. Nevertheless, there is still a long list of possible causes for CLIP's robustness:</p><p>? The large training set size (400 million images)</p><p>? The training distribution ? Language supervision at training time ? Language supervision at test time via prompts ? The contrastive loss function Understanding the mechanism underlying CLIP's robustness is important as it may guide the way towards more reliable machine learning more broadly.</p><p>In this paper, we answer the question of CLIP's robustness via a series of controlled experiments that test the five possible causes listed above. Our main result is that CLIP's robustness is determined almost exclusively by the training distribution. Language supervision at training time does not make the resulting models more robust than standard supervised learning when the images in the training set are the same. Hence language supervision only has an indirect effect on robustness. In particular, language supervision simplifies training on a diverse distribution of images by removing the need for consistent annotation with class labels. The more diverse training distribution --not the language supervision --then leads to more robust representations.  : We compare models trained using different methods and on different datasets, measuring their robustness on a range of natural distribution shifts (ImageNetV2, ImageNet-R, ImageNet-Sketch, and ObjectNet). The CLIP models stand out with their consistent performance in the presence of distribution shift. We find that large gains in effective robustness (improvement over ImageNet models) only come from varying the training distribution. Language supervision alone does not cause robustness.</p><p>Our investigation of CLIP's robustness rests on two further contributions. First, we introduce ImageNet-Captions, a new dataset for training on paired language-image data. ImageNet-Captions augments 463,622 of the 1.2 million images in the ImageNet 2012 training set <ref type="bibr" target="#b28">[29]</ref> with the original text data sourced from the corresponding Flickr images. ImageNet-Captions enables controlled experiments comparing standard (class-based) ImageNet training with language-image training on the same set of images. Such experiments precisely pinpoint the effect of utilizing language when training computer vision models.</p><p>Second, we provide a new baseline for language-image training that minimizes the interaction between the vision and language components yet achieves accuracy similar to CLIP training. Specifically, we introduce the following training procedure and illustrate its behavior on the YFCC-15M dataset <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b26">27]</ref>:</p><p>2. Fine-tune the resulting representation by matching examples in YFCC-15M to ImageNet classes with simple text matches in the corresponding captions.</p><p>In particular, our approach relies on no language model, demonstrating that it is possible to match the performance of CLIP training with much simpler language processing. Besides serving as a useful baseline to understand CLIP training, our simplified approach may open the way for further algorithmic innovations in language-image training.</p><p>The remainder of our paper proceeds as follows. The next two sections introduce relevant background and our new dataset ImageNet-Captions as experimental framework. Sections 4 and 5 then describe our main experiments testing the impact of language supervision and the training distribution on the robustness of the resulting models. Sections 6 and 7 present the evidence against test time prompts and contrastive training losses as causes for CLIP's robustness. We summarize our findings in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Pinpointing the cause of CLIP's robustness requires a precise experimental setup for comparing robustness across a range of models. To this end, we follow the effective robustness framework first proposed by Taori et al. <ref type="bibr" target="#b34">[35]</ref> and later utilized by Radford et al. <ref type="bibr" target="#b26">[27]</ref> to demonstrate the robustness gains of their CLIP models. We first review this measurement framework and then survey further related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Experimental setup for measuring robustness</head><p>An important goal of reliable machine learning is to design models that consistently perform well across a diverse range of test distributions. For instance, a model that achieves 75% accuracy on ImageNet should ideally also achieve 75% accuracy on the closely related ImageNetV2 distribution shift because humans can do so <ref type="bibr" target="#b31">[32]</ref>. But instead of consistent performance, most current ImageNet models see a 12 percentage point (pp) accuracy drop on this distribution shift <ref type="bibr" target="#b27">[28]</ref>. In contrast, the CLIP models of Radford et al. <ref type="bibr" target="#b26">[27]</ref> are more robust and only have a 6 pp accuracy drop on ImageNetV2. Compared to earlier models, CLIP also exhibits substantially smaller accuracy drops on many other distribution shifts <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>More formally, our experiments measure the accuracy of a model f on two test distributions D 1 and D 2 , which we abbreviate as acc D1 (f ) and acc D2 (f ). Usually D 1 is the ImageNet (ILSVRC-2012) test set and D 2 is one of multiple out-of-distribution test sets. An ideal model would achieve close to 100% accuracy on both distributions. Since such machine models currently do not exist, we instead have to compare the robustness of models with varying accuracies across the two distributions. In these comparisons, an important confounder is that simply increasing accuracy on distribution D 1 often already results in accuracy gains on D 2 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref>. For instance, <ref type="figure" target="#fig_1">Figure 1</ref> shows a range of ImageNet models (blue points) in a scatter plot with ImageNet accuracy on the x-axis (acc D1 (f )) and accuracy under distribution shift on the y-axis (acc D2 (f )). The models achieve higher accuracy under distribution shift just by virtue of having higher ImageNet accuracy.</p><p>In order to address the confounder of ImageNet accuracy when evaluating robustness, Taori et al. <ref type="bibr" target="#b34">[35]</ref> quantified robustness as accuracy beyond the baseline given by ImageNet models. The authors called this quantity effective robustness. In <ref type="figure" target="#fig_1">Figure 1</ref>, effective robustness corresponds to the vertical lift of a model above the blue baseline given by ImageNet-trained models. Radford et al. <ref type="bibr" target="#b26">[27]</ref> then demonstrated that their CLIP models achieve high effective robustness (the purple line). Mathematically, we first fit a baseline function ? : R ? R that maps from the accuracy acc D1 (f ) of baseline models f to the corresponding acc D2 (f ). For a new model f , the effective robustness is then given by ?(f ) = acc D2 (f ) ? ?(acc D1 (f )). This is the main quantity we visualize in this paper to understand the robustness of CLIP models.</p><p>Similar to Taori et al. <ref type="bibr" target="#b34">[35]</ref> and Radford et al. <ref type="bibr" target="#b26">[27]</ref>, we focus on natural distribution shifts, which arise from natural variations such as lighting, geographic location, crowdsourcing process, etc. Natural distribution shifts stand in contrast to synthetic distribution shifts, where an existing test set is intentionally computationally modified to reduce model accuracy (e.g., by adding Gaussian noise, blur, or adversarial perturbations). Since natural distribution shifts resemble real data, we choose the following popular distribution shifts: 2</p><p>1. ImageNet-V2 <ref type="bibr" target="#b27">[28]</ref>: a reproduction of the ImageNet validation set with distribution shift due to changes in the crowdsourcing process.</p><p>2. ImageNet-Sketch <ref type="bibr" target="#b36">[37]</ref>: black and white sketches of ImageNet images.</p><p>3. ImageNet-R <ref type="bibr" target="#b18">[19]</ref>: renditions (e.g., art, patterns, etc.) of 200 ImageNet classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ObjectNet [2]</head><p>: real-world objects from ImageNet with crowd-sourced random backgrounds, rotations, and viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ImageNet-A [18]</head><p>: naturally occurring examples filtered so they are misclassified by a ResNet-50 model.</p><p>An important property of effective robustness on these distribution shifts is that only varying the size of the training set (holding its distribution constant) does not influence effective robustness. In particular, Taori et al. <ref type="bibr" target="#b34">[35]</ref> and Miller et al. <ref type="bibr" target="#b22">[23]</ref> showed that randomly sub-sampling the training set changes the accuracy, but not the effective robustness of the resulting models. This rules out the training set size as a cause for CLIP's high effective robustness (the training set size is still important for the ImageNet accuracy of the CLIP models).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Additional related work</head><p>Language image pre-training has been an active area of research for multiple years, including initial contributions such as VirTex <ref type="bibr" target="#b9">[10]</ref>, ICMLM <ref type="bibr" target="#b29">[30]</ref>, and ConVIRT <ref type="bibr" target="#b38">[39]</ref>. Radford et al. <ref type="bibr" target="#b26">[27]</ref> and Jia et al. <ref type="bibr" target="#b20">[21]</ref> continued this line of work and trained on significantly larger datasets to achieve competitive performance on a variety of tasks, as well as obtain models with unprecedented robustness.</p><p>Related recent work also studies exactly where the generalization capabilities of CLIP come from. Devillers et al. <ref type="bibr" target="#b11">[12]</ref> investigate whether models that use multimodal information (such as text &amp; images) have superior generalization capabilities -as measured by few-shot and linear probe performance -to models that use only one type of information (images or text). Their analysis found that for both few-shot and linear probe settings there was no consistent advantage of multimodal models over models using only a single modality. In contrast, our work studies the robustness of CLIP and how language specifically affects its capability to generalize out of distribution. An important difference between our experiments and those of Devillers et al. <ref type="bibr" target="#b11">[12]</ref> is that we control for in-distribution accuracy in our comparison between the models to separate accuracy and robustness. Furthermore, Andreassen et al. <ref type="bibr" target="#b0">[1]</ref> study the effect of fine-tuning on robustness. They find that effective robustness decreases almost monotonically during the fine-tuning process, pointing to the zero-shot capability of CLIP as a source of its robustness.</p><p>Since the original CLIP paper <ref type="bibr" target="#b26">[27]</ref>, there have been a series of follow up works, including ALIGN <ref type="bibr" target="#b20">[21]</ref>, BASIC <ref type="bibr" target="#b25">[26]</ref> and LiT <ref type="bibr" target="#b37">[38]</ref>, each of which has made contributions to improving either the robustness or base accuracy of large pre-trained image-text models. Most related to our experiments in Section 5 is LiT, which uses a pre-trained image model and fine-tunes only the text head of the language-image model to achieve high accuracy on downstream tasks. However, we note that this work differs from our contribution in that LiT still fine-tunes a language model on a dataset of 4 billion image-caption pairs to achieve its zero-shot capability, while we simply convert the captions to class labels using substring matching and train a regular image classifier.</p><p>Image captioning datasets. Existing literature offers a variety of public datasets with image-text pairs. Examples range from medium to large scale, including MS-COCO <ref type="bibr" target="#b7">[8]</ref>, SBU <ref type="bibr" target="#b24">[25]</ref>, Conceptual Captions 3M <ref type="bibr" target="#b32">[33]</ref> and 12M <ref type="bibr" target="#b3">[4]</ref>, RedCaps <ref type="bibr" target="#b10">[11]</ref>, WIT <ref type="bibr" target="#b33">[34]</ref>, YFCC 100M <ref type="bibr" target="#b35">[36]</ref> and LAION 400M <ref type="bibr" target="#b30">[31]</ref>. Compared to these datasets, ImageNet-Captions contains high quality classification labels along with text associated with each image. Moreover, ImageNet-Captions is designed such that the distribution of images strongly resembles that of ImageNet, which is widely used for training and evaluating models, enabling controlled experiments such as comparisons between multiclass supervised training and image-text training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ImageNet-Captions</head><p>We now describe ImageNet-Captions 3 , our new dataset for experiments with image-text supervision. Four desiderata guided the creation of ImageNet-Captions:</p><p>1. To isolate the effect of natural language supervision on effective robustness, we require a dataset that contains both natural language supervision and traditional classification labels. This setup allows us to train classifiers separately with contrastive image-text losses and with standard classification losses on the same images and compare the resulting models. Differences in the models are then solely due to different loss functions, not architectural differences or different training distributions.</p><p>2. The text annotations in the dataset should come from the original image source, as opposed to synthetically generated captions from curated templates or an image captioning model. This helps ensure that the dataset is representative of image-text data "in the wild" and minimizes artifacts from templates or machine models.</p><p>3. The dataset should be related to commonly studied benchmarks, such as ImageNet, in order to have good baselines and comparable training methods.</p><p>4. The dataset should be large enough to support training on contemporary neural networks.</p><p>Before our paper, no dataset satisfied these constraints.</p><p>We constructed ImageNet-Captions to satisfy all four desiderata. ImageNet-Captions is a subset of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 training set, paired with the original  image title, description, and tags from Flickr (recall that a large part of ImageNet was sourced from the Flickr image hosting website). <ref type="figure" target="#fig_3">Figure 3</ref> shows three sample image-text pairs from our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing ImageNet-Captions</head><p>Since ImageNet is a widely used image classification benchmark, our goal was to augment the 2012 ImageNet training set with original text data. A priori, this is a difficult task since the standard 2012 ImageNet release does not contain any metadata for the images. As a starting point, we leveraged three facts about ImageNet:</p><p>? A large fraction of ImageNet is sourced from Flickr.</p><p>? The ImageNet fall 2011 release contained URLs for each image in the full ImageNet dataset.</p><p>? For a given photo identifier, the Flickr API provides the associated text data.</p><p>Our dataset construction began with filtering the <ref type="bibr" target="#b13">14,</ref><ref type="bibr">197,</ref><ref type="bibr">122</ref> image URLs in the ImageNet fall 2011 release to only include images from Flickr. In addition, we restricted the images to just the 1,000 classes included in the 2012 ImageNet competition (every entry in the fall 2011 release contains both a URL and a class label). After this filtering, we were left with 642,147 images belonging to 999 classes (all classes in ILSVRC-2012 except "teddy bear").</p><p>Next, we ran the image deduplication routine of Jain et al. <ref type="bibr" target="#b19">[20]</ref> to remove images that were not in the ILSVRC-2012 training set. In addition, we removed text containing profanity. This left us with a dataset of 463,622 images that are in the ILSVRC-2012 training set, along with the newly obtained corresponding text data. In particular, for each image we extracted a title (the text at the top of the Flickr image), description (the text at the bottom of the Flickr image), and user-provided tags. Since these images are a subset of ILSVRC-2012, we also have a corresponding class label that can be used for standard ImageNet training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Properties of ImageNet-Captions</head><p>The resulting dataset contains captions from a mix of 127 different languages with the bulk (90%) coming from English. We further inspected the quality of image-text pairs by checking for the presence of the desired class label in the associated text. <ref type="table" target="#tab_1">Table 1</ref> summarizes the analysis. We find that for 94% of the images, the name of the ImageNet class is present in the corresponding text. This indicates that most of the captions contain relevant information about the class and are suitable for training image-text models. For additional statistics, see Appendix M.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ImageNet-Captions experiments</head><p>In this section, we use the ImageNet-Captions dataset to investigate the effect of language on robustness.</p><p>ImageNet-Captions provides a simple comparison with vision-only methods because ImageNet is considered the premier benchmark for image classification. We train the ResNet-50 based CLIP model on ImageNet-Captions with a contrastive loss, as well as the vision encoder of that CLIP model with an additional linear layer on the equivalent image classification dataset. Training details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Caption construction</head><p>When constructing ImageNet-Captions, we had to choose which parts of the metadata to include in the caption. To do so, we ran experiments on variants that included just the title, the title followed by the description, and the title followed by the tags followed by the description. Furthermore, Radford et al. <ref type="bibr" target="#b26">[27]</ref> use a filter to keep only images with captions in English. We create additional variants of the dataset by applying a similar filter. As shown in <ref type="table" target="#tab_2">Table 2</ref>, captions that include more information appear to perform better. Furthermore, it seems that filtering for cleaner captions does not make up for the loss of image-caption pairs. In caption construction ablations, images with empty captions were dropped, causing variation in dataset size across the experiments in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Robustness</head><p>To determine the robustness of models trained on ImageNet-Captions, we evaluate on ImageNet and compare with natural distribution shifts in ImageNetV2, ImageNet-R, ImageNet Sketch, ObjectNet, and ImageNet-A.</p><p>In <ref type="figure">Figure 4</ref>, we see that ImageNet-Captions CLIP models roughly follow the same linear trends as ImageNet-  <ref type="figure">Figure 4</ref>: On most natural distribution shifts, models trained with language information from ImageNet-Captions follow the same trend as models trained without it. Neither comes close to achieving the robustness of OpenAI's CLIP models.</p><p>Captions classification models across the various distribution shifts. This shows that CLIP models are not more robust than classification models trained on the same dataset, despite the difference of language supervision. This is a better comparison than that with ImageNet classification models because there is no longer the potential confounding factor of the datasets having different image distributions. Nevertheless, these models do not achieve the robustness seen in CLIP models from Radford et al. <ref type="bibr" target="#b26">[27]</ref>. Additional experiment details can be found in Appendix C and D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pre-training on language</head><p>While the above experiments show that language supervision from ImageNet-Captions does not contribute to a model's robustness, it does not rule out robustness coming from the language supervision of OpenAI's proprietary dataset used to train CLIP. Therefore we ran additional experiments where we loaded the pre-trained OpenAI CLIP model onto the language encoder, while randomly initializing the vision encoder.</p><p>We trained ImageNet-Captions on this setup, with an additional variant where we also freeze the language encoder's weights. As seen in <ref type="figure" target="#fig_5">Figure 5</ref>, while both the unfrozen and frozen variants of the pre-trained language encoder increased the accuracy of the model when compared to the completely randomly initialized model, neither variant provided additional effective robustness. Detailed experiment results can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of using templates</head><p>Given that images in ImageNet-Captions have a corresponding ImageNet class, we can try to leverage this information to investigate the effect of captions and class information on both accuracy and robustness. . This is further evidence that language supervision does not increase robustness. See <ref type="figure">Figure 4</ref> for remaining legend elements.</p><p>Radford et al. <ref type="bibr" target="#b26">[27]</ref> introduces prompt templates in formats similar to "A photo of a {label}." Creating templates for ImageNet-Captions is different than doing so for other image-text datasets because each image already has an assigned label; for other datasets, creating a template requires looking through the caption for classes, which are not guaranteed to be in the caption.</p><p>We found that attaching templates at the beginning of captions (followed by Title+Tags+Description) achieves 34.7% ImageNet top-1 accuracy, which is 3.2% more than without the templates. However, using the templates by themselves as the captions achieves 50.5% ImageNet top-1 accuracy, suggesting that the additional information in the captions hurts ImageNet performance. The model trained on the equivalent classification task achieves 48.7%, which suggests that with additional parameter tuning, classification may be similar to CLIP training on templates. Detailed experiment results can be found in Appendix F.</p><p>While using templates instead of captions can increase ImageNet performance, it does not improve robustness. <ref type="figure">Figure 8</ref> in Appendix F shows that using templates on top of the captions follows linear trends similar to ImageNet-Captions. In fact, training a model on all of ImageNet using templates behaves like an equivalent classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Improving ImageNet performance using captions</head><p>While language supervision does not improve robustness, it is still possible that the additional information may improve ImageNet accuracy. We investigate this by running experiments on ImageNet, augmented with ImageNet-Captions. It is well known that ResNet-50 achieves 77.15% top-1 ImageNet accuracy <ref type="bibr" target="#b14">[15]</ref>. As a similar baseline, we achieve 76.62% top-1 ImageNet accuracy by training the CLIP model with templates as the caption.</p><p>We have tried improving this baseline by initializing the language head with the OpenAI pre-trained model, using the combined ImageNet (templates) and ImageNet-Captions for training, using ImageNet-Captions as text augmentation when available, and contrasting the image encoding with both the template and the ImageNet-Captions caption encodings when available. However, all of these fall within ?1% of the baseline. Note that concatenating the captions to the templates changes the image distributions, while some of the Appendix G presents detailed results. The experiments we have run are non-exhaustive, and we leave it to future work to find whether language information can improve ImageNet performance, and more broadly, vision task performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">YFCC experiments</head><p>Our experiments in the previous section show that language supervision alone does not improve robustness.</p><p>To further understand the source of CLIP's robustness, we now investigate whether it is possible to train a representation with minimal or even no language supervision that still yields the same robustness as CLIP. These results will provide further evidence that CLIP's robustness stems from the more diverse data distribution, not the presence of language supervision.</p><p>Our experiments in this section start with a language-image training set on which CLIP exhibits improved robustness: the Yahoo Flickr Creative Commons dataset (YFCC) <ref type="bibr" target="#b35">[36]</ref>. To test whether the image data in YFCC alone can improve robustness, we contrastively pre-train a "standard" image representation on YFCC that does not involve the language part of the dataset. Building on this image-only representation, we then train a zero-shot classifier with only minimal text processing (substring matches). The resulting classifier achieves effective robustness close to CLIP. This demonstrates that the training distribution, not language supervision at training time, is the main reason behind CLIP's robustness.</p><p>Dataset. In this section, we use the YFCC-15M <ref type="bibr" target="#b26">[27]</ref> dataset, a subset of YFCC-100M <ref type="bibr" target="#b35">[36]</ref> filtered to only images with English titles or descriptions. The dataset contains 14,829,396 images with natural language captions associated with each image.</p><p>To train image classifiers on YFCC-15M, we convert YFCC-15M into a classification dataset with class labels for each image, which we denote YFCC-15M-Cls. We assign ImageNet labels to each image using a simple strategy: if the title or description contains the name of an ImageNet synset or synonym <ref type="bibr" target="#b21">[22]</ref>, we assign the corresponding synset label to the image. If an image contains no or multiple ImageNet synsets, we discard that image. This results in 1,694,125 images (11.4% of the full dataset) covering 953 ILSVRC classes. The least common class has 1 image, while the most common has 280,351 images.</p><p>Classification training. We use a ViT-Base (ViT-B/16) model fine-tuned using the softmax cross-entropy loss on YFCC-15M-Cls. Since this data is only a fraction of YFCC-15M, we initialize the classification model with a SimCLR model pre-trained on YFCC-15M from Mu et al. <ref type="bibr" target="#b23">[24]</ref>. Appendix J includes implementation details and ablations.</p><p>Results. We present our results in <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure" target="#fig_1">Figure 1</ref>. A CLIP model trained on all images and captions from YFCC-15M yields an ImageNet top-1 accuracy of 37.9%. Our baseline classification model 4 , which trains SimCLR on YFCC-15M, but fine-tunes on only a small fraction (about 11%) of the supervision in YFCC-15M, results in an accuracy of 35.7%, which we found surprisingly close to CLIP. Further, as shown in <ref type="figure" target="#fig_1">Figure 1</ref> ("YFCC SimCLR + Classification"), our baseline model's effective robustness is similar to that of CLIP.</p><p>Appendix L provides figures that plot the above results on various distribution shifts, as well as a model trained on YFCC-15M-Cls from scratch. Since the training set is now about nine times smaller than YFCC-15M, the resulting models trained from scratch achieve much lower accuracy and are hard to compare to CLIP.</p><p>Overall, we find that despite largely eschewing language, and training on a fraction of the supervision, our baseline model results in high effective robustness, similar to CLIP. These results indicate that image-only pre-training followed by classification fine-tuning can match the robustness of CLIP, and that language pre-training is not necessary for effective robustness. Models trained on YFCC consistently achieve higher effective robustness than models trained on ImageNet, which shows that different training distributions have different levels of effective robustness.  As another hypothesis, we study whether natural language prompts affect CLIP's robustness. Recall that prompts consist of a template (e.g., "a photo of ") and the name of a class in the dataset. Radford et al. <ref type="bibr" target="#b26">[27]</ref> showed how to use multiple templates by averaging their text representations. Similarly, it is also possible to use multiple class names for each class if synonyms exist (e.g. microwave and microwave oven). To investigate the influence of specific prompts in the robustness of CLIP, we conduct a series of experiments using a trained CLIP model and multiple prompting strategies. Specifically, we vary:</p><p>? The templates used, using one of the following three options: i) Templates from Radford et al. <ref type="bibr" target="#b26">[27]</ref>;</p><p>ii) No templates (i.e., only the class names);</p><p>iii) Random words appended before and after the class name. 5</p><p>? The names of the classes, using one of the following three sources:</p><p>i) Class names from Radford et al. <ref type="bibr" target="#b26">[27]</ref>;</p><p>ii) Class names from WordNet synset <ref type="bibr" target="#b21">[22]</ref>;</p><p>iii) A combination of the previous two sources.</p><p>? The number of templates used, chosen from {1, 2, 4, 8, 16, 32, 80}.</p><p>? The maximum synonyms per class, one of {1, 2, 4}. <ref type="figure" target="#fig_6">Figure 6</ref> (left) shows the results from over a hundred experiments. We find that specific choices of prompts can have a substantial impact on performance. While some prompt variations did increase effective robustness, this increase is entirely due to the substantially reduced accuracy. In particular, one can achieve the same change in effective robustness by simply interpolating with a random classifier (which sees no performance change under distribution shift). We illustrate this behavior with the brown line in <ref type="figure" target="#fig_6">Figure 6</ref> (left). Overall, these results show that prompts are not the source of robustness of CLIP models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Effect of contrastive training losses</head><p>Finally, we explore contrastive pre-training as a potential source of CLIP's robustness. Contrastive pre-training is a popular method for self-supervised representation learning that encourages similar pairs to be close and dissimilar pairs to be far apart in a learned representation space. In Radford et al. <ref type="bibr" target="#b26">[27]</ref>, the similar pairs are images and their corresponding captions. In SimCLR <ref type="bibr" target="#b4">[5]</ref>, similar pairs are the same images with different data augmentation.</p><p>As the contrastive loss is core to CLIP's approach, we explore whether contrastive approaches independently promote effective robustness. <ref type="figure" target="#fig_6">Figure 6 (right)</ref> shows results for various popular contrastive methods, including SimCLRv2 <ref type="bibr" target="#b5">[6]</ref>, SimSiam <ref type="bibr" target="#b6">[7]</ref> and SwAV <ref type="bibr" target="#b2">[3]</ref>, pre-trained on ImageNet. We evaluate on ImageNet and the five distribution shifts. While the methods differ significantly from each other (e.g., different augmentation strategies, memory banks, and feature clustering techniques), they consistently exhibit little to no effective robustness. <ref type="figure" target="#fig_6">Figure 6</ref> leaves out ImageNet-A because of its piecewise behavior. See Appendix H for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The previous sections have systematically ruled out the training set size, language supervision, and the contrastive loss function as explanations for the large robustness gains achieved by the CLIP models of Radford et al. <ref type="bibr" target="#b26">[27]</ref>. In addition, Section 5 has demonstrated that changing the training distribution from ImageNet(-Captions) to YFCC substantially affects the robustness of the resulting models. We arrive at a clear conclusion: CLIP's robustness is dominated by the choice of the training distribution, with other factors playing a small or non-existent role. While language supervision is still helpful for easily assembling training sets, it is not the primary driver for robustness.</p><p>Our paper connects the fields of robustness, learning from language &amp; vision, and data-centric machine learning. Moreover, our results add to a growing body of evidence that the training distribution plays a central role for mitigating real-world distribution shifts. Hence, we believe that the sometimes overlooked area of dataset design offers a promising avenue for increasing the robustness of machine learning models, and we hope that the community invests more efforts into this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>We would like to thank Wieland Brendel, Nicholas Carlini, Yair Carmon, Rahim Entezari, Tatsunori Hashimoto, Jong Wook Kim, Hongseok Namkoong, Alec Radford, and Rohan Taori for valuable conversations while working on this project. This work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML) and Open Philanthropy. ImageNet-Captions classification models are trained with cross-entropy loss for 90 epochs using SGD with Nesterov momentum, setting weight decay to 0.0001, momentum to 0.9, and batch size to 256. The initial learning rate is 0.1, and is decayed by 0.1 at epochs 30, 50, and 70.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Distribution shift examples</head><p>The default augmentation is random resized crop to size 224 with scale set to (0, 9, 1.0), and then normalization. Additional augmentation indicates using random resized crop to size 224, random horizontal flips, and then normalization. Normalization is done with mean set to (0.48145466, 0.4578275, 0.40821073), and standard deviation set to (0.26862954, 0.26130258, 0.27577711).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ImageNet-Captions CLIP subsampled experiments</head><p>All models used here are the ResNet-50 based CLIP model used in Radford et al. <ref type="bibr" target="#b26">[27]</ref>. Experiments are trained on a class-balanced subset of ImageNet-Captions (IN-Captions).  <ref type="figure">Figure 9</ref>: On most natural distribution shifts, models pre-trained on ImageNet with various contrastive objectives do not achieve effective robustness. y-axis is averaged over ImageNetV2, ImageNet-R, ImageNet Sketch, and ObjectNet. ImageNet-A is left out due to its piecewise function behavior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IN-Captions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Data cleaning</head><p>The dataset is cleaned by removing samples thought to be offensive or profane. More specifically, the captions are inspected via three independent mechanisms: i) matches using a list of bad words and expressions; ii) a profanity detector model trained on human-labeled samples; iii) and human annotations, when applicable.</p><p>For the first filtering step, we use better-profanity library. <ref type="bibr" target="#b5">6</ref> The list of words and expressions is initialized from the 835 default expressions from the library. <ref type="bibr" target="#b6">7</ref> The authors manually reviewed these 835 expressions, finding 18 that could potentially be associated with ImageNet classes in non-profane captions. Captions containing any of these 18 expressions were marked for subsequent human validation, while captions that contained any of the remaining 817 expressions were automatically excluded. This step is responsible for the largest portion of filtered samples, around 14 thousand samples (approximately 3% of the data). We now list the 18 expressions (warning, the following words might be offensive): breasts, cock, cocks, coon, cowgirl, dyke, nappy, nipple, nipples, organ, paddy, pot, sandbar, screw, screwed, screwing, sniper, titi.</p><p>Data is additionally filtered using the profanity-check library. <ref type="bibr" target="#b7">8</ref> The library detects profane or offensive language using a linear SVM model trained on 200 thousand human-labeled samples. We use a threshold of 0.95, which filters 482 samples.</p><p>Finally, remaining captions that are found to contain any of the 18 expressions listed above are manually reviewed. A total of 114 samples were found to be offensive or profane, and were removed from the dataset.</p><p>Combined, the three filtering steps filter 14,322 samples, approximately 3% of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J NoCLIP ablations</head><p>We train NoCLIP using RandAug <ref type="bibr" target="#b8">[9]</ref> augmentation with N=3, magnitude=9,and magnitude std=0.5. We use a cosine annealing learning rate initialized at 1e-3 with no warmup, and a batch size of 64, with a class-balanced sampler, training for 1 epoch. We use early stopping because training for more epochs hurts performance. Heckel and Yilmaz <ref type="bibr" target="#b16">[17]</ref> showed that early stopping can be helpful when there is label noise. We present ablations with different hyperparameters in <ref type="table">Table 4</ref>. <ref type="table">Table 4</ref>: NoCLIP ablations. "Label match" indicates whether we search for the ImageNet synset or the synset and synonyms in the YFCC captions. "Init" indicates whether we train from scratch, or using SimCLR pre-training. "Augmentation" indicates different augmentation strategies; for "RandAug" we use N=3, magnitude=9, and magnitude std=0.5. "Sampler" is either class-balanced ('Class-bal') or 'Random'(i.i.d., no balancing). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label match</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L YFCC-15M-Cls additional experiments</head><p>In addition to the experiments that trained on top of a model pre-trained on YFCC-15M, we also train a model on YFCC-15M-Cls from scratch. Note that this model is at a much lower accuracy regime than the rest of the models we look at.  <ref type="figure" target="#fig_1">Figure 11</ref>: The cumulative distribution function of the caption lengths (in number of words) for the title-tag-description variant of ImageNet-Captions. We limit the maximum of the x-axis to 100 words, as only 3.6% of captions are between 100 and 4,924 words. The median caption length is 17 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M ImageNet-Captions additional statistics</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1: We compare models trained using different methods and on different datasets, measuring their robustness on a range of natural distribution shifts (ImageNetV2, ImageNet-R, ImageNet-Sketch, and ObjectNet). The CLIP models stand out with their consistent performance in the presence of distribution shift. We find that large gains in effective robustness (improvement over ImageNet models) only come from varying the training distribution. Language supervision alone does not cause robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the two main training sets in our experiments. (Top) We introduce the ImageNet-Captions dataset, where we augment a subset of the ImageNet 2012 training set images with the corresponding original captions collected from Flickr. (Bottom) We convert the YFCC image-caption dataset into YFCC-Classification by searching for class labels in the YFCC captions and then removing the text annotations. These two datasets allows us to evaluate the impact of language-image training on robustness because we can compare language-image training with standard classification training on the same set of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Three sample images from ImageNet-Captions. Their respective ImageNet labels are: drake, rocking chair, payphone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ImageNet Classification Linear fit (ImageNet Classification) CLIP zero-shot Linear fit (CLIP zero-shot) ImageNet-Captions Classification Linear fit (ImageNet-Captions Classification) ImageNet-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Using the weights from OpenAI's pre-trained CLIP model does not improve robustness, despite the large size of the full CLIP training set (400 million images)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Effect of prompting strategies and contrastive objectives on robustness. (Left) On most natural distribution shifts, effect of prompting on effective robustness is similar to that of random interpolation. (Right) Models pre-trained with various contrastive objectives on ImageNet do not achieve the same effective robustness as CLIP models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>ImageNet(Figure 7 :</head><label>7</label><figDesc>Deng et al.) IN-V2 (Recht et al.) IN-R (Hendrycks et al.) IN-Sketch (Wang et al.) ObjectNet (Barbu et al.) IN-A (Hendrycks et al.) Samples of the class candle from the various distribution shifts that we evaluate on in our experiments.B ImageNet-Captions experiments training detailsCLIP experiments are trained with cross-entropy losses using AdamW optimizer with initial learning rate of 0.001 and a cosine-annealing learning rate schedule with 500 warmup steps. Hyperparameters for AdamW are set at ? 1 = 0.9, ? 2 = 0.999, and =1e-8. The batch size is set to 1024. CLIP models trained on ImageNet-Captions are trained for 32 epochs, while ClIP models trained on all of ImageNet are trained for 90 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>We repeat the prior figure except we plot each distribution shift separately, as well as include ImageNet-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Images from ImageNet-Captions contain three types of metadata: titles, description, and tags. For each type of metadata, this table shows the number of images that have corresponding metadata that contains the class label of the image. For most images, the class label is in at least one text field, indicating that ImageNet-Captions is suitable for language-image training.</figDesc><table><row><cell>Caption Type</cell><cell cols="2"># Images % of Total</cell></row><row><cell>Title Only</cell><cell>239,495</cell><cell>51.6</cell></row><row><cell>Description Only</cell><cell>134,387</cell><cell>28.9</cell></row><row><cell>Tags Only</cell><cell>342,340</cell><cell>73.8</cell></row><row><cell>Title, Tag and Description</cell><cell>435,239</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluating different caption variants across ImageNet (IN) natural distribution shifts. Results are reported in top-1 accuracy (%). The best performing caption uses title, tags, and description. Although the language filter makes captions cleaner, the decrease in overall dataset size decreases performance.</figDesc><table><row><cell>Title Desc Tags Filter Size</cell><cell>Relative</cell><cell>IN</cell><cell cols="5">IN-V2 IN-R IN Sketch ObjectNet IN-A</cell></row><row><cell></cell><cell>Size (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>197K</cell><cell>42.6</cell><cell>15.7</cell><cell>12.2</cell><cell>6.6</cell><cell>1.1</cell><cell>5.5</cell><cell>2.4</cell></row><row><cell>459K</cell><cell>99.0</cell><cell>26.2</cell><cell>20.7</cell><cell>9.5</cell><cell>2.6</cell><cell>8.4</cell><cell>2.7</cell></row><row><cell>312K</cell><cell>67.4</cell><cell>21.9</cell><cell>16.5</cell><cell>8.0</cell><cell>1.7</cell><cell>6.1</cell><cell>2.2</cell></row><row><cell>461K</cell><cell>99.4</cell><cell>27.8</cell><cell>21.6</cell><cell>9.6</cell><cell>3.0</cell><cell>8.0</cell><cell>2.7</cell></row><row><cell>367K</cell><cell>79.3</cell><cell>26.5</cell><cell>20.3</cell><cell>8.9</cell><cell>2.3</cell><cell>7.9</cell><cell>2.5</cell></row><row><cell>464K</cell><cell>100.0</cell><cell>31.5</cell><cell>24.0</cell><cell>10.9</cell><cell>2.7</cell><cell>9.1</cell><cell>3.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparing CLIP training with (language model free) classification models on YFCC-15M. All experiments use a ViT-B/16 backbone. The CLIP results are from Mu et al.<ref type="bibr" target="#b23">[24]</ref>. Image-only contrastive learning followed by a simple text matching stage for classification nearly matches the performance of CLIP with a full language model.</figDesc><table><row><cell>Training style</cell><cell cols="2">ImageNet Avg OOD</cell></row><row><cell>CLIP</cell><cell>37.9</cell><cell>19.9</cell></row><row><cell>SimCLR ? Classification</cell><cell>35.7</cell><cell>18.8</cell></row><row><cell cols="3">other approaches do not. On the other hand, restricting the images used to those within ImageNet-Captions</cell></row><row><cell cols="2">hints that language may help improve ImageNet performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>For the experiments that use templates in Appendix F, the templates do not increase a model's robustness.G ImageNet templates and captions training variation experimentsExperiments in this section use all of ImageNet with the class labels replaced with templates. Experiments that mention captions use captions for the subset of ImageNet in ImageNet-Captions.Figure 9supplements our analysis of self-supervised methods with the recent MAE models of<ref type="bibr" target="#b15">[16]</ref>. In contrast to SimCLRV2 or MoCo, MAE is not a contrastive training approach. While MAE provides more effective robustness than other approaches, it is still much less robust than CLIP. However, the source of this robustness is an open question.</figDesc><table><row><cell cols="4">F ImageNet-Captions template experiments</cell><cell></cell><cell></cell></row><row><cell>Experiment</cell><cell>IN</cell><cell>IN-V2</cell><cell>IN-R</cell><cell cols="3">IN Sketch ObjectNet IN-A</cell></row><row><cell></cell><cell cols="6">(top-1, %) (top-1, %) (top-1, %) (top-1, %) (top-1, %) (top-1, %)</cell></row><row><cell cols="7">10% IN-Captions 20% IN-Captions 30% IN-Captions 40% IN-Captions 60% IN-Captions 80% IN-Captions 100% IN-Captions 100%, Aug ResNet-18, IN-Captions 100% 40.5 12.9 22.8 29.3 33.8 41.2 46.1 48.7 54.3 E ImageNet-Captions language encoder experiments 10.0 5.0 1.0 3.0 18.4 8.3 2.2 4.4 23.1 10.8 3.6 6.0 27.6 13.0 4.7 7.9 33.0 16.7 7.2 11.2 37.4 19.2 9.1 13.4 40.0 21.6 10.8 15.8 45.0 20.8 10.7 18.7 32.3 19.1 8.8 12.9 Title Desc Tags Filter IN IN-V2 IN-R IN Sketch ObjectNet IN-A 1.7 1.9 2.5 2.8 2.9 3.7 3.8 3.5 2.4 (top-1, %) (top-1, %) (top-1, %) (top-1, %) (top-1, %) (top-1, %) Language Initialized 19.9 15.3 8.4 1.9 6.5 2.2 Title+Tags+Description (Base) 31.5 24.0 10.9 2.7 9.1 3.0 Templates+Base 34.7 27.1 10.5 3.0 9.8 2.9 Templates+Base, Aug 42.1 32.9 11.2 3.6 10.8 2.5 Base as Templates 50.5 39.6 17.4 7.5 13.9 3.4 Base as Templates, Aug 59.0 47.6 18.3 8.4 16.1 3.2 Base as Classification 48.7 40.2 21.5 10.8 15.7 3.7 Base as Classification, Aug 54.2 45.0 20.7 10.6 18.7 3.6 15 25 35 45 55 65 75 ImageNet (top-1, %) 10 20 30 40 50 60 ImageNetV2 (top-1, %) y = x ImageNet Cls. Linear fit (ImageNet Cls.) CLIP zero-shot Linear fit (CLIP zero-shot) ImageNet-Captions Classification Linear fit (ImageNet-Captions Classification) Template Usage ImageNet-Captions CLIP 10 20 30 40 50 60 70 80 90 ImageNet (class-subsampled) (top-1, %) 12 22 33 43 52 ImageNet-R (top-1, %) 5 15 25 35 45 55 65 75 85 ImageNet (top-1, %) 11 20 31 ImageNet Sketch (top-1, %) 10 20 30 40 50 60 70 80 90 ImageNet (class-subsampled) (top-1, %) 12 22 33 43 ObjectNet (top-1, %) 15 25 35 45 55 65 75 85 95 ImageNet (class-subsampled) (top-1, %) 1 11 21 31 41 51 ImageNet-A (top-1, %) ImageNet (top-1, %) ImageNet using Templates (Base) 76.6 Base, Language Initialized 76.6 Base concatenated with Captions 76.8 Base concatenated with Captions, Language Initialized 76.9 Base, Captions as Text Augmentation 76.4 Base, Contrast with Template and Captions 76.0 H Self-supervised training variation experiments 45 55 65 75 85 ImageNet (top-1, %) 15 Figure 8: Experiment 25 35 45 55 65 75 y = x ImageNet Classification Linear fit (ImageNet Classification) CLIP zero-shot Linear fit (CLIP zero-shot) SimCLRv2 SimSiam Swav Average over 4 shifts (top-1, %) MAE</cell></row><row><cell></cell><cell>27.2</cell><cell>21.7</cell><cell>10.8</cell><cell>2.8</cell><cell>8.1</cell><cell>3.0</cell></row><row><cell></cell><cell>26.5</cell><cell>20.7</cell><cell>10.4</cell><cell>2.8</cell><cell>8.3</cell><cell>2.9</cell></row><row><cell></cell><cell>30.7</cell><cell>23.5</cell><cell>11.2</cell><cell>3.3</cell><cell>8.8</cell><cell>3.0</cell></row><row><cell></cell><cell>31.2</cell><cell>24.0</cell><cell>12.0</cell><cell>3.2</cell><cell>10.0</cell><cell>2.8</cell></row><row><cell></cell><cell>35.6</cell><cell>28.0</cell><cell>13.3</cell><cell>4.0</cell><cell>10.9</cell><cell>3.1</cell></row><row><cell>Language Initialized and Frozen</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>23.4</cell><cell>18.5</cell><cell>11.0</cell><cell>3.2</cell><cell>8.6</cell><cell>2.9</cell></row><row><cell></cell><cell>32.6</cell><cell>26.0</cell><cell>14.1</cell><cell>4.3</cell><cell>10.4</cell><cell>3.0</cell></row><row><cell></cell><cell>29.3</cell><cell>23.5</cell><cell>12.3</cell><cell>3.6</cell><cell>9.2</cell><cell>2.9</cell></row><row><cell></cell><cell>34.1</cell><cell>27.0</cell><cell>14.7</cell><cell>4.3</cell><cell>11.3</cell><cell>2.9</cell></row><row><cell></cell><cell>35.4</cell><cell>27.5</cell><cell>14.7</cell><cell>4.8</cell><cell>11.0</cell><cell>3.3</cell></row><row><cell></cell><cell>38.3</cell><cell>30.2</cell><cell>15.5</cell><cell>5.0</cell><cell>11.6</cell><cell>3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>There were 47 ImageNet classes that did not show up in the YFCC captions. They are the following: tiger shark, boa constrictor, partridge, bee eater, crane bird, sea lion, toy terrier, Black and Tan Coonhound, English foxhound, Otterhound, Curly-coated Retriever, Brittany dog, Kuvasz, Groenendael dog, Greater Swiss Mountain Dog, Entlebucher Sennenhund, brussels griffon, tiger cat, tiger beetle, guinea pig, bath towel, bell tower, cassette player, cliff dwelling, construction crane, espresso machine, fountain pen, French horn, harp, one-piece bathing suit, measuring cup, missile, oxygen mask, plate rack, radio telescope, rain barrel, balaclava ski mask, slide rule, steel drum, totem pole, waffle iron, whiskey jug, window screen, Windsor tie, acorn squash, bell pepper, gyromitra</figDesc><table><row><cell></cell><cell>Init</cell><cell>Augmentation</cell><cell>Sampler</cell><cell cols="2">Epochs ImageNet (top-1, %)</cell></row><row><cell>Synset</cell><cell cols="3">Scratch Crop, Flip, Jitter Class-bal</cell><cell>1</cell><cell>3.6</cell></row><row><cell>Synset</cell><cell cols="3">Scratch Crop, Flip, Jitter Class-bal</cell><cell>20</cell><cell>5.7</cell></row><row><cell>Synset</cell><cell cols="3">SimCLR Crop, Flip, Jitter Random</cell><cell>1</cell><cell>15.0</cell></row><row><cell>Synset</cell><cell cols="3">SimCLR Crop, Flip, Jitter Class-bal.</cell><cell>1</cell><cell>32.1</cell></row><row><cell>Synset</cell><cell cols="3">SimCLR Crop, Flip, Jitter Class-bal.</cell><cell>2</cell><cell>30.4</cell></row><row><cell>Synset</cell><cell cols="3">SimCLR Crop, Flip, Jitter Class-bal.</cell><cell>5</cell><cell>27.1</cell></row><row><cell cols="4">Synset + Synonyms SimCLR Crop, Flip, Jitter Class-bal.</cell><cell>1</cell><cell>34.5</cell></row><row><cell cols="2">Synset + Synonyms SimCLR</cell><cell>RandAug</cell><cell>Class-bal.</cell><cell>1</cell><cell>35.7</cell></row><row><cell cols="3">K YFCC-15M-Cls classes</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The most frequently occurring languages in ImageNet-Captions, according to PYCLD2 top-1. English also appears as a top-3 language in 91% of the captions.</figDesc><table><row><cell>Language</cell><cell cols="2"># Captions % of Total</cell></row><row><cell>English</cell><cell>416,601</cell><cell>89.9</cell></row><row><cell>Chinese</cell><cell>5,357</cell><cell>1.2</cell></row><row><cell>Spanish</cell><cell>3,893</cell><cell>0.8</cell></row><row><cell>Danish</cell><cell>2,993</cell><cell>0.6</cell></row><row><cell>Italian</cell><cell>2,598</cell><cell>0.6</cell></row><row><cell>German</cell><cell>2,263</cell><cell>0.5</cell></row><row><cell>Portuguese</cell><cell>2,104</cell><cell>0.5</cell></row><row><cell>Dutch</cell><cell>1,924</cell><cell>0.4</cell></row><row><cell>French</cell><cell>1,433</cell><cell>0.3</cell></row><row><cell>Scottish</cell><cell>1,404</cell><cell>0.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See Appendix A for examples of the shifts. In some figures we omit ImageNet-A due to the piecewise linear response created by the adversarial filtering process. We refer the reader to Taori et al.<ref type="bibr" target="#b34">[35]</ref> for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The dataset is available at https://github.com/mlfoundations/imagenet-captions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We call this baseline "NoCLIP", for "Now we use SimCLR+Classification instead of Contrastive Language-Image Pretraining</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Templates are composed by one to ten random words along with the class name, in an arbitrary position. Random words are drawn using https://pypi.org/project/Random-Word/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://pypi.org/project/better-profanity/ 7 https://github.com/snguyenthanh/better_profanity/blob/master/better_profanity/profanity_wordlist.txt 8 https://pypi.org/project/profanity-check/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The evolution of out-of-distribution robustness throughout fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.15831" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.09882" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.08981" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.10029" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2011.10566 (2020). 2020</idno>
		<ptr target="https://arxiv.org/abs/2011.10566" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1504.00325" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.13719" />
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Desai_VirTex_Learning_Visual_Representations_From_Textual_Annotations_CVPR_2021_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11162" to="11173" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Redcaps: Web-curated image-text data created by the people, for the people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zubin Trivadi Aysola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.11431" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Does language help generalization in vision models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavin</forename><surname>Choksi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Bielawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufin</forename><surname>Vanrullen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.08313" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On robustness and transferability of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.08558" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11929" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.06377" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Early stopping in deep networks: Double descent and how to eliminate it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Heckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilmaz</forename><surname>Fatih Furkan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tlV90jvZbw" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.07174" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-ofdistribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2006.16241" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanuj</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Lennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zubin</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat</forename><surname>Tran</surname></persName>
		</author>
		<ptr target="https://github.com/idealo/imagededup" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.05918" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1995-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accuracy on the line: on the strong correlation between out-ofdistribution and in-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2107.04649" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Slip: Self-supervision meets language-image pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.12750" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Combined scaling for zero-shot transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/2111.10050</idno>
		<ptr target="https://arxiv.org/abs/2111.10050" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/radford21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1902.10811" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning visual representations with caption annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>B?lent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58598-3_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58598-3_10" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12353</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.02114" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluating machine accuracy on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horia</forename><surname>Mania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v119/shankar20c.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.01913" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Measuring robustness to natural distribution shifts in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.00644" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.13549" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Lit: Zero-shot transfer with locked-image text tuning. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.07991" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>abs/2010.00747</idno>
		<ptr target="https://arxiv.org/abs/2010.00747" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">ImageNetV2 (top-1, %) y = x ImageNet Classification Linear fit</title>
		<imprint/>
	</monogr>
	<note>ImageNet Classification) CLIP zero-shot Linear fit (CLIP zero-shot</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
