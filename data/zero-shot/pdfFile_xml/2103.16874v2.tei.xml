<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Reference Image Synthetic Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-10">10 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsoo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<email>jchoo@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization Reference Image Synthetic Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-10">10 Sep 2021</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Given a pair of a reference image (containing a person) and a target clothing image, our method successfully synthesizes 1024?768 virtual try-on images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The task of image-based virtual try-on aims to transfer a target clothing item onto the corresponding region of a person, which is commonly tackled by fitting the item to the desired body part and fusing the warped item with the person. While an increasing number of studies have been conducted, the resolution of synthesized images is still limited to low (e.g., 256?192), which acts as the critical limitation against satisfying online consumers. We argue that the limitation stems from several challenges: as the resolution increases, the artifacts in the misaligned areas between the warped clothes and the desired clothing regions become noticeable in the final results; the architectures used in ex-* These authors contributed equally. isting methods have low performance in generating highquality body parts and maintaining the texture sharpness of the clothes. To address the challenges, we propose a novel virtual try-on method called VITON-HD that successfully synthesizes 1024?768 virtual try-on images. Specifically, we first prepare the segmentation map to guide our virtual try-on synthesis, and then roughly fit the target clothing item to a given person's body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS generator to handle the misaligned areas and preserve the details of 1024?768 inputs. Through rigorous comparison with existing methods, we demonstrate that VITON-HD highly surpasses the baselines in terms of synthesized image quality both qualitatively and quantitatively. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-based virtual try-on refers to the image generation task of changing the clothing item on a person into a different item, given in a separate product image. With a growing trend toward online shopping, virtually wearing the clothes can enrich a customer's experience, as it gives an idea about how these items would look on them.</p><p>Virtual try-on is similar to image synthesis, but it has unique and challenging aspects. Given images of a person and a clothing product, the synthetic image should meet the following criteria: (1) The person's pose, body shape, and identity should be preserved. <ref type="bibr" target="#b1">(2)</ref> The clothing product should be naturally deformed to the desired clothing region of the given person, by reflecting his/her pose and body shape. <ref type="bibr" target="#b2">(3)</ref> Details of the clothing product should be kept intact. <ref type="bibr" target="#b3">(4)</ref> The body parts initially occluded by the person's clothes in the original image should be properly rendered. Since the given clothing image is not initially fitted to the person image, fulfilling these requirements is challenging, which leaves the development of virtual try-on still far behind the expectations of online consumers. In particular, the resolution of virtual try-on images is low compared to the one of normal pictures on online shopping websites.</p><p>After Han et al. <ref type="bibr" target="#b9">[10]</ref> proposed VITON, various imagebased virtual try-on methods have been proposed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6]</ref>. These methods follow two processes in common: <ref type="bibr" target="#b0">(1)</ref> warping the clothing image initially to fit the human body;</p><p>(2) fusing the warped clothing image and the image of the person that includes pixel-level refinement. Also, several recent methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref> add a module that generates segmentation maps and determine the person's layout from the final image in advance.</p><p>However, the resolution of the synthetic images from the previous methods is low (e.g., 256?192) due to the following reasons. First, the misalignment between the warped clothes and a person's body results in the artifacts in the misaligned regions, which become noticeable as the image size increases. It is difficult to warp clothing images to fit the body perfectly, so the misalignment occurs as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Most of previous approaches utilize the thin-plate spline (TPS) transformation to deform clothing images. To accurately deform clothes, ClothFlow <ref type="bibr" target="#b8">[9]</ref> predicts the optical flow maps of the clothes and the desired clothing regions. However, the optical flow maps does not remove the misalignment completely on account of the regularization. In addition, the process requires more computational costs than other methods due to the need of predicting the movement of clothes at a pixel level. (The detailed analysis of ClothFlow is included in the supplementary.) Second, a simple U-Net architecture <ref type="bibr" target="#b24">[25]</ref> used in existing approaches is insufficient in synthesizing initially occluded body parts in final high-resolution (e.g., 1024?768) images. As noted in Wang et al. <ref type="bibr" target="#b31">[32]</ref>, applying a simple U-Net-based archi-  tecture to generate high-resolution images leads to unstable training as well as unsatisfactory quality of generated images. Also, refining the images once at the pixel level is insufficient in preserving the details of high-resolution clothing images.</p><p>To address the above-mentioned challenges, we propose a novel high-resolution virtual try-on method, called VITON-HD. In particular, we introduce a new clothingagnostic person representation that leverages the pose information and the segmentation map so that the clothing information is eliminated thoroughly. Afterwards, we feed the segmentation map and the clothing item deformed to fit the given human body to the model. Using the additional information, our novel ALIgnment-Aware Segment (ALIAS) normalization removes information irrelevant to the clothing texture in the misaligned regions and propagates the semantic information throughout the network. The normalization separately standardizes the activations corresponding to the misaligned regions and the other regions, and modulates the standardized activations using the segmentation map. Our ALIAS generator employing ALIAS normalization synthesizes the person image wearing the target product while filling the misaligned regions with the clothing texture and preserving the details of the clothing item through the multi-scale refinement at a feature level. To validate the performance of our framework, we collected a 1024?768 dataset that consists of pairs of a person and a clothing item for our research purpose. Our experiments demonstrate that VITON-HD significantly outperforms the existing methods in generating 1024?768 images, both quantitatively and qualitatively. We also confirm the superior capability of our novel ALIAS normalization module in dealing with the misaligned regions.</p><p>We summarize our contributions as follows:</p><p>? We propose a novel image-based virtual try-on approach called VITON-HD, which is, to the best of our knowledge, the first model to successfully synthesize 1024?768 images. ? We introduce a clothing-agnostic person representation that allows our model to remove the dependency on the clothing item originally worn by the person.</p><p>? To address the misalignment between the warped clothes and the desired clothing regions, we propose ALIAS normalization and ALIAS generator, which is effective in maintaining the details of clothes. ? We demonstrate the superior performance of our method through experiments with baselines on the newly collected dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Conditional Image Synthesis. Conditional generative adversarial networks (cGANs) utilize additional information, such as class labels <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2]</ref>, text <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>, and attributes <ref type="bibr" target="#b27">[28]</ref>, to steer the image generation process. Since the emergence of pix2pix <ref type="bibr" target="#b13">[14]</ref>, numerous cGANs conditioned on input images have been proposed to generate high-resolution images in a stable manner <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b20">21]</ref>. However, these methods tend to generate blurry images when handling a large spatial deformation between the input image and the target image. In this paper, we propose a method that can address the spatial deformation of input images and properly generate 1024?768 images.</p><p>Normalization Layers. Normalization layers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref> have been widely applied in modern deep neural networks. Normalization layers, whose affine parameters are estimated with external data, are called conditional normalization layers. Conditional batch normalization <ref type="bibr" target="#b3">[4]</ref> and adaptive instance normalization <ref type="bibr" target="#b11">[12]</ref> are such conditional normalization techniques and have been used in style transfer tasks. SPADE <ref type="bibr" target="#b19">[20]</ref> and SEAN <ref type="bibr" target="#b38">[39]</ref> utilize segmentation maps to apply spatially varying affine transformations. Using the misalignment mask as external data, our proposed normalization layer computes the means and the variances of the misaligned area and the other area within an instance separately. After standardization, we modulate standardized activation maps with affine parameters inferred from human-parsing maps to preserve semantic information.</p><p>Virtual Try-On Approaches. There are two main categories for virtual try-on approaches: 3D model-based approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref> and 2D image-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b4">5]</ref>. 3D model-based approaches can accurately simulate the clothes but are not widely applicable due to their dependency on 3D measurement data.</p><p>2D image-based approaches do not rely on any 3D information, thus being computationally efficient and appropriate for practical use. Jetchev and Bergmann <ref type="bibr" target="#b14">[15]</ref> proposed CAGAN, which first introduced the task of swapping fashion articles on human images. VITON <ref type="bibr" target="#b9">[10]</ref> addressed the same problem by proposing a coarse-to-fine synthesis framework that involves TPS transformation of clothes. Most existing virtual try-on methods tackle different aspects of VITON to synthesize perceptually convincing photo-realistic images. CP-VTON [31] adopted a geometric matching module to learn the parameters of TPS transfor-mation, which improves the accuracy of deformation. VT-NFP <ref type="bibr" target="#b35">[36]</ref> and ACGPN <ref type="bibr" target="#b34">[35]</ref> predicted the human-parsing maps of a person wearing the target clothes in advance to guide the try-on image synthesis. Even though the image quality at high resolution is an essential factor in evaluating the practicality of the generated images, none of the methods listed above could generate such photo-realistic images at high resolution.</p><p>3. Proposed Method Model Overview. As described in <ref type="figure" target="#fig_2">Fig. 3</ref>, given a reference image I ? R 3?H?W of a person and a clothing image c ? R 3?H?W (H and W denote the image height and width, respectively), the goal of VITON-HD is to generate a synthetic image? ? R 3?H?W of the same person wearing the target clothes c, where the pose and body shape of I and the details of c are preserved. While training the model with (I, c,?) triplets is straightforward, construction of such dataset is costly. Instead, we use (I, c, I) where the person in the reference image I is already wearing c.</p><p>Since directly training on (I, c, I) can harm the model's generalization ability at test time, we first compose a clothing-agnostic person representation that leaves out the clothing information in I and use it as an input. Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in I (Section 3.1). The model generates the segmentation map from the clothing-agnostic person representation to help the generation of? (Section 3.2). We then deform c to roughly align it to the human body (Section 3.3). Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the misleading information in the misaligned area after deforming c. ALIAS generator fills the misaligned area with the clothing texture and maintains the clothing details (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Clothing-Agnostic Person Representation</head><p>To train the model with pairs of c and I already wearing c, a person representation without the clothing information in I has been utilized in the virtual try-on task. Such representations have to satisfy the following conditions: (1) the original clothing item to be replaced should be deleted;</p><p>(2) sufficient information to predict the pose and the body shape of the person should be maintained; (3) the regions to be preserved (e.g., face and hands) should be kept to maintain the person's identity.</p><p>Problems of Existing Person Representations. In order to maintain the person's shape, several approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref> provide a coarse body shape mask as a cue to synthesize the image, but fail to reproduce the body parts elaborately (e.g., hands). To tackle this issue, ACGPN <ref type="bibr" target="#b34">[35]</ref> employs the detailed body shape mask as the input, and the neural network attempts to discard the clothing informa- tion to be replaced. However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information. As a result, the original clothing item that is not completely removed causes problems in the test phase.</p><p>Clothing-Agnostic Person Representation. We propose a clothing-agnostic image I a and a clothing-agnostic segmentation map S a as inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced. We first predict the segmentation map S ? L H?W and the pose map P ? R 3?H?W of the image I by utilizing the pre-trained networks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>, where L is a set of integers indicating the semantic labels. The segmentation map S is used to remove the clothing region to be replaced and preserve the rest of the image. The pose map P is utilized to remove the arms, but not the hands, as they are difficult to reproduce. Based on S and P , we generate the clothing-agnostic image I a and the clothing-agnostic segmentation map S a , which allow the model to remove the original clothing information thoroughly, and preserve the rest of the image. In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we con-catenate I a or S a to the RGB pose map P representing a skeletal structure that improves generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation Generation</head><p>Given the clothing-agnostic person representation (S a , P ), and the target clothing item c, the segmentation generator G S predicts the segmentation map? ? L H?W of the person in the reference image wearing c. We train G S to learn the mapping between S and (S a , P, c), in which the original clothing item information is completely removed. As the architecture of G S , we adopt U-Net <ref type="bibr" target="#b24">[25]</ref>, and the total loss L S of the segmentation generator are written as</p><formula xml:id="formula_0">L S = L cGAN + ? CE L CE ,<label>(1)</label></formula><p>where L CE and L cGAN denote the pixel-wise cross-entropy loss and conditional adversarial loss between? and S, respectively. ? CE is the hyperparameter corresponding to the relative importance between two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Clothing Image Deformation</head><p>In this stage, we deform the target clothing item c to align it with? c , which is the clothing area of?. We employ the geometric matching module proposed in CP-VTON <ref type="bibr" target="#b30">[31]</ref> with the clothing-agnostic person representa- tion (I a , P ) and? c as inputs. A correlation matrix between the features extracted from (I a , P ) and c is first calculated . With the correlation matrix as an input, the regression network predicts the TPS transformation parameters ? ? R 2?5?5 , and then c is warped by ?. In the training phase, the model takes S c extracted from S instead of? c . The module is trained with the L1 loss between the warped clothes and the clothes I c that is extracted from I. In addition, the second-order difference constraint <ref type="bibr" target="#b34">[35]</ref> is adopted to reduce obvious distortions in the warped clothing images from deformation. The overall objective function to warp the clothes to fit the human body is written as</p><formula xml:id="formula_1">L warp = ||I c ? W(c, ?)|| 1,1 + ? const L const ,<label>(2)</label></formula><p>where W is the function that deforms c using ?, L const is a second-order difference constraint, and ? const is the hyperparameter for L const .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Try-On Synthesis via ALIAS Normalization</head><p>We aim to generate the final synthetic image? based on the outputs from the previous stages. Overall, we fuse the clothing-agnostic person representation (I a , P ) and the warped clothing image W(c, ?), guided by?. (I a , P, W(c, ?)) is injected into each layer of the generator. For?, we propose a new conditional normalization method called the ALIgnment-Aware Segment (ALIAS) normalization. ALIAS normalization enables the preservation of semantic information, and the removal of misleading information from the misaligned regions by leveraging? and the mask of these regions.</p><p>Alignment-Aware Segment Normalization. Let us de-</p><formula xml:id="formula_2">note h i ? R N ?C i ?H i ?W i</formula><p>as the activation of the i-th layer of a network for a batch of N samples, where H i , W i , and C i indicate the height, width, and the number of channels of h i , respectively. ALIAS normalization has two inputs: <ref type="figure">Figure 5</ref>: ALIAS normalization. First, the activation is separately standardized according to the regions divided by M misalign , which can be obtained from the difference be-tween? c and M align . Next,? div is convolved to create the modulation parameters ? and ?, and then the standardized activation is modulated with the parameters ? and ?.</p><formula xml:id="formula_3">Standardization Conv Conv ? = ? Conv ?</formula><p>(1) the synthetic segmentation map?; (2) the misalignment binary mask M misalign ? L H?W , which excludes the warped mask of the target clothing image W(M c , ?) from S c (M c denotes the target clothing mask), i.e.,  <ref type="table">Table 1</ref>: Quantitative comparison with baselines across different resolutions. VITON-HD* is a VITON-HD variant where the standardization in ALIAS normalization is replaced by channel-wise standardization as in the original instance normalization. For the SSIM, higher is better. For the LPIPS and the FID, lower is better.</p><formula xml:id="formula_4">M align =? c ? W(M c , ?) (3) M misalign =? c ? M align .<label>(4)</label></formula><p>inferred from? div . The activation value at site (n ? N, k ?</p><formula xml:id="formula_5">C i , y ? H i , x ? W i ) is calculated by ? i k,y,x (? div ) h i n,k,y,x ? ? i,m n,k ? i,m n,k + ? i k,y,x (? div ),<label>(5)</label></formula><p>where h i n,k,y,x is the activation at the site before normalization and ? i k,y,x and ? i k,y,x are the functions that convert S div to modulation parameters of the normalization layer. ? i,m n,k and ? i,m n,k are the mean and standard deviation of the activation in sample n and channel k. ? i,m n,k and ? i,m n,k are calculated by</p><formula xml:id="formula_6">? i,m n,k = 1 |? i,m n | (y,x)?? i,m n h i n,k,y,x<label>(6)</label></formula><formula xml:id="formula_7">? i,m n,k = 1 |? i,m n | (y,x)?? i,m n (h i n,k,y,x ? ? i,m n,k ) 2 ,<label>(7)</label></formula><p>where ? i,m n denotes the set of pixels in region m, which is M misalign or the other region, and |? i,m n | is the number of pixels in ? i,m n . Similar to instance normalization <ref type="bibr" target="#b29">[30]</ref>, the activation is standardized per channel. However, ALIAS normalization divides the activation in channel k into the activation in the misaligned region and the other region.</p><p>The rationale behind this strategy is to remove the misleading information in the misaligned regions. Specifically, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture. Performing a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the final results. In modulation, affine parameters inferred from the segmentation map modulate the standardized activation. Due to injecting semantic information at each ALIAS normalization layer, the layout of the human-parsing map in the final result is preserved.</p><p>ALIAS Generator. <ref type="figure" target="#fig_3">Fig. 4</ref> describes the overview of the ALIAS generator, which adopts the simplified architecture that discards the encoder part of an encoder-decoder network. The generator employs a series of residual blocks (ResBlk) with upsampling layers. Each ALIAS ResBlk consists of three convolutional layers and three ALIAS normalization layers. Due to the different resolutions that Res-Blks operate at, we resize the inputs of the normalization layers,? and M misalign , before injecting them into each layer. Similarly, the input of the generator, (I a , P, W(c, ?)), is resized to different resolutions. Before each ResBlk, the resized inputs (I a , P, W(c, ?)) are concatenated to the activation of the previous layer after passing through a convolution layer, and each ResBlk utilizes the concatenated inputs to refine the activation. In this manner, the network performs the multi-scale refinement at a feature level that better preserves the clothing details than a single refinement at the pixel level. We train the ALIAS generator with the conditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE <ref type="bibr" target="#b19">[20]</ref> and pix2pixHD <ref type="bibr" target="#b31">[32]</ref>. Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Dataset. We collected 1024?768 virtual try-on dataset for our research purpose, since the resolution of images on the dataset provided by Hanet al. <ref type="bibr" target="#b9">[10]</ref> was low. Specifically, we crawled 13,679 frontal-view woman and top clothing image pairs on an online shopping mall website. The pairs are split into a training and a test set with 11,647 and 2,032 pairs, respectively. We use the pairs of a person and a clothing image to evaluate a paired setting, and we shuffle the clothing images for an unpaired setting. The paired setting is to reconstruct the person image with the original clothing item, and the unpaired setting is to change the clothing item on the person image with a different item.</p><p>Training and Inference. With the goal of reconstructing I from (I a , c), the training of each stage proceeds individually. During the training of the geometric matching module and the ALIAS generator, we use S instead of?. While we aim to generate 1024?768 try-on images, we train the segmentation generator and the geometric matching module at 256?192. In the inference phase, after being predicted by the segmentation generator at 256?192, the segmentation map is upscaled to 1024?768 and passed to subsequent stages. Similarly, the geometric matching module predicts the TPS parameters ? at 256?192, and the 1024?768 clothing image deformed by the parameters ? is used in the ALIAS generator. We empirically found that this approach makes these two modules perform better with a lower memory cost than those trained at 1024?768. Details of the model architecture and hyperparameters are described in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Analysis</head><p>We compare VITON-HD with CP-VTON <ref type="bibr" target="#b30">[31]</ref> and ACGPN <ref type="bibr" target="#b34">[35]</ref>, whose codes are publicly available. Following the training and inference procedure of our model, segmentation generators and geometric matching modules of the baselines are trained at 256?192, and the outputs from the modules are upscaled to 1024?768 during the inference.</p><p>Comparison with Baselines. <ref type="figure">Fig. 6</ref> demonstrates that VITON-HD generates more perceptually convincing 1024?768 images compared to the baselines. Our model clearly preserves the details of the target clothes, such as the logos and the clothing textures, due to the multi-scale refinement at a feature level. In addition, regardless of what clothes the person is wearing in the reference image, our model synthesizes the body shape naturally. As shown in <ref type="figure">Fig. 7</ref>, the shape of the original clothing item remains in the synthetic segmentation map generated by ACGPN. On the other hand, the segmentation generator in VITON-HD successfully predicts the segmentation map regardless of the original clothing item, due to our newly proposed clothingagnostic person representation. Although our model surpasses the baselines qualitatively, there are a few limitations to VITON-HD, which are reported in the supplementary with the additional qualitative results. Effectiveness of the ALIAS Normalization. We study the effectiveness of ALIAS normalization by comparing our model to VITON-HD*, where the standardization in ALIAS normalization is replaced by channel-wise standardization, as in the original instance normalization <ref type="bibr" target="#b29">[30]</ref>. <ref type="figure">Fig. 8</ref> shows that ALIAS normalization has the capability to fill the misaligned areas with the target clothing texture by removing the misleading information. On the other hand, without utilizing ALIAS normalization, the artifacts are produced in the misaligned areas, because the background information in the warped clothing image is not removed as described in Section 3.4. ALIAS normalization, however, can handle the misaligned regions properly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Analysis</head><p>We perform the quantitative experiments in both a paired and an unpaired settings, in which a person wears the original clothes or the new clothes, respectively. We evaluate our method using three metrics widely used in virtual try-on. The structural similarity (SSIM) <ref type="bibr" target="#b32">[33]</ref> and the learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b37">[38]</ref> are used in the paired setting, and the frechet inception distance (FID) <ref type="bibr" target="#b10">[11]</ref> score is adopted in the unpaired setting. The inception score <ref type="bibr" target="#b25">[26]</ref> is not included in the experiments, since it cannot reflect whether the details of the clothing image are maintained <ref type="bibr" target="#b9">[10]</ref>. The input of the each model contains different amount of information that offers advantages in reconstructing the segmentation maps, thus we use the segmentation maps from the test set instead of the synthetic segmentation maps in the paired setting.</p><p>Comparison across Different Resolutions. We compare the baselines quantitatively across different resolutions (256?192, 512?384, and 1024?768) as shown in <ref type="table">Table 1</ref>. Our model outperforms the baselines for SSIM and LPIPS across all resolutions. For FID score, our model significantly surpasses CP-VTON, regardless of the resolutions. The FID score in ACGPN is slightly lower than that of our model at the 256?192 resolution. However, at the 1024?768 resolution, our model achieves a lower FID score than ACGPN with a large margin. The results indicate that the baselines cannot handle 1024?768 images, while our model is trained in a stable manner, even at a high resolution. This may be due to the limited capability of the U-Net architecture employed in the baseline models.</p><p>Comparison According to the Degree of Misalignment. To verify the ability of filling the misaligned areas with the clothing texture, we perform experiments in the paired setting according to the degree of the misalignment. According to the number of pixels in the misaligned areas, we divide the test dataset in three types: small, medium, and large. For a fair comparison, each model uses the same segmentation maps and the same warped clothes as inputs to match the misaligned regions. We evaluate LPIPS to measure the semantic distances between the reference images and the reconstructed images. As shown in <ref type="figure">Fig. 9</ref>, the wider the misaligned areas, the worse the performance of models, which means that the misalignment hinders the models from generating photo-realistic virtual try-on images. Compared to the baselines, our model consistently performs better, and the performance of our model decreases less as the degree of misalignment increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose the VITON-HD that synthesizes photorealistic 1024?768 virtual try-on images. The proposed ALIAS normalization can properly handle the misaligned areas and propagate the semantic information throughout the ALIAS generator, which preserves the details of the clothes via the multi-scale refinement. Qualitative and quantitative experiments demonstrate that VITON-HD surpasses existing virtual try-on methods with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Pre-processing Details</head><p>This section introduces the details of generating our clothing-agnostic person representation. To remove the dependency on the clothing item originally worn by a person, regions that can provide any original clothing information, such as the arms that hint at the sleeve length, should be eliminated. Therefore, when generating a clothing-agnostic image I a , we remove the arms from the reference image I. For the same reason, legs should be removed if the pants are the target clothing items. We mask the regions with a gray color, so that the masked pixels of the normalized image would have a value of 0. We add padding to the masks to thoroughly remove these regions, and the width of the padding is empirically determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Model Architectures</head><p>This section introduces the architectures of the segmentation generator, the geometric matching module, and ALIAS generator in detail.</p><p>Segmentation Generator. The segmentation generator has the structure of U-Net <ref type="bibr" target="#b24">[25]</ref>, which consists of convolutional layers, downsampling layers, and upsampling layers. Two multi-scale discriminators <ref type="bibr" target="#b31">[32]</ref> are employed for the conditional adversarial loss. The details of the segmentation generator architecture are shown in <ref type="figure" target="#fig_7">Fig. 10</ref>.</p><p>Geometric Matching Module. The geometric matching module consists of two feature extractors and a regression network. A correlation matrix is calculated from the two extracted features, and the regression network predicts the TPS parameter ? with the correlation matrix. The feature extractor is composed of a series of convolutional layers, and the regression network consists of a series of convolutional layers followed by a fully connected layer. The details are shown in <ref type="figure" target="#fig_9">Fig. 11</ref>.</p><p>ALIAS Generator. The architecture of the ALIAS generator consists of a series of ALIAS ResBlks with nearestneighbor upsampling layers. We employ two multi-scale discriminators with instance normalization. Spectral normalization <ref type="bibr" target="#b17">[18]</ref> is applied to all the convolutional layers. Note that we separately standardize the activation based on the misalignment mask M misalign only in the first five ALIAS ResBlks. The details of the ALIAS generator architecture is shown in <ref type="figure" target="#fig_1">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Training Details</head><p>This section introduces the losses and the hyperparameters for the segmentation generator, the geometric matching module, and the ALIAS generator.</p><p>Segmentation Generator. The segmentation generator G S uses the clothing-agnostic segmentation map S a , the pose map P , and the clothing item c as inputs (? = G S (S a , P, c)) to predict the segmentation map? of the person in the reference image wearing the target clothing item. The segmentation generator is trained with the     cross-entropy loss L CE and the conditional adversarial loss L cGAN , which is LSGAN loss <ref type="bibr" target="#b16">[17]</ref>. The full loss L S for the segmentation generator are written as</p><formula xml:id="formula_8">L S = L cGAN + ? CE L CE<label>(8)</label></formula><formula xml:id="formula_9">L CE = ? 1 HW k?C,y?H,x?W S k,y,x log(? k,y,x )<label>(9)</label></formula><p>L cGAN = E (X,S) [log(D(X, S))]</p><formula xml:id="formula_10">+ E X [1 ? log(D(X,?))],<label>(10)</label></formula><p>where ? CE is the hyperparameter for the cross-entropy loss.</p><p>In the experiment, ? CE is set to 10. In Eq. (9), S yxk and S yxk indicate the pixel values of the segmentation map of the reference image S and? corresponding to the coordinates (x, y) in channel k. The symbols H, W and C indicate the height, width, and the number of channels of S. In Eq. (10), the symbol X indicates the inputs of the generator (S a , P, c), and D denotes the discriminator.</p><p>The learning rate of the generator and the discriminator is 0.0004. We adopt the Adam optimizer with ? 1 = 0.5 and ? 2 = 0.999. We train the segmentation generator for 200,000 iterations with the batch size of 8.</p><p>Geometric Matching Module. The inputs of the geometric matching module are c, P , clothing-agnostic image I a , and? c , which is the clothing area of?. The output is  </p><formula xml:id="formula_11">L warp = ||I c ? W(c, ?)|| 1,1 + ? const L const<label>(11)</label></formula><formula xml:id="formula_12">L const = p?P | (| ||pp 0 || 2 ? ||pp 1 || 2 | + | ||pp 2 || 2 ? ||pp 3 || 2 |) +(|S(p, p 0 ) ? S(p, p 1 )| + |S(p, p 2 ) ? S(p, p 3 )|),<label>(12)</label></formula><p>where W is the function that deforms c using ?, and I c is the clothing item extracted from the reference image I. L const is a second-order difference constraint <ref type="bibr" target="#b34">[35]</ref>, and ? const is the hyperparameter for L const . In the experiment, we set ? const to 0.04. In Eq. (12), the symbol p indicates a sampled TPS control point from the entire control points set P, and p 0 , p 1 , p 2 , and p 3 are top, bottom, left and right point of p, respectively. The function S(p, p i ) denotes the slope between p and p i . The learning rate of the geometric matching module is 0.0002. We adopt the Adam optimizer with ? 1 = 0.5 and ? 2 = 0.999. We train the geometric matching module for 50,000 iterations with the batch size of 8.</p><p>ALIAS Generator. The loss function of ALIAS generator follows those of SPADE <ref type="bibr" target="#b19">[20]</ref> and pix2pixHD <ref type="bibr" target="#b31">[32]</ref>, as it Grid 5x5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid 10x10</head><p>Grid 20x20 ClothFlow <ref type="figure" target="#fig_2">Figure 13</ref>: Qualitative comparisons of TPS transformation with various grid numbers and the flow estimation from ClothFlow.  <ref type="table">Table 2</ref>: denotes a score taken from the ClothFlow paper, and we train VITON-HD in the same setting (e.g., dataset and resolution). We compute MACs of their warping modules at 256?192.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>contains the conditional adversarial loss L cGAN , the feature matching loss L F M , and the perceptual loss L percept . Let D I be the discriminator, I and c be the given reference and target clothing images, and? be the synthetic image generated by the generator. S div is the modified version of the segmentation map S. The full loss L I of our generator is written as</p><formula xml:id="formula_13">L I = L cGAN + ? F M L F M + ? percept L percept<label>(13)</label></formula><p>L cGAN = E I [log(D I (S div , I))]</p><formula xml:id="formula_14">+ E (I,c) [1 ? log(D I (S div ,?))]<label>(14)</label></formula><formula xml:id="formula_15">L F M = E (I,c) T i=1 1 K i [||D (i) I (S div , I) ? D (i) I (S div ,?)|| 1,1 ]<label>(15)</label></formula><formula xml:id="formula_16">L percept = E (I,c) V i=1 1 R i [||F (i) (I) ? F (i) (?)|| 1,1 ],<label>(16)</label></formula><p>where ? F M and ? percept are hyperparameters. In the experiment, both ? F M and ? percept are set to 10. T is the number of layers in D I , and D (i) I and K i are the activation and the number of elements in the i-th layer of D I , respectively. Similarly, V is the number of layers used in the VGG network F <ref type="bibr" target="#b28">[29]</ref>, and F (i) and R i are the activation and the number of elements in the i-th layer of F , respectively. We replace the standard adversarial loss with the Hinge loss <ref type="bibr" target="#b36">[37]</ref>.</p><p>The learning rate of the generator and the discriminator is 0.0001 and 0.0004, respectively. We adopt the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with ? 1 = 0 and ? 2 = 0.9. We train the ALIAS generator for 200,000 iterations with the batch size of 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Comparison with ClothFlow</head><p>To demonstrate that the optical flow estimation does not solve the misalignment completely, we re-implement the flow estimation module of ClothFlow <ref type="bibr" target="#b8">[9]</ref> based on the original paper. <ref type="figure" target="#fig_2">Fig. 13</ref> shows that the misalignment still occurs, although both TPS with a higher grid number (e.g., a 10?10 or 20?20 grid) and the flow estimation module of ClothFlow can reduce the misaligned regions. The reason is that the regularization to avoid the artifacts (e.g., TV loss) prevents the warped clothes from fitting perfectly into the target region. In addition, we evaluate the accuracy and the computational cost of warping modules in VITON-HD and ClothFlow with Warp-SSIM <ref type="bibr" target="#b8">[9]</ref> and MACs, respectively. We also measure how well the models reconstruct the clothing using Mask-SSIM <ref type="bibr" target="#b8">[9]</ref>. <ref type="table">Table 2</ref> shows that the ClothFlow warping module has the better accuracy than ours, whereas the higher Mask-SSIM in VITON-HD proves that ALIAS normalization is more effective at solving the misalignment problem than the improved warping method. We found that the ClothFlow warping module needs a huge computational cost (MACs: 130.03G) at 1024?768, but the cost could be reduced when predicting the optical flow map at 256?192. <ref type="table">Table 2</ref> demonstrates that the ClothFlow warping module still needs more computational cost than ours, yet it is a viable option to combine the flow estimation module with ALIAS generator.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. User Study</head><p>We further evaluate our model and other baselines via a user study in the unpaired setting. We randomly select 30 sets of a reference image and a target clothing image from the test dataset. Given the reference images and the target clothes, the users are asked to rank the 1024?768 outputs of our model and baselines according to the following questions: (1) Which image is the most photo-realistic? (2) Which image preserves the details of the target clothing the most? As shown in <ref type="figure" target="#fig_3">Fig. 14,</ref> it can be observed that our approach achieves the rank 1 votes more than 88% for the both questions. The result demonstrates that our model generates more realistic images, and preserves the details of the clothing items compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Qualitative Results</head><p>We provide additional qualitative results to demonstrate our model's capability of handling high quality image synthesis. <ref type="figure" target="#fig_15">Fig. 16, 17, 18, and 19</ref> show the qualitative comparison of the baselines across different resolutions. <ref type="figure" target="#fig_1">Fig. 20</ref>, 21, 22, and 23 show additional results of VITON-HD at 1024?768 resolution. <ref type="figure" target="#fig_13">Fig. 15</ref> shows the failure cases of our model caused by the inaccurately predicted segmentation map or the inner collar region indistinguishable from the other clothing region. Also, the boundaries of the clothing textures occasionally fade away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Failure Cases and Limitations</head><p>The limitations of our model are as follows. VITON-HD is trained to preserve the bottom clothing items, limiting the presentation of the target clothes (e.g., whether they are tucked in). It can be a valuable future direction to generate multiple possible outputs from a single input pair. Next, our dataset mostly consists of slim women and top clothing images, which makes VITON-HD handle only a limited range of body shapes and clothing during the inference. We believe that VITON-HD has the capability to cover more diverse cases when the images of various body shapes and clothing types are provided. Finally, existing virtual try-on methods including VITON-HD do not provide robust performance for in-the-wild images. We think generating realistic try-on images for the in-the-wild images is an interesting topic for future work.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An example of misaligned regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overview of a VITON-HD. (a) First, given a reference image I containing a target person, we predict the segmentation map S and the pose map P , and utilize them to pre-process I and S as a clothing-agnostic person image I a and segmentation S a . (b) Segmentation generator produces the synthetic segmentation? from (S a , P, c). (c) Geometric matching module deforms the clothing image c according to the predicted clothing segmentation? c extracted from?. (d) Finally, ALIAS generator synthesizes the final output image? based on the outputs from the previous stages via our ALIAS normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>ALIAS generator. (a) The ALIAS generator is composed of a series of ALIAS residual blocks, along with upsampling layers. The input (I a , P, W(c, ?)) is resized and injected into each layer of the generator. (b) A detailed view of a ALIAS residual block. Resized (I a , P, W(c, ?)) is concatenated to h i after passing through a convolution layer. Each ALIAS normalization layer leverages resized? and M misalign to normalize the activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Qualitative comparison of the baselines. Qualitative comparison of the segmentation generator of ACGPN and VITON-HD. The clothing-agnostic segmentation map used by each model is also reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Effects of ALIAS normalization. The orange colored areas in the enlarged images indicate the misaligned regions. LPIPS scores according to the degree of misalignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Segmentation Generator. k ?k Conv (x) denotes a convolutional layer where the kernel size is k and the output channel is x. Also, ConvBlk (x) denotes a block, which consists of two series of 3?3 convolutional layer, instance normalization, and ReLU activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>4x4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Geometric Matching Module. k ? k ?2 Conv (x)denotes a convolutional layer where the kernel size is k, the stride is 2, and the output channel is x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>ALIAS Generator. The segmentation map S and the misalignment mask M misalign are passed to the generator through the proposed ALIAS ResBlks. the TPS transformation parameters ?. The overall objective function is written as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>User study results. We compare our model with CP-VTON<ref type="bibr" target="#b30">[31]</ref> and ACGPN<ref type="bibr" target="#b34">[35]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Failure cases of VITON-HD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Qualitative comparison of the baselines (256?192). Qualitative comparison of the baselines (512?384).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Qualitative comparison of the baselines (1024?768).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Qualitative comparison of the baselines (1024?768). Additional qualitative results of VITON-HD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 21 :</head><label>21</label><figDesc>Sample 1 of VITON-HD. (Left) The synthetic image. (Right) The reference image and the target clothing item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Sample 2 of VITON-HD. (Left) The synthetic image. (Right) The reference image and the target clothing item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 23 :</head><label>23</label><figDesc>Sample 3 of VITON-HD. (Left) The synthetic image. (Right) The reference image and the target clothing item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 5illustrates the workflow of the ALIAS normalization. We first obtain M align and M misalign from Eq. (3) and Eq. (4). We define the modified version of? as? div , wher? S c in? separates into M align and M misalign . ALIAS normalization standardizes the regions of M misalign and the other regions in h i separately, and then modulates the standardized activation using affine transformation parameters256 ? 192 512 ? 384 1024 ? 768 SSIM ? LPIPS ? FID ? SSIM ? LPIPS ? FID ? SSIM ? LPIPS ? FID ? CP-VTON 0.739 0.159 56.23 0.791 0.141 31.96 0.786 0.158 43.28 ACGPN 0.842 0.064 26.45 0.863 0.067 15.22 0.856 0.102 43.39</figDesc><table><row><cell>VITON-HD*</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.893 0.054 12.47</cell></row><row><cell>VITON-HD</cell><cell cols="7">0.844 0.062 27.83 0.870 0.052 14.05 0.895 0.053 11.74</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments.</head><p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420) and Seoul R&amp;BD Program (CD200024) through the Seoul Business Agency (SBA) funded by the Seoul Metropolitan Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Highresolution daytime translation without domain labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Ivan Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Solovev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Korzhenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Silvestrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Nikolenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sterkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7488" to="7497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?mie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards multi-pose guided virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjiang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9026" to="9035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fw-gan: Flow-navigated warping gan for video virtual try-on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance-level human parsing via part grouping network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drape: Dressing any person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loretta</forename><surname>Peng Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clothflow: A flow-based model for clothed person generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Viton: An image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The conditional analogy gan: Swapping fashion articles on people images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Jetchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Bergmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision workshop (ICCVW)</title>
		<meeting>of the IEEE international conference on computer vision workshop (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2287" to="2292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swapping autoencoder for deep image manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouyingcheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7365" to="7375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clothcap: Seamless 4d clothing capture and retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Pujades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonny</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Virtual fitting by single-shot body shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Sugita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Perbet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Nishiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Body Scanning Technologies</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning residual images for face attribute manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4030" to="4038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">stance normalization: The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward characteristicpreserving image-based virtual try-on network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards photo-realistic virtual try-on by adaptively generating-preserving image content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vtnfp: An image-based virtual try-on network with body and clothing feature preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE international conference on computer vision (ICCV)</title>
		<meeting>of the IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5104" to="5113" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
