<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Self-Optimal-Transport Feature Transform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Shalam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Korman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Self-Optimal-Transport Feature Transform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Self-Optimal-Transport (SOT) feature transform is designed to upgrade the set of features of a data instance to facilitate downstream matching or grouping related tasks. The transformed set encodes a rich representation of high order relations between the instance features. Distances between transformed features capture their direct original similarity and their third party 'agreement' regarding similarity to other features in the set. A particular min-cost-max-flow fractional matching problem, whose entropy regularized version can be approximated by an optimal transport (OT) optimization, results in our transductive transform which is efficient, differentiable, equivariant, parameterless and probabilistically interpretable. Empirically, the transform is highly effective and flexible in its use, consistently improving networks it is inserted into, in a variety of tasks and training schemes. We demonstrate its merits through the problem of unsupervised clustering and its efficiency and wide applicability for few-shot-classification, with state-of-the-art results, and large-scale person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Techniques</head><p>Set-to-set or set-to-feature functions Our method can clearly be categorized along with recent techniques that act jointly on a set of items (typically features) to output an updated set (or a single feature), which are typically used for downstream inference tasks on the items individually, or as a set. The pioneering Deep-Sets [44] formalized fundamental requirements from architectures that process sets. Point-Net [27] presented an influential design that learns local and global features on 3D point-clouds, while Maron et.al. [25] study layer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we reassess the design and functionality of features for instancespecif ic problems. In such problems, typically, features computed at test time are mainly compared relative to one another, and less so to the features seen at training time. For such problems the standard practice of learning a generic feature extractor during training and applying it at test time might be suboptimal.</p><p>We aim at finding training and inference schemes that take into account these considerations, being able to exploit large corpuses of training data to learn features that can easily adapt, or be relevant, to the test time task. Our approach to doing so will be in the form of a feature transform that jointly reembeds the set of features of an instance in a way that resembles how recently popular self-attention mechanisms and Transformers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16]</ref> re-embed sets of features.</p><p>Being at the low-to-mid-level of most relevant architectures, advances in such feature re-embeddings have a direct impact and wide applicability in instancespecific problems such as few-shot classification <ref type="bibr" target="#b29">[30]</ref>, clustering <ref type="bibr" target="#b36">[37]</ref>, patch matching <ref type="bibr" target="#b18">[19]</ref> and person re-identification <ref type="bibr" target="#b42">[43]</ref>, to name but a few.</p><p>The general idea of the Self-Optimal-Transport (SOT) feature transform that we propose is depicted and explained in <ref type="figure">Fig. 1</ref>, as part of the general design of networks that work on sets which we illustrate in <ref type="figure">Fig. 2</ref>. <ref type="figure">Fig. 1</ref>: The SOT transform: Its input is a set of n d-dimensional features (each shown as a horizontal gray rectangle, and as a colored point in the input embedding space where color depicts class label or equivalent). Processing is as follows: The unit length (normalized) features are arranged in an n ? d matrix for computing a pairwise n ? n cosine similarity matrix S. Then, the transport-plan matrix W (given a specific OT instance that depends on S) is computed using several Sinkhorn <ref type="bibr" target="#b6">[7]</ref> iterations. Finally, the transformed output features are basically the rows of the matrix W . As we claim and observe in real results, the features are re-embedded in a way that is consistently superior for downstream grouping and matching tasks (observed the better formation of the embedded points, e.g. towards applying a linear classifier or an off-the-shelf clustering procedure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview</head><p>We are given an instance of some inference problem, in the form of a set of n items {x i } n i=1 , represented as vectors in R D , for a fixed dimension D. A generic neural-network ( <ref type="figure">Fig. 2</ref> Left) typically uses a feature embedding (extractor) F :</p><formula xml:id="formula_0">R D ? R d (with d ? D)</formula><p>, which is applied independently on each input item, to obtain a set of features V = {v i } n i=1 = {F (x i )} n i=1 . The features V might be of high quality (concise, unique, descriptive), but are limited in representation since they are extracted based on knowledge acquired for similar examples at train time, with no context of the test time instance that they are part of.</p><p>We adapt a rather simple framework ( <ref type="figure">Fig. 2</ref> Right) in which some transform acts on the entire set of instance features. The idea is to jointly process the set of features to output an updated set (one for each input feature), that re-embeds each feature in light of the joint statistics of the entire instance. The proposed features transform can be seen as a special case of an attention mechanism <ref type="bibr" target="#b28">[29]</ref> specialized to features of instance-specific tasks, with required adaptations. Techniques developed here borrow from and might lend to those used in set-toset <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b24">25]</ref>, self-attention <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> and transformer <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref> architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>We propose a parameter-less transform T , which can be used as a drop-in addition that can convert a conventional network to an instance-aware one (e.g. from <ref type="figure">Fig. 2</ref> Left to Right). We propose an optimal-transport based feature transform which is shown to have the following attractive set of qualities. (i) efficiency: having real-time inference; (ii) differentiability: allowing end-to-end training of the entire 'embedding-transform-inference' pipeline of <ref type="figure">Fig. 2</ref> Right; (iii) equivariance: ensuring that the embedding works coherently under any order of the <ref type="figure">Fig. 2</ref>: Generic designs of networks that act on sets of inputs. These cover relevant architectures, e.g. for few-shot-classification and clustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately goes through a common feature extractor F . (ii) The set of extracted features is the input to a downstream task processing module G. ; Right: A more general structure in which the extracted features undergo a joint processing by a transform T . Our SOT transform (as well as other attention mechanisms) is of this type and its high-level design (within the 'green' module) is detailed in <ref type="figure">Fig. 1</ref>. input items; (iv) capturing relative similarity: The comparison of embedded vectors will include both direct and indirect (third-party) similarity information between the input features; (v) probabilistic interpretation: each embedded feature will encode its distribution of similarities to all other features, by conforming to a doubly-stochastic constraint; (vi) instance-aware dimensionality: embedding dimension (capacity) is adaptive to input size (complexity).</p><p>We provide a detailed analysis of our method and show its flexibility and ease of application to a wide variety of tasks, by incorporating it in leading methods of each kind. A controlled experiment on unsupervised clustering is used to verify its performance, with a detailed analysis. For few-shot-classification we perform an extensive comparison to existing work on several benchmarks, showing that SOT achieves new state-of-art results. Finally, we show that SOT is easily applicable to large-scale benchmarks by using the person re-identification task, for which it consistently improves state-of-art networks that it is incorporated into. designs that approximate equivariant and invariant functions. Unlike the proposed SOT transform, the joint processing in these methods is very limited, amounting to (Siamese) weight-sharing between separate processes and simple joint aggregations like average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><p>The introduction of Relational Networks <ref type="bibr" target="#b31">[32]</ref> and transformers <ref type="bibr" target="#b37">[38]</ref> and their initial applications in vision models <ref type="bibr" target="#b28">[29]</ref> have lead to a surge of following successful works <ref type="bibr" target="#b15">[16]</ref>, many of which are dedicated to few-shot-learning, such as ReNet <ref type="bibr" target="#b14">[15]</ref>, DeepEMD <ref type="bibr" target="#b44">[45]</ref> and FEAT <ref type="bibr" target="#b41">[42]</ref>. Different from these methods, SOT is parameterless, and hence can work at test-time on any pre-trained network. In addition, SOT is the only method that provides an explicit probabilistic global interpretation of the instance data.</p><p>Optimal Transport Optimal transport (OT) problems are tightly related to measuring and calculating distances between distributions or sets of features. In <ref type="bibr" target="#b6">[7]</ref> Cuturi popularized the Sinkhorn algorithm which is a simple, differentiable and fast approximation of entropy-regularized OT problems. The Set transformer <ref type="bibr" target="#b21">[22]</ref> uses an OT-based clustering algorithm, SuperGlue <ref type="bibr" target="#b32">[33]</ref> uses OT in an end-toend manner for feature-point matching, and many state-of-the-art methods in few-shot learning, which we review next, have adopted the Sinkhorn algorithm to model relations between features and class representations. The differentiability and efficiency of regularized OT solvers has recently been shown useful in related domains, to derive a differentiable 'top-k' operator <ref type="bibr" target="#b40">[41]</ref> or for style transfer applications, by viewing styles as a distributions between which distances are approximated <ref type="bibr" target="#b17">[18]</ref>. In this work we focus on self applications of OT, which enables concise modelings of the relative similarities within a set of items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-Shot-Classification (FSC)</head><p>Few-Shot-Classification <ref type="bibr" target="#b38">[39]</ref> is a branch of few-shot-learning in which a classifier needs to learn to recognize classes unseen given a limited number of labeled examples. A FSC task is a self-contained instance that includes both support (labeled) and query (unlabeled) items, hence is a clear instance-specific setup which SOT can handle.</p><p>Some leading FSC approaches follow the meta-learning (or "learn-to-learn") principle in which the training data is split into tasks (or episodes) mimicking the test time tasks to which the learner is required to generalize. The celebrated MAML <ref type="bibr" target="#b9">[10]</ref> "learns to fine-tune" by learning a network initialization from which it can adapt to a novel set of classes with very few gradient update steps on the labeled examples. In ProtoNet <ref type="bibr" target="#b33">[34]</ref>, a learner is meta-trained to predict query feature classes, based on distances from support (labeled) class-prototypes in the embedding space. The trainable version of SOT is a meta-learning algorithm, but unlike the above, it is transductive (see ahead) and exploits the task items as a set, while directly assessing the relative similarity relations between its items.</p><p>Subsequent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> have questioned the benefits of meta-learning, advocating the standard transfer learning procedure of fine-tuning pre-trained networks. In particular, they demonstrate the advantages of using larger and more powerful feature-encoding architectures, as well as the employment of transductive inference, which fully exploits the data of the inference task, including unlabeled images. As mentioned, SOT is a purely transductive method, but it is significantly more flexible in its assumptions, since the transform is based on a general probabilistic grouping action. It does not make any assumptions on (nor does it need to know) the number of classes and the number of items per class in an instance.</p><p>More recently, attention mechanisms were shown to be effective for FSC. We have reviewed some relevant works of this line in the previous section.</p><p>Finally, a large number of works have adopted the Sinkhorn Algorithm <ref type="bibr" target="#b6">[7]</ref> as a parameterless unsupervised classifier that computes fractional matchings between query embeddings and class centers. Many leading FSC works use this approach, including Laplacian-Shot <ref type="bibr" target="#b49">[50]</ref>, CentroidNet <ref type="bibr" target="#b12">[13]</ref> and PT-MAP <ref type="bibr" target="#b11">[12]</ref>. The current state-of-the-art is set by the recent Sill-Net <ref type="bibr" target="#b45">[46]</ref>, which augments training samples with illumination features that are separated from the images in feature space and by PT-MAP-sf <ref type="bibr" target="#b5">[6]</ref>, who propose a DCT-based feature embedding network, encoding detailed frequency-domain information that complements the standard spatial domain features. Both methods are based on PT-MAP <ref type="bibr" target="#b11">[12]</ref>. SOT uses Sinkhorn to solve an entirely different OT problem -that of matching the set of features to itself, rather than against class representations. Nevertheless, SOT can be incorporated into these methods, immediately after their feature extraction stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Unsupervised Clustering and Person Re-Identification (Re-ID)</head><p>These domains are not at the focus of this work therefore we only briefly give some useful pointers for the sake of brevity.</p><p>Unsupervised image clustering is an active area of research, with standardised evaluation protocols (from Cifar-10 <ref type="bibr" target="#b19">[20]</ref> to different subsets of ImageNet <ref type="bibr" target="#b7">[8]</ref>). Prominent works in this area include Deep Adaptive Clustering (DAC) <ref type="bibr" target="#b3">[4]</ref>, Invariant Information Clustering (IIC) <ref type="bibr" target="#b13">[14]</ref> and SCAN <ref type="bibr" target="#b36">[37]</ref>. Clustering has recently gained popularity as a means for self-supervision in feature learning, showing excellent results on unsupervised image classification. See for example Deep-Cluster <ref type="bibr" target="#b1">[2]</ref> and SWAV <ref type="bibr" target="#b2">[3]</ref>. Clustering is a clear case instance specific problem, since most information is relative and unrelated directly to other training data. Our transform can hence be used to upgrade the feature representation quality.</p><p>We chose the Re-ID application as another instance-specific problem, which from our point of view differs from the others considered in two main aspects which we find attractive: (i) The tasks are of larger scale -querying thousands of identities against a target set of (tens of) thousands. (ii) The data is much more real-world compared to the carefully curated classification and clustering tasks. See <ref type="bibr" target="#b42">[43]</ref> for an excellent recent and comprehensive survey on the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Assume we are given a task which consists of an inference problem over a set of n items {x i } n i=1 , where each of the items belongs to a space of input items ? ? R D . The inference task can be modeled as f ? ({x i } n i=1 ), using a learned function f ? , which acts on the set of input items and is parameterized by a set of parameters ?.</p><p>Typically, such functions combine an initial feature extraction stage that is applied independently to each input item, with a subsequent stage of (separate or joint) processing of the feature vectors (see <ref type="figure">Fig. 2</ref> Left or Right, respectively).</p><p>That is, the function f ? takes the form</p><formula xml:id="formula_1">f ? ({x i } n i=1 ) = G ? ({F ? (x i )} n i=1 ),</formula><p>where F ? is the feature extractor (or embedding network) and G ? is the task inference function, parameterized by ? and ? respectively, where ? = ? ? ?.</p><p>The feature embedding F : R D ? R d , usually in the form of a neural-network (with d ? D), could be either pre-trained, or trained in the context of the task function f , along with the inference function G.</p><p>For</p><formula xml:id="formula_2">an input {x i } n i=1 , let us define the set of features {v i } n i=1 = {F (x i )} n i=1 .</formula><p>In the following, we consider these sets of input vectors and features as real-valued row-stacked matrices X ? R n?D and V ? R n?d .</p><p>We suggest a novel re-embedding of the feature set V, using a transform that we denote by T , in order to obtain a new set of features W = T (V), where W ? R n?n . The new feature set W has an explicit probabilistic interpretation, which is specifically suited for tasks related to classification, matching or grouping of items in the input set X . In particular, W will be a symmetric, doubly-stochastic matrix, where the entry w ij (for i ? = j) gives the probability that items x i and x j belong to the same class or cluster.</p><p>The proposed transform T : R n?d ? R n?n (see <ref type="figure">Fig. 1</ref>) acts on the original feature set V as follows. It begins by computing the squared Euclidean pairwise distances matrix D, namely, d ij = ||v i ? v j || 2 , which can be computed efficiently as</p><formula xml:id="formula_3">d ij = 2(1 ? cos(v i , v j )) = 2(1 ? v i ? v T j )</formula><p>, assuming that the rows of V are unit normalized. Or in a compact form, D = 2(1 ? S), where 1 is the all ones n ? n matrix and S = V ? V T is the cosine similarity matrix of V.</p><p>W will be computed as the optimal transport (OT) plan matrix between the n-dimensional all-ones vector 1 n and itself, under the cost matrix D ? , which is the distance matrix D with a very (infinitely) large scalar replacing each of the entries on its diagonal (which were all zero). Explicitly, let D ? = D + ?I, where ? is a very (infinitely) large constant and I is an n ? n identity matrix.</p><p>W is defined to be the doubly-stochastic matrix 1 that is the minimizer of the functional W = arg min</p><formula xml:id="formula_4">W?Bn ?D ? , W?<label>(1)</label></formula><p>where B n is the set (known as the Birkhoff polytope) of n ? n doubly-stochastic matrices and ??, ?? stands for the Frobenius (standard) dot-product.</p><p>This objective can be minimized using simplex or interior point methods with complexity ?(n 3 log n). In practice, we use the highly efficient Sinkhorn-Knopp method <ref type="bibr" target="#b6">[7]</ref>, which is an iterative scheme that optimizes an entropy-regularized version of the problem, where each iteration takes ?(n 2 ). Namely:</p><formula xml:id="formula_5">W = arg min W?Bn ?D ? , W? ? 1 ? h(W) (2) where h(W) = ? i,j w ij log(w ij )</formula><p>is the Shannon entropy of W and ? is the entropy regularization parameter.</p><p>The transport-plan matrix W that is the minimizer of Eq. <ref type="formula">(2)</ref> is the result of our transform, i.e. W = T (V) and each of its rows is the re-embedding of each of the corresponding features (rows) in V. Recall that W is doubly-stochastic and note that it is symmetric 2 . We next explain its probabilistic interpretation.</p><p>The optimization problem in Eq. (1) can be written more explicitly as follows:</p><formula xml:id="formula_6">min W ?D ? , W? s.t. W ? 1 n = W T ? 1 n = 1 n<label>(3)</label></formula><p>which can be seen to be the same as:</p><formula xml:id="formula_7">min W ?D, W? s.t. W ? 1 n = W T ? 1 n = 1 n w ii = 0 for i = 1, . . . n<label>(4)</label></formula><p>since the use of the infinite weights on the diagonal of D ? is equivalent to using the original D with a constraint of zeros along the diagonal of W.</p><p>The optimization problem in Eq. (4) is in fact a fractional matching instance between the set of n original features and itself. It can be posed as a bipartitegraph min-cost max-flow instance. The graph has n nodes on each side, representing the original features {v i } n i=1 (the rows of V). Across the two sides, the cost of the edge (v i , v j ) is the distance d ij and the edges of the type (v i , v i ) have a cost of infinity (or can simply be removed). Each 'left' node is connected to a 'source' node by an edge of cost 0 and similarly each 'right' node is connected to a 'target' (sink) node by an edge of cost 0. All edges in the graph have a capacity of 1 and the goal is to find an optimal fractional self matching, by finding a min-cost max-flow from source to sink. Note that the maximum flow can easily be seen to be n, but a min-cost flow is sought among the max-flows.</p><p>In this set-to-itself matching view, each vector v i is fractionally matched to the set of all other vectors V ? {v i } based on the pairwise distances, but importantly taking into account the fractional matches of the rest of the vectors in order to satisfy the double-stochasticity constraint 3 . Therefore, the ith transformed (re-embedded) feature w i (ith row of W) is a distribution (non-negative entries, summing to 1), where w ii = 0 and w ij is the relative belief that features i and j belong to the same 'class'. Our final set of features W is obtained by replacing the diagonal entries from 0s to 1s, namely W = W + I, where I is the n ? n identity matrix. Please refer to <ref type="figure" target="#fig_0">Fig. 3</ref> for a close look at the application of SOT to a toy clustering problem, where we demonstrate visually the improved embedding obtained through examining the pairwise distances before and after the transform. We can now point out some important properties of this new embedding W:</p><p>Direct and Indirect similarity encoding: Each embedded feature encodes its distribution of similarities to all other features. An important property of our embedding is that the comparison of the embedded vectors w i and w j includes both direct and indirect information about the similarity between the features. Please refer to <ref type="figure">Fig. 4</ref> for a detailed explanation of this property. If we look at the different coordinates k of the absolute difference vector a = |w i ? w j |, SOT captures (i) direct similarity: For k which is either i or j, it holds that a k = 1 ? w ij = 1 ? w ji 4 . This amount measures how high (i.e.close to 1) is the mutual belief of features i and j about one another. (ii) indirect (3rd-party) similarity: For k / ? {i, j}, we have a k = |w ik ? w jk |, which is a comparison of the beliefs of features i and j regarding the (third-party) feature k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameterless-ness:</head><p>Our proposed transform is parameterless, giving it the flexibility to be used in other pipelines, directly over different kinds of embeddings, without the harsh requirement of retraining the entire pipeline 5 . <ref type="figure">Fig. 4</ref>: The (symmetric) embedding matrix W and the absolute difference between its ith and jth rows: We examine the vector |w i ? w j |: (i) Its ith and jth coordinates equal |1 ? w ij | = |1 ? w ji |, giving the direct similarity between the original features, since this amount (in green) is greater when w ij and w ji (the mutual beliefs) are high (closer to 1). ; (ii) Its kth coordinate (for any k / ? {i, j}) gives |w ik ? w jk | which is an indirect (third-party) comparison between the original features through the kth feature. Similarity (in yellow) is stronger when features i and j have a similar belief regarding feature k, i.e. w ik and w jk are close.</p><p>Differentiability: Due to the differentiability of Cuturi's <ref type="bibr" target="#b6">[7]</ref> version of Sinkhorn, back-propagating through the SOT can be done naturally, hence it is possible to (re-)train the hosting network to adapt to the SOT, if desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivariance:</head><p>The embedding works coherently with respect to any change of order of the input items (features). This can be shown by construction, since min-cost max-flow solvers as well as the Sinkhorn OT solver are equivariant with respect to permutations of their inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explainability:</head><p>The non-parametric nature gives SOT an advantage over other set-to-set methods such as transformers in that its output is interpretable (e.g. by visually inspecting the transport-plan matrix W), with a clear probabilistic characterization of the relations it had found.</p><p>Task-Aware Dimensionality: SOT has the unique property that the dimension of the embedded feature depends on (equals) the number of features. On the one hand, this is a desired property, since it is only natural that the feature dimensionality (capacity) depends on the complexity of the task, which typically grows with the number of features (think of the inter-relations which are more complex to model). On the other hand, it might impose a problem in situations in which the downstream calculation that follows the feature embedding expects a fixed input size, for example a pre-trained non-convolutional layer. Nevertheless, in many situations the downstream computation has the flexibility to work with varying input dimensions. Also, in most benchmarks the instance set sizes are fixed, allowing for a single setting of sizes to work throughout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We consider three different applications to evaluate the performance of our method. For unsupervised clustering we designed a specialized synthetic data set with the goal of enabling controlled experiments over a wide range of difficulties, which are determined by data dimensionality and in-cluster spread.</p><p>For few-shot classification we use the standard benchmarks in the literature. The MiniImagenet <ref type="bibr" target="#b38">[39]</ref> dataset is a subset of Imagenet <ref type="bibr" target="#b30">[31]</ref> that contains 100 classes and 600 images of size 84x84 per class. We follow the standard setup of using 64 classes for training and 16 and 20 novel classes for validation and testing. The CIFAR-FS <ref type="bibr" target="#b0">[1]</ref> dataset includes 100 classes with 600 images of size 32 ? 32 per-class. We used the same splits as in MiniImagenet for this dataset. The CUB <ref type="bibr" target="#b39">[40]</ref> dataset includes 200 classes of bird species and has 11,788 images of size 84 ? 84 pixels in total. We followed the split suggested in <ref type="bibr" target="#b10">[11]</ref> into 100 base classes, 50 validation classes and 50 novel classes.</p><p>For person re-identification (ReID) we use two common large-scale datasets. The Market-1501 <ref type="bibr" target="#b46">[47]</ref> and CUHK03 <ref type="bibr" target="#b22">[23]</ref> dataset consists of 1,501 and 1,467 identities and a total of 32,668 and 14,097 images taken from 6 cameras. We use the validation and test sets according to the splits in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training:</head><p>We pre-trained ProtoNet <ref type="bibr" target="#b33">[34]</ref> with a 4-layer Convolution network adapting the procedures of <ref type="bibr" target="#b33">[34]</ref> for training both with and without SOT, training on a 5-way (5/1)-shot 15-query task, using ADAM <ref type="bibr" target="#b16">[17]</ref> with learning rate 0.01 and step size of 20 over 100 episodes (tasks) per epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning:</head><p>We perform fine-tuning on two types of backbone residual networks -a resnet-12 as used in <ref type="bibr" target="#b41">[42]</ref> and a WRN-28-10 as used in <ref type="bibr" target="#b23">[24]</ref>. For Pro-toNet <ref type="bibr" target="#b33">[34]</ref> and ProtoNet-SOT, we fine-tune the base network with parameters taken from <ref type="bibr" target="#b41">[42]</ref>. For PTMAP-SOT, we use meta-training with batches of a single 10-way 5-shot 15-query task per batch. We use ADAM with learning rate 5e ? 5 that decreases with step size 10 for 25 epochs. We train the WRN-28-10 and the resnet-12 backbones for 800 and 100 episodes respectively per epoch. Hyper-parameters: SOT has two hyper-parameters which were chosen through cross-validation and were kept fixed for each of the applications over all datasets. (i) The number of Sinkhorn iterations for computing the optimal transport plan was fixed to 10. (ii) The entropy regularization parameter ? (Eq. (3)) was set to 0.1 for clustering and few-shot-learning experiments and to 1.0 for the ReID experiments. We further ablate these in the supplementaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Clustering on the Sphere</head><p>We first demonstrate the effectiveness of SOT using a controlled synthetically generated clustering experiment, with k = 10 cluster centers that are distributed uniformly at random on a d-dimensional unit-sphere, and 20 points per cluster (200 in total) that are perturbed around the cluster centers by Gaussian noise of increasing standard deviation, of up to 0.75, followed by a re-projection back to the sphere by dividing each vector by its L 2 magnitude. We also apply dimensionality reduction with PCA to d = 50, for dimensions above 50.</p><p>We performed the experiment over a logarithmic 2D grid of combinations of data dimensionalities d in the range <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">1234]</ref> and Gaussian in-cluster noise STD in the range [0.1, 0.75]. Refer to <ref type="figure" target="#fig_5">Fig. 9</ref> (i) for a visualization of the data generation process.  Each point is represented by its d-dimensional euclidean coordinates vector, where the baseline clustering is obtained by running k-means on these location features. In addition, we run k-means on the set of features that has undergone SOT. Hence, the benefits of the transform (embedding) are measured indirectly through the accuracy 6 achieved by running k-means on the embedded vs. original vectors. Evaluation results are reported in <ref type="figure" target="#fig_5">Fig. 9</ref> (ii) as averages over 10 runs, by plotting accuracy vs. dimensionality (for different noise STDs) and accuracy vs noise STDs (for different dimensionalities). The results show (i) general accuracy gains and robustness to wide ranges of data dimensionality (ii) the ability of SOT to find meaningful representations that enable clustering quality to degrade gracefully with the increase in cluster noise level. Note that the levels of noise are rather high, as they are relative to a unit radius sphere (a 3-dimensional example is shown at the top of the figure). We provide further details on this experiment in the supplementaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Few-Shot Classification (FSC)</head><p>Our main experiment is a comprehensive evaluation on the standard few-shot classification benchmarks MiniImagenet <ref type="bibr" target="#b38">[39]</ref>, CIFAR-FS <ref type="bibr" target="#b0">[1]</ref>, and CUB <ref type="bibr" target="#b39">[40]</ref>, with detailed results in <ref type="table">Tables 1 and 2</ref> <ref type="table">(Table 1)</ref> we report on both versions "SOT p " and "SOT t " over a range of backbone architectures, while for the smaller datasets CIFAR-FS and CUB <ref type="table">(Table 2)</ref> we focus on the 'drop-in' version "SOT p " and only the strongest wrn-28-10 architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. For MiniImagenet</head><p>One goal here is to show that we can achieve new state-of-the-art FSC results, when we build on current state-of-the-art. But more importantly, we demonstrate the flexibility and simplicity of applying SOT in this setup, with improvements in the entire range of testing, including: (i) when building on different 'hosting' methods; (ii) when working above different feature embeddings of different  <ref type="table">Table 1</ref>: Few-Shot Classification (FSC) accuracy on MiniImagenet <ref type="bibr" target="#b38">[39]</ref>. The improvements introduced by the variants of SOT (percentages in brackets) are in comparison with each respective baseline hosting method. Bold and underline notations highlight best and second best results per backbone. (*) = from <ref type="bibr" target="#b4">[5]</ref> ; (&amp;) = from <ref type="bibr" target="#b49">[50]</ref> ; ($) = from the method's paper itself ; (#) = our implementation ; complexity backbones; and (iii) whether retraining the hosting network or just dropping-in SOT and performing standard inference.</p><p>To evaluate the performance of the proposed SOT, we applied it to previous FSC methods including the very recent state-of-the-art (PT-MAP <ref type="bibr" target="#b11">[12]</ref>, Sill-NET <ref type="bibr" target="#b45">[46]</ref> and PT-MAP-SF <ref type="bibr" target="#b5">[6]</ref>) as well as a to more conventional methods like the popular ProtoNet <ref type="bibr" target="#b33">[34]</ref>. The detailed results are presented in <ref type="table">Tables 1 and 2)</ref> for the different datasets. Note that SOT is by nature a transductive method 7 , hence we marked its results as so, regardless of whether the hosting network is transductive or not. In the following, we discuss the two modes in which our transform can be used in existing FSC methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FSC benchmark CIFAR-FS [1]</head><p>CUB [40] method 5way-1shot 5way-5shot 5way-1shot 5way-5shot PTMAP($) <ref type="bibr" target="#b11">[12]</ref> 87.69 90.68 91.55 93.99 Sill-Net($) <ref type="bibr" target="#b45">[46]</ref> 87.73 91.09 94.73 96.28 PTMAP-SF($) <ref type="bibr" target="#b5">[6]</ref> 89  <ref type="table">Table 2</ref>: Few-Shot Classification (FSC) accuracy on CIFAR-FS <ref type="bibr" target="#b0">[1]</ref> and CUB <ref type="bibr" target="#b39">[40]</ref>. <ref type="table">Tables  1 and 2</ref>). Recall that the proposed transform is non-parametric. As such, we can simply apply it to a trained network at inference, without the need to re-train. This basic 'drop-in' use of SOT consistently, and in many cases also significantly, improved the performance of the tested methods, including stateof-the-art, across all benchmarks and backbones. SOT p gave improvements of around 3.5% and 1.5% on 1 and 5 shot MiniImagenet tasks. This improvement without re-training the embedding backbone network shows SOT's effectiveness in capturing meaningful relationships between features in a very general sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOT insertion without network retraining (notated by SOT p in</head><p>SOT insertion with network retraining (notated by SOT t in <ref type="table">Table 1</ref>). Due to its differentiability property, the proposed method can be applied while training and hence we expect an adaptation of the hosting network's parameters to the presence of the transform with a potential for improvement. To evaluate this mode, we focused on the MiniImagenet benchmark <ref type="bibr" target="#b38">[39]</ref>, specifically on the same configurations that we used without re-training, to enable a direct comparison. The results in <ref type="table">Table 1</ref> show additional improvements in almost every method. SOT t gave improvements of around 5% and 3% on 1 and 5 shot MiniImagenet tasks, further improving on the pre-trained counterpart. This result indicates the effectiveness of training with SOT in an end-to-end fashion.</p><p>Ablations Within the context of few-shot learning on MiniImagenet, we performed several ablation studies. In <ref type="table">Table 1</ref>, the networks 'PTMAP-COSINE' and 'PTMAP-SOFTMAX' stand for the obvious baseline attempts (found to be unsuccessful) that work in the line of our approach, without the specialized OTbased transform. In the former, we take the output features to be the rows of the (un-normalized) matrix S (rather than those of W) and in the latter we also normalize its rows using soft-max. In the supplementaries we ablate on SOT's two parameters -the number of Sinkhorn iterations and the entropy term ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Person re-Identification (Re-ID)</head><p>In this section, we explore the possibility of using SOT on large-scale datasets by considering the Person re-Identification task. Given a set of query images and a large set of gallery images, the task is to rank the similarities of each single query against the gallery. This is done by computing specialized image features among which similarities are based on Euclidean distances. SOT is applied to such pre-  <ref type="table">Table 3</ref>: Re-ID results on CUHK03 <ref type="bibr" target="#b22">[23]</ref> and Market-1501 <ref type="bibr" target="#b46">[47]</ref> computed image features, refining them with the strong relative information that it is able to capture by applying it on the union of all query and gallery features. We adapted a pre-trained standard resnet-50 architecture <ref type="bibr" target="#b48">[49]</ref> and the popular TopDBNet <ref type="bibr" target="#b27">[28]</ref>, which we tested on the large-scale ReID benchmarks CUHK03 <ref type="bibr" target="#b22">[23]</ref> (on the 'detected' version and similar results on the 'labeled' version in the supplementaries) and Market-1501 <ref type="bibr" target="#b46">[47]</ref>, with and without the re-ranking <ref type="bibr" target="#b47">[48]</ref> procedure. For evaluation, we followed their conventions and compare results using the mAP (mean Average Precision) and Rank-1 metrics.</p><p>The results in <ref type="table">Table 3</ref> show a consistent benefit in using SOT within the different networks. For CUHK03, the results improved by a large margin of +6.8% in mAP for the best configuration. These results demonstrate that the proposed SOT scales well to large-scale problems (with number of features in the thousands) and is attractive for a variety of applications. ReID is not the main focus of this work, hence, we did not re-train the hosting networks with SOT included. Further research is required to measure the possible effects of doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions, Limitations and Future Work</head><p>In this paper, we explored the idea of utilizing global information of features, for instance-specific problems such as clustering, few-shot learning, and person re-identification. We proposed a novel module: the Self-Optimal-Transport (SOT) -a features transform that is non-parametric, differentiable and which can capture high-level relationships between data points in problems of this nature. The proposed method outperforms state-of-the-art networks on popular few-shot classification benchmarks and shows consistent improvements on tested ReID benchmarks. Based on these promising results, we believe that exploring its full potential can lead to improvements in a variety of fields and open new possibilities.</p><p>In future work, we plan to address some current limitations. (i) Regarding the output dimensionality of the embedding, which is dictated by the input set size. We will aim at being able to obtain an arbitrary dimension, for increased usage flexibility; (ii) We plan to investigate the usage of SOT in unsupervised settings, which would be possible by utilizing its informative representation for self-supervision; (iii) It would likely be beneficial to have a variant of SOT in which the transform is enriched with learnable parameters, similar to transformers, to extend its modeling capacity even further; (iv) SOT is purely transductive. We plan to explore non-transductive variants, possibly by comparing each sample separately to the support or gallery sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Sinkhorn iterations</head><p>In <ref type="table" target="#tab_4">Table 4</ref> we ablate the number of normalization iterations in the Sinkhorn-Knopp (SK) <ref type="bibr" target="#b6">[7]</ref> algorithm at test-time. We measured accuracy on the validation set of MiniImagenet <ref type="bibr" target="#b38">[39]</ref>, using ProtoNet-SOT p (which is the non-fine-tuned drop-in version of SOT within ProtoNet <ref type="bibr" target="#b33">[34]</ref>). As was reported in prior works following <ref type="bibr" target="#b6">[7]</ref>, we empirically observe that a very small number of iterations (around 5) provide rapid convergence. We observed similar behavior for other hosting methods, and therefore chose to use a fixed number of 10 iterations throughout the experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 OT entropy regularization parameter ?</head><p>We measured the impact of using different values of the optimal-transport entropy regularization parameter ? (the main parameter of the Sinkhorn algorithm) on a variety of configurations (ways and shots) in Few-Shot-Classification (FSC) on MiniImagenet <ref type="bibr" target="#b38">[39]</ref> in <ref type="figure" target="#fig_3">Fig. 6</ref> as well as on the Person-Re-Identification (RE-ID) experiment on Market-1501 <ref type="bibr" target="#b46">[47]</ref> in <ref type="figure" target="#fig_4">Fig. 7</ref>. In both cases, the ablation was executed on the validation set. For FSC, in <ref type="figure" target="#fig_3">Fig. 6</ref>, the left plot shows that the effect of the choice of ? is similar across tasks with a varying number of ways. The right plot shows the behavior as a function of ? across multiple shot-values, where the optimal value of ? can be seen to have a certain dependence on the number of shots. Recall that we chose to use a fixed value of ? = 0.1, which gives an overall good accuracy trade-off. Note that a further improvement could be achieved by picking the best values for the particular cases. Notice also the log-scale of the x-axes to see that performance is rather stable around the chosen value.</p><p>For Re-ID, in <ref type="figure" target="#fig_4">Fig. 7</ref>, we experiment with a range of ? values on the validation set of the Market-1501 dataset. The results (shown both for mAP and rank-1 measures) reveal a strong resemblance to those of the FSC experiment in <ref type="figure" target="#fig_3">Fig.  6</ref>, however, the optimal choices for ? are slightly higher, which is consistent with the dependence on the shots number, since the re-ID tasks are typically large ones. In this re-ID ablation, we found that a value of ? = 0.25 gives good results across both datasets. We ask to note that in the paper we mistakenly reported that we used ? = 1.0, while in practice all our results were obtained using ? = 0.25. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Unsupervised Clustering -further details</head><p>In this section we provide further details (due to lack of space in main paper) on the experiment on unsupervised clustering on the unit sphere (Exp. 5.1). <ref type="figure">Fig. 8</ref> depicts the average percentile of the in-class and out-class distances computed by the original and the SOT points. Each panel presents the distributions of both types of distances, for instances of a different level of noise. We compute the mean (and plus-minus half-std) percentiles, with respect to the entire set of pair-wise distances, for a fixed level of in-class noise (increasing from top-left to bottom-right panels), for a range of data dimensionality (x-axis). Naturally, the <ref type="figure">Fig. 8</ref>: intra (in) vs. inter (out) class distances before and after SOT. A strong indicative property of an embedding that works on class (cluster) objects is its ability to reduce embedded intra-class (pink shaded) pairwise feature distances compared to inter-class (green shaded) ones. SOT (red lines) consistently improves this separation compared to the baseline (brown lines) -leading to better downstream clustering and classification. x-axis represents data dimensionality; y-axis represents percentiles of pair-wise distances; The four panels present results for the noise standard deviations levels in {0.15, 0.19, 0.23, 0.29} overlap between in-class and between-class distances increases both with dimensionality and with in-class noise. Nevertheless, across almost all sampled points, the situation is far better after SOT application (in red), compared to prior to SOT application (in brown). This can explain, in part, the effectiveness of using SOT in Euclidean-based downstream methods, like k-means and ProtoNet <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Separation between inter-and intra-class features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Evaluation on an extended set of measures</head><p>In <ref type="figure" target="#fig_5">Fig. 9</ref> we evaluate the performance on additional popular clustering metrics, NMI and ARI (in addition to the accuracy measure we reported on in <ref type="figure" target="#fig_2">Figure 5</ref> of the paper). The results shows the same trend as with accuracy, perhaps even stronger for NMI, where SOT significantly improves the clustering performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>(a) orig.(b) S (c) W (d) D (e) DW (f ) SOT (orig. embedding) (cos-similarity) (SOT features) (orig. dists) (SOT dists) (SOT embedding) A close look at the SOT transform as it operates on a 10-way 20-shot supervised clustering task: The input is a set of 200 33-dimensional unit-length feature vectors that are visualized on the plane in (a) using a t-SNE dimension reduction<ref type="bibr" target="#b35">[36]</ref>, where colors refer to the 10 classes. In (b) is the pairwise cosine similarity matrix S, which is linearly related to the Euclidean pairwise distances D shown in (d). Next, in (c) we show the SOT matrix W whose rows (or columns, symmetrically) consist of our new embedding of the features. These 200-dimensional features are shown again on the plane in (f ). Notice the visually apparent improvement in point gathering by class, from (a) to (f ), which can be explained by comparing the matrices D and D W , which are the self-pairwise distances of the original and SOT embedding respectively. Notice the greater contrast in D W between inter-and intra-cluster points. Note, that like in the visualizations ofFig. 1, we show the matrices with row/col order based on the true classes, purely for ease of visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>10 Random cluster centers on the unit sphere, perturbed with increasing noise STD ?. (ii) Clustering accuracy across dimensions d (left) and noise levels ? (right). For each configuration, k-means accuracy is reported when applied with original (solid) and SOT (dashed) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Clustering on the d-dimensional sphere. Left (i): the data generation process (illustrated for the 3D case). Right (ii): detailed k-means accuracy results. The SOT (dashed) features give superior results throughout a majority of the space of settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Ablation study on ? in Few-Shot-Classification (FSC): Considering different 'ways' (left), and different 'shots' (right). See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Ablation study on ? in Person-Re-Identification (Re-ID): Using the validation set of the Market-1501 dataset and considering both mAP and Rank-1 measures. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>A controlled clustering experiment on the d-dimensional sphere -Extension of results from Figure 5 of the paper, with 2 additional measures: It can be seen that the SOT (dashed ---) shows superior results in all aspects (see text for explanations and interpretation). Clustering accuracy across different noise levels ? and dimensions d. Note: For each configuration, SOT is shown by a dashed line while the baseline features are shown by a solid line. For all 3 measures -the higher the better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>4%) 91.12 (+0.5%) 91.90 (+0.4%) 94.63 (+0.7%) Sill-Net-SOTp 87.30 (-0.5%) 91.40 (+0.3%) 94.86 (+0.1%) 96.61 (+0.3%) PTMAP-SF-SOTp 89.94 (+0.6%) 92.83 (+0.8%) 95.80 (+0.4%) 97.12 (+0.4%)</figDesc><table><row><cell></cell><cell>.39</cell><cell>92.08</cell><cell>95.45</cell><cell>96.70</cell></row><row><cell>PTMAP-SOTp</cell><cell>87.37 (-0.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>+6.9%) 80.4 (+6.2%) 88.1 (+2.8%) 94.4 (+0.1%) TopDBNet-rerank-SOTp 87.9 (+0.9%) 88.0 (+1.0%) 94.0 (0.0%) 95.0 (-0.3%)</figDesc><table><row><cell>ReID benchmark</cell><cell cols="2">CUHK03-detected [23]</cell><cell cols="2">Market-1501 [47]</cell></row><row><cell>network</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell>TopDBNet [28]</cell><cell>72.9</cell><cell>75.7</cell><cell>85.7</cell><cell>94.3</cell></row><row><cell>TopDBNet-rerank [28]</cell><cell>87.1</cell><cell>87.1</cell><cell>94.0</cell><cell>95.3</cell></row><row><cell>TopDBNet-SOTp</cell><cell>77.9 (</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sinkhorn</figDesc><table /><note>iterations ablation study: See text for details.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">a square (n ? n) matrix of non-negative real values, each of whose rows and columns sums to 1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The symmetry of W is as a result of the symmetry of D and the double-stochasticity of W.<ref type="bibr" target="#b2">3</ref> The construction constrains the maximum flow to exactly have a total outgoing flow of 1 from each 'left' node and a total incoming flow of 1 from each 'right' node.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note: (i) wii = wjj = 1 ; (ii) wij = wji from the symmetry of W ; (iii) all elements of W are ? 1 and hence the | ? | can be dropped ;<ref type="bibr" target="#b4">5</ref> Retraining is certainly possible, and beneficial in many situations, but not mandatory, as our experiments work quite well without it.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Accuracy is measured by comparison with the optimal permutation of the predicted labels, found by the Hungarian Algorithm<ref type="bibr" target="#b20">[21]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">SOT is transductive in the sense that it needs to jointly process the data, but importantly, unlike other methods it does not gain its benefit in being so from making limiting assumptions about the structure of the instance, like knowing the number of classes, or the number of items per class.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metalearning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joulin</surname></persName>
		</author>
		<idno>abs/2006.09882, 2020. 5</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shiming Xiang, and Chunhong Pan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<idno>2017. 5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint/>
	</monogr>
	<note>Deep adaptive image clustering</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Few-shot learning by integrating spatial and frequency representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05348</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate few-shot classification. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Leveraging the feature distribution in transfer-based few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2006.03806</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Are few-shot learning benchmarks too simple? solving them without task supervision at test-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08605</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relational embedding for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
	</analytic>
	<monogr>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Style transfer by relaxed optimal transport and self-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coherency sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Korman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutationinvariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Charting the right manifold: Manifold mixup for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vineeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On learning sets of symmetric elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Alexandre d&apos;Aspremont, and Julien Mairal. A trainable optimal transport embedding for feature aggregation and its relationship to attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Top-db-net: Top dropblock for activation enhancement in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helio</forename><surname>Pedrini</surname></persName>
		</author>
		<idno>2020. 14</idno>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Superglue: Learning feature matching with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandenhende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Differentiable top-k with optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minshuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno>2020. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Few-shot learning via embedding adaptation with set-to-set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Han-Jia Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>De-Chuan Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sill-net: Feature augmentation with separated illumination representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03539</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Torchreid: A library for deep learning person reidentification in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10093</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Imtiaz Masud Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
