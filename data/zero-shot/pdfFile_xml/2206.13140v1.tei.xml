<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressing Features for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunrong</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
						</author>
						<title level="a" type="main">Compressing Features for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Label noise</term>
					<term>compression</term>
					<term>bias-variance decom- position</term>
					<term>information sorting</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised learning can be viewed as distilling relevant information from input data into feature representations. This process becomes difficult when supervision is noisy as the distilled information might not be relevant. In fact, recent research <ref type="bibr" target="#b0">[1]</ref> shows that networks can easily overfit all labels including those that are corrupted, and hence can hardly generalize to clean datasets. In this paper, we focus on the problem of learning with noisy labels and introduce compression inductive bias to network architectures to alleviate this over-fitting problem. More precisely, we revisit one classical regularization named Dropout [2] and its variant Nested Dropout <ref type="bibr" target="#b2">[3]</ref>. Dropout can serve as a compression constraint for its feature dropping mechanism, while Nested Dropout further learns ordered feature representations w.r.t. feature importance. Moreover, the trained models with compression regularization are further combined with Co-teaching [4] for performance boost.</p><p>Theoretically, we conduct bias-variance decomposition of the objective function under compression regularization. We analyze it for both single model and Co-teaching. This decomposition provides three insights: (i) it shows that over-fitting is indeed an issue in learning with noisy labels; (ii) through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise; (iii) it gives explanations on the performance boost brought by incorporating compression regularization into Co-teaching. Experiments show that our simple approach can have comparable or even better performance than the state-of-the-art methods on benchmarks with real-world label noise including Clothing1M [5] and ANIMAL-10N [6]. Our implementation is available at https://yingyichencyy.github.io/CompressFeatNoisyLabels/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>defective data, including querying commercial search engines <ref type="bibr" target="#b6">[7]</ref>, downloading images from social media <ref type="bibr" target="#b7">[8]</ref>, and various web crawling strategies <ref type="bibr" target="#b8">[9]</ref>. Correspondingly, persistent efforts have been paid in literature to learn with imperfect data, among which learning with noisy labels has always been attached great significance.</p><p>The problem of learning with noisy labels dates back to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. The mainstream methods include (i) training on reweighted samples <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> where samples possibly clean are assigned larger weights than those possibly corrupted; (ii) employing robust loss functions to resist noise <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>; (iii) conducting label correction <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> where original labels are often substituted by the possible clean predictions; (iv) semi-supervised learning methods <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> where samples are first identified as clean or corrupted, and then networks are trained in a semisupervised manner with only the clean labels used. Moreover, label noise itself also plays an important role in understanding the generalization puzzle of deep learning. Empirical experiments in <ref type="bibr" target="#b0">[1]</ref> show that deep neural networks (DNNs), such as AlexNet <ref type="bibr" target="#b25">[26]</ref>, can achieve almost zero training errors on randomly labelled datasets. This analysis demonstrates that the capacities of DNNs are often high enough to memorize the entire noisy training information. Since over-fitting is mainly due to the model capacities, an alternative way to address the problem of training with noisy labels is to introduce explicit compression inductive bias to the model architecture, which is the main focus of this work.</p><p>In this paper, we propose to combat this over-fitting problem by introducing compression inductive bias to networks. More precisely, rather than relying on the prediction of deterministic DNNs, we introduce feature compression to the hidden features in networks via Dropout <ref type="bibr" target="#b1">[2]</ref> and its variant Nested Dropout <ref type="bibr" target="#b2">[3]</ref>. Dropout can be served as a compression constraint for its feature dropping mechanism, while Nested Dropout further learns ordered feature representations w.r.t. feature importance. Leveraging Nested Dropout, we can not only constrain the model capacity, but also filter out the irrelevance while preserves the relevance w.r.t. the learning task. Moreover, compared to Dropout, the information sorting property of Nested Dropout is particularly useful for conducting signal-to-noise separation in the feature level. Note that we may also consider other compression strategies such as principal component analysis (PCA) and kernel principal component analysis (kernel PCA) <ref type="bibr" target="#b26">[27]</ref>, but Dropout/Nested Dropout is a plug-and-play component to networks, thus bringing much convenience to the implementation.</p><p>In addition to Dropout/Nested Dropout's bringing feature-level compression to networks, we find that they are suitable for incorporating into Co-teaching <ref type="bibr" target="#b3">[4]</ref> which is a strong method for learning with noisy labels, for performance boost. Specifically, Co-teaching trains two networks simultaneously where networks update themselves based on the small-loss mini-batch samples selected by their peer. Intuitively, this sample selection mechanism discards samples with possibly wrong labels, and preserves those that are possibly clean. We will show in this paper that the sample selection during the cross-update process together with compression techniques will further prevent networks from over-fitting the noisy labels. On account that good performance of Co-teaching requires the two base networks to be reliable enough, we propose our twostage method:</p><p>? Train two Dropout / Nested Dropout networks separately until convergence; ? Fine-tune these two networks with Co-teaching. Note that Dropout/Nested Dropout is maintained in the second stage for fine-tuning. The efficacy of our two-stage compression approach is validated on benchmark real-world datasets by achieving comparable or even better performance than the state-of-the-art approaches. For example, on Clothing1M <ref type="bibr" target="#b4">[5]</ref>, our method obtains 75.0% in accuracy, which achieves comparable performance to DivideMix <ref type="bibr" target="#b24">[25]</ref> and ELR+ <ref type="bibr" target="#b27">[28]</ref>. On ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>, we achieve 84.5% in accuracy while the state-of-the-art method PLC <ref type="bibr" target="#b21">[22]</ref> is 83.4%.</p><p>Beyond the empirical contributions, we provide theoretical explanations on why compression can combat label noise. In particular, we conduct bias-variance decomposition of the objective function where Dropout/Nested Dropout is formulated into latent variable model. This decomposition provides three insights: (i) it shows that over-fitting is indeed an issue in learning with noisy labels. The bias term determines how close the model fits noisy labels, while the variance term promotes a consensus among individual models in latent variable model. Deterministic DNNs have zero variance term and thus focus on minimize the bias term during training, leading to overfitting on noisy labels. (ii) Through an information bottleneck formulation, it explains why the proposed feature compression helps in combating label noise. Dropout/Nested Dropout can serve as compression constraints since they can be formulated as optimizing an information bottleneck. These compression constraints bring non-zero variance term and thus reduce the impact of the bias term. (iii) It explains the performance boost brought by incorporating compression regularization into Coteaching. The cross-update strategy of Co-teaching together with the compression constraints bring larger variance term to further diminish the influence of the bias term, leading to even less over-fitting on the noisy labels. This paper is based on our previous work <ref type="bibr" target="#b28">[29]</ref> which mainly focuses on the empirical results. We enrich it with theoretical understanding of our method, the learning with noisy labels problem itself, and more detailed numerical assessments. This paper is structured as follows: Section II summarizes the related works in learning with noisy labels. Section III presents our algorithm. Section IV provides a theoretical understanding of our method. Section V shows illustrative toy example and experiments on benchmark realworld datasets. Finally, we conclude this paper in Section VI. Implementation is available at https://yingyichencyy.github.io/CompressFeatNoisyLabels/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section, we briefly review the existing works related to learning with label noise. Extensive literature reviews can be found in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. a) Over-fitting prevention: The idea of preventing networks from over-fitting for better generalization has been considered in <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In particular, <ref type="bibr" target="#b32">[33]</ref> proposes that appropriately tuned explicit regularization prevents DNNs from over-fitting noisy datasets while maintains generalization on clean data, and <ref type="bibr" target="#b33">[34]</ref> proposes to understand the generalization of DNNs by investigating the dimensionality of the deep representation subspace of training samples. C2D <ref type="bibr" target="#b34">[35]</ref> uses self-supervised pre-training to learn more meaningful information before over-fitting to noise. ELR/ELR+ <ref type="bibr" target="#b27">[28]</ref> proposes an early-learning regularization to resist over-fitting, while AugDesc <ref type="bibr" target="#b35">[36]</ref> achieves this by employing different augmentation strategies. Although starting from the point of preventing networks from over-fitting, our method is different from their works mainly in that (i) we theoretically verify that over-fitting is indeed an issue by conducting bias-variance decomposition while <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref> are more from an experimental perspective. (ii) we inject extrinsic compression to filter out noisy information, while <ref type="bibr" target="#b33">[34]</ref> identifies network's intrinsic compression point and adapts the corresponding loss. b) Samples reweighting: Samples reweighting scheme learns to assign small weights to those samples supposed to be corrupted. ActiveBias <ref type="bibr" target="#b36">[37]</ref> reweights samples based on the variance of prediction probabilities and the closeness between the prediction probabilities and the decision threshold. MentorNet <ref type="bibr" target="#b11">[12]</ref> trains its student network based on the clean samples selected by its teacher network. Co-teaching <ref type="bibr" target="#b3">[4]</ref> crossupdates its two base models based on the samll-loss samples selected by their peers. Decoulping <ref type="bibr" target="#b12">[13]</ref> updates the networks based on samples where the predictions of the two predictors are different, that is, the "disagreement" strategy. As for Co-teaching+ <ref type="bibr" target="#b13">[14]</ref>, it combines Co-teaching with the "disagreement" strategy to further improve the performance. Different from above where networks are based on "disagreement", JoCoR <ref type="bibr" target="#b14">[15]</ref> trains two networks as a whole by a joint loss following the "agreement" strategy, and select the small-loss examples to update themselves. This "agreement" strategy shows improvement over the previous methods. Note that we still base our method on Co-teaching since it is easier and also effective for both implementation and analysis. <ref type="bibr" target="#b37">[38]</ref> proposes a statistic, namely AUM, which differentiates clean samples from mislabeled samples by exploiting their training dynamics. The mislabeled ones are discarded during training. <ref type="bibr" target="#b37">[38]</ref> is categorized here since discarding samples is equivalent to assigning zero weights to them. c) Robust loss function: Robust losses have been applied to achieve noise-tolerant classifications including ramp loss <ref type="bibr" target="#b38">[39]</ref>, unhinged loss <ref type="bibr" target="#b39">[40]</ref>, mean absolute error <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. <ref type="figure" target="#fig_4">Fig. 1</ref>: Both Dropout and Nested Dropout are ways to induct ensemble of models, not merely regularizations. To be specific, network with them applied can be regarded as a latent variable model q(y|x) := Z q(y|z)q(z|x) dz, where q(z|x) is the encoder, and q(y|z) is the decoder as in <ref type="bibr" target="#b5">(6)</ref>. "Models 0-2" are models corresponding to different trials of z. For instance, "Model 1" of Dropout is the model corresponding to trial z 1 where the second neuron is masked out. In this case, the entire Dropout model can be viewed as an ensemble of different trial models since the integral is over all possible z ? Z. Similar explanations also apply to Nested Dropout.</p><p>However, the fact that DNNs can learn arbitrary labels may dampen the effectiveness of these losses in the context of deep learning. In deep learning, losses are corrected to be robust to noisy samples, or more exactly, to eliminate the influence of noisy samples. Based on the estimated noise transition matrix, Forward and Backward <ref type="bibr" target="#b15">[16]</ref> modify the loss function and build an end-to-end framework. HOC <ref type="bibr" target="#b42">[43]</ref> recently proposes to work on clusterable feature representations so as to efficiently estimate noise transition matrix, and further conduct better loss correction. Other loss correction strategies include <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Different from these methods, we use the cross-entropy loss albeit adapt it for latent variable models. d) Label correction: JO <ref type="bibr" target="#b19">[20]</ref> is a joint optimization framework where network parameters and class labels are optimized alternatively in training. Inspired but quite unlike <ref type="bibr" target="#b19">[20]</ref>, rather than correcting labels by using the running average of network predictions, PENCIL <ref type="bibr" target="#b20">[21]</ref> corrects labels via an updating label distribution in an end-to-end manner. Moreover, those noisy labels are only utilized for initializing the label distributions, and the network loss function is computed using the label distributions. SELFIE <ref type="bibr" target="#b5">[6]</ref> selects refurbishable samples which are of low uncertainty and can be corrected with a high precision, then replaces their labels based on past model outputs. These corrected samples together with other low-loss instances are later used to update the network. Another stateof-the-art method named PLC <ref type="bibr" target="#b21">[22]</ref> focuses more on featuredependent label noise where labels are progressively corrected based on the confidence of the noisy classifier. Notably, we keep using all the labels including those noisy ones instead of conducting label correction which is more complicated. e) Semi-supervised methods: In <ref type="bibr" target="#b22">[23]</ref>, a two-stage method is proposed where samples are identified as clean or corrupted in the first stage, and then networks are trained in a semisupervised manner with only the clean labels utilized in stage two. <ref type="bibr" target="#b23">[24]</ref> also conducts a similar two-stage method with Renyi entropy regularization used in stage two. DivideMix <ref type="bibr" target="#b24">[25]</ref> is one of the state-of-the-art methods achieving high accuracy on real noisy datasets. Specifically, it dynamically divides training data into a labeled clean set and an unlabelled corrupted set, and then trains models on both sets in a semi-supervised manner with improved MixMatch <ref type="bibr" target="#b43">[44]</ref> strategy. It can be seen that these methods mainly differ in adopting different criteria for semi-supervised learning step after dividing the training set into clean and corrupted subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we present our approach for learning with noisy labels. We start with recalling compression techniques including Dropout <ref type="bibr" target="#b1">[2]</ref> and its structured variant named Nested Dropout <ref type="bibr" target="#b2">[3]</ref> in Section III-A. Next, we combine them with one commonly accepted approach named Co-teaching <ref type="bibr" target="#b3">[4]</ref> (Section III-B) in Section III-C. The reason for this combination will be discussed in detail in Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Compression regularizations</head><p>Here, we consider two compression regularizations that are plug-and-play modules, which can be inserted into common network architectures. For the sake of clarity, we summarize some necessary notations here. LetZ ? R channels?height?width be the hidden feature representation obtained by the feature network f , i.e.,Z = f (X). Note that we set the number of channels to be K and leave out the rest for simplicity, that is, R K???? . In this paper, we treat compression methods as applying masks to the obtained featureZ. In this manner, let M ? M be the feature mask where the space M can vary for different compression methods. The feature with mask applied is denoted by Z = M Z where is the elementwise product. Then, Z will be fed into the subsequent network structures. The two compression methods are given w.r.t. their specific mask distributions as follows:</p><p>1) Dropout: Dropout <ref type="bibr" target="#b1">[2]</ref> is one classical method for feature compression where each feature in the network layer it applies is dropped according to a Bernoulli distribution. The space of its feature mask M is defined by</p><formula xml:id="formula_0">M := {M ? R K???? | ? 1 ? k ? K, M k ? B(p drop )} (1)</formula><p>where B is the Bernoulli distribution with M k being either 1 or 0, and p drop is the drop rate.</p><p>2) Nested Dropout: Nested Dropout <ref type="bibr" target="#b2">[3]</ref> learns ordered representations with different dimensions having different degrees of importance. Although it is originally proposed to perform fast information retrieval and adaptive data compression, we find that it can properly regularize a network to combat label noise. In particular, while Nested Dropout is applied, the meaningless representations can be dropped, which leads to <ref type="figure">Fig. 3</ref>: Overview of our method. In stage one, the hidden acti-vationZ is computed by a feature extractor f . Dropout/Nested Dropout is applied toZ by masking some of the features to zeros, i.e., Z = M Z . The compressed feature Z is then fed into the network structure d, which can simply be a fully connected layer (FC), to perform the final prediction. In stage two, the two base networks are fine-tuned with Co-teaching. a compressed network <ref type="bibr" target="#b44">[45]</ref>. Considering above, these ordered representations can be adapted to learning with noisy labels since representations learned from noisy data are supposed to be meaningless. Consequently, Nested Dropout may serve as a strong substitute of Dropout.</p><p>In order to obtain an ordered feature representation, in each training iteration, we only keep the first k dimensional feature ofZ and mask the rest to zeros, that is, M ? M where</p><formula xml:id="formula_1">M := {M ? R K???? | ? k ? C(p 1 , . . . , p K ), ? 1 ? i ? k, M i = 1 and ? k &lt; i ? K, M i = 0},<label>(2)</label></formula><p>with 1 and 0 being all-ones and all-zeros tensors, respectively. Moreover, k is sampled from a categorical distribution denoted by C with corresponding parameters as follows:</p><formula xml:id="formula_2">p k ? exp ? k 2 2 ? 2 nest , ?k = 1, . . . , K<label>(3)</label></formula><p>where ? nest is the major hyper-parameter in Nested Dropout. In this case, smaller k is preferred if ? nest is small. Moreover, though we could compute E P M (Z) := E P M (M Z ) with P M being (3) exactly during inference, we find it more efficient to verify which k yields the best performance on the validation set, and then keep the model induced by k for testing. Note that rather than treating Dropout and Nested Dropout merely as regularizations, we focus on their ability of inducting ensemble of models, i.e., <ref type="bibr" target="#b5">(6)</ref>, which will be carefully discussed in Section IV-B. We underline this ensemble property in <ref type="figure" target="#fig_4">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Co-teaching</head><p>Co-teaching <ref type="bibr" target="#b3">[4]</ref> is a baseline method for learning under label noise. It trains two deep networks with identical architecture, i.e., h 1 and h 2 , simultaneously where each network selects its 100(1?? forget ) percent small-loss instances, leading to D 1 and D 2 respectively, where ? forget is the forget rate. Note that ? forget is a crucial hyper-parameter in the Co-teaching architecture. </p><formula xml:id="formula_3">&gt; 0) by (2), (3). while h 1 , h 2 not converge do Train h 1 , h 2 independently on D with loss L q under (Nested) Dropout; end while Fine-tune with Co-teaching do Randomly separate mini-batch D m into two subsets: D m1 , D m2 with |D m1 | = |D m2 |; h 1 selects (1 ? ? forget )|D m1 | small-loss dataD m1 ; h 2 selects (1 ? ? forget )|D m2 | small-loss dataD m2 ; Train h 1 onD m2 , h 2 onD m1 independently with loss L q under (Nested) Dropout; end Output: (h 1 + h 2 )/2</formula><p>Networks update themselves basing on the data subset selected by their peers. We provide a illustration for clarity in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Co-teaching bases on the concept that small-loss instances are more likely to be clean <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Therefore, classifiers trained on them are supposed to be more resistant to noisy labels. However, one non-negligible premise is that base models should be reliable enough to select samples which are indeed clean. To prevent constantly bad selections, a scheduling has been proposed in <ref type="bibr" target="#b3">[4]</ref>. That is, Co-teaching first keeps all the samples in the mini-batch, then gradually decreases the sample size in D 1 and D 2 till the predefined Nth epoch, after which the sample size used for training kept fixed. Nevertheless, we experimentally find that the tuning of N is not stable since N varies with different levels of label noise. Therefore, rather than training Co-teaching with random initialized base models and tuning on N , we employ well-trained models as initialization for better and stable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Combination</head><p>Now we combine the compression regularizations with Coteaching in a two-stage manner:</p><p>? Train two Dropout / Nested Dropout networks separately until convergence; ? Fine-tune these two networks with Co-teaching. Co-teaching is chosen for fine-tuning since its cross-update mechanism would help in alleviating the over-fitting issue over a single model. As mentioned in <ref type="bibr" target="#b3">[4]</ref>, different classifiers are able to generate different decision boundaries and then have different abilities to learn. During training on noisy labels, we expect the two neural networks to adaptively compress out the noisy information left by their peer networks where samples with obviously corrupted labels have been already excluded. Hence, the base networks are less likely to overfit the corrupted labels. The above idea is validated in Section IV-E with the help of a bias-variance decomposition for Co-teaching.</p><p>We now specify our method. In the first stage, two networks are trained independently until convergence so as to provide better base models for Co-teaching. Moreover, a learning rate warm-up is set to cope with the difficulty of training with Dropout/Nested Dropout in the early epochs, which results from the high probability of dropping most of the channels in the feature layer when p drop is large or ? nest is small. In the second stage, since we only fine-tune the networks with Coteaching, Dropout/Nested Dropout is maintained during the training of each model except for the selection procedure of small-loss data subsets D 1 , D 2 . Note that the performance of Co-teaching also depends on the diversity of the base models. In this case, we modify the original Co-teaching <ref type="bibr" target="#b3">[4]</ref> with batch separation strategy where each batch is divided into two data subsets with equivalent size, and small-loss data selections are then conducted on these two subsets separately. The final result is the accuracy of the ensembled model. The workflow of our two-stage method is given in <ref type="figure">Fig. 3</ref> and Algorithm 1 for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL ANALYSIS</head><p>This section provides the motivation and validity of our method. Notations and basic concepts in need are given in Section IV-A. We state that the key issue in combating label noise is to prevent networks from over-fitting based on a bias-variance decomposition in Section IV-B. To this end, we recall two compression regularizations which can be treated as implicit information bottleneck in Section IV-C. More on Nested Dropout is in Section IV-D. Finally, we verify that the Co-teaching combination leads to even less over-fitting based on the bias-variance decomposition in IV-E. For the sake of clarity, all the proofs in this section are given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>First of all, we formulate the problem of learning with noisy labels. Let X ? X be the input variable where X is the input feature space. We consider the data generation process for the training set:</p><formula xml:id="formula_4">x ? p(x), ? ? p(?), y ? p(y|x, ?),<label>(4)</label></formula><p>where ? is the noise occurred during labelling. In this manner, we denote by Y ? Y the contaminated label where Y is the corresponding signal space. The goal is to learn a model on the corrupted dataset for testing on clean data drawn from the same generative process expect that ? 0. Next, we cover some basic concepts in information theory <ref type="bibr" target="#b46">[47]</ref>. The entropy gives the amount of information coded in a distribution or equivalently the uncertainty about a random variable, and it is defined as the average code length:</p><formula xml:id="formula_5">H p (Y ) := ? Y p(y) log p(y) dy</formula><p>where p(y) is a fixed probability measure on Y. Similarly, conditional entropy gives the amount of information about one random variable given another random variable:</p><formula xml:id="formula_6">H p (Y |X) := ? Y X p(x, y) log p(y|x) dxdy</formula><p>where p(x, y) is the joint probability measure on X ? Y and p(y|x) is the conditional p.d.f. The cross entropy with respect to a model distribution q(y|x) is defined by</p><formula xml:id="formula_7">H p,q (Y |X) := E p(x) E p(y|x) [? log q(y|x)] = E p(x) E p(y|x) ? log p(y|x) + log p(y|x) q(y|x) ? H p (Y |X),<label>(5)</label></formula><p>which upper bounds the conditional entropy as in <ref type="bibr" target="#b4">(5)</ref>. Note that the inequality holds for that the Kullback-Leibler divergence, i.e., the second term, is always non-negative. A related quantity is the cross-entropy loss ? log q(y|x). The mutual information measures the statistical dependency between random variables X and Y by comparing their joint density with the product of each marginal density:</p><formula xml:id="formula_8">I(X; Y ) := Y X p(x, y) log p(x, y) p(x)p(y) dxdy.</formula><p>Note that since the mutual information is a function of p(x, y), we modify the notation to I p (X; Y ) for better emphasizing on the actual variable. Moreover, given a factorization p(x, y) = p(x)p(y|x), we have</p><formula xml:id="formula_9">I p (X; Y ) = H p (Y ) ? H p (Y |X)</formula><p>which leads to another interpretation, that is, the reduction in uncertainty of Y by knowing X. In addition, the conditional mutual information is defined as follows:</p><formula xml:id="formula_10">I p (X; Y |Z) := Z Y X p(x, y, z) log p(x, y|z) p(x|z)p(y|z)</formula><p>dxdydz.</p><p>Here and subsequently, we let E p(x) [x] ? E P X [X] stand for the expected value of X and D KL (?) for the Kullback-Leibler divergence. Besides, we denote the capital Roman alphabet for random variables or matrices and their lowercase for the values. Moreover, we denote by Z i the i-th row if Z is a matrix or the i-th channel if Z is a 3-dimensional tensor. We also write Z i:j for the slice from the i-th channel to the j-th channel, and for the element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bias-variance decomposition for noisy labels</head><p>Considering that deterministic networks are likely to overfit the noisy training set, we introduce an ensemble of models and rely on the intersection of these models to extract consistent information. The idea is that the information learned from the noise are less likely to be consistent across different models. This motivates us to consider the latent variable model since it can be treated as an ensemble of models:</p><formula xml:id="formula_11">q(y|x) := Z q(y|z)q(z|x) dz,<label>(6)</label></formula><p>where q(z|x) is the encoder, and q(y|z) is the decoder or can even be an individual model induced by a particular instance of z. For practical reasons, we would like to use existing network architectures, such as ResNet <ref type="bibr" target="#b47">[48]</ref>, to construct the encoder q(z|x) and the decoder q(y|z). Our strategy is to split an entire network architecture, e.g. a ResNet-18, into two parts as shown in <ref type="figure">Fig. 3</ref>. In this case, it is natural to take the second part plus a softmax layer to implement q(y|z). The first part is however insufficient to implement q(z|x) as we will discuss later.</p><p>Since the cross-entropy loss ? log q(y|x) is intractable due to the integral in <ref type="formula" target="#formula_11">(6)</ref>, we consider a surrogate quantity to q(y|x) using Jensen's inequality:</p><formula xml:id="formula_12">q(y|x) ? exp E q(z|x) log q(y|z) ? q(y|x)<label>(7)</label></formula><p>and therefore we define the new loss function as the negative log-likelihood with respect toq(y|x):</p><formula xml:id="formula_13">L q (x, y) := E q(z|x) ? log q(y|z) ? ? logq(y|x).<label>(8)</label></formula><p>Based on (4), <ref type="formula" target="#formula_12">(7)</ref> and <ref type="formula" target="#formula_13">(8)</ref>, we now derive the bias-variance decomposition of the proposed latent variable with the new loss function under label noise as follows: <ref type="formula" target="#formula_13">(8)</ref>, the risk has a bias-variance decomposition:</p><formula xml:id="formula_14">Theorem 1. Let x, y ? p(x, y) = p(x)p(y|x) where p(y|x) = p(y|x, ?)p(?) d?, for loss L q (x, y) defined in</formula><formula xml:id="formula_15">E p(x,y) L q (X, Y ) = E p(x) D KL p(y|x) q(y|x) bias + E q(z|x) D KL q(y|x) q(y|z) variance + const<label>(9)</label></formula><p>whereq(y|x) ? exp E q(z|x) log q(y|z) is the average or an ensemble of models.</p><p>Intuitively, the bias term in (9) determines how close the average modelq(y|x) is to p(y|x) and p(y|x) is the conditional probability for the noisy Y , while the variance term promotes a consensus among individual models. The variance term also serves as a regularization to combat label noise in the sense that the consensus downweights the influence of the incorrect labels. Unlike learning with clean data, we do not expect low bias as it indicates model's over-fitting to label noise. Instead, we rely on the variance term and early stopping to provide good training signals. In regard of above, the problem of learning with noisy labels can be simplified to how can we prevent models from over-fitting the noisy training labels?</p><p>It is worth mentioning that a careful design of the encoder q(z|x) is necessary in order to make better use of the variance term. For instance, if we choose q(z|x) to be a Dirac delta function, i.e., a deterministic mapping from x to z, the variance term is zero. As a result, the training will focus on minimizing the bias term, leading to an easy over-fitting to the distribution of noisy labels. On the other hand, if the variance is too large, there will be little consensus among individual models, and therefore, no consistent information could be learned by the latent variable model. Thus in Section IV-C, we propose to design q(z|x) by incorporating a compression inductive bias for better combating label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compression regularizations</head><p>We would like to create an information bottleneck for the latent variable model. In this manner, the noisy information can be filtered out systematically. To be specific, we propose to create an information bottleneck by masking and damping the output of the feature extractor f (X):</p><formula xml:id="formula_16">min p ?I p (Y ; Z)<label>(10)</label></formula><formula xml:id="formula_17">s.t. Z = M f (X), M ? P M</formula><p>where the distributions of M , i.e., P M , is an extrinsic source of randomness, which is tuned on a held-out clean dataset.</p><p>Here M is also called mask as defined in <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>. However, here comes the question that why don't we use Tishby's information bottleneck <ref type="bibr" target="#b48">[49]</ref> directly for compression? As mentioned in <ref type="bibr" target="#b48">[49]</ref>, the relationship among the input X, the label Y and the feature representation Z in the network is given by a Markov chain: Y ? X ? Z. In this consideration, <ref type="bibr" target="#b48">[49]</ref> proposes to learn a good feature representation by minimizing the weighted sum of the data fitting term ?I p (Y ; Z) and the complexity term I p (X; Z) with respect to the distribution p(x, y, z), that is</p><formula xml:id="formula_18">min p ?I p (Y ; Z) + ?I p (X; Z).<label>(11)</label></formula><p>However, when learning with noisy labels, there is no clear causal relationship between Y and X. Therefore, Tishby's information bottleneck principle cannot be applied in this case.</p><p>To be more specific, we may argue in terms of the information diagrams shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. The left image in <ref type="figure" target="#fig_1">Fig. 4</ref> visualizes the Markov chain Y ? X ? Z, which is called the Mickey Mouse I-diagram in <ref type="bibr" target="#b49">[50]</ref> since I(Y ; Z|X) = 0, which implies that I(X; Y ; Z) = I(Y ; Z). In this case, I(Y ; Z) is always smaller than I(X; Z), hence we can prevent Z's over-fitting to Y by reducing I(X; Z). However, in general, I(Y ; Z|X) = 0 as shown in the middle in <ref type="figure" target="#fig_1">Fig. 4</ref>, and we can find cases where I(X; Z) is small but Z overfits label noise. To conclude, the traditional information bottleneck (11) may not be effective when dealing with label noise. Comparing to Tishby's IB (11), we discard the term I(X; Z) in (10) completely and rely only on the held-out clean dataset to remove noisy information. Since I p (Y ; Z) is intractable, we further adjust it to be computationally available. Instead of estimating the joint distribution p(x, y, z), we consider a surrogate joint distribution q(x, y, z) = p(x, y)q(z|x) ? p(x, y, z), and access to p(x, y) only through its samples. Note that the idea is similar to the variational information bottleneck by <ref type="bibr" target="#b50">[51]</ref>. Specifically, we first identify that for the representation learning, and hence we approximate</p><formula xml:id="formula_19">?I p (Y ; Z) = H p (Y |Z) ? H p (Y ) where H p (Y ) is a constant</formula><formula xml:id="formula_20">H p (Y |Z) by H p (Y |Z) ? H p,q (Y |Z) = E p(x,y,z) [? log q(y|z)] ? E p(x,y) E q(z|x) ? log q(y|z) = E p(x,y) L q (X, Y ) .</formula><p>Now, we rewrite our learning with noisy labels as follows:</p><formula xml:id="formula_21">min q E p(x,y) L q (X, Y ) s.t. Z = M f (X), M ? P M<label>(12)</label></formula><p>where L q (x, y) := E q(z|x) ? log q(y|z) as in <ref type="bibr" target="#b7">(8)</ref>. As such, we only need to learn the proposed q(z|x) and q(y|z). In particular, we propose an implicit parameterization for q(z|x) as specified in <ref type="formula" target="#formula_1">(12)</ref>, which involves a feature extractor f (X) and an external random variable M . We also proposed to learn q(z|x) and q(y|z) jointly by minimizing E p(x,y) L q (X, Y ) , which will be optimized by stochastic gradient descent (SGD) as we only have access to the samples of X, Y and Z. 1) Dropout: If we set P M to a Bernoulli distribution as in (1), then (12) exactly covers Dropout. However, different from the original formulation in <ref type="bibr" target="#b1">[2]</ref>, we formulate Dropout into the framework of latent variable models with our loss <ref type="bibr" target="#b7">(8)</ref>. In this manner, Dropout is the baseline of incorporating compression inductive bias for combating label noise.</p><p>2) Nested Dropout: If we specify M ? M where M is (2), and P M as (3), then (12) exactly covers the Nested Dropout. More properties are provided in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Nested Dropout</head><p>Nested Dropout is a variant of Dropout where the importance of each feature channel is sorted from high to low, while channels in Dropout model are of equal importance. For better understanding of Nested Dropout's sorting property, we theoretically work on it through mutual information.</p><p>Before presenting the theorem on the information sorting property, we need the following Assumption 1 where hidden features are supposed to be exchangeable <ref type="bibr" target="#b52">[53]</ref>. Assumption 1. Let X be the input, f be the feature extractor, d be the subsequent network structure including the classification head, andZ = f (X) ? [Z k ] K k=1 be the hidden feature representation. The hidden feature representationZ is exchangeable, and d is also exchangeable. That is, for any permutation ?, the model satisfies that</p><formula xml:id="formula_22">(Z 1 ,Z 2 , . . . ,Z K ) D = (Z ?1 ,Z ?2 , . . . ,Z ? K ),<label>(13)</label></formula><formula xml:id="formula_23">Y |Z 1 ,Z 2 , . . . ,Z K D = Y |Z ?1 ,Z ?2 , . . . ,Z ? K ,<label>(14)</label></formula><p>where D = denotes equivalence in distribution.</p><p>We find this can serve as a valid assumption considering the network architectures. According to de Finetti's theorem <ref type="bibr" target="#b53">[54]</ref>, a sequence of random variables (Z 1 ,Z 2 , . . .) is infinitely exchangeable iff, p(z 1 ,z 2 , . . . ,z n ) = n i=1 p(z i |?)P (d?), for all n ? N + and some measure P on ?. If we consider a simple</p><formula xml:id="formula_24">MLP wherez = f (x) = W 1 x + b 1 ,? = d(z) = W 2z + b 2 and denote ? = [W 1 , b 1 , x].</formula><p>In this way, for any permutation of the hidden featurez ? := ?(z), it can be obtained by ?(W 1 )?(x)+?(b 1 ). As it requires to integrate all the possible ? to obtain p(z) and p(z ? ), we then have p(z) = p(z ? ). Since? ? = ?(W 2 )z ? + ?(b 2 ) =?, then d is exchangeable. Moreover, similar argument can be derived by considering the features after global average pooling in CNNs.</p><p>Theorem 2. Let X be the input, f be the feature extractor, and Z = f (X) ? [Z k ] K k=1 be the hidden feature representation. Suppose that the model satisfies Assumption 1. Then, we have</p><formula xml:id="formula_25">for Z k = M k Z k , I(Y ;Z 1 ) = ? ? ? = I(Y ;Z K ),<label>(15)</label></formula><formula xml:id="formula_26">I(Y ; Z 1 ) ? ? ? ? ? I(Y ; Z K ).<label>(16)</label></formula><p>This sorting property is also discussed in <ref type="bibr" target="#b2">[3]</ref>, although from the perspective of I(Y ; Z 1:k ) ? I(Y ; Z 1:(k+1) ) with Z 1:k := [Z 1 , Z 2 , . . . , Z k ]. In addition to the theoretical analysis, we conduct an experiment on CIFAR-10 [52] using ResNet-18 to verify Theorem 2 empirically. We plot the empirical estimate of the variational lower bound of I(Y ; Z k ), i.e., H(Y |Z k ), for Z computed by <ref type="bibr" target="#b11">(12)</ref> and Z =Z = f (X). The comparison is in <ref type="figure" target="#fig_2">Fig. 5</ref> where the one without Nested Dropout is tagged as baseline. A clear information sorting has been achieved comparing to the baseline training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Combination with Co-teaching</head><p>In this subsection, we consider the stage two in our method where two networks are further fine-tuned with Co-teaching. Recall that during the cross-update state, one network selects its small-loss instances D 1 and send them to its peer. Intuitively, the above process resembles the teacher and student mechanism where the teacher selects possibly clean instances for the student to learn. In this regard, let q t (y|x) be the teacher network, q(y|x) in (6) be the student network. The sample selection mechanism only preserves those with small loss ? log q t (y|x), i.e., large q t (y|x). If we consider this selection w.r.t. probability together with the following student network training, we reformulate the student's loss as: where q t (y|x) represents (x, y)'s probability to be selected. Moreover, by regarding sample selection and student network training as a whole, we redefine student network's decoder by q co (y|x, z) ? exp[q t (y|x) log q(y|z)]. In order to distinguish it from the original student network's decoder q(y|z), we call q co (y|x, z) the taught student decoder. The following Theorem 3 gives the bias-variance decomposition when networks are further fine-tuned with Co-teaching. Theorem 3. Let q t (y|x) be the Co-teaching teacher network, and q co (y|x, z) = exp[q t (y|x) log q(y|z)]/C 1 (x, z) where C 1 (x, z) := Y exp[q t (y|x) log q(y|z)] dy ? 1 be the taught student decoder. For the Co-teaching student loss L s q defined in <ref type="bibr" target="#b16">(17)</ref>, the risk has a bias-variance decomposition:</p><formula xml:id="formula_27">L s q (x, y) := q t (y|x)L q (x, y) = E q(z|x) ? q t (y|x) log q(y|z)<label>(17)</label></formula><formula xml:id="formula_28">E p(x,y) L s q (X, Y ) = E p(x) D KL p(y|x) q co (y|x) bias + E q(z|x) D KL q co (y|x) q co (y|x, z) variance + const whereq co (y|x) ? exp E q(z|x) log q co (y|x, z)</formula><p>is the average or an ensemble of models. Moreover, by defining ?(y|x) := Eq (y|x) exp[q t (y|x)] / exp[q t (y|x)], we then have:</p><formula xml:id="formula_29">(i) If ?(y|x) ? 1, then D KL p(y|x) q co (y|x) ? D KL p(y|x) q(y|x) . (18) (ii) If ?(y|x) ? C 1 (x, z), then E q(z|x) D KL q co (y|x) q co (y|x, z) ? E q(z|x) D KL q(y|x) q(y|z) .<label>(19)</label></formula><p>Remark 1. The condition ?(y|x) ? 1 equals to q t (y|x) ? log Eq (y|x) exp[q t (y|x)] where the right-hand side measures the difference between a single training network q(y|x) and the teacher network q t (y|x) of Co-teaching. The larger the difference, the smaller the value, and vise versa. Hence, to obtain smaller bias term than in (9), the sample selection of Co-teaching only chooses those (x, y) with large q t (y|x) so as to meet the condition. As</p><formula xml:id="formula_30">C 1 (x, z) ? 1 by definition, if further ?(y|x) ? C 1 (x, z), i.e., q t (y|x) ? log Eq (y|x) exp[q t (y|x)]/C 1 (x, z)</formula><p>with larger right-hand side value, then we will have larger variance term than that in <ref type="bibr" target="#b8">(9)</ref>.</p><p>Theorem 3 and Remark 1 together demonstrate that choosing samples with large q t (y|x) during selection of Co-teaching leads to smaller bias term and larger variance term than those in <ref type="bibr" target="#b8">(9)</ref>. That is, the impact of the bias term can be even diminished. Consequently, the sample selection mechanism during Co-teaching's cross-update process helps in further preventing networks from over-fitting on noisy labels, thus achieving better performance on clean datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we present our experimental results. First of all, we focus on how Dropout and Nested Dropout cope with the regression noise by a toy example in Section V-A. For better understanding of our methods, we assess them on real datasets albeit with synthetic label noise in Section V-B. In Section V-C, we compare our method with the state-ofthe-art methods on two real-world datasets: Clothing1M <ref type="bibr" target="#b4">[5]</ref> and ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>. Finally, we conduct ablation study on ANIMAL-10N and Clothing1M in Section V-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Toy example: a simple regression with noise</head><p>This subsection provides an intuitive better understanding on the reason why Nested Dropout <ref type="bibr" target="#b2">[3]</ref> and Dropout <ref type="bibr" target="#b1">[2]</ref> are able to resist label noise. To this end, we give a simulated regression experiment. Specifically, we generate a dataset of noisy observations from y i = x i + i for i = 1, . . . , 64 where x i is evenly spaced between [0, 10] and i ? N (0, 1) are i.i.d sampled. We employ a multilayer perceptron (MLP) consisting of three linear layers with input and output dimensions being 1 ? 64 ? 128 ? 1. Moreover, we add ReLU activations to all layers except the last one. When training model with Nested Dropout/Dropout, we only apply it to the last layer of the MLP, and the corresponding model is (a) Symmetric noise on CIFAR-10 and CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) Cross-Entropy <ref type="bibr" target="#b24">[25]</ref> 85.0 F-correction <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref> 87.2 M-correction <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b24">[25]</ref> 87.4 Iterative-CV <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b24">[25]</ref> 88.6 PENCIL <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b24">[25]</ref> 88.5 JO <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref> 88.9 MLNT <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b24">[25]</ref> 89.2 DivideMix <ref type="bibr" target="#b24">[25]</ref> 93  <ref type="formula" target="#formula_2">(3)</ref> where ? nest = 200. <ref type="figure" target="#fig_3">Fig. 6</ref> gives the results after 100k epochs. The drop ratio p drop of Dropout varies in {0.9, 0.7, 0.5, 0.3} where the compression ratio decreases. As in <ref type="figure" target="#fig_3">Fig. 6</ref>, MLP overfits the label noise while MLP+Nested with the first k = 1, k = 10 channels recover the groundtruth y = x better. Nevertheless, MLP+Nested gradually overfits the label noise due to over parameterization as the number of channels increases. As for MLP+Dropout, with p drop decreasing, the models become over-fitting the noisy labels. However, MLP+Nested with k = 1 still gives the best performance. To conclude, both compression methods prevent networks from over-fitting the noisy patterns. Notably, for MLP+Nested, the main data structure information is contained in the first few channels, while noisy information is likely to be encoded in channels towards the end.</p><p>B. Model analysis on synthetic noise 1) Datasets: We evaluate our methods on CIFAR-10 [52] and CIFAR-100 <ref type="bibr" target="#b51">[52]</ref> with synthetic label noise following <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b56">[57]</ref>. For the training data, we manually corrupt the label according to a transition matrix Q with Q ij = P(y = j|y clean = i), i, j ? {1, . . . , C} denoting the probability of flipping clean y clean to noisy y. One representative structure of the matrix Q is the symmetric flipping <ref type="bibr" target="#b39">[40]</ref>, that is, P(y = i|y clean = i) = 1 ? ? , P(y = i|y clean = i) = ? /(C ? 1) where C is the number of classes and ? is called the noise ratio. The other representative structure is the asymmetric (or pair) flipping <ref type="bibr" target="#b15">[16]</ref> where label mistakes only happen within very similar classes, therefore should be tailored for different datasets. For example, in CIFAR-10, the asymmetric flippings follow: truck ? auto-mobile, bird ? airplane, deer ? horse and cat ? dog. The probability is ? for flipping from groundtruth to inaccurate class, while 1?? for remaining uncorrupted.</p><p>2) Implementation details: Our methods are implemented with PyTorch. Following the previous works <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>, experiments on CIFAR-10, CIFAR-100 are with PreAct ResNet-18 <ref type="bibr" target="#b54">[55]</ref> trained from scratch. In the first stage, we use SGD optimizer with a momentum of 0.9, a weight decay of 1e-4, an initial learning rate of 0.1, and batch size of 128. We apply learning rate warm-up with 6000 iterations and the number of epochs is 200 with learning rate decayed by 0.1 at 100 and 150 epochs. Mixup data augmentation <ref type="bibr" target="#b55">[56]</ref> is adopted in the first stage for better performance as in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b57">[58]</ref>. We apply Dropout/Nested Dropout on the average pooled Conv5 features. In the second stage, two well-trained models are set as base models for Co-teaching. The initial learning rate is 1e-3 and we still employ SGD as optimizer. Moreover, ? forget is tuned under different noise ratio, batch norm is frozen and no warm-up is applied. Models are trained for 100 epochs with the learning rate decayed by 0.1 after 50 epochs. We set ? nest = 50, p drop = 0.5 for cases on CIFAR-10, and ? nest = 100, p drop = 0.3 on CIFAR-100. Note that when training with Nested Dropout, we record the optimal number of channels k * of the model and use only these first k * channels when testing. We further show that our methods can also serve as good complementary strategies to other state-of-the-art methods to achieve even better performance. In particular, we propose an additional data preprocessing step before training our methods, which is named "Pre-Cleaning". During this precleaning, for example, we substitute the original labels of the dataset with the predictions of a well-trained DivideMix <ref type="bibr" target="#b24">[25]</ref> model, resulting in a pre-cleaned dataset. We later train our methods on this pre-cleaned dataset so as to further exceed the performance of DivideMix. Note that we can employ any state-of-the-art methods to conduct the "Pre-Cleaning".</p><p>3) Results on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b51">[52]</ref>: We compare our two-stage methods with multiple state-of-the-art methods on CIFAR-10 and CIFAR-100 under different types and levels of synthetic label noise in <ref type="table" target="#tab_1">Table I</ref>. We consider the performance of our methods with and without the "Pre-Cleaning" step separately. Without the pre-cleaning step, our simple Nested+Co-teaching and Dropout+Co-teaching achieve the top-3 performance for all except the extreme 90% label noise ratio cases. Moreover, by pre-cleaning with DivideMix, our methods achieve the best performance for all cases. Note that we also compare with M-correction <ref type="bibr" target="#b57">[58]</ref> and MLNT <ref type="bibr" target="#b56">[57]</ref> with the pre-cleaning step in <ref type="table" target="#tab_1">Table I</ref>. It can be seen that although both M-correction and MLNT improve upon their own results with the help of pre-cleaning, they fail to surpass the performance of DivideMix. Therefore, they cannot serve as complementary strategies to DivideMix to enhance performance. In contrast, our Dropout+Co-teaching improves upon DivideMix by a maximum of 5.9% in accuracy under 90% symmetric noise on CIFAR-100. We also consider the training and inference time of our methods. It takes 2.7 hours for training the complete two-stage model on a single NVIDIA V100 GPU, and both Nested+Co-teaching and Dropout+Coteaching take 0.24 milliseconds per image for inference. In regard of above, our methods not only perform well on their own, but can also serve as effective and efficient complementary strategies to other state-of-the-art methods with only a little extra time. Moreover, we provide additional insight that by using ImageNet <ref type="bibr" target="#b59">[60]</ref> pre-trained models as in <ref type="table" target="#tab_1">Table II</ref>, we can achieve even better performance.</p><p>C. Comparison with state-of-the-art methods on real datasets 1) Datasets: The following experiments are conducted on two real-world datasets with real label noise: Clothing1M <ref type="bibr" target="#b4">[5]</ref> and ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>. Clothing1M is a benchmark dataset containing 1 million clothing images with 14 categories from online shopping websites, and its overall estimated noise ratio is 38.5% according to <ref type="bibr" target="#b4">[5]</ref>. Moreover, this dataset provides 50k, 14k and 10k manually verified clean data for training, validation and testing. Note that we do not use the clean training set during training. In our experiment, we randomly sample a balanced subset that includes 260k images with 18.5k images per category, from the noisy training set as in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. This balanced subset is used as our training set and classification accuracies are reported on the 10k clean test data. We follow the data augmentations in <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b57">[58]</ref>, which includes Mixup data augmentation <ref type="bibr" target="#b55">[56]</ref>. ANIMAL-10N is another benchmark dataset recently proposed by <ref type="bibr" target="#b5">[6]</ref>. It contains 10 animal classes with confusing appearance. There are 50k training, 5k testing images, and an estimated label noise rate 8%. No data augmentation is applied so as to follow the settings in <ref type="bibr" target="#b5">[6]</ref>.</p><p>2) Implementation details:</p><p>The following experiments are implemented on PyTorch. Experiments on Clothing1M <ref type="bibr" target="#b4">[5]</ref> are with ResNet-50 <ref type="bibr" target="#b47">[48]</ref> pre-trained on ImageNet <ref type="bibr" target="#b59">[60]</ref> following <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b56">[57]</ref>. Dropout/Nested Dropout is applied right TABLE III: Test accuracy (%) of state-of-the-art methods on Clothing1M <ref type="bibr" target="#b4">[5]</ref>. All approaches are implemented with ResNet-50 <ref type="bibr" target="#b47">[48]</ref> architecture. Results with "*" use a balanced subset or a balanced loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) Cross-Entropy <ref type="bibr" target="#b24">[25]</ref> 69.2 F-correction <ref type="bibr" target="#b15">[16]</ref> 69.8 M-correction <ref type="bibr" target="#b57">[58]</ref> 71.0 JO <ref type="bibr" target="#b19">[20]</ref> 72.2 ELR* <ref type="bibr" target="#b27">[28]</ref> 72.9 HOC* <ref type="bibr" target="#b42">[43]</ref> 73.4 PENCIL* <ref type="bibr" target="#b20">[21]</ref> 73.5 MLNT <ref type="bibr" target="#b56">[57]</ref> 73.5 PLC* <ref type="bibr" target="#b21">[22]</ref> 74.0 C2D* <ref type="bibr" target="#b34">[35]</ref> 74.6 ELR+* <ref type="bibr" target="#b27">[28]</ref> 74.8 DivideMix* <ref type="bibr" target="#b24">[25]</ref> 74.8 Ours Nested* 73.2 Nested+Co-teaching* 75.0 Dropout* 72.9 Dropout+Co-teaching* 74.0 before the linear classifier in the network with ? nest = 250, p drop = 0.5. First, SGD optimizer is used for stage one model training with momentum 0.9, weight decay 5e-4, initial learning rate 2e-2, and batch size 96. Learning rate warmup is utilized for 6000 iterations in stage one, and model is later trained for 30 epochs with the learning rate decayed by 0.1 after the 10-th epoch. Second, the two models trained in stage are fine-tuned through Co-teaching in stage two. SGD optimizer is utilized with the same settings and follows a cosine learning rate decay <ref type="bibr" target="#b60">[61]</ref> with a maximum learning rate of 5e-5 and a minimum of 1e-5 and without learning rate warm-up. Forget rate ? forget is set to be 0.5, batch norms are frozen, and we train for 10 epochs. Note that when training with Nested Dropout, the optimal number of channels k * of the model is recorded, and only these first k * channels are used for testing. For ANIMAL-10N, VGG-19 <ref type="bibr" target="#b61">[62]</ref> is used with batch normalization <ref type="bibr" target="#b62">[63]</ref> as in <ref type="bibr" target="#b5">[6]</ref>. The two Dropout layers in the original VGG-19 architecture are substituted with Nested Dropout when "Nested" is applied. SGD optimizer is applied. For more stable training, we use alternative training strategy for these two layers of Dropout/Nested Dropout. That is, for each feed-forward, Nested Dropout is either applied to the first or the second layers. In stage one, following <ref type="bibr" target="#b5">[6]</ref>, the network is trained for 100 epochs with initial learning rate 0.1. The learning rate is later decayed by 0.2 at 50-th and 75-th epochs. Moreover, models are trained with learning rate warm-up for 6000 iterations. In stage two, forget rate ? forget is set to be 0.2, batch norms are frozen and no warm-up is applied. The initial learning rate is 4e?3 and is decayed by 0.2 after the 5-th epoch with 30 epochs in total.</p><p>3) Results on the Clothing1M <ref type="bibr" target="#b4">[5]</ref>: We compare our methods to state-of-the-art methods in <ref type="table" target="#tab_1">Table III</ref>. Notably, a single model trained with Nested Dropout or Dropout can not only surpass M-correction <ref type="bibr" target="#b57">[58]</ref>, JO <ref type="bibr" target="#b19">[20]</ref>, ELR <ref type="bibr" target="#b27">[28]</ref>, but also achieve comparable performance to HOC <ref type="bibr" target="#b42">[43]</ref> and PENCIL <ref type="bibr" target="#b20">[21]</ref>. The combination of Dropout+Co-teaching boosts the performance of a single Dropout model by 1.1%. Moreover, the combina-  <ref type="bibr" target="#b5">[6]</ref>. All approaches are implemented with VGG-19 <ref type="bibr" target="#b61">[62]</ref> architecture. Results with "*" use two networks for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. (%) Cross-Entropy <ref type="bibr" target="#b5">[6]</ref> 79.4 ? 0.1 Co-teaching* <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> 80.2 ? 0.1 ActiveBias <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b5">[6]</ref> 80.5 ? 0.3 SELFIE <ref type="bibr" target="#b5">[6]</ref> 81.8 ? 0.1 PLC <ref type="bibr" target="#b21">[22]</ref> 83.  <ref type="table" target="#tab_1">Table IV</ref> gives the results on ANIMAL-10N. It can be seen that our single Dropout model can achieve comparable performance to SELFIE <ref type="bibr" target="#b5">[6]</ref>. Moreover, the combination with Co-teaching provides a consistent performance boost, which is in line with the results on Clothing1M <ref type="bibr" target="#b4">[5]</ref>. Notably, our best performance by using Dropout+Co-teaching achieves 84.5% accuracy outperforms recent approach PLC <ref type="bibr" target="#b21">[22]</ref> by 1.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation study with real label noise</head><p>This section provides ablation study of ? nest , p drop , and ? forget on ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>. Note that same as many state-of-the-art methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b56">[57]</ref>, our hyper-parameters need to be tuned on a clean validation set. Moreover, we also evaluate our methods using different backbones on Clothing1M <ref type="bibr" target="#b4">[5]</ref>.</p><p>1) Ablation on ? nest : As in <ref type="table">Table V</ref> (a), Nested Dropout provides consistent improvement compared to training with standard cross-entropy loss and the performance gain is also robust to the choices of the hyper-parameter ? nest . Moreover, fine-tuning through Co-teaching provides clear performance boost for all the models. We also present the optimal number of channels of each model (entry "k * "). Although there are two layers of Nested Dropout applied to the classifier of VGG-19, the optimal number of channels k * is recorded w.r.t. the last Nested Dropout layer for simplicity. Interestingly, models trained with Nested Dropout achieve better performance with only less than 1% of channels comparing to their counterparts with cross-entropy loss.</p><p>2) Ablation on p drop : We experiment on different p drop with results given in Table V (a). Note that results with p drop ? 0.5 are not given in the table since the training of single VGG-19 fails on ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>. The performance under different choices of p drop are less robust compared to those of Nested Dropout. However, what is in common is that the combination with Co-teaching again brings significant performance boost for all the models.</p><p>3) Ablation on ? forget : We focus on how forget rate ? forget of Co-teaching influences the performance in <ref type="table">Table V (b)</ref>. We present with the settings where Nested Dropout and Dropout have the best performance. To be specific, we set ? nest = 100 and p drop = 0.1 for the training, respectively.</p><p>For Nested+Co-teaching, the performance drops with ? forget increasing, and the accuracy remains 83.3% for ? forget ? 0.5. The performance with ? forget ? 0.5 are not given in the Table V (b) for simplicity. Moreover, this performance boost of 83.3% compared to a single Nested Dropout network actually results from the ensemble estimation, not the crossupdate mechanism of Co-teaching. Therefore, considering that the estimated label noise ratio of ANIMAL-10N is 8%, Coteaching's core mechanism is not suitable for cases where the difference between ? forget and the ground-truth noise ratio is large. Furthermore, performance of Dropout+Co-teaching again verify the above analysis where the best performance is achieved by ? forget = 0.2. 4) Ablation on different backbones: As given in <ref type="table" target="#tab_1">Table VI</ref>, the best performance is achieved by using ResNet-50 as backbone, which is 75.0% in accuracy. Even though EfficientNet-B2 <ref type="bibr" target="#b63">[64]</ref> cannot achieve the best performance possibly due to its capacity issue, Nested+Co-teaching indeed improves upon a single Nested Dropout model. The same applied to Dropout+Co-teaching. These results suggest the effectiveness of our approach with different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we investigate the problem of image classification in the presence of label noise. In particular, we find that preventing networks from over-fitting the corrupted labels is one key problem in learning with noisy labels based on a biasvariance decomposition. To this end, we introduce compression inductive bias to networks to increase the variance term so as to weaken the influence of the bias term which is associated with over-fitting. This inductive bias is realized by applying simple compression regularizations such as Dropout <ref type="bibr" target="#b1">[2]</ref> and its variant named Nested Dropout <ref type="bibr" target="#b2">[3]</ref> to networks. Notably, Nested Dropout is proved to learn ordered feature representations in this paper. Therefore, this information sorting property can bring interpretability w.r.t. channel importance to networks while filter out the noisy patterns. Moreover, we combine these compression regularizations with Co-teaching <ref type="bibr" target="#b3">[4]</ref>, leading to a two-stage method. We then theoretically verify that this combination is in line with our bias-variance trade-off since Co-teaching further increases the variance term, hence further prevent networks from over-fitting. Our method is validated on benchmark real datasets under synthetic label noise and realworld label noise including Clothing1M <ref type="bibr" target="#b4">[5]</ref> and ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>. Our method achieves comparable or even better performance than state-of-the-art approaches. Our approach is simple compared to many existing methods. Therefore, we hope that our approach can serve as a strong baseline for future research on learning with noisy labels. V: Average test accuracy (%) with standard deviation (3 runs) of (a) different ? nest for Nested Dropout, p drop for Dropout. The corresponding optimal number of channels k * for each Nested Dropout model is given (entry "k * "). We report test accuracy of single model (entry "Acc.") as well as the accuracy with the combination of Co-teaching (entry "Co-teaching Acc.") (b) different ? forget for Co-teaching on ANIMAL-10N <ref type="bibr" target="#b5">[6]</ref>.  (i) [Proof of <ref type="bibr" target="#b14">(15)</ref>]: For the hidden representationZ, since we have (13) held, ? i, j ? {1, . . . , K}, and i &lt; j without loss of generality, we exchange the i-th and j-th arguments: pZ i (z i ) = ? ? ? pZ 1,Z2,...,ZK (z 1 , . . . ,z i , . . . ,z j , . . . ,z K ) dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = ? ? ? pZ 1,Z2,...,ZK (z 1 , . . . ,z j , . . . ,z i , . . . ,z K ) dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = pZ j (z i ). Therefore, pZ i (z i ) = pZ j (z i ) andZ i andZ j are identically distributed. Since i, j are arbitrarily chosen, we have</p><formula xml:id="formula_31">{Z i } K i=1 identically distributed.</formula><p>Similarly, for the joint distributions of Y,Z i and Y,Z j with i, j arbitrarily chosen, we have p Y,Zi (y,z i ) = ? ? ? p Y,Z1,Z2,...,Z K (y,z 1 , . . . ,z i , . . . ,z j , . . . ,z K )</p><formula xml:id="formula_32">dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = ? ? ? p Y |Z1,Z2,...,Z K (y| . . . ,z i , .</formula><p>. . ,z j , . . .) pZ 1,Z2 ,...,Z K (. . . ,z i , . . . ,z j , . . .)dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = ? ? ? p Y |Z1,Z2,...,Z K (y| . . . ,z j , . . . ,z i , . . .) pZ 1,Z2 ,...,Z K (. . . ,z j , . . . ,z i , . . .)dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = ? ? ? p Y,Z1,Z2,...,Z K (y,z 1 , . . . ,z j , . . . ,z i , . . . ,z K ) dz 1 ? ? ? dz i?1 dz i+1 ? ? ? dz K = p Y,Zj (y,z i ),</p><p>where the third equivalence holds for the permutation invariant (13), <ref type="bibr" target="#b13">(14)</ref>. Hence, p Y,Zi (y,z i ) = p Y,Zj (y,z i ) for arbitrary i, j, that is, {(Y,Z i )} K i=1 are identically distributed. Considering the formulation of mutual information, we have ? i, j ? {1, . . . , K}:</p><formula xml:id="formula_33">I(Y ;Z i ) : = Y Z i p Y,Zi (y,z i ) log p Y,Zi (y,z i ) p Y (y)pZ i (z i ) dydz i = Y Z j p Y,Zj (y,z j ) log p Y,Zj (y,z j ) p Y (y)pZ j (z j ) dydz j =: I(Y ;Z j ).</formula><p>This completes the proof of <ref type="bibr" target="#b14">(15)</ref>.</p><p>(ii) [Proof of (16)]: By the definition of Nested Dropout in (2) and (3), we have Z = [Z 1:k , 0, . . . , 0] with k ? C(p 1 , . . . , p K ),</p><p>We first rewrite the above calculation equivalently as and ? 0 = 1, Z k = T k k , ? k = 1, . . . , K.</p><p>The above recursion can be solved and specified by Hence, the equivalence holds if</p><formula xml:id="formula_35">p 1 ? 1 ? ? 1 , p k ? k?1 j=1 ? j (1 ? ? k )</formula><p>for k = 2, . . . , K ? 1, and p K ? Given the construction of the network, we obtain a Markov chain Y ? X ? T 1 k ? ? ? ? ? T K k , which simply means that T i k is conditionally independent of Y given X or T j k for j &lt; i. Now, we need the following data processing inequality: Lemma 2 (Data processing inequality). Let three random variables form the Markov chain A ? B ? C, meaning that C is conditionally independent of A given B. Then, Proof of Theorem 3. Recall that the taught student decoder is defined by q co (y|x, z) = exp[q t (y|x) log q(y|z)]/C 1 (x, z) where C 1 (x, z) := Y exp[q t (y|x) log q(y|z)] dy. For a given input x, E p(y|x) [L s q (x, Y )] = E p(y|x) E q(z|x) ? q t (y|x) log q(y|z) = E p(y|x) E q(z|x) ? log q co (y|x, z) ? E q(z|x) [log C 1 (x, z)] = E p(?) E q(z|x) D KL p(y|x, ?) q co (y|x, z) + const <ref type="bibr" target="#b21">(22)</ref> where const = H(Y |X = x, ?) ? E q(z|x) [log C 1 (X = x, z)], and the last equation is according to <ref type="bibr" target="#b19">(20)</ref>. With Lemma 1 applied, the first term in <ref type="bibr" target="#b21">(22)</ref> can be decomposed as where ?(y|x) := Eq (y|x) exp[q t (y|x)] / exp[q t (y|x)]. For ?(y|x) ? 1, that is, q t (y|x) ? log Eq (y|x) exp[q t (y|x)] , we have Co-teaching bias term satisfying D KL p(y|x) q co (y|x) ? D KL p(y|x) q(y|x) .</p><p>(ii) For the variance term, we have To conclude, if ?(y|x) ? C 1 (x, z), then E q(z|x) D KL q co (y|x) q co (y|x, z) ? E q(z|x) D KL q(y|x) q(y|z) .</p><p>This completes the proof.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Co-teaching trains two networks simultaneously where base network updates itself based on the small-loss mini-batch samples selected by its peer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Information diagrams for the relationship among three random variables. Each circle represents the entropy of the corresponding random variable. The intersection between two circles is the mutual information. Left: if Y ? X ? Z holds. Middle: the general case. Right: an over-fitting case where I(Y ; Z) is large but I(X; Z) is small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>The mutual information of different channels of models trained on the classification task on CIFAR-10<ref type="bibr" target="#b51">[52]</ref>. The result is the average after 3 runs with ?95% confidence interval. We compare the model trained with and without Nested Dropout. Baseline refers to the one without Nested Dropout.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Comparisons of regression between standard MLP and MLP trained with Nested Dropout [3] and Dropout [2] on a synthetic noisy label dataset. (a) MLP with standard training; (b-d) predictions of MLP+Nested using only the first k ? {1, 10, 100} channels; (e-h) predictions of MLP+Dropout with drop ratio p drop ? {0.9, 0.7, 0.5, 0.3}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>T 1 k</head><label>1</label><figDesc>=Z k , ? k = 1, . . . ? i T i k , k &gt; i , ? i = 1, . . . , K ? 1, ? k = 1, . . . , K where ? i = b ? Bernoulli (? i ) , ? i?1 = 1 , 0 , o.w.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Z 1 =Z 1 , Z k = k? 1 j=1? 1 j=1 ? j ( 1 ?</head><label>11111</label><figDesc>j Z k , ?k = 2, . . . , K, which suggests thatP(Z = [Z 1 , 0, . . . , 0]) = P(Z 1 =Z 1 , Z 2 = 0) = P(? 1 = 0) = 1 ? ? 1 ,and for k = 2, . . . , K ? 1, we haveP(Z = [Z 1:k , 0, . . . , 0]) = P(Z k =Z k , Z k+1 = 0) = P(? 1 = 1, . . . , ? k?1 = 1, ? k = 0) = k?? k ),and also have P(Z =Z) = P(Z K?1 =Z K?1 , Z K =Z K ) = P(? 1 = 1, . . . , ? K?1 = 1) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>the equivalence holds if ? 1 = 1 ? p 1 ,</head><label>111</label><figDesc>for k = 2, . . . , K ? 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>I</head><label></label><figDesc>(A; B) ? I(A; C). The equality holds if and only if I(A; B | C) = 0.Combining the above data processing inequality with both<ref type="bibr" target="#b14">(15)</ref> and(21), we haveI(Y ; Z k+1 ) = I(Y ; T k+1 k+1 ) ? I(Y ; T k k+1 ) = I(Y ; T k k ) = I(Y ; Z k ). Note that I(Y ; T k k+1 ) = I(Y ; T k k )is due to the fact that ? j = 1 , 0, o.w. and I(Y ;Z k+1 ) = I(Y ;Z k ). This completes the proof.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>E</head><label></label><figDesc>p(?) E q(z|x) D KL p(y|x, ?) q co (y|x, z) = D KL p(y|x) q co (y|x)+ E q(z|x) D KL q co (y|x) q co (y|x, z) + I(Y ; ?)where we have p(y|x) = E p(?) p(y|x, ?), and als?q co (y|x) ? exp E q(z|x) log q co (y|x, z) ? exp E q(z|x) q t (y|x) log q(y|z) = exp q t (y|x)E q(z|x) log q(y|z) ? exp[q t (y|x)]q(y|x). Sinceq co (y|x) ? exp[q t (y|x)]q(y|x), then we haveq co (y|x) followsq co (y|x) = exp[q t (y|x)]q(y|x)/C 2 (x) with C 2 (x) := Y exp[q t (y|x)]q(y|x) dy.The proof follows if we plug in the above decomposition when computing E p(x,y) L s q (X, Y ) . Note that both H(Y |X, ?) and I(Y ; ?) are constant with respect to the model q(y|x).(i) For the bias term, we haveD KL p(y|x) q co (y|x) = Y p(y|x) log p(y|x) q co (y|x) dy = Y p(y|x) log C 2 (x) ? p(y|x) exp[q t (y|x)]q(y|x) dy = Y p(y|x) log ?(y|x) ? p(y|x) q(y|x) dy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>E=</head><label></label><figDesc>q(z|x) D KL q co (y|x) q co (y|x, z)= E q(z|x) Yq co (y|x) logq co (y|x) q co (y|x, z) dy = E q(z|x) Yq (y|x) ?(y|x) logq (y|x)/?(y|x) exp[q t (y|x) log q(y|z)]/C 1 (x, z) dy ? E q(z|x) Yq (y|x) ?(y|x) log C 1 (x, z) ?(y|x)q (y|x) q(y|z) dywhere the last inequality is due to exp[q t (y|x) log q(y|z)] ? q(y|z) as the output of the teacher network is derived by soft-max layer with 0 ? q t (y|x) ? 1. Similarly, C 1 (x, z) := Y exp[q t (y|x) log q(y|z)] dy ? Y exp[log q(y|z)] dy = 1. In these regards, if we further have ?(y|x) ? C 1 (x, z), then E q(z|x) E q(z|x) D KL q(y|x) q(y|z) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Two-stage compression training Input: training data D with size |D|, compression hyper-parameters {? nest , p drop }, two initialized networks h 1 , h 2 , loss L q (8), forget rate ? forget . Ensure: Either train with Dropout (p drop &gt; 0) by (1), or Nested Dropout (? nest</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Test accuracy (%) of state-of-the-art methods under (a) symmetric noise on CIFAR-10<ref type="bibr" target="#b51">[52]</ref> and CIFAR-100<ref type="bibr" target="#b51">[52]</ref>, (b) 40% asymmetric noise on CIFAR-10. All approaches are implemented with PreAct ResNet-18<ref type="bibr" target="#b54">[55]</ref> architecture.</figDesc><table><row><cell>Methods / Noise Ratio (%)</cell><cell>20%</cell><cell cols="2">CIFAR-10 50% 80%</cell><cell>90%</cell><cell>20%</cell><cell cols="2">CIFAR-100 50% 80%</cell><cell>90%</cell></row><row><cell>Cross-Entropy [25]</cell><cell>86.8</cell><cell>79.4</cell><cell>62.9</cell><cell>42.7</cell><cell>62.0</cell><cell>46.7</cell><cell>19.9</cell><cell>10.1</cell></row><row><cell>Bootstrap [18]</cell><cell>86.8</cell><cell>79.8</cell><cell>63.3</cell><cell>42.9</cell><cell>62.1</cell><cell>46.6</cell><cell>19.9</cell><cell>10.2</cell></row><row><cell>F-correction [16]</cell><cell>86.8</cell><cell>79.8</cell><cell>63.3</cell><cell>42.9</cell><cell>61.5</cell><cell>46.6</cell><cell>19.9</cell><cell>10.2</cell></row><row><cell>Co-teaching+ [14], [25]</cell><cell>89.5</cell><cell>85.7</cell><cell>67.4</cell><cell>47.9</cell><cell>65.6</cell><cell>51.8</cell><cell>27.9</cell><cell>13.7</cell></row><row><cell>Mixup [56]</cell><cell>95.6</cell><cell>87.1</cell><cell>71.6</cell><cell>52.2</cell><cell>67.8</cell><cell>57.3</cell><cell>30.8</cell><cell>14.6</cell></row><row><cell>PENCIL [21], [25]</cell><cell>92.4</cell><cell>89.1</cell><cell>77.5</cell><cell>58.9</cell><cell>69.4</cell><cell>57.5</cell><cell>31.1</cell><cell>15.3</cell></row><row><cell>MLNT [57], [25]</cell><cell>92.9</cell><cell>89.3</cell><cell>77.4</cell><cell>58.7</cell><cell>68.5</cell><cell>59.2</cell><cell>42.4</cell><cell>19.5</cell></row><row><cell>M-correction [58]</cell><cell>94.0</cell><cell>92.0</cell><cell>86.8</cell><cell>69.1</cell><cell>73.9</cell><cell>66.1</cell><cell>48.2</cell><cell>24.3</cell></row><row><cell>DivideMix [25]</cell><cell>96.1</cell><cell>94.6</cell><cell>93.2</cell><cell>76.0</cell><cell>77.3</cell><cell>74.6</cell><cell>60.2</cell><cell>31.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Nested+Co-teaching</cell><cell>95.3</cell><cell>91.9</cell><cell>78.8</cell><cell>55.0</cell><cell>77.5</cell><cell>66.7</cell><cell>43.0</cell><cell>14.2</cell></row><row><cell>Dropout+Co-teaching</cell><cell>95.0</cell><cell>90.2</cell><cell>78.8</cell><cell>56.0</cell><cell>76.7</cell><cell>67.0</cell><cell>44.1</cell><cell>18.4</cell></row><row><cell></cell><cell></cell><cell cols="3">Pre-Cleaning with DivideMix</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>M-correction</cell><cell>94.5</cell><cell>93.5</cell><cell>92.6</cell><cell>73.5</cell><cell>66.4</cell><cell>63.8</cell><cell>54.0</cell><cell>29.1</cell></row><row><cell>Nested+Co-teaching</cell><cell>96.0</cell><cell>94.9</cell><cell>93.5</cell><cell>78.3</cell><cell>78.3</cell><cell>76.0</cell><cell>63.6</cell><cell>36.1</cell></row><row><cell>Dropout+Co-teaching</cell><cell>96.1</cell><cell>95.0</cell><cell>93.4</cell><cell>78.0</cell><cell>79.5</cell><cell>76.8</cell><cell>63.2</cell><cell>37.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="7">Test accuracy (%) of our Nested+Co-teaching</cell></row><row><cell cols="8">and Dropout+Co-teaching on CIFAR-10 and CIFAR-100 [52]</cell></row><row><cell cols="8">under symmetric noise. Methods based on ImageNet [60]</cell></row><row><cell cols="8">pre-trained models are marked with "". All approaches are</cell></row><row><cell cols="6">implemented with ResNet-18 [48] architecture.</cell><cell></cell><cell></cell></row><row><cell>Methods /</cell><cell>Pre-</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise Ratio</cell><cell>trained</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell><cell>20%</cell><cell>50%</cell><cell>80%</cell></row><row><cell>Nested+</cell><cell></cell><cell>91.7</cell><cell>86.9</cell><cell>53.4</cell><cell>69.0</cell><cell>60.6</cell><cell>28.0</cell></row><row><cell>Co-teaching</cell><cell></cell><cell>92.9</cell><cell>87.8</cell><cell>72.2</cell><cell>71.4</cell><cell>63.3</cell><cell>36.4</cell></row><row><cell>Dropout+</cell><cell></cell><cell>91.9</cell><cell>86.6</cell><cell>59.0</cell><cell>72.4</cell><cell>62.4</cell><cell>33.2</cell></row><row><cell>Co-teaching</cell><cell></cell><cell>91.9</cell><cell>89.7</cell><cell>77.9</cell><cell>74.2</cell><cell>65.0</cell><cell>39.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Average test accuracy (%) with standard deviation (3 runs) of state-of-the-art methods on ANIMAL-10N</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="4">: Test accuracy (%) on Clothing1M [5] with differ-</cell></row><row><cell cols="4">ent backbones: ResNet-18, ResNet-50, EfficientNet-B2 [64].</cell></row><row><cell cols="4">Results with "*" use a balanced subset or a balanced loss.</cell></row><row><cell>Method / Acc. (%)</cell><cell cols="3">Backbones ResNet-18 ResNet-50 EfficientNet-B2</cell></row><row><cell>Cross-Entropy</cell><cell>67.2 [15]</cell><cell>69.2 [25]</cell><cell>69.8</cell></row><row><cell>DivideMix* [25]</cell><cell>-</cell><cell>74.8</cell><cell>-</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell></row><row><cell>Nested*</cell><cell>73.1</cell><cell>73.2</cell><cell>72.5</cell></row><row><cell>Nested+Co-teaching*</cell><cell>74.9</cell><cell>75.0</cell><cell>73.5</cell></row><row><cell>Dropout*</cell><cell>72.5</cell><cell>72.9</cell><cell>72.5</cell></row><row><cell>Dropout+Co-teaching*</cell><cell>74.0</cell><cell>74.0</cell><cell>73.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We appreciate Qinghua Tao for helpful discussions, and also the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning ordered representations with nested dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SELFIE: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bharambe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Web crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from noisy examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="370" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7010" to="7018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with feature-dependent label noise: A progressive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A semi-supervised twostage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recycling: Semi-supervised learning with noisy labels in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandewalle</surname></persName>
		</author>
		<title level="m">Least squares support vector machines. World scientific</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Earlylearning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20" to="331" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting co-teaching with compression regularization for label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2688" to="2692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08199</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A survey of label-noise representation learning: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04406</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on deep learning with noisy labels: How to train your model when you cannot trust on the annotations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contrast to divide: Self-supervised pre-training for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1657" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Augmentation strategies for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8022" to="8031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17" to="044" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Support vector machines with the ramp loss and the hard margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
	<note>Neural processing letters</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clusterability as an alternative to anchor points when learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning sparse networks using targeted dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kamalakara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13678</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unpacking information bottlenecks: Unifying information-theoretic objectives in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12537</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Exchangeability and related topics,&quot; in ?-school of ?t? de Probabilit? of Saint-Flour XIII -1983</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Aldous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">De finetti&apos;s theorem on exchangeable variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="188" to="189" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A bias-variance decomposition for bayesian deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brofos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Lederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 4th Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
