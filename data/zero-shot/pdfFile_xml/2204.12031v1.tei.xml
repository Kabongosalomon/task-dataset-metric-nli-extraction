<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boundary Smoothing for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enwei</forename><surname>Zhu</surname></persName>
							<email>zhuenwei@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HwaMei Hospital</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Ningbo Institute of Life and Health Industry</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Li</surname></persName>
							<email>lijinpeng@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HwaMei Hospital</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Ningbo Institute of Life and Health Industry</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boundary Smoothing for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity probabilities from annotated spans to the surrounding ones. Built on a simple but strong baseline, our model achieves results better than or competitive with previous stateof-the-art systems on eight well-known NER benchmarks. 1 Further empirical analysis suggests that boundary smoothing effectively mitigates over-confidence, improves model calibration, and brings flatter neural minima and more smoothed loss landscapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is one of the fundamental natural language processing (NLP) tasks with extensive investigations. As a common setting, an entity is regarded as correctly recognized only if its type and two boundaries exactly match the ground truth.</p><p>The annotation of boundaries is more ambiguous, error-prone, and raises more inconsistencies than entity types. For example, the CoNLL 2003 task contains four entity types (i.e., person, location, organization, miscellaneous), which are easy to distinguish between. However, the boundaries of a entity mention could be ambiguous, because of the "boundary words" (e.g., articles or modifiers). Considerable efforts are required to specify the "gold standard practice" case by case. <ref type="table" target="#tab_0">Table 1</ref> presents some examples from CoNLL 2003 An- notation Guidelines. <ref type="bibr">2</ref> In addition, some studies have also reported that incorrect boundary is a major source of entity recognition error <ref type="bibr" target="#b9">Eberts and Ulges, 2020)</ref>.</p><p>Recently, span-based models have gained much popularity in NER studies, and achieved state-ofthe-art (SOTA) results <ref type="bibr" target="#b9">(Eberts and Ulges, 2020;</ref><ref type="bibr" target="#b42">Yu et al., 2020;</ref><ref type="bibr" target="#b19">Li et al., 2021)</ref>. This approach typically enumerates all candidate spans and classifies them into entity types (including a "non-entity" type); the annotated spans are scarce and assigned with full probability to be an entity, whereas all other spans are assigned with zero probability. This creates noticeable sharpness between the classification targets of adjacent spans, and may thus plague the trainability of neural networks. In addition, empirical evidence shows that these models easily encounter the over-confidence issue, i.e., the confidence of a predicted entity is much higher than its correctness probability. This is a manifestation of miscalibration <ref type="bibr" target="#b10">(Guo et al., 2017)</ref>.</p><p>Inspired by label smoothing <ref type="bibr" target="#b37">(Szegedy et al., 2016;</ref><ref type="bibr" target="#b29">M?ller et al., 2019)</ref>, we propose boundary smoothing as a regularization technique for span-based neural NER models. By explicitly reallocating entity probabilities from annotated spans to the surrounding ones, boundary smoothing can effectively mitigate over-confidence, and result in consistently better performance.</p><p>Specifically, our baseline employs the contextualized embeddings from a pretrained Transformer of a base size (768 hidden size, 12 layers), and the biaffine decoder proposed by <ref type="bibr" target="#b42">Yu et al. (2020)</ref>. With boundary smoothing, our model outperforms previous SOTA on four English NER datasets <ref type="bibr">(CoNLL 2003</ref><ref type="bibr">, OntoNotes 5, ACE 2004</ref><ref type="bibr">and ACE 2005</ref> and two Chinese datasets (Weibo NER and Resume NER), and achieves competitive results on other two Chinese datasets (OntoNotes 4 and MSRA). Such extensive experiments support the effectiveness and robustness of our proposed technique.</p><p>In addition, we show that boundary smoothing can help the trained NER models to preserve calibration, such that the produced confidences can better represent the precision rate of a predicted entity. This corresponds to the effect of label smoothing on the image classification task <ref type="bibr" target="#b29">(M?ller et al., 2019)</ref>. Further, visualization results qualitatively suggest that boundary smoothing can lead to flatter solutions and more smoothed loss landscapes, which are typically associated with better generalization and trainability <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b20">Li et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Named Entity Recognition The mainstream NER systems are designed to recognize flat entities and based on a sequence tagging framework. <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> introduced the linear-chain conditional random field (CRF) into neural networkbased sequence tagging models, which can explicitly encode the transition likelihoods between adjacent tags. Many researchers followed this work, and employed LSTM as the encoder. In addition, character-level representations are typically used for English tasks <ref type="bibr" target="#b13">(Huang et al., 2015;</ref><ref type="bibr" target="#b17">Lample et al., 2016;</ref><ref type="bibr" target="#b28">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b3">Chiu and Nichols, 2016)</ref>, whereas lexicon information is helpful for Chinese NER <ref type="bibr" target="#b43">(Zhang and Yang, 2018;</ref><ref type="bibr" target="#b27">Ma et al., 2020;</ref><ref type="bibr" target="#b21">Li et al., 2020a)</ref>.</p><p>Nested NER allows a token to belong to multiple entities, which conflicts with the plain sequence tagging framework. <ref type="bibr" target="#b15">Ju et al. (2018)</ref> proposed to use stacked LSTM-CRFs to predict from inner to outer entities. <ref type="bibr" target="#b36">Strakov? et al. (2019)</ref> concatenated the BILOU tags for each token inside the nested entities, which allows the LSTM-CRF to work as for flat entities. <ref type="bibr" target="#b22">Li et al. (2020b)</ref> reformulated nested NER as a machine reading comprehension task. <ref type="bibr" target="#b35">Shen et al. (2021)</ref> proposed to recognize nested entities by the two-stage object detection method widely used in computer vision.</p><p>Recent years, a body of literature emerged on span-based models, which were compatible with both flat and nested entities, and achieved SOTA performance <ref type="bibr" target="#b9">(Eberts and Ulges, 2020;</ref><ref type="bibr" target="#b42">Yu et al., 2020;</ref><ref type="bibr" target="#b19">Li et al., 2021)</ref>. These models typically enumerate all possible candidate text spans and then classify each span into entity types. In this work, the biaffine model <ref type="bibr" target="#b42">(Yu et al., 2020)</ref> is chosen and re-implemented with slight modifications as our baseline, because of its high performance and compatibility with boundary smoothing.</p><p>In addition, pretrained language models, also known as contextualized embeddings, were also widely introduced to NER models, and significantly boosted the model performance <ref type="bibr" target="#b33">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>. They are used in our baseline by default. <ref type="bibr" target="#b37">Szegedy et al. (2016)</ref> proposed the label smoothing as a regularization technique to improve the accuracy of the Inception networks on ImageNet. By explicitly assigning a small probability to non-ground-truth labels, label smoothing can prevent the models from becoming too confident about the predictions, and thus improve generalization. It turned out to be a useful alternative to the standard cross entropy loss, and has been widely adopted to fight against the over-confidence <ref type="bibr" target="#b44">(Zoph et al., 2018;</ref><ref type="bibr" target="#b4">Chorowski and Jaitly, 2017;</ref><ref type="bibr" target="#b39">Vaswani et al., 2017)</ref>, improve the model calibration <ref type="bibr" target="#b29">(M?ller et al., 2019)</ref>, and denoise incorrect labels <ref type="bibr" target="#b26">(Lukasik et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing</head><p>Our proposed boundary smoothing applies the smoothing technique to entity boundaries, rather than labels. This is driven by the observation that entity boundaries are more ambiguous and inconsistent to annotate in NER engineering. <ref type="bibr">3</ref> To the best of our knowledge, this study is the first that focuses on the effect of smoothing regularization on NER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Biaffine Decoder</head><p>A neural network-based NER model typically encodes the input tokens to a sequence of representations x = x 1 , x 2 , . . . , x T of length T , and then decodes these representations to task outputs, i.e., a list of entities specified by types and boundaries.</p><p>We follow <ref type="bibr" target="#b42">Yu et al. (2020)</ref> and use the biaffine decoder. Specifically, the representations x are separately affined by two feedforward networks, resulting in two representations h s ? R T ?d and h e ? R T ?d , which correspond to the start and end positions of spans. For c entity types (a "nonentity" type included), given a span starting at the i-th token and ending at the j-th token, a scoring vector r ij ? R c can be computed as: <ref type="bibr">2d+dw)</ref> and b ? R c are learnable parameters. r ij is then fed into a softmax layer:</p><formula xml:id="formula_0">r ij = (h s i ) T U h e j + W (h s i ? h e j ? w j?i ) + b, (1) where w j?i ? R dw is the (j ? i)-th width em- bedding from a dedicated learnable matrix; U ? R d?c?d , W ? R c?(</formula><formula xml:id="formula_1">y ij = softmax(r ij ),<label>(2)</label></formula><p>which yields the predicted probabilities over all entity types. The ground truth y ij ? R c is an one-hot encoded vector, with value being 1 if the index corresponds with the annotated entity type, and 0 otherwise. Thus, the model can be optimized by the standard cross entropy loss for all candidate spans:</p><formula xml:id="formula_2">L CE = ? 0?i?j&lt;T y T ij log(? ij ).<label>(3)</label></formula><p>In the inference time, the spans predicted to be "non-entity" are first discarded, and the remaining ones are ranked by their predictive confidences. Spans with lower confidences would also be discarded if they clash with the boundaries of spans with higher confidences. Refer to <ref type="bibr" target="#b42">Yu et al. (2020)</ref> for more details. <ref type="figure" target="#fig_0">Figure 1a</ref> visualizes the ground truth y ij for an example sentence with two annotated entities. The valid candidate spans cover the upper triangular area of the matrix. In existing NER models, the annotated boundaries are considered to be absolutely reliable. Hence, each annotated span is assigned The example sentence has ten tokens and two entities of spans (1, 2) and (3, 7), colored in red and blue, respectively. The first subfigure presents the entity recognition targets of hard boundaries. The second subfigure presents the corresponding targets of smoothed boundaries, where the span (1, 2) is smoothed by a size of 1, and the span (3, 7) is smoothed by a size of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Boundary Smoothing</head><p>with the full probability to be an entity, whereas all unannotated spans are assigned with zero probability. We refer to this probability allocation as hard boundary, which is, however, probably not the best choice. As aforementioned, the entity boundaries may be ambiguous and inconsistent, so the spans surrounding an annotated one deserve a small probability to be an entity. <ref type="figure" target="#fig_0">Figure 1b</ref> visualizes? ij , the boundary smoothing version of y ij . Specifically, given an annotated entity, a portion of probability is assigned to its surrounding spans, and the remaining probability 1 ? is assigned to the originally annotated span. With smoothing size D, all the spans with Manhattan distance d (d ? D) to the annotated entity equally share probability /D. After such entity probability re-allocation, any remaining probability of a span is assigned to be "non-entity". We refer to this as smoothed boundary.</p><p>Thus, the biaffine model can be optimized by the boundary-smoothing regularized cross entropy loss:</p><formula xml:id="formula_3">L BS = ? 0?i?j&lt;T? T ij log(? ij ).<label>(4)</label></formula><p>Empirically, the positive samples (i.e., groundtruth entities) are sparsely distributed over the candidate spans. For example, the CoNLL 2003 dataset has about 35 thousand entities, which represent only 0.93% in the 3.78 million candidate spans. By explicitly assigning probability to surrounding spans, boundary smoothing prevents the model from concentrating all probability mass on the scarce positive samples. This intuitively helps alleviate over-confidence.</p><p>In addition, hard boundary presents noticeable sharpness between the classification targets of positive spans and surrounding ones, although they share similar contextualized representations.</p><p>Smoothed boundary provides more continuous targets across spans, which are conceptually more compatible with the inductive bias of neural networks that prefers continuous solutions <ref type="bibr" target="#b12">(Hornik et al., 1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets We use four English NER datasets: CoNLL 2003 <ref type="bibr" target="#b38">(Tjong Kim Sang and Veenstra, 1999)</ref>, OntoNotes 5 4 , ACE 2004 5 and ACE 2005 6 ; and four Chinese NER datasets: OntoNotes 4 7 , MSRA <ref type="bibr" target="#b18">(Levow, 2006)</ref>, Weibo NER <ref type="bibr" target="#b31">(Peng and Dredze, 2015)</ref> and Resume NER <ref type="bibr" target="#b43">(Zhang and Yang, 2018</ref>). Among them, ACE 2004 and ACE 2005 are nested NER tasks, and the others are flat tasks.</p><p>Hyperparameters For English corpora, we use RoBERTa  followed by a BiL-STM layer to produce the contextualized representations. For Chinese, we choose the BERT pretrained with whole word masking <ref type="bibr" target="#b7">(Cui et al., 2019</ref>  <ref type="bibr" target="#b25">Lu and Roth (2015)</ref>. 7 https://catalog.ldc.upenn.edu/ LDC2011T03; Data splits follow <ref type="bibr" target="#b1">Che et al. (2013)</ref>.</p><p>The BiLSTM has one layer and 200 hidden size with dropout rate of 0.5. The biaffine decoder follows <ref type="bibr" target="#b42">Yu et al. (2020)</ref>, with the affine layers of hidden size 150 and dropout rate 0.2. We additionally introduce a span width embedding of size 25. Note that the pretrained language models are all of the base size (768 hidden size, 12 layers), and the model is free of any additional auxiliary embeddings; this configuration is relatively simple, compared with those in related work.</p><p>The boundary smoothing parameter is selected</p><formula xml:id="formula_4">in {0.1, 0.2, 0.3}; smoothing size D is selected in {1, 2}.</formula><p>All the models are trained by the AdamW optimizer <ref type="bibr" target="#b24">(Loshchilov and Hutter, 2018)</ref> with a gradient clipping at L2-norm of 5.0 <ref type="bibr" target="#b30">(Pascanu et al., 2013)</ref>. The models are trained for 50 epochs with batch size of 48. The learning rate is searched between 1e-3 and 3e-3 on the randomly initialized weights, and between 8e-6 and 3e-5 on the pretrained weights; a scheduler of linear warmup in the first 20% steps followed by linear decay is applied.</p><p>Evaluation A predicted entity is considered correct if its type and boundaries exactly match the ground truth. Hyperparameters are tuned according to the F 1 scores on the development set, and the evaluation metrics (precision, recall, F 1 score) are reported on the testing set.   discourages over-confidence when recognizing entities, which implicitly leads the model to establish a more critical threshold to admit entities. Given the use of well pretrained language models, most of the performance gains are relatively marginal. However, boundary smoothing can work effectively and consistently for different languages and datasets. In addition, it is easy to implement and integrate into any span-based neural NER models, with almost no side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We perform ablation studies on CoNLL <ref type="formula" target="#formula_1">2003</ref>   <ref type="table" target="#tab_6">Table 4</ref>. Most combinations of the two hyperparameters can achieve higher F 1 scores than the baseline, which suggests the robustness of boundary smoothing. On the other hand, the best smoothing parameters are different across datasets, which are probably related to the languages/domains of the text, the entity types, and the annotation scheme (e.g., flat or nested NER). Hence, if the best performance is desired for a new NER task in practice, hyperparameter tuning would be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Smoothing</head><p>We replace boundary smoothing with label smoothing in the span classifier. Label smoothing cannot improve, or may even impair the performance of the model, compared with the baseline (see <ref type="table" target="#tab_6">Table 4</ref>). As aforementioned, we hypothesize that the semantic differences between the typical entity types are quite clear, so it is ineffective to smooth between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Language Models</head><p>We test if the performance gain by boundary smoothing is robust to different baselines. For English datasets, we use BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> of the base and large sizes, and RoBERTa    2020) of the base and large sizes, and boundary smoothing still performs positively and consistently, with an improvement of 0.2-0.3 percentage F 1 scores on Resume NER (see <ref type="table" target="#tab_8">Table 5</ref>).</p><p>It is noteworthy that boundary smoothing achieves performance gains roughly comparable to the gains by switching the pretrained language model from the base size to the large size. This suggests that the effect of boundary smoothing is quite considerable, although the performance improvements seem small in magnitude.</p><p>In addition, our results show that RoBERTa substantially outperforms the original BERT on English NER. This is probably because that (1) RoBERTa is trained on much more data; and (2) RoBERTa focuses on the token-level task (i.e., masked language modeling) by removing the sequence-level objective (i.e., next sentence prediction), hence, it is particularly suitable for withinsequence downstream tasks, e.g., NER. This is also the reason why we choose RoBERTa for our baseline.</p><p>BiLSTM Layer We remove the BiLSTM layer, directly feeding the output of pretrained language model into the biaffine decoder. The results show that this does not change the positive effect of boundary smoothing (see <ref type="table" target="#tab_8">Table 5</ref>). In addition, absence of the BiLSTM layer will result in drops of the F 1 scores by about 0.3, 0.5 and 0.1 percentages on the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Further In-Depth Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Over-Confidence and Entity Calibration</head><p>The model performance (evaluated by, e.g., accuracy or F 1 score) is certainly important. However, the confidences of model predictions are also of interest in many applications. For example, when it requires the predicted entities to be highly reliable (i.e., precision is of more priority than recall), we may filter out the entities with confidences lower than a specific threshold.</p><p>However, <ref type="bibr" target="#b10">Guo et al. (2017)</ref> have indicated that modern neural networks are poorly calibrated, and typically over-confident with their predictions. By calibration, they mean the extent to which the prediction confidences produced by a model can represent the true correctness probability. We find neural NER models also easy to become miscalibrated and over-confident. We observe that, with the standard cross entropy loss, both the development loss and F 1 score increase in the later training stage, which goes against the common perception that the loss and F 1 score should change in the opposite directions. This phenomenon is similar to the disconnect between negative likelihood and accuracy in image classification described by <ref type="bibr" target="#b10">Guo et al. (2017)</ref>. We suppose that the model becomes over-confident with its predictions, including the incorrect ones, which contributes to the increase of loss (see Appendix A for more details).</p><p>To formally investigate the over-confidence issue, we plot the reliability diagrams and calculate expected calibration error (ECE). In brief, for an NER model, we group all the predicted entities by the associated confidences into ten bins, and then calculate the precision rate for each bin. If the model is well calibrated, the precision rate should be close to the confidence level for each bin (see Appendix B for more details). <ref type="figure" target="#fig_1">Figure 2</ref> compares the reliability diagrams and ECEs between models with different smoothness on CoNLL 2003 and OntoNotes 5. For the baseline model ( = 0), the precision rates are much lower than corresponding confidence levels, suggesting significant over-confidence. By introducing boundary smoothing and increasing the smoothness , the over-confidence is gradually mitigated, and shifted to under-confidence ( = 0.3). In general, the model presents best reliability diagrams when is 0.1 or 0. to 0.013 and 0.034.</p><p>In conclusion, boundary smoothing can prevent the model from becoming over-confident with the predicted entities, and result in better calibration. In addition, as mentioned previously, spans with lower confidences are discarded if they clash with those of higher confidences when decoding. With the better calibration, the model can obtain a very marginal but consistent increase in the F 1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loss Landscape Visualization</head><p>How does boundary smoothing improve the model performance? We originally conjectured that boundary smoothing can de-noise the inconsistently annotated entity boundaries <ref type="bibr" target="#b26">(Lukasik et al., 2020)</ref>, but failed to find enough evidence -the performance improvement did not significantly increase when we injected boundary noises into the training data. <ref type="bibr">8</ref> As aforementioned, positive samples are very sparse among the candidate spans. Without boundary smoothing, the annotated spans are regarded to be entities with full probability, whereas all other spans are assigned with zero probability. This creates noticeable sharpness between the targets of the annotated spans and surrounding ones, although their neural representations are similar. Boundary smoothing re-allocates the entity probabilities across contiguous spans, which mitigates the sharpness and results in more continuous targets. Conceptually, such targets are more compatible with the inductive bias of neural networks that prefers continuous solutions <ref type="bibr" target="#b12">(Hornik et al., 1989)</ref>. <ref type="bibr" target="#b20">Li et al. (2018)</ref> have shown that residual connections and well-tuned hyperparameters (e.g., learning rate, batch size) can produce flatter minima and less chaotic loss landscapes, which account for the better generalization and trainability. Their findings provide important insights into the geometric properties of non-convex neural loss functions. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the loss landscapes for models with different smoothness on CoNLL 2003 and OntoNotes 5, following <ref type="bibr" target="#b20">Li et al. (2018)</ref>. In short, for a trained model, a direction of the parameters is randomly sampled, normalized and fixed, and the loss landscape is computed by sampling over this direction (refer to Appendix C for more details).</p><p>The visualization results qualitatively show that, the solutions found by the standard cross entropy are relatively sharp, whereas boundary smoothing can help arrive at flatter minima. As many theoretical studies regard the flatness as a promising predictor for model generalization <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b14">Jiang et al., 2019)</ref>, this result may explain why boundary smoothing can improve the model performance. In addition, boundary smoothing is associated with more smoothed landscapes -the surrounding local minima are small, shallow, and thus easy for the optimizer to escape. Intuitively, such geometric property suggests that the underlying loss functions are easier to train <ref type="bibr" target="#b20">(Li et al., 2018)</ref>.</p><p>We believe that the sharpness in the span-based NER targets is probably the reason for the sharp and chaotic loss landscape. Boundary smoothing can effectively mitigate the sharpness, and result in loss landscapes of better generalization and trainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this study, we propose boundary smoothing as a regularization technique for span-based neural NER models. Boundary smoothing re-assigns entity probabilities from annotated spans to the surrounding ones. It can be easily integrated into any span-based neural NER systems, but consistently bring improved performance. Built on a simple but strong baseline (a base-sized pretrained language model followed by a BiLSTM layer, and the biaffine decoder), our model achieves SOTA results on eight well-known NER benchmarks, covering English and Chinese, flat and nested NER tasks.</p><p>In addition, experimental results show that boundary smoothing leads to less over-confidence, better model calibration, flatter neural minima and more smoothed loss landscapes. These properties plausibly explain the performance improvement. Our findings shed light on the effects of smoothing regularization technique in the NER task.</p><p>As discussed, boundary smoothing typically increases the overall F 1 score at the risk of a slight drop in the recall rate; hence, one may be careful to use it for recall-sensitive applications. Future work will apply boundary smoothing to more variants of span-based NER models, and investigate its effect in a broader range of information extraction tasks.</p><p>For most machine learning tasks, the desired metric (e.g., accuracy or F 1 score) is non-differentiable and thus cannot be optimized via back-propagation. The loss, on the other hand, is a designed differentiable proxy such that minimizing it can increase the original metric.</p><p>However, as illustrated in <ref type="figure" target="#fig_3">Figure 4a</ref>, when training an NER model by the standard cross entropy loss, although the development F 1 score keeps increasing throughout, the development loss also increases in the later stage (e.g., after ten epochs) of the training process. <ref type="bibr" target="#b10">Guo et al. (2017)</ref> describe this phenomenon as a disconnect -the neural network overfits to the loss without overfitting to the metric. They regard this as indirect evidence for miscalibration.</p><p>One plausible explanation is that in the later training stage, the model becomes too confident with its predicted outcomes, including both the correct and incorrect ones. Therefore, although slightly more spans are correctly classified on the development set (as the F 1 score increases), a small portion of incorrectly classified spans is assigned with much more confidence and contributes to the increase of loss. <ref type="figure" target="#fig_3">Figure 4b</ref> presents the curves for boundary smoothing loss. The development loss decreases throughout the training process, opposite to the increasing F 1 score. This result suggests that boundary smoothing can help mitigate over-confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Reliability Diagrams and Expected Calibration Error</head><p>We generally follow <ref type="bibr" target="#b10">Guo et al. (2017)</ref>'s approach to plot reliability diagrams and calculate expected calibration error (ECE). Given an NER dataset and a model trained on it, denote the gold and predicted entity sets as E and?, respectively; the model produces a confidencep e for each entity e ??. With K confidence interval bins, the predicted entities are grouped such that those with confidences falling into the k-th bin constitute a subset: E k = e | e ??,p e ? k ? 1 K , k K .</p><p>The precision rate (equivalent to the accuracy with regard to a predicted set) of k-th group? k is:</p><formula xml:id="formula_5">Prec k = |? k ? E| |? k | ,</formula><p>and the corresponding average confidence is:</p><formula xml:id="formula_6">Conf k = e?? kp e |? k | .</formula><p>The reliability diagrams plot Prec k against Conf k for k = 1, 2, . . . , K. ECE is estimated by the weighted average of absolute difference between Prec k and Conf k :</p><formula xml:id="formula_7">ECE = K k=1 |? k | |?| ? Prec k ? Conf k</formula><p>By definition, a perfectly calibrated model will have Prec k = Conf k for k = 1, 2, . . . , K. In this case, the reliability diagrams should lie along the identity line, and ECE equals to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Loss Landscape Visualization</head><p>We generally follow <ref type="bibr" target="#b20">Li et al. (2018)</ref>'s approach to visualize the loss landscape. Given a trained model of parameters ? , we sample a random direction ? from a normal distribution, and rescale it by:</p><formula xml:id="formula_8">? i ? ? i ? i ? i ,</formula><p>where ? i is the i-th weight of ?. 9 On a data set/split D, the loss landscape plots the function:</p><formula xml:id="formula_9">f (?) = L(D; ? + ??),</formula><p>where L(D; ?) is the average loss value (in the evaluation mode) on D if the model takes parameters of ?. In practice, we evenly sample 51 points in the interval [?1, 1] for ?, and plot the loss values against ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of hard and smoothed boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>2. In addition, the ECEs of the baseline model are 0.072 and 0.063 on CoNLL 2003 and OntoNotes 5, respectively; with of 0.1, the ECEs are reduced &amp;RQILGHQFH Reliability diagram of recognized entities on CoNLL 2003 and OntoNotes 5. Results are computed on ten bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of loss landscapes on CoNLL 2003 and OntoNotes 5. Training, development and testing losses are in orange, green and red, respectively. CE and BS mean cross entropy and boundary smoothing, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Training/development losses and F 1 scores of models with cross entropy loss and boundary smoothing loss on CoNLL 2003. Both the cross entropy loss and corresponding F 1 score on the development set experience an ascending trend after about ten epochs, suggesting the existence of over-confidence. However, the boundary smoothing loss on the development set keeps decreasing through the whole training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of CoNLL 2003 Annotation Guidelines and potential alternatives. The gold annotations are marked in blue [*], whereas the alternative annotations are in red [*].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>https://catalog.ldc.upenn.edu/ LDC2013T19; Data splits follow<ref type="bibr" target="#b34">Pradhan et al. (2013)</ref>. Data splits follow<ref type="bibr" target="#b25">Lu and Roth (2015)</ref>.</figDesc><table><row><cell>).</cell></row></table><note>45 https://catalog.ldc.upenn.edu/ LDC2005T09;6 https://catalog.ldc.upenn.edu/ LDC2006T06; Data splits follow</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>presents the evaluation results on four English datasets, in which CoNLL 2003 and OntoNotes 5 are flat NER corpora, whereas ACE 2004 and ACE 2005 contains a high proportion of nested entities. Compared with previous SOTA systems, our simple baseline (RoBERTa-base + BiLSTM + Biaffine) achieves on-par or slightly inferior performance. Provided the strong baseline, our experiments show that boundary smoothing can effectively and consistently boost the F 1 score of entity recognition across different datasets. With the help of boundary smoothing, our model outperforms the best of the previous SOTA systems by a magnitude from 0.2 to 0.5 percentages.Table 3presents the results on four Chinese datasets, which are all flat NER corpora. Again, boundary smoothing consistently improves model performance against the baseline (BERT-basewwm + BiLSTM + Biaffine) across all datasets. In addition, our model outperforms previous SOTA</figDesc><table><row><cell cols="2">CoNLL 2003</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell>Lample et al. (2016)</cell><cell>-</cell><cell>-</cell><cell>90.94</cell></row><row><cell cols="4">Chiu and Nichols (2016) ? 91.39 91.85 91.62</cell></row><row><cell>Peters et al. (2018)</cell><cell>-</cell><cell>-</cell><cell>92.22</cell></row><row><cell>Akbik et al. (2018) ?</cell><cell>-</cell><cell>-</cell><cell>93.07</cell></row><row><cell>Devlin et al. (2019)</cell><cell>-</cell><cell>-</cell><cell>92.8</cell></row><row><cell>Strakov? et al. (2019) ?</cell><cell>-</cell><cell>-</cell><cell>93.38</cell></row><row><cell>Wang et al. (2019) ?</cell><cell>-</cell><cell>-</cell><cell>93.43</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">92.33 94.61 93.04</cell></row><row><cell>Yu et al. (2020) ?</cell><cell>93.7</cell><cell>93.3</cell><cell>93.5</cell></row><row><cell>Baseline</cell><cell cols="3">92.93 94.03 93.48</cell></row><row><cell>Baseline + BS</cell><cell cols="3">93.61 93.68 93.65</cell></row><row><cell cols="2">OntoNotes 5</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell>Chiu and Nichols (2016)</cell><cell cols="3">86.04 86.53 86.28</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">92.98 89.95 91.11</cell></row><row><cell>Yu et al. (2020)</cell><cell>91.1</cell><cell>91.5</cell><cell>91.3</cell></row><row><cell>Baseline</cell><cell cols="3">90.31 92.13 91.21</cell></row><row><cell>Baseline + BS</cell><cell cols="3">91.75 91.74 91.74</cell></row><row><cell cols="2">ACE 2004</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="2">Katiyar and Cardie (2018) 73.6</cell><cell>71.8</cell><cell>72.7</cell></row><row><cell>Strakov? et al. (2019) ?</cell><cell>-</cell><cell>-</cell><cell>84.40</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">85.05 86.32 85.98</cell></row><row><cell>Yu et al. (2020)</cell><cell>87.3</cell><cell>86.0</cell><cell>86.7</cell></row><row><cell>Shen et al. (2021)</cell><cell cols="3">87.44 87.38 87.41</cell></row><row><cell>Baseline</cell><cell cols="3">86.67 88.42 87.54</cell></row><row><cell>Baseline + BS</cell><cell cols="3">88.43 87.53 87.98</cell></row><row><cell cols="2">ACE 2005</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="2">Katiyar and Cardie (2018) 70.6</cell><cell>70.4</cell><cell>70.5</cell></row><row><cell>Strakov? et al. (2019) ?</cell><cell>-</cell><cell>-</cell><cell>84.33</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">87.16 86.59 86.88</cell></row><row><cell>Yu et al. (2020)</cell><cell>85.2</cell><cell>85.6</cell><cell>85.4</cell></row><row><cell>Shen et al. (2021)</cell><cell cols="3">86.09 87.27 86.67</cell></row><row><cell>Baseline</cell><cell cols="3">84.29 88.97 86.56</cell></row><row><cell>Baseline + BS</cell><cell cols="3">86.25 88.07 87.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of English named entity recognition. BS means boundary smoothing. ? means that the model is trained with both the training and development splits. by 2.16 and 0.55 percentages on Weibo and Resume NER datasets, and achieves comparable F 1 scores on OntoNotes 4 and MSRA. Note that almost all previous systems solve these tasks within a sequence tagging framework; this work adds to the literature by introducing a span-based approach and establishing SOTA results on multiple Chinese NER benchmarks.In five out of the above eight datasets, integrating boundary smoothing significantly increases the precision rate with a slight drop in the recall, resulting in a better overall F 1 score. This is consistent with our expectation, because boundary smoothing</figDesc><table><row><cell cols="2">OntoNotes 4</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="4">Zhang and Yang (2018) 76.35 71.56 73.88</cell></row><row><cell>Ma et al. (2020)</cell><cell cols="3">83.41 82.21 82.81</cell></row><row><cell>Li et al. (2020a)</cell><cell>-</cell><cell>-</cell><cell>81.82</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">82.98 81.25 82.11</cell></row><row><cell>Chen and Kong (2021)</cell><cell cols="3">79.25 80.66 79.95</cell></row><row><cell>Wu et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>82.57</cell></row><row><cell>Baseline</cell><cell cols="3">82.79 81.27 82.03</cell></row><row><cell>Baseline + BS</cell><cell cols="3">81.65 84.03 82.83</cell></row><row><cell cols="2">MSRA</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="4">Zhang and Yang (2018) 93.57 92.79 93.18</cell></row><row><cell>Ma et al. (2020)</cell><cell cols="3">95.75 95.10 95.42</cell></row><row><cell>Li et al. (2020a)</cell><cell>-</cell><cell>-</cell><cell>96.09</cell></row><row><cell>Li et al. (2020b)</cell><cell cols="3">96.18 95.12 95.75</cell></row><row><cell>Wu et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>96.24</cell></row><row><cell>Baseline</cell><cell cols="3">95.82 95.78 95.80</cell></row><row><cell>Baseline + BS</cell><cell cols="3">96.37 96.15 96.26</cell></row><row><cell cols="2">Weibo NER</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell>Zhang and Yang (2018)</cell><cell>-</cell><cell>-</cell><cell>58.79</cell></row><row><cell>Ma et al. (2020)</cell><cell>-</cell><cell>-</cell><cell>70.50</cell></row><row><cell>Li et al. (2020a)</cell><cell>-</cell><cell>-</cell><cell>68.55</cell></row><row><cell>Shen et al. (2021)</cell><cell cols="3">70.11 68.12 69.16</cell></row><row><cell>Chen and Kong (2021)</cell><cell>-</cell><cell>-</cell><cell>70.14</cell></row><row><cell>Wu et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>70.43</cell></row><row><cell>Baseline</cell><cell cols="3">68.65 74.40 71.41</cell></row><row><cell>Baseline + BS</cell><cell cols="3">70.16 75.36 72.66</cell></row><row><cell cols="2">Resume NER</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Prec.</cell><cell>Rec.</cell><cell>F1</cell></row><row><cell cols="4">Zhang and Yang (2018) 94.81 94.11 94.46</cell></row><row><cell>Ma et al. (2020)</cell><cell cols="3">96.08 96.13 96.11</cell></row><row><cell>Li et al. (2020a)</cell><cell>-</cell><cell>-</cell><cell>95.86</cell></row><row><cell>Wu et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>95.98</cell></row><row><cell>Baseline</cell><cell cols="3">95.81 96.87 96.34</cell></row><row><cell>Baseline + BS</cell><cell cols="3">96.63 96.69 96.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Results of Chinese named entity recognition. BS means boundary smoothing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of smoothing parameters. F 1 Boundary Smoothing Parameters We train the model with in {0.1, 0.2, 0.3} and D in {1, 2}; the corresponding results are reported in</figDesc><table><row><cell>scores are reported. BS and LS mean boundary smooth-</cell></row><row><cell>ing and label smoothing, respectively.</cell></row><row><cell>flat/nested and English/Chinese datasets), to eval-</cell></row><row><cell>uate the effects of boundary smoothing parameter</cell></row><row><cell>and D, as well as other components of our NER</cell></row><row><cell>system.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies of model structure. F 1 scores are reported. BS means boundary smoothing.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www-nlpir.nist.gov/related_ projects/muc/proceedings/ne_task.html.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We note that<ref type="bibr" target="#b35">Shen et al. (2021)</ref> also allocate a weight to the non-entity but partially matched spans; however, boundary smoothing additionally regularizes the weight of entity spans, which is intuitively crucial for mitigating over-confidence.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">On the other hand, this cannot rule out the de-noising effect of boundary smoothing, because the synthesized boundary noises are distributed differently from the real noises.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9"><ref type="bibr" target="#b20">Li et al. (2018)</ref> use filter-wise normalization for convolutional networks, whereas our models have no convolutional layers, so we simplify it as weight-wise normalization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Yiyang Liu for his efforts in data processing, and the anonymous reviewers for their insightful comments and feedback. This work is supported by the National Natural Science Foundation of China (No. 62106248), Ningbo Science and Technology Service Industry Demonstration Project (No. 2020F041), and Ningbo Public Service Technology Foundation (No. 2021S152).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing entity boundary detection for better Chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00104</idno>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="523" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for Chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.58</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08101</idno>
		<title level="m">Pre-training with whole word masking for Chinese BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with Transformer pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th European Conference on Artificial Intelligence</title>
		<meeting>the 24th European Conference on Artificial Intelligence<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<title level="m">Bidirectional LSTM-CRF models for sequence tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fantastic generalization measures and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meizhi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1079</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The third international Chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A span-based model for joint overlapped and discontinuous named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.372</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4814" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6391" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FLAT: Chinese NER using flatlattice transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.611</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6836" to="6842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint mention extraction and classification with mention hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simplify the usage of lexicon in Chinese NER</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.528</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5951" to="5960" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Named entity recognition for Chinese social media with jointly trained embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1064</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Locate and label: A two-stage identifier for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.216</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2782" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representing text chunks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn</forename><surname>Veenstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CrossWeigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5154" to="5163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MECT: Multi-metadata embedding based crosstransformer for Chinese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.121</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1529" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.577</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
