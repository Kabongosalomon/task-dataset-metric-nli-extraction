<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Deep Learning Baselines for Ubuntu Corpus Dialogs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
							<email>rudolfkadlec@cz.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague 4</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
							<email>martin.schmid@cz.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague 4</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM</orgName>
								<address>
									<addrLine>Watson V Parku 4</addrLine>
									<settlement>Prague 4</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Deep Learning Baselines for Ubuntu Corpus Dialogs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents results of our experiments for the next utterance ranking on the Ubuntu Dialog Corpus -the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for the next utterance ranking on this dataset. Finally, we discuss our future plans using this corpus. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Ubuntu Dialogue Corpus is the largest freely available multi-turn based dialog corpus <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b0">1</ref> . It was constructed from the Ubuntu chat logs 2 -a collection of logs from Ubuntu-related chat rooms on the Freenode IRC network. Although multiple users can talk at the same time in the chat room, the logs were preprocessed using heuristics to create two-person conversations. The resulting corpus consists of almost one million two-person conversations, where a user seeks help with his/her Ubuntu-related problems (the average length of a dialog is 8 turns, with a minimum of 3 turns). Because of its size, the corpus is well-suited for explorations of deep learning techniques in the context of dialogue systems. In this paper, we introduce our preliminary research and experiments with this corpus, and report state-of-the-art results.</p><p>The rest of the paper continues as follows: 1. we introduce the setup -the data as well as the evaluation of the task; 2. we briefly describe the previously evaluated models; 3. we introduce three different models (one of them being the same as in the previous work); <ref type="bibr" target="#b3">4</ref>. we evaluate these models and experiment with different amount of training data; 5. we conclude and discuss our plans for future works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>In this section we briefly describe the data and evaluation metrics used in <ref type="bibr" target="#b0">[1]</ref>. First, all the collected data was preprocessed by replacing named entities with corresponding tags (name, location, organization, url, path). This is analogical to the prepossessing of <ref type="bibr" target="#b1">[2]</ref> (note that the IT helpdesk dataset used there is not publicly available). Second, these data are further processed to create tuples of (context, response, f lag). The f lag is a Boolean variable indicating whether the response is correct or incorrect.</p><p>To form the training set, each utterance (starting from the third one) is considered as a potential response, while the previous utterances form its context. So a dialogue of length n yields (n ? 2) training examples (context, response, 1) and (n ? 2) training examples (context, response , 0). The negative response response is a randomly sampled utterance from the entire corpus. Finally, the training examples are shuffled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Metric</head><p>A randomly selected 2% of the conversations are used to create a test set. The proposed task is that of the best response selection. The system is presented with n response candidates, and it is asked to rank them. To vary the task's difficulty (and to remedy that some of the sampled candidates flagged as incorrect can very well be correct), the system's ranking is considered correct if the correct response is among the first k candidates. This quantity is denoted as Recall@k. The baselines were reported with (n, k) of (2, 1), (10, 1), (10, 2) and (10, 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>This task can naturally be formulated as a ranking problem which is often tackled by three techniques <ref type="bibr" target="#b2">[3]</ref>: (i) pointwise; (ii) pairwise and (iii) listwise ranking.</p><p>While pairwise and listwise ranking approaches are empirically superior to the pointwise ranking approach, our preliminary experiments use pointwise ranking approach for its simplicity. Note that pointwise method was also used in the original baselines <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pointwise Ranking</head><p>In pointwise ranking, only the context and the response are directly used to compute the probability of the pair. All the pairs are then sorted by their probabilities. We denote the function that outputs the probability of the pair as g(context, response). In our settings, the function g is represented by a neural network (learned using the training data). We describe the details of the network architectures used in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Previous Work</head><p>The pointwise architectures reported in <ref type="bibr" target="#b0">[1]</ref> included (i) TF-IDF, (ii) RNN and (iii) LSTM. In this section, we briefly describe these models. <ref type="figure">Figure 1</ref>: Neural Network Embedding approach. A neural network is used to compute the embedding for the context and the response, denoted as c and r. These are fed through a sigmoid function to compute the pairwise probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TF-IDF</head><p>The motivation here is that the correct response tends to share more words with the context than the incorrect ones. First, the TF-IDF vectors are calculated for the context and each of the candidate responses. Next, the cosine similarity between the context vector and each response vector is used to rank the responses.</p><formula xml:id="formula_0">tf idf context (w) = tf (w, context) ? idf (w, D) (1) tf idf document (w) = tf (w, document) ? idf (w, D) (2) g(context, response) = tf idf context ? tf idf context</formula><p>(3) tf idf context and tf idf response are the resulting TF-IDF vectors for context and response respectively. D stands for the corpus and w is a word. The dimension of the resulting vectors is thus equal to the dictionary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Network Embeddings</head><p>A neural network is used to create an embedding of both the context and the candidate response. These embeddings, denoted as c and r, are then multiplied using a matrix M and the result is fed into the sigmoid function to score the response.</p><formula xml:id="formula_1">c = f (context) (4) r = f (response) (5) g(context, response) = ?(c M r + b)<label>(6)</label></formula><p>c and r are the resulting embeddings of the context and response, computed using a neural network. We present some different architectures to compute these embeddings. <ref type="figure">Figure 1</ref> illustrates the approach. Note that matrix M , bias b and parameters of the function f (which is a neural network) are all learned using the training data.</p><p>One can think of this approach as a predictive one -given the context, we predict the embedding of the response as r = c M , and measure the similarity of the predicted response r to the actual response r using the dot product (or vice-versa, predicting the context from the response as c = M r)</p><p>The authors experimented with vanilla RNN and LSTM <ref type="bibr" target="#b3">[4]</ref> as the underlying networks producing the embeddings. LSTM significantly outperformed RNN in the author's experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Our Architectures</head><p>All our architectures fall within the neural network embedding based approach. We implemented three different architectures (i) CNN <ref type="bibr" target="#b4">[5]</ref> (ii) LSTM and (iii) Bi-Directional <ref type="bibr" target="#b5">[6]</ref> LSTM. We also report an ensemble of our models.</p><p>All of our architectures share the same design where the words from the input sequence (context or response) are projected into the words' embeddings vectors. Thus, if the input sequence consist of 42 words, we project these words into a matrix E which has a dimension e ? 42, where e is dimensionality of the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CNN</head><p>While originating from computer vision <ref type="bibr" target="#b6">[7]</ref>, CNN models have recently been very successfully applied in NLP problems <ref type="bibr" target="#b4">[5]</ref>. At the very heart of the CNN model, the convolving filters are sequentially applied over the input sequence. The width of the filters might vary, and in NLP typically range from 1 to 5 (the filters can be thought of here as a form of n-grams). These filters are followed by a max-pooling layer to get a fixed-length input. In our architecture, the output of the max-pooling operation forms the context/response embedding. Thus, the resulting embedding has a dimension equal to the number of filters. <ref type="figure" target="#fig_0">Figure 2a</ref> displays this architecture with two filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">LSTM</head><p>Long short-term memory (LSTM) is a recurrent neural network (RNN) architecture designed to remedy the vanishing gradient problem of vanilla RNN <ref type="bibr" target="#b3">[4]</ref>. Thus, LSTM networks are well-suited for working with (very) long sequences <ref type="bibr" target="#b7">[8]</ref>. We use the same model as the authors' LSTM network <ref type="bibr">[?]</ref>. LSTM iterates over the sequence embeddings, and the resulting embedding is the last state of the LSTM's cells. <ref type="figure" target="#fig_0">Figure 2b</ref> illustrates this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Bi-Directional LSTM</head><p>Although the LSTM is tailor-made to keep context over large sequences, empirically it can be problematic for the network to capture the meaning of the entire sequence as it gets longer. If the important parts of the sequence are found at the beginning of a long sequence, the LSTM might struggle to get well-performing embedding. We decided to experiment with Bi-LSTMs to see whether this is the case in our settings. Bi-directional <ref type="bibr" target="#b5">[6]</ref> LSTMSs feed the sequence into two recurrent networks -one reads the sequence as it is, the second reads the sequence from the end to the beginning. To avoid forming cycles, only the outputs of the recurrent networks (not the state-to-state connections) lead to same units in the next layers. <ref type="figure" target="#fig_0">Figure 2c</ref> illustrates this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Method</head><p>To match the original setup of <ref type="bibr" target="#b0">[1]</ref> we use the same training data 3 . We use one million training examples and we use the same word vectors pre-trained by GloVe <ref type="bibr" target="#b8">[9]</ref>. All our models were implemented using Theano <ref type="bibr" target="#b9">[10]</ref> and Blocks <ref type="bibr" target="#b10">[11]</ref>. For training we use ADAM learning rule <ref type="bibr" target="#b11">[12]</ref> and binary negative log-likelihood as training objective. We stop the training once Recall@1 starts increasing on a validation set. The experiments were executed on Nvidia K40 GPUs. The best meta-parameters were found by simple grid search.</p><p>In all architectures we tried both: (i) learning separate parameters for the networks encoding context and response and (ii) learning shared parameters for both networks. Here we report only the results for the architectures with shared parameters, since they consistently achieved higher accuracy.</p><p>Aside from learning single models, we also experimented with model ensembles. We found that averaging predictions of multiple models further improves performance, which is common in many machine learning tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Our best classifier is an ensemble of 11 LSTMs, 7 Bi-LSTMs and 10 CNNs trained with different meta-parameters. <ref type="table">Table 1</ref> shows performance of the models with the best metaparameters in each category. An example prediction from the ensemble is shown in A: i ' ve been missing updates because my normal process is sudo bash -c " aptitude update &amp;&amp; aptitude safe-upgrade -y ". ahh , " e : some index files failed to download . they have been ignored , or old ones used instead .". so i guess the issue is that " aptitude update " is n't giving an error at all N-Best Confidence Response 1 ****** 0.598 does the internet work on that box ? 2 **** 0.444 what time is it saying to going to be released ?? 3 *** 0.348 ahh ok 4 ** 0.245 nice <ref type="table">Table 2</ref>: A dialog context with three turns and a set of four ranked possible responses. The highest ranked response is the ground truth response in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>Our ensemble of classifiers sets a new state-of-the art performance for response ranking on the Ubuntu Dialog Corpus -the largest, publicly available multi-turn dialog corpus. Interestingly LSTMs and Bi-LSTMs achieve almost the same accuracy. We hypothesise that: (i) either utterances that appear at the beginning of the context are less important than the later utterances or, (ii) LSTMs successfully capture all of the important parts of the sequence. When we inspect accuracy of individual models we see that recurrent models are superior to CNNs. However, CNNs proved to significantly improve performance of the ensemble. An ensemble without the 10 CNNs had Re-call@1 accuracy of only 66.8 compared to 68.3 of the larger ensemble. This shows that CNNs learned representations that are complementary to the recurrent models. We believe that our results are important, since they can be used as baselines for more complicated models (see the Future Work).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Varying Training Data Size</head><p>We also experimented with different training data sizes in order to see how this affects the resulting models. We trained all networks on a training data size ranging from 100, 000 to the full 1, 000, 000 examples. The graph in <ref type="figure" target="#fig_1">Figure 3</ref> shows the Recall@1 for all the three models (reported on the test data). There are two main observations here: (i) CNNs outperform recurrent models if the training dataset is small. We believe that this is mostly due to the max operation performed on top of the feature maps. Thanks to the simplicity of this operation, the model does not over-fit the data and generalizes better when learned on small training datasets. On the other hand, the simplicity of the operation does not allow the model to properly handle more complicated dependencies (such as the order in which the n-grams occur in the text), thus recurrent models perform better given enough data; (ii) the recurrent models have not made its peak yet, suggesting that adding more training data would improve the model's accuracy. This agrees with <ref type="figure" target="#fig_1">Figure 3</ref> of the previous evaluation <ref type="bibr" target="#b0">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Future Work</head><p>In our future work, we plan to investigate applicability of neural networks architectures extended with memory (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>) on this task. It is an appealing idea to bootstrap the system with external source of information (e.g., user manual or man pages) to help the system pick the right answer. For successful application of this paradigm in the domain of reinforcement learning, see <ref type="bibr" target="#b17">[18]</ref>.</p><p>An alternative direction for future research might be to extend the model with attention <ref type="bibr" target="#b18">[19]</ref> over sentences in the dialog context. This would allow the model to explain which facts in the context were the most important for its prediction. Therefore, the prediction could be better interpreted by a human.</p><p>Additional accuracy improvements might be also achieved by different text pre-processing pipelines. For instance, in the current dataset all named entities were replaced with generic tags, which could possibly harm the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work we achieved a new state-of-the-art results on the next utterance ranking problem recently introduced in <ref type="bibr" target="#b0">[1]</ref>. The best performing system is an ensemble of multiple diverse neural networks.</p><p>In the future, we plan to use our system as a base for more complicated models going beyond the standard neural network paradigm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different architectures to compute the embedding of the context/reponse: a) CNN with two feature maps: the green one has feature width one, the red one has feature width three. Since the output out of the maxpooling forms our context embedding, the resulting embedding has dimension equal to the number of filters (2 in this example) b) LSTM network. Embeddings is the last hidden state, thus the dimension of the embedding equals to the number of LSTM units. c) Bi-Directional network. Embedding is a concatenation of the corresponding LSTM states: c = c1.c2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Training data size ranging from 100, 000 to the full 1, 000, 000 examples (X axis) and the resulting Recall@1 (Y axis). The CNN has 500, 100 and 100 filters of length 1, 2 and 3. The LSTM and Bi-LSTM has both 300 hidden units in each recurrent layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .Table 1 :</head><label>21</label><figDesc>The performance was evaluated after every epoch of training. Most of the models achieved the best cost on validation data after a single epoch of training. However, the best Recall metrics were usually recorded in the second epoch of training. Results of our experiments compared to the results reported in [1]. Meta-parameters of our architectures are the following: our CNN had 400 filters of length 1, 100 filters of length 2 and 100 filters of length 3; our LSTM had 200 hidden units and our bidirectional LSTM had 250 hidden units in each network. For CNNs and LSTMs, the best results were achieved with batch size 256. For Bi-LSTM, the best batch size was 128.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Baselines from [1]</cell><cell>Our Architectures</cell></row><row><cell></cell><cell></cell><cell cols="2">TF-IDF RNN</cell><cell>LSTM CNN</cell><cell>LSTM Bi-LSTM Ensemble</cell></row><row><cell cols="2">1 in 2 R@1</cell><cell>65.9%</cell><cell cols="2">76.8% 87.8% 84.8% 90.1% 89.5%</cell><cell>91.5%</cell></row><row><cell cols="3">1 in 10 R@1 41.0%</cell><cell cols="2">40.3% 60.4% 54.9% 63.8% 63.0%</cell><cell>68.3%</cell></row><row><cell cols="3">1 in 10 R@2 54.5%</cell><cell cols="2">54.7% 74.5% 68.4% 78.4% 78.0%</cell><cell>81.8%</cell></row><row><cell cols="3">1 in 10 R@5 70.8%</cell><cell cols="2">81.9% 92.6% 89.6% 94.9% 94.4%</cell><cell>95.7%</cell></row><row><cell cols="3">Turn User Text</cell><cell></cell></row><row><cell>1</cell><cell>A:</cell><cell cols="3">anyone know why " aptitude update " returns a non-successful status (255) ?</cell></row><row><cell>2</cell><cell>B:</cell><cell cols="3">does apt-get update work ?</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The dataset in binary format is available at http://cs.mcgill.ca/?jpineau/datasets/ ubuntu-corpus-1.0/ubuntu_blobs.tgz [accessed 25.9.2015]   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Precise Timing with LSTM Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Theano: new features and speed improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Blocks and Fuel : Frameworks for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sequence to Sequence Learning with Neural Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural Turing Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a montecarlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="621" to="659" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
