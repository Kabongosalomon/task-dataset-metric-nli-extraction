<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExpansionNet v2: Block Static Expansion in fast end to end training for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Cheng Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cavicchioli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Capotondi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Modena and Reggio Emilia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ExpansionNet v2: Block Static Expansion in fast end to end training for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Expansion methods explore the possibility of performance bottlenecks in the input length in Deep Learning methods. In this work, we introduce the Block Static Expansion which distributes and processes the input over a heterogeneous and arbitrarily big collection of sequences characterized by a different length compared to the input one. Adopting this method we introduce a model called ExpansionNet v2, which is trained using our novel training strategy, designed to be not only effective but also 6 times faster compared to the standard approach of recent works in Image Captioning. The model achieves the state of art performance over the MS-COCO 2014 captioning challenge with a score of 143.7 CIDEr-D in the offline test split, 140.8 CIDEr-D in the online evaluation server and 72.9 All-CIDEr on the nocaps validation set. Source code available at: https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image Captioning describes the problem of generating a natural language description of an image without human intervention. It's a multi-modal problem that requires from the system both language comprehension and visual understanding. Over the past years, thanks to an easy access to a huge amount of data and the increasing affordability of massive parallel computational resources, neural networks became the currently most popular and most performing tools in many Computer Vision and Natural Language Processing related research field. Neural image captioning systems consist typically of an encoder responsible of extracting the visual information for the decoder which instead generates a linguistically acceptable description, at the early stages they mostly respectively consisted of CNN and recurrent layers <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">27]</ref> since for many years they were the most performing tools in many Computer Vision and Natural Language Processing tasks. In recent years however, transformer based models <ref type="bibr" target="#b28">[29]</ref> gained momentum and it either replaced or merged with existing approaches further improving the effectiveness of captioning models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17]</ref>. All encoders <ref type="figure">Figure 1</ref>: The expansion mechanism transform the input data into a new one featuring a different sequence length during the forward phase and performs the reverse operation in the backward pass in order to enable the network into processing the input unconstrained by the number of elements. The Block Static Expansion provides a way of performing these operations over a collection of arbitrary and diverse lengths at the same time.</p><p>until recent year relied on a convolutional backbone <ref type="bibr" target="#b30">[31]</ref>, an aspect that was changed in <ref type="bibr" target="#b32">[33]</ref> which introduces the first fully attentive end to end architecture for Image Captioning, called PureT <ref type="bibr" target="#b32">[33]</ref>, enabled in particular by the Swin-Transformer <ref type="bibr" target="#b20">[21]</ref>. Despite the impressive results of existing methods, in <ref type="bibr" target="#b10">[11]</ref> we challenged the most popular approaches with the expansion layers, which consists of leveraging different sequence lengths compared to the one provided in the input. In this work we further explore the idea and improve the encoder by leveraging a set of multiple arbitrary sequence lengths, while preserving the efficiency of the original formulation. We design a training strategy that is not only effective but also several times faster than the standard one and we achieve the state of arts results on the MS-COCO 2014 challenge and competitives results on the nocaps validation set. The overall contributions of this work are the following: -a novel architecture for Image Captioning that achieves state of art performance in the popular MS-COCO 2014 challenge and competitive results on the validation nocaps dataset;</p><p>-the Block Static Expansion layer;</p><p>-a novel learning strategy that is more efficient and effective compared to the standard one, based upon an alternation of partial and end to end training, lowering the computational requirements of recent technologies in image captioning;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Image Captioning models benefitted a lot from Deep Learning methods, from hand-crafted sentences aided by object detection systems <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref> they now consist of a neural encoder and decoder structure where the first is responsible of extracting meaningful visual information and the latter is responsible of generating the caption. The encoder in the early formulations consisted of a convolutional backbone <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> but it was later replaced by an object detector <ref type="bibr" target="#b2">[3]</ref> setting up a multi-modal sequence modeling problem over a position invariant collection of salient objects. Consequently, most of the methods, originally designed in the field of Natural Language Processing, could be adopted in Image Captioning as well, in particular, the works of <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30]</ref> relied on the Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, whereas most recent architectures exploited fully attentive models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b22">23]</ref>. The application of the Transformer has been proved to be surprisingly effective, despite the fact it was originally designed for the NLP field. However, due to the quadratic cost with respect to the input size, it couldn't easily replace the role of a convolutional backbone, in fact, from the sheer number of elements perspective images are typically orders of magnitude greater than text sequences, regardless of sub-word tokenization techniques. Despite the premise, the recent work of <ref type="bibr" target="#b20">[21]</ref> introduced a fully attentive backbone for images and achieved state of art performances in many Vision related task and <ref type="bibr" target="#b32">[33]</ref> proved it to be very effective in the Image Captioning as well. Additionally, in contrast to the previous captioning systems, <ref type="bibr" target="#b32">[33]</ref> also provided the first end to end architecture achieving outstanding results at the cost of high computational requirements. Finally, in <ref type="bibr" target="#b10">[11]</ref> we proposed an alternative method to the self-attention layer based upon the idea of changing the sequence length, in contrast to <ref type="bibr" target="#b6">[7]</ref> however, we focused on the architectural front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Our model consists of the standard encoder decoder structure implemented on top of the Swin-Transformer which details are provided in <ref type="bibr" target="#b20">[21]</ref>. The image A, it is first fed into the backbone:</p><formula xml:id="formula_0">X 0 = Swin-T ransf (A)<label>(1)</label></formula><p>and generates the initial set of processed visual fea-</p><formula xml:id="formula_1">tures X 0 = {x 0 1 , x 0 2 , . . . , x 0 N }, x 0 i ? R dm .</formula><p>The result is fed into the encoder which is made of N enc Static Expansion ? FeedForward blocks, skip connection and pre-layer normalization <ref type="bibr" target="#b33">[34]</ref> are adopted and the whole encoder is described by the following formulas for each n = 1, . . . , N enc :</p><formula xml:id="formula_2">B n = X n?1 + StaticExp n (N orm SE n (X n?1 ) X n = B n + F F n (N orm F F n (B n ))<label>(2)</label></formula><p>Similarly, given Y 0 = {y 0 1 , y 0 2 , . . . , y 0 M }, y 0 i ? R dm is the generic input sequence (during training stage so we can omit the time axis), the decoder is made of N dec Dynamic Expansion ? Cross-Attention ? FeedForward blocks, skip connection and normalization is applied on each component. Each layer n = 1, . . . , N dec are described by the following equations:</p><formula xml:id="formula_3">B n = Y n?1 + DynamicExp n (N orm DE n (Y n?1 )) W n = B n + Attention n (N orm CA n (B n ), X Nenc ) Y n = W n + F F n (N orm F F n (W n ))<label>(3)</label></formula><p>where the final output Y n is fed to the classification layer. A visual representation of the architecture's main components is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Block Static and Dynamic Expansion Layers</head><p>The attention mechanism <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref> was first introduced to spread the input information throughout all encoder hidden vectors. In <ref type="bibr" target="#b10">[11]</ref> instead we explored the idea of distributing the input content over an arbitrary number of elements. In its essence the expansion principle consists of transforming the input sequence into another one featuring a different length by means a "Forward expansion" and retrieving the original length back in the complementary backward operation. In particular two versions currently exist, called Static Expansion and Dynamic Expansion according to the nature of the transformation. The latter is designed to preserve the auto-regression property whereas the first can be operated in both bidirectional processing and decoding stage. The main differences lies in how the forward operation is performed, in particular, in the vector generation phase. In the Static Expansion case the expansion coefficient N E defines the final size of the expanded sequence regardless of the input sequence length L (despite what the name suggests N E can be also smaller than L) and it involves two parameter matrices Q E , B E of size N E ? d m called expansion queries and biases respectively. Whereas, in the Dynamic Expansion, given the input sequence X = {x 1 , x 2 , . . . , x L } and the expansion factor N E , considering two parameter matrices E Q , E B ? R N E ?dm , the expansion queries Q E and expansion biases B E are defined by:</p><formula xml:id="formula_4">Q E = (C H E ) + (E Q I E ) B E = (C H E ) + (E B I E )<label>(4)</label></formula><p>where C ? R L?dm defines a linear projection of the input and H E ? R L?(L?N E ) is defined as:</p><formula xml:id="formula_5">H E = ? ? ? ? ? 1 0 . . . 0 0 1 . . . 0 . . . . . . . . . . . . 0 0 . . . 1 ? ? ? ? ? , 1, 0 ? R 1?N E and I E ? R N E ?(L?N E )</formula><p>is defined by the column-wise concatenation of L identity matrices of size N E ? N E :</p><formula xml:id="formula_6">I E = I L I L . . . I L , I L ? R N E ?N E</formula><p>The second forward part and the remaining operations involve 4 linear projections K, V 1 , V 2 , S ? R L?dm . First of all, the "Length Transformation Matrix" denoted as M is computed using K and the expanded queries previously generated:</p><formula xml:id="formula_7">M = Q E K ? d m .<label>(5)</label></formula><p>Given row-wise normalization operator ? :</p><formula xml:id="formula_8">(X, ) ? Y , X, Y ? R N1?N2 , ? R + /{0}: ?(X, ) ij = x ij N2 z=1 x iz +<label>(6)</label></formula><p>where the coefficient ensures the feasibility of the operation, the forward processing consists of:</p><formula xml:id="formula_9">R f w i = ?(ReLU ((?1) i M ), ) i ? {1, 2} F f w i = R f w i V i + B E i ? {1, 2}<label>(7)</label></formula><p>In the backward step the original sequence length is retrieved back by transposing the length transformation matrix in Equation 5:</p><formula xml:id="formula_10">R bw i = ?(ReLU ((?1) i M ), ) i ? {1, 2} B bw i = R bw i F f w i i ? {1, 2}<label>(8)</label></formula><p>Finally the selection phase combine the results produced by the two paths:</p><formula xml:id="formula_11">out = ?(S) B bw 1 + (1 ? ?(S)) B bw 2 .<label>(9)</label></formula><p>In order to increase the effectiveness of the static expansion, we propose an improved version called Block Static Expansion which performs the Forward and Backward operations on a collection of several target lengths instead of one. The layer not only enables the network to leverage a variety of processing perspectives of the input during the features refinement, but eases the hyper parameters exploration as it is more likely to find the most suitable expansion coefficients for a given problem. From a formulation perspective, the vector generation phase is repeated over a given group of arbitrary number of lengths</p><formula xml:id="formula_12">G = {N 1 E , N 2 E , . . . , N N G E }.</formula><p>Despite the increase of the number of sequence lengths, the number of parameters are not increased thanks to the parameters sharing and it is implemented in a way such that both forward and backward steps are performed over all targets at the same time, in particular the grouped expansion queries and biases are now defined as:</p><formula xml:id="formula_13">E G Q = {(E 1 Q ) , (E 2 Q ) , . . . , (E N G Q ) } E G B = {(E 1 B ) , (E 2 B ) , . . . , (E N G B ) }<label>(10)</label></formula><p>thus the computational efficiency of the original formulation is preserved. During the backward stage the length transformation matrix is scaled by the inverse number of lengths in each block 1 N G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training objectives</head><p>The model is first pre-trained using the Cross-Entropy loss L XE :</p><formula xml:id="formula_14">L XE (?) = ? M t log(p ? (y * t |y * 1:t?1 , I))<label>(11)</label></formula><p>where p ? (y * t |y * 1:t?1 , I) is the probability assigned by the model ? to the target y * t given the image I and the previous words y * 1:t?1 . Then the self-critical optimization <ref type="bibr" target="#b25">[26]</ref> is performed according the following formula:</p><formula xml:id="formula_15">L RF (?) = ?(r ? b) ? M t log(p ? (y t |y 1:t?1 ))<label>(12)</label></formula><p>where b is the baseline computed according to <ref type="bibr" target="#b21">[22]</ref>, r is the reward assigned to the sampled sequence referred as y 1:M and y * 1:M is the ground truth caption. Although we adopt only two losses, in this work each training stage is split into two additional steps in order to allow a broader number of computational resources to reproduce this work. More details are provided in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The training dataset consists of the popular Microsoft COCO benchmark <ref type="bibr" target="#b18">[19]</ref> split according to <ref type="bibr" target="#b15">[16]</ref> resulting in 113.287 training images and 5000 validation images and 5000 testing cases. Each reference caption is preprocessed by a simple pipeline that consists of lowering casing, punctuation removal and filtering out words that do not occur at least 5 times (vocabulary of size 10.000). Additionally, the final model is evaluated over the Novel Object Captioning at Scale dataset (nocaps) validation set <ref type="bibr" target="#b0">[1]</ref> which consists of three class of images called in-domain, near-domain and out-domain, according to the familiarity of the classes with respect to those contained in the training set. This dataset is subject to the same pre-processing of MS-COCO and serves the purpose of further challenging the model in unfavourable conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model details</head><p>Three models are implemented for the experiment results. Each one rely on top of the same backbone, the Swin-Transformer in the Large configuration <ref type="bibr" target="#b20">[21]</ref> pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. All images are subject to a minimal pre-processing: first they are resized into a 3?384?384 tensor, then RGB values are converted into a [0, 1] range and further normalized using mean=(0.485, 0.456, 0.406) and std=(0.229, 0.224, 0.225). The baseline, which is the Base Transformer, and our main model, referred as "ExpansionNet v2", are implemented with the follow-</p><formula xml:id="formula_16">ing configurations d m =512, d f f =2048, N enc =N dec =3.</formula><p>In the latter, the dynamic expansion coefficients is set to 16 and the improved static expansion coefficients consist of G={32, 64, 128, 256, 512} (more details in Section 4.4). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training algorithm</head><p>From the sheer number of parameters perspective alone it can be observed that the Swin-Transformer backbone is the biggest part of the system, hence it represents the most computationally expensive portion of the training phase. Following this observation, in order to enable the end-to-end training step to a broader number of computational architectures, our training is divided in four steps, in particular each phase, during either the pre-training and the reinforcement stages, consists of an initial training in which the backbone's weights are freezed and a fine-tuning step during which gradients flow throughout the whole system:</p><p>1. Cross-Entropy -Freezed backbone. The model is trained using batch size 48, an initial learning rate of 2e-4, a warmup of 10.000 and is annealed by 0.8 every 2 epochs for 8 epochs;</p><p>2. Cross Entropy -End to end. The whole system is trained for 2 additional epochs, using batch size 48 and an initial learning rate of 3e-5 annealed by 0.55 every epoch; 4. CIDEr-D optimization -End to end. The whole system is fine-tuned for few more iterations up to an additional epoch using a batch size of 48 and fixed learning rate 2e-6. This step is optional since it only slightly contribute to the final performances and can be skipped in case no improvements are observed.</p><p>Despite its apparent complexity, it is much more computational friendly compared to the standard method consisting of a small batch size of 10 for 30 epochs for both optimization steps. As a matter of fact, only a much smaller number of training epochs are dedicated for the fine-tuning of the whole system. This not only means that most of the time the cost of computing the required information related to backbone's gradient is avoided, but the cost of the forward operation can be drastically reduced as well. In particular, in our implementation, during step 1 and 3 the backbone's forward pass is performed only once for each image in the data set therefore the forward cost is replaced by a memory read and copy. This configuration not only results in a significant decrease of the training time, as show in Table 1, but it is able to achieve even higher performances compared to the end-to-end training showcased in <ref type="bibr" target="#b32">[33]</ref> (see <ref type="table" target="#tab_4">Table 4</ref>). All steps are trained using the RAdam optimizer <ref type="bibr" target="#b19">[20]</ref> (? 1 = 0.9, ? 2 = 0.98).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance comparison</head><p>COCO Offline Evaluation. <ref type="table" target="#tab_4">Table 4</ref> reports the score comparison between ExpansionNet v2 and the best performing models in recent years. Up-Down <ref type="bibr" target="#b2">[3]</ref> introduced the idea of extracting a collection of features from the images using an object detector like Faster-RCNN <ref type="bibr" target="#b24">[25]</ref> in contrast to the classification backbone <ref type="bibr" target="#b34">[35]</ref>. The idea was adopted in most of the  following architectures as well, for instance in case of GCN-LSTM <ref type="bibr" target="#b37">[38]</ref> and SGAE <ref type="bibr" target="#b35">[36]</ref>, which, additionally, implemented a graph convolutional network on top of it in order to exploit the information provided by a scene graph. AoANet <ref type="bibr" target="#b12">[13]</ref> instead, relied on a Transformer based architecture and improved the attentive components with two gates serving the purpose of simulating an additional level of attention over the inputs and augmented the language modeling part with a LSTM. X-Transformer <ref type="bibr" target="#b23">[24]</ref> on the other hand, adopted a fully attentive architecture and further refined the attentive blocks by means of bilinear pooling techniques. The most recent and performing architectures redefined the best way of feeding the visual input into the network, for instance RSTNet <ref type="bibr" target="#b39">[40]</ref> showcased the effectiveness of grid features over regions, GET <ref type="bibr" target="#b14">[15]</ref> processed the images using a global representation as well in addition to the local ones, DLCT <ref type="bibr" target="#b22">[23]</ref> instead exploited the advantages of both regions and grid visual features. Finally PureT <ref type="bibr" target="#b32">[33]</ref>      <ref type="bibr" target="#b2">[3]</ref>. However, it's worth noting that it is still ultimately outperformed by many recent works such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> which results are omitted because of a different experiment setup. <ref type="table" target="#tab_9">Table 8</ref> and 9 provide some examples of captions. Regardless of the complexity, ExpansionNet v2 appears not only able to correctly describe the subjects depicted in the scenes but also showcases a good level of semantic understanding by describing also the goals and interactions (Images 1, 8 and 12). Unfortunately, our model seem to struggle with out-of-domain objects as showcased in <ref type="table" target="#tab_8">Table 7</ref> where, because of objects and terms unknown to the model, predictions are either imprecise (2 nd image) or incorrect (1 st and 3 rd image). Nonetheless, it seems to be able of providing a rough description of the images context. Finally, we showcase an example of attention visualization in <ref type="figure" target="#fig_1">Figure 3</ref>, despite the absence of an object detector, the scattered focus correctly outlines the main subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel architecture for image captioning based upon the expansion methods and in particular the Block Static Expansion enabling the model to process sequences in multiple different forms each one defined by an arbitrary number of elements in contrast to the one provided by the input, which ultimately increases the quality of features refinement. To support this claim we implemented an end to end encoderdecoder architecture called ExpansionNet v2 which parameters are found by means of an alternating full and partial end-to-end training strategy. Experiments conducted over the COCO and nocaps dataset resulted in the new state of art performance in the first and very competitive results for the latter.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>ExpansionNet v2 architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Attention visualization of one head in one of our main model layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between our training strategy and the standard one using a single Nvidia A100 GPU.</figDesc><table><row><cell>Training</cell><cell>Standard</cell><cell>Ours</cell><cell>Speed-up</cell></row><row><cell>Cross-Entropy</cell><cell>10 days</cell><cell>1.5 days</cell><cell>6.7</cell></row><row><cell>Reinforcement</cell><cell>5 days</cell><cell>1 day</cell><cell>5</cell></row><row><cell>Complete</cell><cell>15 days</cell><cell>2.5 days</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study in the first stage of Cross-Entropy training using beam size 3 over the Karpathy test split.</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell>B1</cell><cell>B2</cell><cell>B3</cell><cell>B4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>Baseline</cell><cell>Baseline</cell><cell cols="8">74.8 58.4 44.8 34.2 28.3 56.7 115.8 21.5</cell></row><row><cell>Static Exp. G={64}</cell><cell>Dyn. Exp.</cell><cell cols="8">76.9 61.1 47.5 36.6 28.8 57.6 121.9 22.3</cell></row><row><cell>Static Exp G={128, 128, 128, 128, 128}</cell><cell>Dyn. Exp.</cell><cell cols="8">76.4 60.8 47.2 36.5 29.2 57.8 122.6 22.6</cell></row><row><cell>Static Exp G={384, 384, 384, 384, 384}</cell><cell>Dyn. Exp.</cell><cell cols="8">77.3 61.4 47.8 36.8 29.1 57.9 122.7 22.6</cell></row><row><cell>Static Exp G={512, 512, 512, 512, 512}</cell><cell>Dyn. Exp.</cell><cell cols="8">76.9 60.9 47.1 36.1 29.2 57.6 122.9 22.7</cell></row><row><cell>Static Exp G={8, 16, 32, 64, 96, 128, 256}</cell><cell>Dyn. Exp.</cell><cell cols="8">77.1 61.4 47.6 36.5 29.0 57.8 123.0 22.5</cell></row><row><cell>Static Exp G={16, 32, 64, 128, 196, 256, 320}</cell><cell>Dyn. Exp.</cell><cell cols="8">77.0 61.2 47.6 36.6 29.2 57.7 123.3 22.6</cell></row><row><cell>Static Exp G={128, 256, 384, 512}</cell><cell>Dyn. Exp.</cell><cell cols="8">77.4 61.6 47.8 36.8 29.1 57.8 123.4 22.6</cell></row><row><cell>Static Exp. G={32, 64, 128, 256, 512}</cell><cell>Dyn. Exp.</cell><cell cols="8">77.4 61.6 47.8 36.8 29.3 58.0 123.8 22.7</cell></row><row><cell cols="2">3. CIDEr-D optmization -Freezed backbone.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Reinforcement phase adopt a batch size of 48, an</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">initial learning rate of 1e-4, no warmup, annealed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>by 0.8 every epoch for 9 epochs;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>we compare the results of the improved Static Expansion over the previous one. We adopt several configurations covering a wide range of lengths. First of all, there is no apparent relation between the size of the length groups and the performance, as more targets does not necessarily yield better performances,</figDesc><table /><note>in contrast to the diversity, where instances like {128, 128, 128, 128, 128}, {256, 256, 256, 256, 256} and {512, 512, 512, 512, 512} lead to a less significant increase in performance compared to {8, 16, 32, 64, 96, 128, 256} or {16, 32, 64, 128, 196, 256, 320}. The most successful configuration in terms of CIDEr, which will</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Two examples of captions sampled from the Karpathy test split.</figDesc><table><row><cell>Image</cell><cell>Captions</cell></row><row><cell></cell><cell>Baseline: A cat looking in front of</cell></row><row><cell></cell><cell>a mirror.</cell></row><row><cell></cell><cell>ExpansionNet v2: A cat looking</cell></row><row><cell></cell><cell>at its reflection in a mirror.</cell></row><row><cell></cell><cell>Gt: { A cat looking at his reflection</cell></row><row><cell></cell><cell>in the mirror. ; A cat that is look-</cell></row><row><cell></cell><cell>ing in a mirror. ; A cat looking at</cell></row><row><cell></cell><cell>itself in a mirror. ; A cat looking at</cell></row><row><cell></cell><cell>itself adoringly in a mirror. ; A cat</cell></row><row><cell></cell><cell>stares at itself in a mirror. }</cell></row><row><cell></cell><cell>Baseline: A cat on a leash next to</cell></row><row><cell></cell><cell>a water bottle.</cell></row><row><cell></cell><cell>ExpansionNet v2: Two pictures of</cell></row><row><cell></cell><cell>a cat and a dog on a leash.</cell></row><row><cell></cell><cell>Gt: { A gray and white cat sitting</cell></row><row><cell></cell><cell>on top of a table. ; A double pic-</cell></row><row><cell></cell><cell>ture with one featuring a dog and</cell></row><row><cell></cell><cell>the other a cat. ; A dog is shown</cell></row><row><cell></cell><cell>above a cat picture, A dog and cat</cell></row><row><cell></cell><cell>in a coupe of photos. ; Two pictures</cell></row><row><cell></cell><cell>of a dog and a cat sitting. }</cell></row><row><cell cols="2">be adopted in the remaining experiments, is {32, 64,</cell></row><row><cell cols="2">128, 256, 512} and it suggests that in addition to the</cell></row><row><cell cols="2">diversity, the model benefits from adopting both short</cell></row><row><cell cols="2">and long expansion sequences. For instance, remov-</cell></row><row><cell cols="2">ing either the smallest or greatest end from the targets</cell></row><row><cell cols="2">length interval, like in the case of {128, 256, 384, 512},</cell></row><row><cell cols="2">leads to a performance downgrade. Remarkably, all</cell></row><row><cell cols="2">instances perform better than the baseline across all</cell></row><row><cell>metrics.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Offline comparison of state of art models over the Karpathy test split.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cross-Entropy</cell><cell></cell><cell></cell><cell></cell><cell cols="4">CIDEr-D optimization</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">B@1 B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell><cell>B@1</cell><cell>B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>XXX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Single Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Up-Down [3]</cell><cell>77.2</cell><cell>36.2</cell><cell cols="4">27.0 56.4 113.5 20.3</cell><cell>79.8</cell><cell>36.3</cell><cell>27.7</cell><cell>56.9</cell><cell>120.1</cell><cell>21.4</cell></row><row><cell>GCN-LSTM [38]</cell><cell>77.3</cell><cell>36.8</cell><cell cols="4">27.9 57.0 116.3 20.9</cell><cell>80.5</cell><cell>38.2</cell><cell>28.5</cell><cell>58.3</cell><cell>127.6</cell><cell>22.0</cell></row><row><cell>SGAE [36]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.8</cell><cell>38.4</cell><cell>28.4</cell><cell>58.6</cell><cell>127.8</cell><cell>22.1</cell></row><row><cell>AoANet [13]</cell><cell>77.4</cell><cell>37.2</cell><cell cols="4">28.4 57.5 119.8 21.3</cell><cell>80.2</cell><cell>38.9</cell><cell>29.2</cell><cell>58.8</cell><cell>129.8</cell><cell>22.4</cell></row><row><cell>X-Transformer [24]</cell><cell>77.3</cell><cell>37.0</cell><cell cols="4">28.7 57.5 120.0 21.8</cell><cell>80.9</cell><cell>39.7</cell><cell>29.5</cell><cell>59.1</cell><cell>132.8</cell><cell>23.4</cell></row><row><cell>GET [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.5</cell><cell>39.5</cell><cell>29.3</cell><cell>58.9</cell><cell>131.6</cell><cell>22.8</cell></row><row><cell>DLCT [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.4</cell><cell>39.8</cell><cell>29.5</cell><cell>59.1</cell><cell>133.8</cell><cell>23.0</cell></row><row><cell>RSTNet [40]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.8</cell><cell>40.1</cell><cell>29.8</cell><cell>59.5</cell><cell>135.6</cell><cell>23.3</cell></row><row><cell>PureT [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.1</cell><cell>40.9</cell><cell>30.2</cell><cell>60.1</cell><cell>138.2</cell><cell>24.2</cell></row><row><cell>ExpansionNet v2</cell><cell>78.1</cell><cell>38.1</cell><cell cols="4">30.1 58.9 128.2 23.5</cell><cell cols="6">82.8 41.5 30.3 60.5 140.4 24.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>XXX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Ensemble Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN-LSTM [38]</cell><cell>77.4</cell><cell>37.1</cell><cell cols="4">28.1 57.2 117.1 21.1</cell><cell>80.9</cell><cell>38.3</cell><cell>28.6</cell><cell>58.5</cell><cell>128.7</cell><cell>22.1</cell></row><row><cell>SGAE [36]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.0</cell><cell>39.0</cell><cell>28.4</cell><cell>58.9</cell><cell>129.1</cell><cell>22.2</cell></row><row><cell>AoANet [13]</cell><cell>78.7</cell><cell>38.1</cell><cell cols="4">28.5 58.2 122.7 21.7</cell><cell>81.6</cell><cell>40.2</cell><cell>29.3</cell><cell>59.4</cell><cell>132.0</cell><cell>22.8</cell></row><row><cell>X-Transformer [24]</cell><cell>77.8</cell><cell>37.7</cell><cell cols="4">29.0 58.0 122.1 21.9</cell><cell>81.7</cell><cell>40.7</cell><cell>29.9</cell><cell>59.7</cell><cell>135.3</cell><cell>23.8</cell></row><row><cell>GET [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.1</cell><cell>40.6</cell><cell>29.8</cell><cell>59.6</cell><cell>135.1</cell><cell>23.8</cell></row><row><cell>DLCT [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.2</cell><cell>40.8</cell><cell>29.9</cell><cell>59.8</cell><cell>137.5</cell><cell>23.3</cell></row><row><cell>PureT [33]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.4</cell><cell>42.1</cell><cell>30.4</cell><cell>60.8</cell><cell>141.0</cell><cell>24.3</cell></row><row><cell>ExpansionNet v2</cell><cell>78.5</cell><cell>38.5</cell><cell cols="4">29.9 58.8 128.7 23.6</cell><cell cols="6">83.5 42.7 30.6 61.1 143.7 24.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Online server results on the MS-COCO 2014 test set which ground truth are unknown.</figDesc><table><row><cell>Model</cell><cell>B1</cell><cell></cell><cell>B2</cell><cell></cell><cell>B3</cell><cell></cell><cell>B4</cell><cell></cell><cell cols="2">METEOR</cell><cell cols="2">ROUGE-L</cell><cell cols="2">CIDEr-D</cell></row><row><cell></cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell></row><row><cell>SCST [26]</cell><cell>78.1</cell><cell>93.7</cell><cell>61.9</cell><cell>86.0</cell><cell>47.0</cell><cell>75.9</cell><cell>35.2</cell><cell>64.5</cell><cell>27.0</cell><cell>35.5</cell><cell>56.3</cell><cell>70.7</cell><cell>114.7</cell><cell>116.0</cell></row><row><cell>Up-Down [3]</cell><cell>80.2</cell><cell>95.2</cell><cell>64.1</cell><cell>88.8</cell><cell>49.1</cell><cell>79.4</cell><cell>36.9</cell><cell>68.5</cell><cell>27.6</cell><cell>36.7</cell><cell>57.1</cell><cell>72.4</cell><cell>117.9</cell><cell>120.5</cell></row><row><cell>GCN-LSTM [38]</cell><cell>-</cell><cell>-</cell><cell>65.5</cell><cell>89.3</cell><cell>50.8</cell><cell>80.3</cell><cell>38.7</cell><cell>69.7</cell><cell>28.5</cell><cell>37.6</cell><cell>58.5</cell><cell>73.4</cell><cell>125.3</cell><cell>126.5</cell></row><row><cell>SGAE [36]</cell><cell>81.0</cell><cell>95.3</cell><cell>65.6</cell><cell>89.5</cell><cell>50.7</cell><cell>80.4</cell><cell>38.5</cell><cell>69.7</cell><cell>28.2</cell><cell>37.2</cell><cell>58.6</cell><cell>73.6</cell><cell>123.8</cell><cell>126.5</cell></row><row><cell>AoANet [13]</cell><cell>81.0</cell><cell>95.0</cell><cell>65.8</cell><cell>89.6</cell><cell>51.4</cell><cell>81.3</cell><cell>39.4</cell><cell>71.2</cell><cell>29.1</cell><cell>38.5</cell><cell>58.9</cell><cell>74.5</cell><cell>126.9</cell><cell>129.6</cell></row><row><cell>X-Transformer [24]</cell><cell>81.9</cell><cell>95.7</cell><cell>66.9</cell><cell>90.5</cell><cell>52.4</cell><cell>82.5</cell><cell>40.3</cell><cell>72.4</cell><cell>29.6</cell><cell>39.2</cell><cell>59.5</cell><cell>75.0</cell><cell>131.1</cell><cell>133.5</cell></row><row><cell>RSTNet [40]</cell><cell>82.1</cell><cell>96.4</cell><cell>67.0</cell><cell>91.3</cell><cell>52.2</cell><cell>83.0</cell><cell>40.0</cell><cell>73.1</cell><cell>29.6</cell><cell>39.1</cell><cell>59.5</cell><cell>74.6</cell><cell>131.9</cell><cell>134.0</cell></row><row><cell>GET [15]</cell><cell>81.6</cell><cell>96.1</cell><cell>66.5</cell><cell>90.9</cell><cell>51.9</cell><cell>82.8</cell><cell>39.7</cell><cell>72.9</cell><cell>29.4</cell><cell>38.8</cell><cell>59.1</cell><cell>74.4</cell><cell>130.3</cell><cell>132.5</cell></row><row><cell>DLCT [23]</cell><cell>82.4</cell><cell>96.6</cell><cell>67.4</cell><cell>91.7</cell><cell>52.8</cell><cell>83.8</cell><cell>40.6</cell><cell>74.0</cell><cell>29.8</cell><cell>39.6</cell><cell>59.8</cell><cell>75.3</cell><cell>133.3</cell><cell>135.4</cell></row><row><cell>PureT [33]</cell><cell>82.8</cell><cell>96.5</cell><cell>68.1</cell><cell>91.8</cell><cell>53.6</cell><cell>83.9</cell><cell>41.4</cell><cell>74.1</cell><cell>30.1</cell><cell>39.9</cell><cell>60.4</cell><cell>75.9</cell><cell>136.0</cell><cell>138.3</cell></row><row><cell>ExpansionNet v2</cell><cell>83.3</cell><cell>96.9</cell><cell>68.8</cell><cell>92.6</cell><cell>54.4</cell><cell>85.0</cell><cell>42.1</cell><cell>75.3</cell><cell>30.4</cell><cell>40.1</cell><cell>60.8</cell><cell>76.4</cell><cell>138.5</cell><cell>140.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performances comparison on nocaps validation set.</figDesc><table><row><cell cols="2">Domain Metric</cell><cell cols="3">Enc-Dec[5] Up-Down[3] Ours</cell></row><row><cell>In</cell><cell>C S</cell><cell>72.8 11.1</cell><cell>78.1 11.6</cell><cell>83.8 12.6</cell></row><row><cell>Near</cell><cell>C S</cell><cell>57.1 10.2</cell><cell>57.7 10.3</cell><cell>79.2 12.4</cell></row><row><cell>Out</cell><cell>C S</cell><cell>34.1 8.3</cell><cell>31.3 8.3</cell><cell>54.0 9.3</cell></row><row><cell>All</cell><cell>C S</cell><cell>54.7 10.0</cell><cell>55.3 10.1</cell><cell>72.9 11.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Three examples of nocaps out-of-domain images, for the sake of brevity only 3 ground truth descriptions are reported.Nocaps Evaluation. We evaluate ExpansionNet v2 over the nocaps validation set. In particular, we adopt a single model trained exclusively on Cross-Entropy Loss, using no additional pre-training data sets and the predictions are generated by the standard Beam Search algorithm (beam size 3) in contrast to the CBS<ref type="bibr" target="#b1">[2]</ref>. A limited comparison is reported inTable 6which showcases that our model achieves very competitive results among the architectures trained in similar configurations, with a overall lead of 17.6 CIDEr and 1.4 SPICE over the Up-Down model</figDesc><table><row><cell>Image</cell><cell>Captions</cell></row><row><cell></cell><cell>Pred: A close up of a fish in a</cell></row><row><cell></cell><cell>body of water.</cell></row><row><cell></cell><cell>Gt: { A seahorse in an aquarium</cell></row><row><cell></cell><cell>full of water with some plants grow-</cell></row><row><cell></cell><cell>ing in the background. ; A blue sea-</cell></row><row><cell></cell><cell>horse is swimming near sea plants</cell></row><row><cell></cell><cell>on back. ; A very small seahorse</cell></row><row><cell></cell><cell>is in the water along with other</cell></row><row><cell></cell><cell>pieces. }</cell></row><row><cell></cell><cell>Pred: Three pictures of a blender</cell></row><row><cell></cell><cell>with red liquid in it.</cell></row><row><cell></cell><cell>Gt: { A picture of three blenders</cell></row><row><cell></cell><cell>with a strawberry looking beverage</cell></row><row><cell></cell><cell>inside. ; A white mixer in the pro-</cell></row><row><cell></cell><cell>cess of making a smoothie. ; The</cell></row><row><cell></cell><cell>steps of making a smoothie in a</cell></row><row><cell></cell><cell>blender are shown. }</cell></row><row><cell></cell><cell>Pred: A birthday cake is decorated</cell></row><row><cell></cell><cell>with a house.</cell></row><row><cell></cell><cell>Gt: { A gingerbread house has a</cell></row><row><cell></cell><cell>red frosting roof and several candy</cell></row><row><cell></cell><cell>pieces. ; A gingerbread house that</cell></row><row><cell></cell><cell>is red, brown, white, and green. ;</cell></row><row><cell></cell><cell>A log cabin is made out of dessert</cell></row><row><cell></cell><cell>treats. }</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Examples of captions in single model configuration (beam search 5).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>More examples.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Gt1: A dog catching a Frisbee in the grass. Gt2: The black and white dog just got the frisbee. Gt3: A dog in a vest catching a fristbee. Gt4: a dig jumps in the air to catch a frisbe. Gt5: Dog wearing blue vest catching red flying disc at outdoor event.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id</head><p>Image Networks prediction Ground Truth 1 Baseline: A man holding a tennis ball on a tennis court.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A man jumping in the air to hit a tennis ball.</p><p>Gt1: A tennis player jumps and swats at the ball. Gt2: A tennis player hitting a tennis ball on a court. Gt3: Professional tennis player immediately after returning a shot. Gt4: A man with a tennis racket plays a game of tennis. Gt5: A man playing tennis with two people watching the game.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Baseline: A giraffe standing next to a man in a field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A giraffe eating from a feeder in a park.</p><p>Gt1: A giraffe sticking its head in a feeding basket with trees in background. Gt2: Giraffe reaching into basked with trees in background. Gt3: A very tall adult giraffe eating from a basket. Gt4: A giraffe is eating out of a basket. Gt5: A giraffe eating from a man made feeder.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>A little girl brushing her hair with a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A little girl brushing her hair with a pink brush.</p><p>Gt1: A young girl tries to comb her own hair. Gt2: A young child brushing her hair with a big pink brush. Gt3: A young girl is trying to brush her hair with a pink brush. Gt4: A little girl with a bunny shirt brushing her hair with a pink brush. Gt5: A young girl is combing her hair and looking at the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Baseline: Three horses grazing in the snow with a sheep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>Three horses grazing in the snow in a field.</p><p>Gt1: Two brown horses standing next to a white horse on a snow covered field. Gt2: A group of horses grazing in the snow. Gt3: Horses eating grass through the snow in a field. Gt4: Three horses are grazing in the snow covered pasture. Gt5: Three horses grazing in the snow with trees in the background. Baseline: A bird is sitting in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A large bird swimming in the middle of a body of water.</p><p>Gt1: A large bird floating out in the water. Gt2: A bird swimming in wavy water, with a island in the background. Gt3: A pelican stands alone in the middle of the ocean. Gt4: There is a large seagull that is swimming in the water. Gt5: A bird is sitting out on the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>Baseline:</p><p>A woman taking a picture of a bathroom mirror.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A woman taking a picture of herself in a bathroom mirror.</p><p>Gt1: A woman is taking a selfie in her bathroom mirror. Gt2: A curly haired girl with glasses takes a selfie infront of the mirror. Gt3: A woman takes a selfie with her phone in the bathroom mirror. Gt4: Woman in green and maroon shirt taking a picture of herself. Gt5: A person in front of a mirror with a cell phone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>Base:</p><p>A woman sitting on a bench.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A young girl sitting on a skateboard.</p><p>Gt1: Cute girl sitting on a skateboard in the driveway. Gt2: A girl sitting on a skateboard in the driveway. Gt3: A girl is sitting on a skateboard outside. Gt4: A girl is sitting on the sidewalk on her skate board. Gt5: A girl in an orange sweater is sitting on a skateboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Baseline: A dog jumping in the air with a frisbee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ExpansionNet v2:</head><p>A dog jumping in the air to catch a frisbee.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8948" to="8957" />
		</imprint>
	</monogr>
	<note>Nocaps: Novel object captioning at scale</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00576</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3558" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Length-controllable image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="712" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Image captioning: Transforming objects into words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05963</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation 9</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ExpansionNet: exploring the sequence length bottleneck in the Transformer for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia Cheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cavicchioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Capotondi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03327</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling up vision-language pre-training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022</meeting>
		<imprint>
			<biblScope unit="page" from="17980" to="17989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">TRANS-BLSTM: Transformer with bidirectional LSTM for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07000</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving image captioning by leveraging intra-and inter-layer global representation in transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1655" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A Better Variant of Self-Critical Sequence Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09971</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dual-level collaborative transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2286" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">X-Linear Attention Networks for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020</meeting>
		<imprint>
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selfcritical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving image captioning with better use of captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11807</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="966" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An integrative review of image captioning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">journal of physics: conference series</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, Recall, and Tell: Image Captioning with Recall Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<idno>07. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12176" to="12183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Endto-End Transformer Based Model for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15350</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">I2t: Image parsing to text description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mun</forename><forename type="middle">Wai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 98</title>
		<meeting>the IEEE 98</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1485" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Vinvl: Making visual representations matter in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RSTNet: Captioning with adaptive attention on visual and non-visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15465" to="15474" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
