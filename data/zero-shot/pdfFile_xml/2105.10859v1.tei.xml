<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse to Fine Multi-Resolution Temporal Convolutional Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
							<email>dipika16@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Rahaman</surname></persName>
							<email>rahul.rahaman@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
							<email>ayao@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse to Fine Multi-Resolution Temporal Convolutional Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal convolutional networks (TCNs) are a commonly used architecture for temporal video segmentation. TCNs however, tend to suffer from over-segmentation errors and require additional refinement modules to ensure smoothness and temporal coherency. In this work, we propose a novel temporal encoder-decoder to tackle the problem of sequence fragmentation. In particular, the decoder follows a coarse-to-fine structure with an implicit ensemble of multiple temporal resolutions. The ensembling produces smoother segmentations that are more accurate and bettercalibrated, bypassing the need for additional refinement modules. In addition, we enhance our training with a multiresolution feature-augmentation strategy to promote robustness to varying temporal resolutions. Finally, to support our architecture and encourage further sequence coherency, we propose an action loss that penalizes misclassifications at the video level. Experiments show that our stand-alone architecture, together with our novel feature-augmentation strategy and new loss, outperforms the state-of-the-art on three temporal video segmentation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We tackle the problem of temporal video segmentation and classification for untrimmed videos of complex activities. A complex activity is usually goal-oriented, e.g. 'frying eggs' and composed of multiple steps or sub-actions in some sequence, e.g. 'Pour Oil', 'Crack Egg', ... 'Put to Plate' over time. The standard framework for temporal segmentation is MS-TCN <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9]</ref>. MS-TCN uses temporal convolutions with progressively larger dilations to maintain a constant temporal resolution throughout the feedforward architecture. Multiple works further augment the MS-TCN model <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46]</ref> with additional model training or postprocessing smoothing.</p><p>We posit that maintaining a constant temporal resolution is suboptimal for handling video sequences. In this work, * indicates equal contribution.</p><p>we propose an encoder-decoder instead: the encoder reduces the temporal resolution to some bottleneck feature before a symmetric decoder gradually recovers the sequence back to the original temporal resolution. Such a shrinkthen-stretch strategy is in line with many other works in vision for image segmentation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b13">14]</ref>, depth and flow estimation <ref type="bibr" target="#b43">[44]</ref> and landmark detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>In fact, encoder-decoders have also been used for video sequence understanding in the past <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>, though their performance is not strong. We believe that possible causes include the simple bottleneck and decoder design. Furthermore, the decoder must be carefully designed for action segmentation because of the significant variation in the subaction lengths.</p><p>To handle different sub-actions lengths, we propose a novel "coarse-to-fine ensemble" of decoder output layers. Our ensemble is not only more accurate, but also has a smoothing effect and yields more accurately calibrated sequences. Incorporation of "coarse-to-fine ensemble" of decoder outputs is a key novelty of our work in comparison to previous temporal Encoder-Decoder architectures <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>. To support our proposed architecture, we incorporate the following two simple yet effective novelties.</p><p>Video-level loss: Firstly, we propose a video-level "Action Loss". Currently, temporal convolutional frameworks for videos are all trained with frame-level losses; however, these do not adequately penalize sequence-level missclassification. To augment the frame-level losses, we introduce a novel video level "Action Loss" to penalize subactions not associated with the complex activity label. Such a loss is highly advantageous in mitigating the effects of over-segmentation and prevent fragmented sequence segmentation.</p><p>Multi-resolution feature augmentation: Although augmentations are quite common in deep learning, as per our knowledge, none of the previous work considers data augmentation for sequence segmentation, likely due to the following reasons. A direct translation of augmentation techniques from images would require perturbations to the video frames <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref>. This would not only be computa-tionally expensive but suffer domain gap problems since the standard is to use pre-computed features from pre-trained networks. As an alternative, we consider augmentation at a feature level <ref type="bibr" target="#b7">[8]</ref> specifically for video sequences. Specifically, we propose "multi-resolution feature-augmentation" strategy, which augments video sequences by considering features of different sampling rates. This adds robustness to the model and allows the network to learn to handle subsampled resolutions of video. Previous works <ref type="bibr" target="#b8">[9]</ref> found that using lower frame-rates drops the accuracy, motivating the need for a full (temporal) resolution framework which is computationally expensive. We can derive higher accuracy with frame rates much lower than that of the original video through our augmentation strategy.</p><p>Uncertainty quantification in Action Segmentation: Due to the direct application of video action segmentation in real-life human activities, models with over-confidently wrong predictions can lead to disastrous consequences. Thus, it is highly crucial for the models to have good prediction uncertainty. Traditional neural networks are known to be over-confident in their predictions <ref type="bibr" target="#b14">[15]</ref>. Among all the available solutions that combat over-confidence, probability ensembles are one of the most effective <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> especially in the presence of abundant training set <ref type="bibr" target="#b35">[36]</ref>. We thus highlight the calibration performance of our proposed method in this work.</p><p>To summarize, our main contributions are (1) We design an novel temporal Encoder-Decoder architecture C2F-TCN for temporal segmentation. This utilizes the coarse-to-fine resolutions of decoder outputs to give more calibrated, less fragmented, and accurate outputs.</p><p>(2) We propose a multi-resolution temporal feature-level augmentation strategy that is computationally more efficient and gives a significant improvement in accuracy.</p><p>(3) We propose an "Action Loss" which penalizes misclassifications at the video level and enhances our framelevel predictions. This global video-level loss serves as an auxiliary loss that complements the frame-level loss without any additional network structure.</p><p>(4) We adapt our temporal segmentation model C2F-TCN to recognize the minutes-long video and achieves SOTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action Recognition. The task of recognition requires classifying the video as whole whereas task of segmentation requires classifying each frame of the video. For short trimmed videos(2?10 sec) architectures for action recognition include two-stream 2D CNNs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>, CNNs together with LSTMs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref>, 3D CNNs <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b42">43]</ref>. More recent architectures include a two-stream 3D-CNN (I3D) <ref type="bibr" target="#b1">[2]</ref> , non-local blocks <ref type="bibr" target="#b49">[50]</ref>, SlowFast network <ref type="bibr" target="#b10">[11]</ref> and the Temporal Shift Module <ref type="bibr" target="#b32">[33]</ref>.</p><p>For longer, untrimmed videos, the computational cost of directly training such deep 3D convolutional architectures is computationally very expensive and data deficit. Most approaches resort to extracting snippet-level features based on the previously-mentioned architectures and add additional dedicated sequence-level processing for either recognition or segmentation. Examples include TCN networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref>, RNNs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref>, graphs <ref type="bibr" target="#b20">[21]</ref>, dedicated architectures like the temporal inception network <ref type="bibr" target="#b19">[20]</ref> or attention modelling <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. Our work falls into this category where we model temporal relationships on top of snippet level I3D features.</p><p>For only long video recognition, some use temporal convolutions <ref type="bibr" target="#b19">[20]</ref>, attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> or graph modelling <ref type="bibr" target="#b20">[21]</ref>; others use aggregating models likes ActionVLAD <ref type="bibr" target="#b12">[13]</ref> and some generating framework for recognition <ref type="bibr" target="#b24">[25]</ref>. Different from these we adapt our segmentation model to be used for temporal action recognition task as it captures both global relationships and local fine-grained information for recognizing the video action.</p><p>Action Segmentation. Initially, models for temporal video segmentation were based on statistical length modelling <ref type="bibr" target="#b36">[37]</ref>, RNNs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref>, and RNNs combined with Hidden Markov models <ref type="bibr" target="#b26">[27]</ref>. The recursive nature of RNNs make them slow and difficult to train, especially for long sequences. More recent architectures treat the entire video sequence as a whole and model coarse units within the sequence either via non-local attention <ref type="bibr" target="#b38">[39]</ref> or graph convolutional networks <ref type="bibr" target="#b18">[19]</ref>. However, these approaches cannot account for the fine-grained temporal changes in videos, required for segmentation.</p><p>Several works have shown the effectiveness of temporal convolutional networks (TCN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>. MS-TCN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref> stacks a temporal convolutions with dilation, without max-pool to capture the relationships in long videos. Encoder-decoder TCN's <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> uses gradual reduce and expanded resolution convolutions. In line with Encoder-Decoder TCN's, we design our model architecture C2F-TCN. However, we are first to explore the coarse-tofine decoder layer's ensemble. Segmentation with TCN still suffers from over-fragmentation errors, which are handled with refinement layers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, LSTMs on top of refinement layers <ref type="bibr" target="#b45">[46]</ref>, or separate boundary detection model with postprocessing smoothing <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b22">23]</ref>. Different from these, we handle over-fragmentation with our implicit model's components ensembling which does not require any additional network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The aim of video action segmentation is to assign a label to each frames of a video. More formally, it deals with video V ? R T ?H?W , where for each temporal location t ? T := {1, ..., T }, frame V t ? R H?W is an image of dimension H ? W . With a pre-defined set of C action classes A := {1, ..., C}, the task of action segmentation is to find a mapping? : T ? A that maps each frame V t to an action label? t ? A. Our method is supervised; the model M takes a video V as input and produces predictions</p><formula xml:id="formula_0">M (V) = {p t } T t=1 where each p t ? R C is a probability vector of dimension C.</formula><p>The predicted label for each t is then obtained by? t = arg max p t and the corresponding probability byp t = max p t , with the max and arg max over all possible actions in A. To overcome the computational challenge of training an end-to-end model, the standard practice is to use pre-trained frame level features. We work with such pre-trained feature representation of every frame t denoted by f t ? R d . Furthermore, instead of using the features at full temporal length T , we down-sample f to obtain coarser features f in of temporal dimension T in , which we use as input to our model. We discuss the details of our down-sampling strategy in subsection 3.3. However, it should be noted that for each video during inference, we always up-sample our predictions {? t : t ? T in } to the original temporal dimension T , in order to compare with original ground truth {y t } T t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture:</head><p>Our model's architecture can be separated into three components, i.e. M := (? : ? : ?), consisting of the encoder ?, bottleneck ? and decoder ?. Below we discuss each component of the architecture in more details.</p><p>Encoder network ?: The input to the encoder network is the down-sampled frame-level features f in ? R T in ?d , where d denotes channel dimension of input feature. The encoder network consists of a 1-D convolution unit ? (0) and six sequential encoder blocks {? (i) : i ? 6}. At the beginning, ? (0) projects f in to feature of dimension T in ?d 0 ; for i ? 1, the outputs of ? (i) are R Ti?di , where T i and d i are the temporal and feature dimensions of each block i re-spectively. Each encoder consists of a convolutional block and a max-pooling unit that halves its input's temporal dimension.</p><p>Bottleneck network ?: To ensure flexibility in the input video resolution and to facilitate temporal augmentations, we introduce a pyramid pooling <ref type="bibr" target="#b16">[17]</ref> along the temporal dimension. Pyramid pooling has been used in the past to handle multi-resolution inputs for segmentation, detection, and recognition task in both images and videos <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>The input to the bottleneck ? is the output f en of the last encoder layer ? <ref type="bibr" target="#b5">(6)</ref> . We create four redundant representations of f en of varying temporal resolutions via four parallel max-pooling units of varying kernel sizes {w ? i : i ? 4}. The max-pooling reduces f en to a smaller temporal dimension T en w ? i ; each feature is then collapsed by a shared 1D convolution of kernel size 1 to a single latent dimension while the temporal dimension is kept fixed. These four redundant representations are then up-sampled with linear interpolation back to the original temporal dimension T en . Along with the features f en , the four features of dimension T en ? 1 are concatenated along latent dimension to produce a bottleneck output of R T en ?(4+d en ) . Owing to the shared nature of the single 1-D convolution unit, the bottleneck network contains marginal parameters.</p><p>Decoder network ?: Structurally, the decoder network is symmetric to the encoder; it has six decoder layers {? (i) : i ? 6}, each containing an up-sampling unit and the same convolution block as the encoder (see <ref type="figure" target="#fig_0">Fig. 1</ref>). For each i ? 1, the up-sampling unit linearly interpolates or stretches inputs to an output of twice the temporal length. This is then concatenated with ? (6?i) , i.e. the output of the (6 ? i) th encoder block via a skip connection. The output of the i th decoder block thus has temporal dimension same as T 6?i and latent dimension 128. The skip-connection ensures that both global information (from the decoder) and local information (from the encoder). The last decoder layer ? <ref type="bibr" target="#b5">(6)</ref> , with a skip connection from ? (0) generates output of</p><formula xml:id="formula_1">R T in ?128 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Coarse-to-Fine Ensemble (C2F Ensemble):</head><p>Rather than taking the last decoder layer as the output, we propose to ensemble the results from several decoder layers. For each i ? 1, we project the output of the i th decoder block ? (i) to C dimensions, i.e. the number of actions and follow with a softmax operation to produces probability vectors p (i) which get up-sampled with linear interpolation to the input temporal dimension T in and ensembled. Thus, for any 1 ? t ? T in , the ensembled prediction p ens t ? R C takes the form -</p><formula xml:id="formula_2">p ens t = i ? i ? Up p (i) , t , i ? i = 1, ? i &gt; 0 (1)</formula><p>where ? i are the ensemble weights of the i th decoder layer, and Up(?, t) is a function that returns the interpolated vector at time t. The sum in Eq. 1 is performed action class-wise, with the final predicted action label a?</p><formula xml:id="formula_3">y t = arg max k?A p ens t .<label>(2)</label></formula><p>The ensembled predictions p ens is used during both training and inference; we refer to it as a coarse-to-fine ensemble (C2F ensemble). Our rationale for using an ensemble is twofold. First, the earlier decoder layers are less susceptible to fragmentation errors by being coarser in their temporal resolution. Incorporating these outputs helps to mitigate over-segmentation. Such multi-resolution ensemble is made possible due to the shrink-then-stretch policy of encoder-decoder architecture. In contrast, architectures such as MS-TCN keep the temporal dimension unchanged, requiring additional refinement stages to correct over-segmentation. Secondly, standard network outputs tend to be over-confident in their predictions <ref type="bibr" target="#b14">[15]</ref> which we discuss later in section 4.5. Probability ensembles are an effective way <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> to reduce overconfidence, especially with sufficient training data <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Resolution Feature Augmentation:</head><p>As discussed earlier, we down-sample the pre-trained feature representations f to obtain the input feature vector f in t ? R d and ground truth y in t ? A. Instead of standard subsampling, we use max-pooling, which has been found to be beneficial for representing video segments in <ref type="bibr" target="#b38">[39]</ref>. Instead of max-pooling with a fixed window size, we consider multiple resolution of the features by varying the window. At time t, for some temporal window w &gt; 0, we max-pool along the temporal dimension within a temporal window of w:</p><formula xml:id="formula_4">f w t = max ? ?[wt,wt+w) f ? ,<label>(3)</label></formula><p>while taking the ground truth action that is most frequent in the window [wt, wt + w) as the corresponding label</p><formula xml:id="formula_5">y w t = arg max k?A wt+w ? =wt I[y ? = k]<label>(4)</label></formula><p>The pooled features f w := {f w t } and ground truth y w := {y w t } both have a temporal dimension of T w := T w . By using a varying set of windows W ? N, a corresponding set of feature-ground truth pair D aug := {(f w , y w ) : w ? W} can thus be obtained.</p><p>By equipping W, and by extension D aug , with a probability distribution ?, we formulate a stochastic augmentation strategy. We work with a specific class of probability distribution ? := {?(? ; w 0 ) : w 0 ? N} parameterized by a "base window" w 0 . For a base window, we define an upper and lower bound w max := 2w 0 and w min := w0 2 , and r := w max ? w min . Then we define distribution ? as</p><formula xml:id="formula_6">?(w; w 0 ) = ? 0 : w = w 0 (1 ? ? 0 )/r : otherwise</formula><p>where 0 ? ? 0 ? 1 is the probability of sampling the base window. We assign zero probability to any window size outside the bounds. In our experiments we found ? 0 = 1 2 to be most effective.</p><p>Apart from the obvious advantage of the increased effective size of the training data, this augmentation strategy encourages model robustness with respect to a wide range of temporal resolutions. At test time, we can also combine predictions for different temporal windows.</p><p>Before combining, all the predictions are interpolated to the original temporal dimension T of the test example. We compute the expectation of the prediction probabilities under the distribution ?. Formally, for any t ? T and k ? A our final predictive likelihood is</p><formula xml:id="formula_7">P(? t = k) = E w?? P(? t = k | f w )<label>(5)</label></formula><p>where P(? t = k | f w ) is the conditional probability of the frame t being labelled with action class k given input features f w max-pooled with window w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Losses:</head><p>During our training procedure, we make use of three different losses. The first, L CE , is the standard frame-level cross-entropy for action classification:</p><formula xml:id="formula_8">L CE = ? 1 T t k?A I[y t = k] log P[? t = k]<label>(6)</label></formula><p>where y t ,? t are the ground truth and the predicted label respectively. The second is the transition loss used in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>  L TR :</p><formula xml:id="formula_9">L TR = 1 T t min (? t , ? max ) 2 ? t := log p ens t ? log p ens t?1 .<label>(7)</label></formula><p>to encourage neighbouring frames to have the same action label. Here, p ens t ? R C + is our multi-resolution probability outputs, and the ? max ? R + is the cutoff used to clip the inter-temporal absolute difference of log-probabilities. All the vector operations in equation 7 are performed elementwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Video-Level Action Loss L AL</head><p>The complex activity itself serves as a very strong cue for the actions present depending on the complex activity. For example, 'crack egg' should not appear in 'making coffee', but the standard frame-level cross-entropy would penalizes it the same as other wrong but feasible actions. To stronger enforce the relationships of complex activities to the actions, we propose a novel video-level auxiliary loss. For a video V we define A V ? A to be the set of unique ground truth actions present in the video. Let ? pres k := I[k ? A V ] be the indicator whether the action k is present in video V and ? pres k := max t P[? t = k] be the maximum action probability assigned to class k across the whole video. We define our Video-level Action Loss as</p><formula xml:id="formula_10">L AL = ? k?A ? pres k ? log ? pres k ? k?A (1 ? ? pres k ) ? log(1 ? ? pres k ).<label>(8)</label></formula><p>This loss ensures that we maximize the ? pres k for all actions present in a video. More importantly, though, it allows us to minimize ? pres k for any action k not present in the entire video, thereby limiting misclassifications by actions not related to the complex activity.</p><p>The three loss terms can be summed into a joint loss L:</p><formula xml:id="formula_11">L = L AL + L CE + ? TR L TR<label>(9)</label></formula><p>where ? TR = 0.15 as suggested by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complex Activity Recognition:</head><p>Our framework can be adopted easily from video segmentation to classify the overall complex activity. We use the encoder-decoder architecture as described in Section 3.1 to obtain output p t and then max-pool over time before applying a two-layer MLP followed by a softmax:</p><formula xml:id="formula_12">p V = MLP max t (log p t ) ,<label>(10)</label></formula><p>where p V ? R K + is the probability vector for the K complex activities. Intuitively, max-pooling along the temporal dimension retains the important information over time and is is invariant to permutation of action orders within the video. Similar to other works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, we do not use any frame wise sub-action labels. Instead, we train our segmentation network separately with the following loss</p><formula xml:id="formula_13">L V = ? K k=1 I[y V = k] log P[? V = k].<label>(11)</label></formula><p>where y V ,? V are the ground truth and the predicted complex activity of video sequence V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Calibration:</head><p>The calibration of a prediction is a measurement of over/under-confidence. Earlier we defined maximum probability prediction of a frame t to bep t := max p t . In calibration literature,p t ? [0, 1] is termed as confidence of the prediction? t = arg max p t . The accuracy of a confidence value p ? [0, 1], denoted by Acc(p) is the action classification accuracy of frames with maximum probability predictionp t equal to p. Ideally, one would like the Acc to be high for high values of confidence and vice-versa. A model is calibrated if Acc(p) = p, ?p ? [0, 1] and it is called over-confident (or under-confident) if Acc(p) ? p (or Acc(p) &gt; p). The above definition of Acc is often made practical by calculating the accuracy for a range of confidence values P ? [0, 1], rather than one particular value p. Thus the modified definition becomes -</p><formula xml:id="formula_14">Acc(P) := t I[? t = y t ] ? I[p t ? P] t I[p t ? P]</formula><p>.</p><p>For P = [0, 1], the definition of accuracy reduces to the standard classification accuracy. We use this notion of confidence and accuracy to later measure the calibration performance of temporal segmentation in sub-section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets, Evaluation, and Implementation:</head><p>We evaluate on three standard action segmentation benchmarks: Breakfast Actions <ref type="bibr" target="#b24">[25]</ref>, 50Salads <ref type="bibr" target="#b41">[42]</ref> and GTEA <ref type="bibr" target="#b9">[10]</ref>. Breakfast Actions is a third-person view dataset of 1.7k videos of 52 subjects performing ten highlevel tasks for making breakfast. On average, the videos are 2.3 minutes long with 6 subactions (a total of 48 possible actions). 50Salads has top-view videos of 25 people preparing 2 mixed salads each, totally 50 videos with 19 different sub-actions. The videos have average length of 6.4 minutes and an average of 20 actions. GTEA captures 28 egocentric videos of 7 complex activities with 11 sub-actions. The average duration of videos is 1.5 minutes with 20 sub-action instances.</p><p>For evaluation, we report Mean-over-frames(MoF), segment-wise edit distance (Edit) and F 1-scores with IoU thresholds of 0.10, 0.25 and 0.50(F 1@{10, 25, 50}). For all three datasets, we use features pre-extracted from an I3D model <ref type="bibr" target="#b1">[2]</ref> pre-trained on Kinetics, and follow the k-fold cross-validation averaging to report our final results. Here k = {4, 5, 4} for Breakfast, 50Salads and GTEA respectively. The evaluation metrics and features follow the convention of other recent temporal video segmentation methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Implementation Details In our experiments we train using an Adam optimizer for 600 epochs, with learning rates of {10 ?4 , 3?10 ?4 , 5?10 ?4 }, weight decay of {3 ? 10 ?3 , 10 ?3 , 3 ? 10 ?4 }, batch size of {100, 25, 11} and a base window for sampling w 0 of {10, 20, 4} for Breakfast, 50Salads and GTEA respectively. While choosing the base window we ensure it is small enough not to drop any sub-action or fragments. The ensemble weights used were ? i = 1 4 , ?i ? 3 for all three datasets. We find for i ? 2, the results of the ensemble degrade due to very coarse temporal resolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies &amp; Model Analysis:</head><p>In <ref type="table">Table 1</ref> we perform a component-wise analysis of our core contributions. In the first row, we start with the base model M := (? : ? : ?) with the probability outputs p <ref type="bibr" target="#b5">(6)</ref> from the last decoder layer. This base model already outperforms other temporal encoder-decoder architectures such as ED-TCN <ref type="bibr" target="#b28">[29]</ref> and Residual Deformable ED-TCN <ref type="bibr" target="#b30">[31]</ref> (see <ref type="table">Table 2</ref>). The gradual addition of each component which we detail below, shows a steady improvement of results.</p><p>Impact of Coarse-to-Fine Ensemble: Adding the decoder ensemble to the base model gives a significant improvement in Edit and F1 scores on all three datasets. The Edit score improves by +5.7%, +7.5 %, +2.7 % on Breakfast, 50Salads and GTEA; the extent of improvement directly aligns with the average length of the videos. The improvement over the base model in F1@50 is also a significant +5.7 %, +3.9 %, +4.4 %; this demonstrates the usefulness of our decoder ensembling. With only the ensembling, we can exceed the performance of MS-TCN++ (see <ref type="table">Table 2</ref> row 5) in all the scores on Breakfast Actions dataset.</p><p>Analysis of Coarse-to-Fine Ensemble: <ref type="figure">Figure 2</ref> shows a qualitative and quantitative analysis of the ensemble compared to the individual decoder layers ? (i) which form the ensemble. The left plot shows our model's segmentation output on a sample video where each color denotes a subaction. The output from the last decoder layer ? <ref type="bibr" target="#b5">(6)</ref> becomes over-fragmented (blue patch), which does not corrupt the ensembled result. Plotting the MoF, Edit, and F 1@50 scores (see <ref type="figure">Fig. 2 right)</ref>, we find that the fourth decoder layer(? (4) ) has the strongest individual performance and that these results erode with further decoding. However, incorporating all the layers in the C2F ensemble yields a significant boost, especially in Edit distance, which is the measure most affected by fragmentation.</p><p>Number of Layers used in C2F Ensemble: We mention that we use ensemble weights with ? i = 1 4 , ?i ? 3 and ? i = 0, ?i ? 2. This is found using experimental validation shown in <ref type="table">Table 3</ref>. The first column shows the unnormalized weights. Here we perform an ablation study in Breakfast dataset on how many layers of Decoder's output are useful for ensembling. We use our final proposed model with C2F Ensemble, Action Loss, Train, and Test Augmentation and only modify the weights ? i in the ensemble to perform the ablation. We see that we get the maximum of Edit, MoF and F1 scores at Last four layers, i.e. when ? i = 1/4, ?i ? 3 and ? i = 0, i ? 2.</p><p>Impact of Multi-Resolution Feature-Augmentation Strategy: <ref type="table">Table 1</ref>   <ref type="table">Table 3</ref>. Ablation study on number of layers used in C2F Ensemble on Breakfast dataset. mentation strategy, with the highest increase for 50Salads with +10.1% F1@50. Interestingly, we also observe some decrements for GTEA; we speculate that this is due to some very short segments getting vanished in some of the coarser windows of test time augmentation. Impact of Video-Level Action Loss L AL : <ref type="table">Table 1</ref> row 4 shows that adding this loss is also beneficial. Edit Scores' improvement is +1.7%, +0.4%, +0.6% in Breakfast, 50Salads, and GTEA datasets. The maximum improvement in out-of-activity prediction is on Breakfast Actions because it has the most sub-actions 48 compared to 19 and 11 for 50Salads and GTEA.</p><p>Temporal Pyramid Pooling layer ?: In the last row of <ref type="table">Table 1</ref> we show that removing the Temporal Pooling Layer lowers the scores in all the datasets and metrics. It highlights the importance of including a multi-resolution hidden feature representation at the bottleneck. <ref type="table">Table 2</ref> compares our performance against recent and related SOTA segmentation approaches. All the listed works use the same I3D features and evaluation splits. There are few other works on temporal segmentation which are not directly comparable <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5]</ref>. SSTDA [5] uses self-supervised task's adapted features to train the segmentation architecture, Alleviating-Over-segmentation <ref type="bibr" target="#b22">[23]</ref> uses features extracted from fully-trained MS-TCN <ref type="bibr" target="#b8">[9]</ref> architecture to train segmentation architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with SOTA:</head><p>Our model outperforms the SOTA scores by +5.6%, +2.6 %, +3.2 % on MoF, F1@50 and F1@25 scores on Breakfast, the largest of three datasets. Our Edit score is slightly lower than GateR <ref type="bibr" target="#b45">[46]</ref>. However, GateR's MoF is -8.3 % less than ours. For the smaller 50Salads, we outperform the SOTA scores by +2.1 %, +2.0 % on Edit and F1@10. For F1@50, we are slightly lower than BCN <ref type="bibr" target="#b50">[51]</ref>, but for all other measures and datasets, BCN is worse. On GTEA, we outperform SOTA scores on all metrics with +1.0%, 2.0%, 1.7% on MoF, Edit, and F1@25 scores. We conclude from these strong evaluation scores that our method can generalize to different types of videos and datasets.</p><p>Impact of Video Length: For a closer look, we split the videos into three length categories and tally our results. In comparison, we train an MS-TCN++ <ref type="bibr" target="#b31">[32]</ref> model, which achieves comparable or higher scores than reported in the original paper in all metrics. In the <ref type="table" target="#tab_2">Table 5 we</ref>  Unlike the standard calibration curve, we plot the difference between accuracy and confidence in our y-axis. The ideal line is y = 0, with y &gt; 0 and y &lt; 0 denoting under-confidence and over-confidence respectively. The first plot shows the comparison between layers, and the middle plot compares the calibration of MSTCN++, our C2F ensemble, and our final prediction (with test time augment). Performance-wise our final predictions are more calibrated than C2F ensemble, which is more calibrated than any other decoder layer and MSTCN++. (Rightmost) plot is the density of the entropy of probability for incorrect predictions. Our C2F ensemble is more uncertain about wrong predictions than MSTCN++.</p><p>MoF % for various video lengths. We observe that after training augmentation (row 3), our performance improves regardless of the videos' length. For the longer videos (? 2.5 mins), our final proposal achieves +5.7% MoF over the MS-TCN++ model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of Complex Activity Recognition:</head><p>In <ref type="table">Table 4</ref> we show our results on Action Recognition task. We compare against several methods with varying architectures <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> ranging from graphs to multiscale self-attention dedicated in their design for long-video recognition. Unlike these works, we adapt a segmentation architecture but find that we can still outperform these other works. For a fair comparison, we use the same splits provided by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>, which has {1357 : 335} videos for training and evaluation. We use features from I3D pretrained on Kinetics <ref type="bibr" target="#b1">[2]</ref> dataset, and it is not fine-tuned for the Breakfast Action dataset. Our base model is +15.4 % above other methods without fine-tuned features and competitive with other methods that do use fine-tuned features. Adding the subsequent components steadily improves our results surpassing SOTA that uses fine-tuned I3D features by +5% even though our features are not fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Uncertainty Quantification:</head><p>As motivated in section 1 it is crucial to evaluate models dealing with videos based on uncertainty quantification. Hence, we evaluate our model by comparing its calibration and prediction uncertainty with the trained MS-TCN++ model of 4.3. We follow the notations defined in sub-section 3.6. To make the calibration plot, we partition the unit interval [0, 1] into N equal length bins P n := ( n N , n+1 N ], and compute Acc n := Acc(P n ). We then obtain the 2-D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Feature Not Fine Fine Tuned Tuned ActionVLAD <ref type="bibr" target="#b12">[13]</ref> I3D 65.5 82.7 VideoGraph <ref type="bibr" target="#b20">[21]</ref> I3D 69.5 -Timeception <ref type="bibr" target="#b19">[20]</ref> I3D 71.3 86.9 Generative <ref type="bibr" target="#b25">[26]</ref> FV <ref type="bibr" target="#b46">[47]</ref> 73.3 -PIC <ref type="bibr" target="#b21">[22]</ref> I3D -89.9 Actor-Focus <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>. Complex Activity Recognition: we outperform previous SOTA without using fine-tuned features curve {( 2n+1 2N , Acc n ) : 1 ? n ? N } in which the x-axis denotes confidence (mid-point of interval P m ) and y-axis denotes its corresponding accuracy. For better visualization, we plot the difference between accuracy and confidence in the y-axis. In the first two plots of <ref type="figure" target="#fig_2">figure 3</ref> we compare the calibration performance of predictions from our final proposed stack (with train and test augment), C2F ensemble prediction, MSTCN++, and prediction from different decoder layers. Our final proposal gives the most calibrated result. In addition, our C2F ensemble is more calibrated than MSTCN++. For the last plot, we calculate the Shannon Entropy of probability predictions for incorrect segmentation. Higher entropy indicates more uncertainty in prediction. We plot the density of the calculated entropy for MSTCN++ and our C2F ensemble. Our ensemble predictions are more uncertain when the model is wrong.  In addition to comparing MSTCN++ with our proposed method (denoted as our proposed setup), we also compare the resource used when our proposed method is run with full resolution inputs (denoted by our model (full resolution)) as done in MSTCN++. All the graphs terminate when the batch size could not be fit into a single GPU. For better visualization, some graphs use a logarithmic scale on the x-axis (batch size). Our method takes considerably less computing time and memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Computational efficiency:</head><p>In this section, we discuss the computational benefits in terms of memory-usage and compute-time of our proposed method compared to MSTCN++ <ref type="bibr" target="#b31">[32]</ref>.</p><p>The computation gains stem from two main reasons:</p><p>1. Shrink-then-stretch of temporal resolution compared to maintaining the same temporal resolution architecture of MS-TCN/MS-TCN++ <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>2. Feature sub-sampling obtained from temporal pooling as described in Multi-Resolution Feature Augmentation subsection 3.3 compared to using full resolution features as motivated in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>We report two metrics: (1) Average Training Compute Time per video, (2) Maximum GPU Memory usage during training. To report the compute time, we exclude the time taken for data preparation and transfer time between devices, thus only capturing the time taken for the forward and backward pass during training. We report all our metrics based on a single Nvidia GeForce RTX 2080 Ti GPU with 10.76 Gb of memory. We compare our architecture to the base model MS-TCN++ <ref type="bibr" target="#b31">[32]</ref>. We further note that other methods in the SOTA comparison, like BCN <ref type="bibr" target="#b50">[51]</ref> and Gat-edR <ref type="bibr" target="#b45">[46]</ref>, all use MS-TCN++ as their base architecture, with additional model components to resolve over-segmentation. We, therefore, assume that these works would have simi-lar or higher time and memory consumption compared to MS-TCN++.</p><p>In <ref type="figure" target="#fig_4">Figure 4</ref>, we compare our method with MSTCN++ in terms of the two metrics defined above. MS-TCN++ uses a full-resolution setup, and we show that our model C2F-TCNworks best with a sub-sampled version. For a fair comparison, we show our C2F-TCNusing a full-resolution set of features (i.e., where the window of temporal pooling is w 0 = 1), similar to MS-TCN++, and also compare C2F-TCNwith sub-sampling by a factor of 10, (i.e., where w 0 = 10 as in our final proposal). The blue curve denotes MSTCN++; the orange curve denotes our C2F-TCNwith full-resolution features; the red curve denotes our C2F-TCNwith sub-sampled features. The x-axis of each plot shows the batch size.</p><p>We increase the batch size from 5 to the maximum that fits in the one single RTX2080 GPU (20 for MS-TCN++, 25 for our model at full resolution, and 400 for sub-sampled features). In a similar setup, with the maximum batch size, the minimum processing time for a single video for MS-TCN++ is 0.11 seconds, while our model is 0.03 seconds at full-resolution and 0.003 seconds when subsampled, i.e. a speedup of more than 30X. In conclusion, we show that our method takes less resources even when we use full resolution features; using sub-sampled features as proposed in our setup allows for even further reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Details of Encoder-Decoder Architecture</head><p>Here we give the detailed model architecture explained in subsection 3.1. To define our model, we first define a block called double conv block where double conv(in c, out c) = Conv1D(in c, out c, kernel=3, pad=1) ? ? Batch-Norm1D(out c) ? ? ReLU() ? ? Conv1D(out c, out c, ker-nel=3, pad=1) ? ? BatchNorm1D(out c) ? ? ReLU(); in c denotes input channel's dimension and out c denotes the output channel's dimension. Using this block, we define our model M detailed in the <ref type="table" target="#tab_3">Table 6</ref>. The output from ? i is then projected to number of classes and followed by a softmax operation to produce probability vectors p (i) as described in subsection 3.1. Our model has a total of 4.08 million trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We design a temporal encoder-decoder model with a coarse-to-fine ensemble of decoding layers which achieves state-of-the-art performance in temporal segmentation. Our model produces calibrated predictions with better uncertainty measures which is otherwise crucial for real-world deployment. In addition, we propose a simple and computa-tionally effective augmentation strategy at the feature level which significantly improves results. Such an augmentation strategy can be applied in other works in segmentation or sequence processing. Interestingly, our segmentation architecture allows us to achieve state-of-the-art performance in the complex activity recognition task, opening the possibility for further investigation along this front.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>InputFigure 1 .</head><label>1</label><figDesc>Our segmentation architecture: Depiction of the architecture of our model (? : ? : ?). We utilize our multi-resolution features to produce Coarse-to-fine Ensemble predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 TimeFigure 2 .</head><label>62</label><figDesc>Performance of different decoder layers: The left plot shows qualitative example of our model's video segmentation result, where each color denotes an sub-action. We see C2F ensemble(C2F ens) matches best to the ground truth(GT) than other layers. Additionally, over-fragmentation (blue) patch removed from the last decoder layer (? 6 ). The right bar chart shows quantitative overall performance of different layers and C2F ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Uncertainty quantification: (Left and middle) plot show calibration curves of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>x) vs. Average seconds per video (y) Batch size (x) vs. Maximum memory GB (y) MSTCN++ Our model (full resolution) Our proposed setup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Computational resources: In the first two figure, we compare the average time (in seconds) per example used during training on Breakfast dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>row 3 shows that adding feature augmentation during training provides a boost in all scores across all datasets. The maximum improvement of scores with train augmentation strategy is seen in Breakfast Action dataset with +3.2% Mof and +3.4% Edit scores. In row 5, we observe additional improvement with test-time aug-1@{10,<ref type="bibr" target="#b24">25</ref>, 50} Edit MoF F 1@{10, 25, 50} Edit MoF F 1@{10, 25, 50} Edit MoF Base Model ?, ?, ? 56.6 52.5 43.4 57.4 65.8 67.5 64.3 53.9 59.1 77.5 87.1 82.6 69.3 81.4 77.3 (+) C2F Ensemble 64.5 60.4 49.1 63.1 70.2 72.3 68.8 57.8 66.6 78.4 88.1 86.8 73.7 84.1 78.5 (+) Train Augment 69.4 65.9 55.1 66.5 73.4 75.8 73.1 62.3 68.8 79.4 90.1 87.8 74.9 86.7 79.5 (+) Action Loss 70.1 66.6 56.2 68.2 73.5 76.6 73.0 62.5 69.2 80.1 90.5 88.5 77.1 87.3 80.3 (+) Test Aug. (final) 72.2 68.7 57.6 69.6 76.0 84.3 81.8 72.6 76.4 84.9 90.3 88.8 77.7 86.4 80.8 (-) TPP layer ? 69.9 66.6 56.5 66.9 75.1 81.7 79.9 71.0 74.0 83.9 89.6 88.3 77.4 86.3 80.4Ablation study on each component of our proposal. We gradually add (+) each part of our proposed method to show its effectiveness. To highlight the fact that temporal pyramid pooling is most effective when inputs are of varying resolution, we show its ablation as removal (-) only after we add train and test augmentation to our method stack. Comparison with recent related work. Our proposed model exceeds in most of the scores across all datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Breakfast</cell><cell></cell><cell>50Salads</cell><cell>GTEA</cell></row><row><cell>Method</cell><cell></cell><cell cols="3">F Breakfast</cell><cell></cell><cell>50Salads</cell><cell>GTEA</cell></row><row><cell>Method</cell><cell cols="4">F 1@{10, 25, 50} Edit</cell><cell cols="2">MoF F 1@{10, 25, 50} Edit</cell><cell>MoF F 1@{10, 25, 50} Edit</cell><cell>MoF</cell></row><row><cell>ED-TCN[29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">43.3 68.0 63.9 52.6 52.6 64.7 72.2 69.3 56.0 -</cell><cell>64.0</cell></row><row><cell>TDRN[31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">72.9 68.5 57.2 66.0 68.1 79.2 74.4 62.7 74.1 70.1</cell></row><row><cell>MSTCN[9]</cell><cell cols="7">52.6 48.1 37.9 61.7 66.3 76.3 74.0 64.5 67.9 80.7 85.8 83.4 69.8 79.0 76.3</cell></row><row><cell>GTRM[19]</cell><cell cols="7">57.5 54.0 43.3 58.7 65.0 75.4 72.8 63.9 67.5 82.6 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MSTCN++[32]</cell><cell cols="7">64.1 58.6 45.9 65.6 67.6 80.7 78.5 70.1 74.3 83.7 87.8 86.2 74.4 82.6 78.9</cell></row><row><cell>GatedR[46]</cell><cell cols="7">71.1 65.7 53.6 70.6 67.7 78.0 76.2 67.0 71.4 80.7 89.1 87.5 72.8 83.5 76.7</cell></row><row><cell>BCN[51]</cell><cell cols="7">68.7 65.5 55.0 66.2 70.4 82.3 81.3 74.0 74.3 84.4 88.5 87.1 77.3 84.4 79.8</cell></row><row><cell>Ours proposed</cell><cell cols="7">72.2 68.7 57.6 69.6 76.0 84.3 81.8 72.6 76.4 84.9 90.3 88.8 77.7 86.4 80.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">25, 50} Edit MoF</cell></row><row><cell>{0, 0, 0, 0, 0, 1}</cell><cell></cell><cell cols="5">65.7 62.2 52.7 65.3 75.8</cell></row><row><cell>{0, 0, 0, 0, 1, 1}</cell><cell></cell><cell cols="5">67.0 63.2 52.9 65.0 75.5</cell></row><row><cell>{0, 0, 0, 1, 1, 1}</cell><cell></cell><cell cols="5">67.6 64.2 54.6 66.9 75.3</cell></row><row><cell>{0, 0, 1, 1, 1, 1}</cell><cell></cell><cell cols="5">72.2 68.7 57.6 69.6 76.0</cell></row><row><cell>{0, 1, 1, 1, 1, 1}</cell><cell></cell><cell cols="5">71.4 67.8 57.1 68.8 74.8</cell></row><row><cell>{1, 1, 1, 1, 1, 1}</cell><cell></cell><cell cols="5">70.6 66.5 55.6 67.6 74.4</cell></row></table><note>{? 1 , ? 2 , ? 3 , ? 4 , ? 5 , ? 6 } F1@{10,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>MoF for varying lengths of videos from Breakfast Dataset.</figDesc><table><row><cell>Duration</cell><cell cols="3">? 1 min &gt; 1 and ? 2.5 &gt; 2.5 min</cell></row><row><cell>No. of Videos</cell><cell>534</cell><cell>584</cell><cell>594</cell></row><row><cell>MS-TCN++[32]</cell><cell>68.7</cell><cell>70.5</cell><cell>70.2</cell></row><row><cell>Ours C2F Ensemble</cell><cell>68.9</cell><cell>69.8</cell><cell>69.7</cell></row><row><cell>(+) Train-Aug</cell><cell>72.9</cell><cell>72.9</cell><cell>72.7</cell></row><row><cell>(+) Test-Aug (final)</cell><cell>73.0</cell><cell>73.3</cell><cell>75.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Encoder-Decoder Architecture M = (?, ?, ?)</figDesc><table><row><cell></cell><cell></cell><cell>Upsample1D(2)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>concat ?1(128, 256)</cell><cell>T in 2 ? 128</cell></row><row><cell></cell><cell></cell><cell>double conv(384, 128)</cell><cell></cell></row><row><cell>?6</cell><cell>T in 2 ? 128 Tin ? 256</cell><cell>Upsample1D(2) concat ?0(128, 256)</cell><cell>Tin ? 128</cell></row><row><cell></cell><cell></cell><cell>double conv(384, 128)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Long-term behaviour recognition in videos with actorfocused region attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ombretta</forename><surname>Strafforello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klamer</forename><surname>Schutte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action segmentation with joint selfsupervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature re-learning with data augmentation for video relevance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ce-net: Context encoder network for 2d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaying</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR 70</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34 th International Conference on Machine Learning</title>
		<meeting>the 34 th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving action segmentation via graph-based temporal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Timeception for complex action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05143</idno>
		<title level="m">Videograph: Recognizing minutes-long human activities in videos</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pic: Permutation invariant convolution for recognizing long-range activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noureldien</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08275</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alleviating over-segmentation errors by detecting action boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seito</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimitsu</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">tional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
	<note>3d convolu</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="765" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal deformable residual networks for action segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6742" to="6751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ms-tcn++: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Abufarha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent assistance: crossdataset training of lstms on kitchen tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Uncertainty quantification and deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">H</forename><surname>Thiery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1961" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time depth estimation with an optimized encoderdecoder architecture on embedded devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfa</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition in video sequences using deep bi-directional lstm with cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamil</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1155" to="1166" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gated forward refinement network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">407</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal pyramid pooling-based convolutional neural network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhouhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Boundary-aware cascade networks for temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziteng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stacked hourglass network for robust facial landmark localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial-temporal pyramid based convolutional neural network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoyun</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqi</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="446" to="455" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
