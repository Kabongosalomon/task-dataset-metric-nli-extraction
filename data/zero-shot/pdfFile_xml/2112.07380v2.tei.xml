<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRACER: Extreme Attention Guided Salient Object Tracing Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Seok</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wooseok</forename><surname>Shin</surname></persName>
							<email>wsshin95@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Won</forename><surname>Han</surname></persName>
							<email>swhan@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRACER: Extreme Attention Guided Salient Object Tracing Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing studies on salient object detection (SOD) focus on extracting distinct objects with edge information and aggregating multi-level features to improve SOD performance. To achieve satisfactory performance, the methods employ refined edge information and low multi-level discrepancy. However, both performance gain and computational efficiency cannot be attained, which has motivated us to study the inefficiencies in existing encoder-decoder structures to avoid this trade-off. We propose TRACER, which detects salient objects with explicit edges by incorporating attention guided tracing modules. We employ a masked edge attention module at the end of the first encoder using a fast Fourier transform to propagate the refined edge information to the downstream feature extraction. In the multi-level aggregation phase, the union attention module identifies the complementary channel and important spatial information. To improve the decoder performance and computational efficiency, we minimize the decoder block usage with object attention module. This module extracts undetected objects and edge information from refined channels and spatial representations. Subsequently, we propose an adaptive pixel intensity loss function to deal with the relatively important pixels unlike conventional loss functions which treat all pixels equally. A comparison with 13 existing methods reveals that TRACER achieves state-of-the-art performance on five benchmark datasets. We have released TRACER at https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>To improve the performance of salient object detection (SOD), existing methods can be categorized into two approaches: those that improve the edge representation and those that reduce discrepancies during multi-level aggregation. To refine edges, existing methods incorporated shallow encoder representations, which contain sufficient edge information <ref type="bibr" target="#b42">(Zhang et al. 2017;</ref><ref type="bibr" target="#b31">Wang et al. 2018;</ref><ref type="bibr" target="#b37">Wu, Su, and Huang 2019a)</ref>, and deeper encoder outputs <ref type="bibr" target="#b23">Qin et al. 2019;</ref><ref type="bibr" target="#b38">Wu, Su, and Huang 2019b;</ref>. Although multi-level aggregation can explicitly represent edges, multiple encoder outputs have different data distributions. For the discrepancy reduction, skip-connected or cascaded decoder structures have been proposed <ref type="bibr" target="#b44">(Zhang et al. 2018b;</ref><ref type="bibr" target="#b48">Zhou et al. 2018;</ref><ref type="bibr" target="#b4">Feng, Lu, and Ding 2019;</ref><ref type="bibr" target="#b21">Pang et al. 2019;</ref><ref type="bibr" target="#b23">Qin et al. 2019;</ref>). These existing approaches improved SOD performance; however, they are incapable of simultaneously achieving the performance and computational efficiency. Therefore, to improve both performance and computational efficiency, this study focuses on reducing inefficiencies, which can develop in existing encoder-decoder structures, and applying adaptive pixel-wise weights to conventional loss functions.</p><p>In existing encoder-decoder structures, previous studies used multi-level encoder representations across a network <ref type="bibr" target="#b23">Qin et al. 2019;</ref><ref type="bibr" target="#b38">Wu, Su, and Huang 2019b;</ref> to extract edge information. Although these studies improve SOD performance because of the explicit edge information, they extensively employ edge refinement modules, which require all or part of the encoder outputs. These approaches increase parameter overhead and computational inefficiency. Moreover, the methods in these studies do not leverage the refined edges in the downstream feature extraction phases because the edge generating approaches depend on other encoder outputs. Because low-and high-level information can be effective for detecting edge features and semantic representations, the use of the refinement modules at both levels should be determined selectively and minimized for network efficiency.</p><p>The purpose of the decoder is to minimize discrepancies during multi-level aggregation when incorporating low-and high-level representations <ref type="bibr" target="#b44">(Zhang et al. 2018b;</ref><ref type="bibr" target="#b21">Pang et al. 2019;</ref><ref type="bibr" target="#b23">Qin et al. 2019;</ref>. Because a decoder that uses multiple encoder outputs consumes a large amount of memory, recent studies have proposed a cascaded decoder, which reduces the low-level connections or aggregates high-level representations <ref type="bibr" target="#b37">(Wu, Su, and Huang 2019a;</ref>. Subsequently, others have proposed object and edge refinement modules <ref type="bibr" target="#b13">(Kuen, Wang, and Wang 2016;</ref><ref type="bibr" target="#b1">Chen et al. 2018;</ref><ref type="bibr" target="#b18">Liu, Han, and Yang 2018;</ref><ref type="bibr" target="#b43">Zhang et al. 2018a;</ref><ref type="bibr" target="#b23">Qin et al. 2019;</ref><ref type="bibr" target="#b32">Wang et al. 2019;</ref><ref type="bibr" target="#b46">Zhao and Wu 2019)</ref> or multi-decoder structures <ref type="bibr" target="#b38">(Wu, Su, and Huang 2019b;</ref>) to reduce the discrepancies in different representations. However, it is yet to be ascertained whether the multiple modules and multi-decoder are efficient for computation. To improve performance and network efficiency, the focus should be on determining which representations over multiple levels are important while minimizing the use of attention modules in the multi-level aggregation and decoder structure. Because the different levels contain different data distributions, the distinct representations should be emphasized during multi-level aggregation.</p><p>In the process of applying adaptive pixel-wise weights to the loss function, each pixel is treated independently with binary cross entropy (BCE) and IoU losses, which are globally adopted for loss functions <ref type="bibr" target="#b22">Pang et al. 2020)</ref>. It is evident that the pixels adjacent to fine or explicit edges should be more focused than the pixels in the background or center of the salient object. To treat this pixel inconsistency, existing study  proposed the weighted BCE and weighted IoU loss functions. Although the study allocated larger weights to relatively important pixels, they also imposed significance on redundant pixels, including background regions while not covering explicit and fine edges. Consequently, it is necessary to employ adaptive pixel-wise weights to delineate fine or explicit edge regions while excluding redundant areas.</p><p>This study proposes an extreme attention guided salient object tracing network called TRACER. To address the inefficiencies in existing approaches, we apply three attention guided modules (i.e., masked edge, union, and object attention modules) in the shallow encoder, multi-level aggregation process, and decoder, respectively. The masked edge attention module enhances the edge features in low-level representations using a fast Fourier transform and propagates the edge-refined representation to the next encoder. The union attention module aggregates multi-level encoder outputs to reduce discrepancies in the distributions. Subsequently, this module determines the more important context in the aggregated channel-and spatial-wise representations. Following the integration, the object attention module incorporates lowlevel encoder representations and decoder outputs to identify the salient objects. To deal with the relative significance of pixels, we propose an adaptive pixel intensity loss function. We aggregate adjacent pixels around the target pixel by employing multiple kernel aggregation and exclude weights outside the edges. When the target pixel consists of fine or explicit edges, it is assigned a higher intensity than the other pixels. The main contributions of this study are as follows.</p><p>? We study the inefficiencies of the existing encoderdecoder structures and propose an efficient network called TRACER, which overcomes the trade-off between SOD performance improvement and computational efficiency. ? We propose masked edge, union, and object attention modules, which effectively and efficiently identify salient objects and edges in encoder-decoder structures with minimal use. ? The adaptive pixel intensity loss function focuses on the relatively significant pixels, which are adjacent to explicit or fine edges. It enables the network to optimize noisy label robustness and local-global structure awareness. ? TRACER significantly outperforms the 13 existing meth-ods on five benchmark datasets under four evaluation metrics and achieves computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network performance and efficiency</head><p>Since the introduction of FCN <ref type="bibr" target="#b20">(Long, Shelhamer, and Darrell 2015)</ref> and U-Net <ref type="bibr" target="#b24">(Ronneberger, Fischer, and Brox 2015)</ref>, existing studies have used VGG <ref type="bibr" target="#b42">(Zhang et al. 2017;</ref><ref type="bibr" target="#b4">Feng, Lu, and Ding 2019;</ref><ref type="bibr" target="#b32">Wang et al. 2019;</ref><ref type="bibr" target="#b36">Wu et al. 2019;</ref> and ResNet <ref type="bibr" target="#b23">(Qin et al. 2019;</ref><ref type="bibr" target="#b38">Wu, Su, and Huang 2019b;</ref><ref type="bibr" target="#b22">Pang et al. 2020)</ref> as backbone encoders in the U-shaped encoder-decoder structure. Using the VGG (Simonyan and Zisserman 2014) and ResNet <ref type="bibr" target="#b7">(He et al. 2016a)</ref>, which have satisfactory generalization performance, existing methods employed various object refinement modules and multi-decoder structures to improve SOD performance. The modules extracted explicit edge information <ref type="bibr" target="#b23">Qin et al. 2019;</ref><ref type="bibr" target="#b38">Wu, Su, and Huang 2019b;</ref> or detected elaborate object regions to enhance salient object representations <ref type="bibr" target="#b1">(Chen et al. 2018;</ref><ref type="bibr" target="#b22">Pang et al. 2020;</ref><ref type="bibr" target="#b36">Wu et al. 2019)</ref>. In terms of decoder efficiency, an existing study proposed a cascaded partial decoder, which aggregates deeper encoder representations and excludes shallow outputs to reduce computational inefficiency <ref type="bibr" target="#b37">(Wu, Su, and Huang 2019a)</ref>. Furthermore, recent studies have improved SOD performance by applying object and edge refinement modules with multi-decoder structures <ref type="bibr" target="#b38">(Wu, Su, and Huang 2019b;</ref>. Although these studies help avoid increases in the parameter overhead despite the use of multiple decoders, few approaches focus on improving computational efficiency and performance simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge refinement module efficiency</head><p>Enhancing the edge representation, which improves the detection of a salient object, can lead to a substantial improvement in SOD performance. To represent edge information, existing methods aggregated multi-level representations or applied edge refinement modules in a multi-decoder structure <ref type="bibr" target="#b23">Qin et al. 2019;</ref><ref type="bibr" target="#b38">Wu, Su, and Huang 2019b;</ref>). However, using multi-level representations or refinement modules with multidecoders is inefficient with respect to memory and computation. Moreover, previous approaches could not use edges to enhance the encoder outputs in the feature extraction phase because these methods required deeper encoder outputs to obtain distinct edges. To generate edges efficiently, minimizing the use of multiple outputs is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention module efficiency</head><p>In CNNs, attention mechanisms have been employed for various tasks to extract important features and enhance the original representation. Images are represented both channelwise and spatial-wise in CNNs; thus, existing studies have focused on these properties to detect informative features <ref type="bibr" target="#b10">(Hu, Shen, and Sun 2018;</ref><ref type="bibr" target="#b35">Woo et al. 2018</ref>). For the SOD task, PFANet (Zhao and Wu 2019) employed channel and spatial attention modules separately in shallow and deeper representations, respectively. Another proposed method, PA-GENet , organized the pyramid attention module that enhances multi-scale representations. In terms of spatial attention, other studies have applied spatial attention weights to each encoder representation to emphasize salient objects <ref type="bibr" target="#b1">(Chen et al. 2018;</ref><ref type="bibr" target="#b18">Liu, Han, and Yang 2018;</ref><ref type="bibr" target="#b43">Zhang et al. 2018a</ref>). However, attention modules are overused in the decoder because existing methods commonly use all the encoder outputs to discriminate salient objects. As a result, computational and memory efficiency is not preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRACER</head><p>In this section, we describe TRACER, which comprises an efficient backbone encoder along with attention guided salient object tracing modules (i.e., masked edge, union, and object attention modules), as depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture overview</head><p>Because existing backbone encoders, VGG16 (14.7M) and ResNet50 (23.5M), have a vulnerability in feature extraction performance and memory efficiency, an alternative backbone is necessary. Therefore, we employ EfficientNet (Tan and Le 2019) as the backbone encoder and incorporate the existing seven blocks into four blocks in which output resolution is shifted except for the initial convolution blocks. Here, we denote each encoder block output as E i and the masked edge attention module is initially applied to the first encoder block output E 1 , which has sufficient boundary representations, to leverage the enhanced edge information and improve memory efficiency. At the decoder, we implement the union and object attention modules, which aggregate multi-level features and incorporate the encoder and decoder outputs, respectively. In the union attention module, we integrate three encoder block outputs E 2 , E 3 , and E 4 , which are obtained by multikernel based receptive field blocks, at the scale of E 2 . The multi-level representations have different distributions; thus, we emphasize more distinct channel and spatial information through the union attention module. The object attention module extracts the distinct object with complementary edge information and utilizes this supplementary information to reduce the discrepancy between the shallow encoder and decoder representations <ref type="bibr" target="#b44">(Zhang et al. 2018b;</ref><ref type="bibr" target="#b21">Pang et al. 2019)</ref>. Moreover, the object attention module consist of depthwise convolution blocks <ref type="bibr" target="#b9">(Howard et al. 2017)</ref>, minimizing the number of learning parameters to increase computational efficiency. Finally, TRACER generates four deep supervision maps (DS i ) <ref type="bibr" target="#b14">(Lee et al. 2015)</ref>, which are the output the union (DS 0 ), object attention modules (DS 1 and DS 2 ), and an ensemble of the DS maps (DS e ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention guided tracing modules</head><p>Detecting distinct objects with edges is critical for improving SOD performance. Using the proposed convolution modules (designed for computational efficiency), we trace the objects and edges through attention guided salient object tracing modules (ATMs) to improve performance. Masked edge attention: For tracing edge information, we propose the masked edge attention module (MEAM), which extracts an explicit boundary by employing a fast Fourier transform (F F T ) <ref type="bibr" target="#b25">(Shanmugam, Dickey, and Green 1979;</ref><ref type="bibr" target="#b39">Xu 1996;</ref><ref type="bibr" target="#b0">Abdel-Qader, Abudayyeh, and Kelly 2003)</ref> and enhances the first encoder output boundaries. Existing methods employ the edge information, but they cannot leverage the explicit edges in the feature extraction phases because these methods require the outputs of deeper encoders to obtain the distinct edges. Therefore, we employ the F F T to extract the explicit edge from first encoder representation only. Using F F T and F F T ?1 , the first encoder representation is separated into high and low frequencies as follows:</p><formula xml:id="formula_0">X H = F F T ?1 (f H r (F F T (X)))<label>(1)</label></formula><p>Here, X indicates the input feature, and F F T (?), F F T ?1 (?) denote the fast Fourier transform and its inverse transform, respectively. Moreover, f H r (?) is a high-pass filter, which eliminates all frequencies except those in radius r. To discriminate the explicit edges, we utilize the high frequencies obtained by the high-pass filter, which have sufficient boundary information <ref type="bibr" target="#b6">(Haddad, Akansu et al. 1991;</ref><ref type="bibr" target="#b39">Xu 1996;</ref><ref type="bibr" target="#b29">Wang et al. 2020b</ref>). Moreover, X H contains the background noise when X H is transformed from the frequency domain to the spatial domain. Thus, we eliminate noise by applying the receptive field operation RFB(?) and generate explicit edge as follows: E = RFB(X H ). Finally, we compute the edgerefined representation X E as follows:</p><formula xml:id="formula_1">X E = X + E.</formula><p>Using the E, we compute the explicit edge loss. Union attention: A union attention module (UAM) is designed to aggregate multi-level features and detect the more important context from both channel and spatial representations. Here, f (?) and cat(?) denote the convolutional operation and channel-wise feature concatenation, respectively. Each encoder output E i?{2,3,4} , aggregated to 32, 64, and 128 channels respectively, is integrated as follows:</p><formula xml:id="formula_2">E 2 = E 2 ? f (U p(E 3 )) ? f (U p(U p(E 4 )))</formula><p>,</p><formula xml:id="formula_3">E 3 = f (cat[E 3 ? f (U p(E 4 )), f (U p(E 4 ))]), E 2 = f (U p(E 3 ))<label>(2)</label></formula><p>We obtain an aggregated representation, which is the scale of</p><formula xml:id="formula_4">E 2 , through X = f (cat[E 2 , E 2 ]) ? R (32+64+128)?H2?W2 .</formula><p>Following aggregation, it remains that which contextual information is relatively significant in both channel and spatial features. However, existing studies <ref type="bibr" target="#b46">Zhao and Wu 2019)</ref> have applied channel and spatial attention modules independently to the decoder and receptive field blocks despite of dependency of both spaces. Thus, we first discriminate the relatively significant channel-wise context and emphasize the spatial information based on complementary confidence scores obtained from the channel context.</p><formula xml:id="formula_5">? c = ? exp(F q ( X)(F k ( X)) ) exp(F q ( X)(F k ( X)) ) F v ( X)<label>(3)</label></formula><p>In Eq. 3, X ? R C?1?1 is the channel-wise pooled representation, and F(?) denotes the convolution operation using 1?1 kernel size. Context information is obtained by using the self-attention method and the softmax function to discriminate significant channels ? c ? R C?1?1 with a sigmoid function. To refine the aggregated representation X, we apply confidence channel weight as follows:</p><formula xml:id="formula_6">X c = (X ? ? c ) + X.</formula><p>Subsequently, we retain confidence channels based on the distribution of ? c and the confidence ratio ?, as follows:</p><formula xml:id="formula_7">X c = X c ? mask mask = 1, if ? c &gt; F ?1 (?) mask = 0, otherwise<label>(4)</label></formula><p>Here, F ?1 (?) denotes ? quantile of ? c . We exclude an area of ? in the lower tail of the distribution ? c . Then, the refined input X c is computed spatially to discriminate the salient object and generate the first decoder representation D 0 ? R 1?H2?W2 , as shown in Eq. 5.</p><formula xml:id="formula_8">D 0 = exp(G q ( X c )(G k ( X c )) ) exp(G q ( X c )(G k ( X c )) ) G v ( X c ) + G v ( X c ) (5)</formula><p>Here, G(?) projects the input features to X c ? R 1?H2?W2 using convolutional operation with 1 ? 1 kernel size. The D 0 is upsampled to DS 0 to obtain a deep supervision map.</p><p>Object attention: To reduce the distribution discrepancy between encoder and decoder representations using minimal parameters, we organize an object attention module (OAM) as a decoder. In contrast to the existing studies <ref type="bibr" target="#b1">(Chen et al. 2018;</ref>), we maintain D as a single channel for decoder efficiency, and the OAM traces both object and complementary edges from each decoder representation D i ? R 1?H?W . To refine the salient object, the object weight ? O is calculated as follows: ? O = ?(D i ). However, ? O cannot always detect the entire object with explicit edge regions; thus, we generate a complementary edge weight ? E to cover the undetected regions, as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>. For each pixel x ij in D, we reverse the detected areas and eliminate background noise corresponding to the denoising ratio d for missed region detection, as shown in Eq 6.</p><formula xml:id="formula_9">? E = 0, if (??(x ij ) + 1) &gt; d ??(x ij ) + 1, otherwise<label>(6)</label></formula><p>We incorporate the encoder output E i?{2,1} and decoder feature D i?{0,1} , as shown in Eq. 7. To reduce discrepancy, we exploit a receptive field operation RFB(?) and upsample D i+1 to generate DS i+1 . </p><formula xml:id="formula_10">D i+1 = RFB((? O ? E 2?i ) + (? E ? E 2?i )) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive pixel intensity loss</head><p>For a loss function, we combine the binary cross entropy (BCE), IoU, and L1 loss functions to reduce the discrepancy between the object and background. Although the BCE and IoU are globally employed for loss functions, these functions cause a class discrepancy between the foreground and background when all pixels are considered equally. Pixels that are adjacent to fine or explicit edges require more attention compared to pixels in the background and center of the salient object. Thus, we propose adaptive pixel intensity (API) loss, which applies the pixel intensity ? to each pixel as follows:</p><formula xml:id="formula_11">? ij = (1 ? ?) k?K h,w?Aij y k hw h,w?Aij 1 ? y ij y ij<label>(8)</label></formula><p>Here, we aggregate adjacent pixels (h, w) around the target pixel A ij by using multiple kernel size K and excluding weights outside the edges. As depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>, if the target pixel consists of fine edges, multi-kernel aggregation is employed to allocate more intensity to the target pixel than to other pixels. ? is an overriding weight that penalizes when employing multi-kernel aggregation because hierarchical aggregation imposes more weights on the pixels at the explicit edges. We empirically set the penalty term ? to 0.5, and the kernel size of K ? {3, 15, 31}. The pixel intensity ? is used for an adaptive BCE (aBCE) loss, as shown in Eq. 9. Here, y and? denote the label and predicted probability corresponding to binary class c, respectively. By using ?, the aBCE loss enables the network to focus more on the local structures related to explicit or fine edges unlike the BCE loss.</p><formula xml:id="formula_12">L a BCE = ? H i W j (1+? ij ) 1 c=0 (y c log(? c )+(1?y c )log(1?? c )) H i W j (1.5+? ij )<label>(9)</label></formula><p>In contrast, the adaptive IoU loss optimizes the global structures based on the intensive features corresponding to ?. As shown in Eq. 10, the pixels highly associated with the intensive regions are discriminated and emphasized compared to the original IoU loss.</p><formula xml:id="formula_13">L a IoU = 1 ? H i W j (y ij?ij )(1+? ij ) H i W j (y ij +? ij ?y ij?ij )(1+? ij )<label>(10)</label></formula><p>Moreover, to further improve the network for equivariance learning and to reduce the divergence discrepancy, we measure the L1 distance, which enables the network to learn robustly against noisy labels <ref type="bibr" target="#b5">(Ghosh, Kumar, and Sastry 2017;</ref><ref type="bibr" target="#b28">Wang et al. 2020a</ref>). The L1 loss also deals equally with all pixels; thus, we apply the pixel intensity ? to the L1 loss to discriminate relatively significant pixels and exclude the noisy pixels adjacent to explicit or fine edges as follows:</p><formula xml:id="formula_14">L a L1 = H i W j |y ij ?? ij |(1 + ? ij ) H ? W H i W j ? ij<label>(11)</label></formula><p>To incorporate the above local and global structural intensities, we combine the API loss functions as follows: L AP I (y,?) = L a BCE (y,?) + L a IoU (y,?) + L a L1 (y,?) (12) Based on the combined loss function L AP I , we optimize the final loss using the ground truth G, three deep supervisions DS i?{0,1,2} , an ensemble of three supervisions DS e , and an explicit edge E obtained from the MEAM, as follows:</p><formula xml:id="formula_15">L = i L AP I (G, DS i ) + L AP I (E,?)<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Dataset</head><p>We performed the evaluation on five benchmark datasets: DUTS, DUT-OMRON, ECSSD, HKU-IS, and PASCAL-S. DUTS ) is the largest benchmark dataset for SOD. It contains 10,553 training and 5,019 test images. DUT-OMRON <ref type="bibr" target="#b41">(Yang et al. 2013)</ref> has 5,168 images, that include one or more salient objects with relatively complex backgrounds. ECSSD <ref type="bibr" target="#b40">(Yan et al. 2013</ref>) comprises 1,000 structurally complex and semantically meaningful scenes. HKU-IS <ref type="bibr" target="#b15">(Li and Yu 2015)</ref> consist of 4,447 images, that include two or more objects with various backgrounds. PASCAL-S <ref type="bibr" target="#b16">(Li et al. 2014</ref>) has 850 challenging images containing objects of different levels of importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental setup</head><p>Hyper-parameters: We set the batch size to 32 and number of epochs to 100. We used the Adam optimizer with a learning rate of 5?10 ?5 and a weight decay of 10 ?4 . We observed the validation loss and reduced the learning rate by 10 ?1 if the loss did not decrease after five epochs. Additionally, an early stopping operation was applied when the validation loss did not decrease after 10 epochs. We empirically obtained the frequency radius r = 16 and denoising ratio d = 0.93 for the masked edge and object attention modules, respectively. To separate insignificant channels in the union attention module, the confident ratio ? was set to 0.1. For a fair comparison, we fixed all random seeds to 42. Implementation details: We trained TRACER using the DUTS-TR dataset and used the other datasets for testing, following existing studies ). In the training phase, data augmentation techniques (e.g., flip, random contrast, and blur) were applied to enhance the network generalization ability. The backbone network weights were initialized with a pre-trained ImageNet dataset. To measure TRACER performance, we used four evaluation metrics MaxF, MeanF, MAE, and Smeasure, which are widely adopted <ref type="bibr" target="#b22">Pang et al. 2020;</ref>). For the F-measure, F ? was calculated from precision-recall pairs, where ? 2 was set to 0.3. The S-measure, calculating the object-aware (S o ) and region-aware (S r ) structural similarities, was calculated as S m = ? ? S o + (1 ? ?) ? S r , where ? = 0.5 <ref type="bibr" target="#b3">(Fan et al. 2017)</ref>. TRACER was implemented using the PyTorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with state-of-the-art methods</head><p>We compared TRACER with 13 existing methods <ref type="bibr">(Feng,</ref>    when compared with the previous 13 methods on the five benchmark datasets. TE2 showed a relatively similar performance compared to LDF, which was the previous outstanding method; however, TE2 required 2.3? fewer learning parameters and was 6? faster than LDF. In addition, TE4 was 1.2? smaller model size than LDF, however, TE4 was 3.6? faster and showed a significant performance improvement than LDF. Based on the model parameters, GCPANet (67.06M) and PoolNet (68.21M) required a similar level of learning parameters compared with TE7 (66.27M); however, TE7 significantly outperformed other methods by a large margin, that is, it was 4.6? and 9.4? faster, respectively. Saliency map comparison: We evaluated TRACER with 10 previous methods <ref type="bibr" target="#b23">(Qin et al. 2019;</ref><ref type="bibr">Wu, Su, and Huang 2019a,b;</ref><ref type="bibr" target="#b2">Chen et al. 2020;</ref><ref type="bibr" target="#b17">Liu et al. 2019;</ref><ref type="bibr" target="#b22">Pang et al. 2020;</ref><ref type="bibr" target="#b47">Zhou et al. 2020</ref>) by visualizing the saliency maps. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, we sampled large-scale (rows 1 and 2) and small-scale salient objects with relatively complex backgrounds (rows 3 and 4). All the methods detected the large-scale object; however, the existing methods could not obtain the complete detailed region. For the detection of small-scale objects, TRACER could extract the salient objects precisely; in contrast, the previous methods identified objects that included the background or only a few areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation study</head><p>We have demonstrated that TRACER improves SOD performance substantially with respect to memory efficiency by employing a smaller backbone and attention guided tracing modules (ATMs). To evaluate the effectiveness of TRACER framework and subsequently enhance model explainability, we conducted ablation studies on TRACER. TRACER framework effectiveness: We organized the backbone encoder with EfficientNet because the existing backbone encoder had unsatisfactory feature extraction performance and memory efficiency. To evaluate the effective-   ness of the TRACER framework, we compared it with existing methods <ref type="bibr" target="#b38">(Wu, Su, and Huang 2019b;</ref>, which showed outstanding performance. For a fair comparison, we measured model training and inference times, that is, minutes per epoch (MPE) and frames per second (FPS), respectively, under the same conditions. We adopted TRACER-ResNet (TR-R) and TE2, which have the same backbone or input size. As listed in the Tab.2, the TE2 performed at least 2.9? to maximum 4.1? faster than the existing methods on training each epoch and at least 3.8? to maximum 6.4? faster on inference times. Indeed, TR-R occupied approximately 12.9% of of total GFLOPs at the decoder structures. In contrast, the multi-decoder frameworks proposed from the existing methods occupied 32.6% (SCRN), 38.2% (F3Net), and 34.6% (LDF) of total GFLOPs, respectively. As a result, we observed that when we employed a conventional backbone, the TRACER framework improved network efficiency. ATMs effectiveness: To validate the performance improvement originating from proposed attention modules, we eliminated the ATMs in TE7. In Tab. 3, we observed a change in performance improvement with and without ATMs. In particular, when we eliminated the UAM, TRACER performance decreased significantly (rows 1 to 4). Because the UAM enhances the aggregated multi-level representation, which contains various levels of importance, the UAM contributed more to the improvement than the MEAM and OAM. It means that discriminating distinct features from the aggregated multilevel representation, neither the edge representation leverage nor decoder structure, determined much of the SOD performance. Based on the UAM application, employing the OAM outperformed the MEAM result (rows 5 to 6). That is, the decoder structure secondly contributed to the performance gain because it complemented fine edges and undetected regions by aggregating encoder representations.  <ref type="table">Table 4</ref>: Comparison of effects on the adaptive pixel intensity in the loss function. ? indicates API loss function.</p><p>Adaptive pixel intensity: In Tab. 4, the proposed API loss functions (rows 5 to 7) displayed outstanding local-global structure awareness and network robustness against noisy labels, more so than other loss function combinations. In rows 2 and 3, we observed that using API was more effective for local and global structure awareness than the weighted BCE and IoU losses proposed by the existing study . When we additionally adopted L1 loss (row 4), MAE improved while preserving S m performance. In particular, applying API to MAE yielded a higher performance improvement than the conventional loss combination. Moreover, we examined the performance variation corresponding to the penalty term ?. The gain was unsatisfactory at a higher ? because it highly penalized ? assigned to the pixels adjacent to the fine edges. When we fully reflected the ? on the pixels, it showed limited performance gain of the local-global structure awareness and robustness because of weight overwhelming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We studied the inefficiencies in the existing encoder-decoder structure to improve the SOD performance along with network efficiency. We proposed TRACER, which discriminates salient objects by employing ATMs. TRACER detects the objects and edges in both channel and spatial-wise representations using minimal learning parameters. To treat the relative importance of pixels, we propose an adaptive pixel intensity loss function. For model explainability, we observe the model GFLOPs using the effectiveness of ATMs, through ablation studies. TRACER improves the performance and computational efficiency in all evaluation metrics in comparison to the 13 existing methods on the five benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional modules</head><p>Basic convolutional module: The attention guided salient object tracing modules mainly comprise three convolutional modules, as depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. We design the post-activation structure employed in ResNet <ref type="bibr">(He et al. 2016a,b)</ref> to regularize a network and leverage a generalization effect. The original structure used ReLU, whereas our basic convolutional module adopts SELU <ref type="bibr" target="#b12">(Klambauer et al. 2017)</ref>, which can control negative values and make networks more robust than ReLU. To achieve computational efficiency, as demonstrated by a depthwise convolutional (DWConv) in a MobileNet <ref type="bibr" target="#b9">(Howard et al. 2017)</ref>, we also reorganize the DWConv and depthwise separable convolutional (DWSConv) modules by applying post-activation after each convolutional layer. Depthwise dilated receptive field module: Existing receptive field blocks <ref type="bibr" target="#b19">(Liu, Huang et al. 2018</ref>) are organized with naive convolutional combinations; however, these combinations are inefficient in larger channels. Thus, we design a depthwise dilated receptive field module (DDRM) based on three convolutional modules, which are BasicConv, DWConv, and DWSConv modules, as illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>. Although depthwise convolutional has a lower performance than naive convolutional modules <ref type="bibr" target="#b9">(Howard et al. 2017)</ref>, parameter reduction is effective in large channels. We employ the DDRM in the masked edge attention and object attention modules, which require receptive representations and large computational resources (e.g., resolution or number of channels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence ratio</head><p>To observe the confidence channel effectiveness with a confidence ratio ?, we calibrated ? as depicted in Tab. 5. The network performance significantly decreased as ? increased. Because channels were significantly eliminated when using the large values of ? for detecting distinct representations, we obtained an unsatisfactory performance with large values  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge leverage</head><p>We observed the effect of the location of the MEAM on performance, as listed in Tab. 6. When the MEAM was located at the end of the deeper encoder, TRACER performance decreased gradually, and this MEAM in the deeper encoder resulted in memory and computational inefficiency. Because the deeper encoder output contains a more abstract representation than the shallow encoder, the MEAM is ineffective on deeper outputs. Moreover, it cannot propagate an edgerefined representation to the downstream feature extraction. Therefore, positioning the MEAM in the shallow encoder improved performance and network efficiency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency radius and denoising ratio</head><p>We calibrated the frequency radius r and denoising ratio d, which were used in the masked edge attention and object attention modules. As shown in Tab. 7, we found that when r = 16 and d = 0.93, TRACER performed optimally. When   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel and spatial dependency</head><p>This study has addressed that channel and spatial-wise representations could be complementary. To verify the effectiveness of a complementary attention structure, we organized both modules parallel. Tab. 8 demonstrated the complementary structure outperformed the parallel structure. Therefore, the complementary structure, which eliminated relatively insignificant channels in the spatial attention module following the channel attention weights, was effective compared to the parallel structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative ATMs comparison</head><p>We visualized heatmaps and compared the results corresponding to the application of ATMs for the module explainability, as depicted in <ref type="figure">Fig 7.</ref> When the ATMs are applied, the network not only discriminates the fine edges and redundant regions but also shows robustness against noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weighted adaptive pixel intensity loss combination</head><p>We experimented with a weighted combination of the proposed adaptive pixel intensity (API) loss function to observe the influence on local-global structure awareness and robustness against noisy labels. As listed in Tab. 9, when we im-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical multi-kernel aggregation effect</head><p>We examined the effectiveness of using multiple kernel sizes K for API loss. As listed in Tab. 11, employing hierarchical multi-kernel aggregation (rows 6 and 7) displayed outstanding performance compared to single or bi-kernel aggregation. Indeed, as shown in rows 1 to 3, increasing the kernel size enabled the network to learn local and global structure awareness. Based on the kernel size increase, we observed that the capacity of noisy label robustness and structural awareness improved when we incorporated multiple kernel sizes. Although adopting multi-kernel aggregation was more effective than single or bi-kernel aggregation, performance improvement was limited when we applied five different kernel sizes. Thus, hierarchical aggregation with three kernels (e.g., K ? {3, 15, 31}) could be empirically obtained for a reasonable level of API loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework Effectiveness</head><p>To observe the TRACER framework effectiveness, we experimented with other alternative backbones that had similar model sizes. Following the encoder aggregation strategy as   applied in TRACER, we integrated the backbones in four encoder blocks. Since TRACER required a different input size for each encoder output (E 2 to E 4 ), we modified the last dense block in DenseNet by employing a pooling operation. As depicted in Tab. 10, DenseNet  required fewer parameters and demonstrated suitable computational efficiency, although the modifications made in the dense block4 for applying TRACER resulted in an unsatisfactory performance. In contrast, Xception (Chollet 2017) achieved outstanding performance with remarkable network efficiency when compared to that of existing methods and other alternative backbones; however, this Xception backbone was not able to outperform TE4, which had significant performance improvements and was 3? faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention guided object tracing</head><p>As the OAM enhanced salient object with undetected regions, we visualized object and complementary edge attention maps to observe the OAM effectiveness as depicted in <ref type="figure">Fig. 8</ref>. The input feature X-(a), which was obtained from the encoder outputs, was clarified by the object attention ? O -(b) and transformed to (c). Then, we obtained the complementary edge attention ? E -(d) through the ? O -(b) to cover the undetected areas. In refined object (e), we examined the regions, which were emphasized by ? O -(b) and ? E -(d).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of TRACER architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Object and complementary edge detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Pixel intensity ? visualization corresponding to the kernel size K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Saliency map comparison of 10 state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Convolutional modules applied in TRACER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Overview of depthwise dilated receptive field module. MAE S m MAE S m MAE S m - * 0.025 0.916 0.049 0.853 0.022 0.930 0.027 0.935 0.1 0.022 0.919 0.045 0.855 0.020 0.932 0.026 0.935 0.5 0.025 0.918 0.048 0.849 0.022 0.930 0.028 0.934 0.7 0.026 0.909 0.047 0.835 0.023 0.927 0.029 0.931 0.9 0.028 0.899 0.048 0.821 0.025 0.920 0.031 0.926</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>MAE S m MAE S m MAE S m 0.87 0.029 0.913 0.050 0.852 0.023 0.930 0.028 0.933 0.89 0.025 0.916 0.048 0.853 0.021 0.932 0.028 0.934 0.91 0.023 0.917 0.046 0.855 0.021 0.930 0.027 0.933 0.93 0.022 0.919 0.045 0.855 0.020 0.932 0.026 0.935 0.95 0.024 0.916 0.047 0.853 0.022 0.931 0.026 0.934 0.97 0.025 0.919 0.046 0.855 0.021 0.932 0.027 0.935 0.99 0.028 0.917 0.049 0.853 0.022 0.930 0.027 0.931</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>MAE F A S m F m MAE F A S m F m MAE F A S m F m MAE F A S m F m MAE F A S m BASNet 87. .847 AFNet 34.3M -.873 .046 .811 .866 .818 .057 .734 .825 .929 .036 .888 .905 .941 .042 .905 .913 .884 .071 .830 .849 EGNet 112M 240.3 .902 .039 .836 .885 .842 .053 .751 .838 .943 .031 .902 .918 .957 .037 .919 .924 .889 .075 .828 .853 PoolNet 68.21M 178 .901 .037 .838 .886 .831 .054 .742 .829 .944 .030 .902 .919 .956 .035 .918 .926 .900 .065 .842 .866 SCRN 25.25M 30.2 .906 .040 .834 .885 .846 .056 .750 .837 .943 .033 .895 .916 .961 .037 .916 .927 .902 .064 .838 .868 MLMSNet 72.24M -.863 .049 .791 .862 .791 .063 .708 .809 .927 .038 .879 .907 .937 .045 .890 .911 .873 .074 .814 .845 PAGENet 47.40M 204 .834 .052 .794 .855 .789 .062 .743 .825 .917 .037 .885 .903 .932 .042 .905 .912 .884 .071 .830 .849 MINet 162M 174.22 .894 .038 .841 .884 .829 .056 .752 .833 .942 .029 .906 .919 .954 .034 .919 .925 .884 .065 .833 .856 ITSD 26.47M 31.92 .893 .041 .839 .885 .842 .061 .767 .840 .940 .031 .903 .917 .953 .035 .921 .925 .890 .065 .839 .862 GCPANet 67.06M 87.98 .897 .038 .836 .890 .831 .057 .749 .838 .942 .032 .898 .920 .954 .036 .912 .927 .890 .063 .832 .866 F3Net 25.54M 32.86 .904 .035 .851 .888 .838 .053 .764 .838 .943 .028 .909 .917 .956 .033 .925 .924 .894 .062 .840 .860 LDF 25.15M 31.02 .903 .034 .860 .892 .832 .052 .768 .839 .943 .028 .913 .919 .955 .034 .927 .925 .892 .061 .846 .862 TE0 7.45M 4.26 .890 .034 .849 .880 .834 .049 .775 .839 .935 .028 .910 .913 .949 .031 .923 .921 .889 .059 .847 .860 TE1 9.96M 4.28 .893 .033 .856 .885 .836 .048 .781 .843 .937 .027 .912 .916 .951 .031 .928 .923 .893 .056 .855 .865 TE2 11.09M 5.20 .900 .030 .866 .891 .841 .047 .786 .846 .939 .027 .915 .918 .950 .031 .927 .924 .894 .055 .855 .866 TE3 14.02M 6.26 .909 .028 .873 .898 .840 .046 .785 .847 .944 .024 .919 .922 .954 .028 .930 .929 .900 .053 .960 .870 TE4 20.71M 8.64 .915 .028 .879 .902 .851 .047 .798 .854 .948 .023 .925 .925 .958 .027 .936 .931 .900 .052 .862 .873 TE5 31.3M 11.52 .918 .026 .883 .909 .853 .047 .805 .861 .952 .020 .923 .933 .960 .024 .938 .937 .901 .048 .865 .880 TE6 43.47M 14.84 .928 .025 .896 .915 .848 .049 .797 .854 .954 .021 .932 .932 .963 .025 .941 .937 .908 .049 .870 .881 TE7 66.27M 18.86 .932 .022 .904 .919 .849 .045 .798 .855 .954 .020 .934 .932 .961 .026 .940 .935 .909 .047 .874 .882</figDesc><table><row><cell>Model #Params GFLOPs</cell><cell>F m</cell><cell>DUTS-TE</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell><cell>PASCAL-S</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Lu,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">and Ding 2019; Liu et al. 2019; Qin et al. 2019; Wang et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">2019; Wu et al. 2019; Wu, Su, and Huang 2019a,b; Zhao</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">et al. 2019; Chen et al. 2020; Wei, Wang, and Huang 2020;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Wei et al. 2020; Pang et al. 2020; Zhou et al. 2020). For a fair</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">comparison, the model parameters, GFLOPs, and saliency</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">maps were obtained from the released code or pre-computed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>by the authors.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Model performance and efficiency: As demonstrated in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Tab. 1, TRACER achieved a state-of-the-art performance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">and computational efficiency in all the evaluation metrics</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of performance with 13 existing methods on five benchmark datasets. Larger is better in the MaxF (F m ) , MeanF (F A ), and S-measure metrics, whereas smaller is better in MAE. The best and existing best results are highlighted.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>FPS SCRN 352 25.25M 30.18 10.04m .040 .885 41.29 .056 .837 41.52 .033 .916 44.03 .037 .927 47.81 F3Net 352 25.54M 32.86 7.24m .035 .888 60.51 .053 .838 63.22 .028 .917 78.63 .033 .924 76.37 LDF 352 25.15M 31.02 7.05m .034 .892 64.41 .052 .839 67.00 .028 .919 82.89 .034 .925 79.92 TR-R 352 25.28M 25.94 3.73m .035 .890 145.48 .050 .845 154.38 .028 .919 154.08 .033 .925 140.21 TE2 352 11.09M 5.20 2.46m .030 .891 242.92 .047 .846 267.25 .027 .918 260.35 .031 .924 231.31</figDesc><table><row><cell>Models Size #Params GFLOPs MPE</cell><cell>DUTS-TE MAE S m FPS MAE S m DUT-O</cell><cell>HKU-IS FPS MAE S m</cell><cell>ECSSD FPS MAE S m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of TRACER effectiveness for ResNet based methods. MAE S m MAE S m MAE S m - ? .038 .874 .059 .822 .033 .905 .042 .907 MEAM .035 .876 .059 .831 .030 .910 .042 .915 OAM .034 .880 .057 .828 .027 .912 .036 .917 UAM .027 .909 .056 .838 .026 .929 .032 .927 UAM+MEAM .027 .911 .053 .844 .024 .926 .030 .933 UAM+OAM .024 .913 .048 .842 .022 .928 .027 .934 UA+MEA+OA .022 .919 .045 .855 .020 .932 .026 .935</figDesc><table><row><cell>Modules</cell><cell>DUTS-TE MAE S m</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the TE7 performance for the combination of ATMs. ? indicates that all the ATMs are eliminated.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>MAE S m MAE S m MAE S m BCE+IoU .038 .895 .055 .835 .034 .915 .035 .923 wBCE+wIoU .034 .896 .053 .839 .033 .920 .034 .925 ?(BCE+IoU) ? .030 .909 .051 .851 .031 .929 .032 .930 BCE+IoU+L1 .027 .896 .049 .836 .027 .917 .030 .925 API (?=0.9) ? .026 .906 .048 .840 .023 .920 .030 .927 API (?=0.5) ? .022 .919 .045 .855 .020 .932 .026 .935 API (?=0.0) ? .024 .912 .047 .849 .020 .930 .027 .931</figDesc><table><row><cell>Loss functions</cell><cell>DUTS-TE MAE S m</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of TRACER performance with respect to confidence ratio ?. * indicates the condition in which confidence ratio was not applied. of ?. Moreover, removal of the complementary masking resulted in poor performance compared to when masking was applied.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>MAE S m MAE S m MAE S m 1 66.27M 18.86 .022 .919 .045 .855 .020 .932 .026 .935 2 66.29M 18.72 .025 .916 .048 .847 .023 .924 .026 .935 3 66.56M 18.86 .027 .913 .051 .841 .024 .922 .028 .931 4 68.74M 19.18 .028 .912 .053 .839 .024 .920 .029 .929</figDesc><table><row><cell>E i #Params GFLOPs</cell><cell>DUTS-TE MAE S m</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the effects of different locations of the MEAM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison of frequency radius r and denoising ratio d. The best results in each experiment are highlighted. MAE S m MAE S m MAE S m UAM-parallel .027 .914 .050 .851 .023 .930 .030 .932 UAM .022 .919 .045 .855 .020 .932 .026 .935</figDesc><table><row><cell>Modules</cell><cell>DUTS-TE MAE S m</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparison of channel and spatial-wise attention structures in UAM.r was between 14 and 18, TRACER exhibited robust performance. Because d clarified edges with undetected regions in the OAM, we obtained an unsatisfactory MAE performance as d decreased. Moreover, high d values also excluded the undetected regions, resulting in poor TRACER performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison of weighted API loss combinations.</figDesc><table><row><cell>posed more weights on adaptive binary cross entropy (aBCE),</cell></row><row><cell>we obtained F-measure (i.e., MaxF and MeanF) improve-</cell></row><row><cell>ment but the MAE and S m were unsatisfactory. In contrast,</cell></row><row><cell>although higher weights on adaptive IoU displayed outstand-</cell></row><row><cell>ing local-global structure awareness (S m ) compared to other</cell></row><row><cell>combinations, they could not handle noisy labels. Indeed,</cell></row><row><cell>some combinations displayed outstanding performance at</cell></row><row><cell>specific measures compared to the equally weighted com-</cell></row><row><cell>bination on DUTS-TE and DUT-O. However, the equally</cell></row><row><cell>weighted combination, (1.0*L a BCE + 1.0*L a IoU + 1.0*L a L1 ),</cell></row><row><cell>was adopted as it achieved satisfactory generalization perfor-</cell></row><row><cell>mance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>MaxF MAE MeanF S m MaxF MAE MeanF S m MaxF MAE MeanF S m TR-ResNet50 25.28M 25.94 0.905 0.035 0.855 0.890 0.849 0.050 0.776 0.845 0.943 0.028 0.910 0.919 0.955 0.033 0.925 0.925 TR-DenseNet201 19.28M 22.26 0.891 0.039 0.840 0.883 0.837 0.053 0.761 0.837 0.931 0.033 0.895 0.911 0.949 0.038 0.912 0.920 TR-ResNeXt50 24.75M 26.68 0.904 0.035 0.853 0.891 0.845 0.052 0.774 0.844 0.942 0.028 0.907 0.920 0.956 0.032 0.924 0.926 TR-Res2Net50 27.49M 27.98 0.910 0.033 0.860 0.895 0.844 0.050 0.765 0.840 0.942 0.028 0.905 0.920 0.956 0.035 0.920 0.923 TR-Xception 21.99M 25.38 0.906 0.032 0.859 0.897 0.846 0.050 0.775 0.845 0.944 0.027 0.910 0.921 0.955 0.032 0.924 0.927 TE4 20.71M 8.64 0.915 0.028 0.879 0.902 0.851 0.047 0.798 0.854 0.948 0.023 0.925 0.925 0.958 0.027 0.936 0.931</figDesc><table><row><cell>Models</cell><cell>#Params GFLOPs</cell><cell>DUTS-TE MaxF MAE MeanF S m</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Comparison of TRACER framework effectiveness on other backbones. MAE S m MAE S m MAE S m .024 .905 .048 .842 .023 .923 .030 .927 .023 .910 .047 .845 .022 .925 .029 .927 .023 .916 .046 .851 .021 .930 .027 .929 .024 .909 .046 .849 .022 .926 .029 .930 .023 .915 .046 .852 .021 .931 .026 .934 .022 .919 .045 .855 .020 .932 .026 .935 .022 .921 .044 .854 .021 .933 .026 .936</figDesc><table><row><cell>Kernel size</cell><cell>DUTS-TE</cell><cell>DUT-O</cell><cell>HKU-IS</cell><cell>ECSSD</cell></row><row><cell cols="2">3 9 15 23 31 MAE S m</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Comparison of TRACER performance with respect to multiple kernel sizes in API loss.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by National Research Foundation of Korea (NRF-2019R1F1A1060250). This research was also supported by Brain Korea 21 FOUR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of edge-detection techniques for crack identification in bridges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Abdel-Qader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Abudayyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing in Civil Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="255" to="263" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global Figure 8: Input feature and attention map visualization. The feature maps are sampled from the original channels. context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00651</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Xception: Deep learning with depthwise separable convolutions</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A class of fast Gaussian binomial filters for speech and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Akansu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="723" to="727" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3668" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards bridging semantic gap to improve semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4230" to="4239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-Scale Interactive Network for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9413" to="9422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An optimal frequency domain filter for edge detection in digital pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A noise-robust framework for automatic segmentation of COVID-19 pneumonia lesions from CT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Highfrequency component helps explain the generalization of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="8684" to="8694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with imagelevel supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detect globally, refine locally: A novel approach to saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3127" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">F 3 Net: Fusion, Feedback and Focus for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12321" to="12328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Label Decoupling Framework for Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13025" to="13034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">So</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A mutual learning method for salient object detection with intertwined multi-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8150" to="8159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7264" to="7273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identifying fabric structures with fast Fourier transform techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Textile Research Journal</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="496" to="506" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">EGNet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pyramid feature attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3085" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactive Two-Stream Decoder for Accurate and Fast Saliency Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9141" to="9150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
