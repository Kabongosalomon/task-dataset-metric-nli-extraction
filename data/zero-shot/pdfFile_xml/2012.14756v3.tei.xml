<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialogue Response Selection with Hierarchical Curriculum Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
							<email>qingyuzhou@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
							<email>yunbocao@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<email>shumingshi@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab</orgName>
								<orgName type="institution" key="instit1">University of Cambridge</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong ? Tencent Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dialogue Response Selection with Hierarchical Curriculum Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an "easy-to-difficult" scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. As for IC, it progressively strengthens the model's ability in identifying the mismatching information between the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years <ref type="bibr" target="#b22">(Shum et al., 2018;</ref><ref type="bibr" target="#b11">Kollar et al., 2018)</ref>. An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses <ref type="bibr" target="#b21">(Ritter et al., 2011)</ref>.</p><p>To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate <ref type="bibr" target="#b34">(Wu et al., 2017;</ref><ref type="bibr" target="#b18">Lu et al., 2019;</ref><ref type="bibr" target="#b5">Gu et al., 2019)</ref>. Despite their differences, * The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. <ref type="bibr">Yan</ref> Wang is the corresponding author.  most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scores are higher than the negative ones.</p><p>Recently, some researchers <ref type="bibr" target="#b14">Lin et al., 2020)</ref> have raised the concern that randomly sampled negative responses are often too trivial (i.e., totally irrelevant to the dialogue context). Models trained with trivial negative responses may fail to handle strong distractors in real-world scenarios. Essentially, the problem stems from the ignorance of the diversity in context-response matching degree. In other words, all random responses are treated as equally negative regardless of their different distracting strengths. For example, Ta-ble 1 shows a conversation between two speakers and two negative responses (N1, N2) are presented. For N1, one can easily dispel its appropriateness as it unnaturally diverges from the TV show topic. On the other hand, N2 is a strong distractor as it overlaps significantly with the context (e.g., fantasy series and Game of Thrones). Only with close observation we can find that N2 does not maintain the coherence of the discussion, i.e., it starts a parallel discussion about an actor in Game of Thrones rather than elaborating on the enjoyable properties of the TV series. In addition, we also observe a similar phenomenon on the positive side. For different training context-response pairs, their pairwise relevance also varies. In <ref type="table" target="#tab_1">Table 1</ref>, two positive responses (P1, P2) are provided for the given context. For P1, one can easily confirm its validity as it naturally replies the context. As for P2, while it expatiates on the enjoyable properties of the TV series, it does not exhibit any obvious matching clues (e.g., lexical overlap with the context). Therefore, to correctly identify P2, its relationship with the context must be carefully reasoned by the model. Inspired by the above observations, in this work, we propose to employ the idea of curriculum learning (CL) <ref type="bibr" target="#b0">(Bengio et al., 2009)</ref>. The key to applying CL is to specify a proper learning scheme under which all training examples are learned. By analyzing the characteristics of the concerned task, we tailor-design a hierarchical curriculum learning (HCL) framework. Specifically, our learning framework consists of two complementary curriculum strategies, corpus-level curriculum (CC) and instance-level curriculum (IC), covering the two distinct aspects of response selection. In CC, the model gradually increases its ability in finding matching clues through an easy-to-difficult arrangement of positive context-response pairs. In IC, we sort all negative responses according to their distracting strength such that the model's capability of identifying the mismatching information can be progressively strengthened.</p><p>Notably, our learning framework is independent to the choice of matching models. For a comprehensive evaluation, we evaluate our approach on three representative matching models, including the current state of the art. Results on three benchmark datasets demonstrate that the proposed learning framework leads to remarkable performance improvements across all evaluation metrics.</p><p>In a nutshell, our contributions can be summa-rized as: (1) We propose a hierarchical curriculum learning framework to tackle the task of dialogue response selection; and (2) Empirical results on three benchmark datasets show that our approach can significantly improve the performance of various strong matching models, including the current state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><formula xml:id="formula_0">Given a dataset D = {(c i , r i )} |D| i=1</formula><p>, the learning of a matching model s(?, ?) is to correctly identify the positive response r i conditioned on the dialogue context c i from a set of negative responses R ? i . The learning objective is typically defined as</p><formula xml:id="formula_1">L s = m j=1 max{0, 1 ? s(c i , r i ) + s(c i , R ? ij )},<label>(1)</label></formula><p>where m is the number of negative responses associated with each training context-response pair.</p><p>In most existing studies <ref type="bibr" target="#b34">(Wu et al., 2017;</ref><ref type="bibr" target="#b5">Gu et al., 2019)</ref>, the training negative responses R ? i are randomly selected from the dataset D. Recently,  and <ref type="bibr" target="#b14">Lin et al. (2020)</ref> proposed different approaches to strengthen the training negatives. In testing, for any contextresponse (c, r), the models give a score s(c, r) that reflects their pairwise matching degree. Therefore, it allows the user to rank a set of response candidates according to the scores for response selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We propose a hierarchical curriculum learning (HCL) framework for training neural matching models. It consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the relationship between these two strategies. In CC ( ?3.2), the training context-response pairs with lower difficulty are presented to the model before harder pairs. This way, the model gradually increases its ability to find the matching clues contained in the response candidate. As for IC ( ?3.3), it controls the difficulty of negative responses that associated with each training context-response pair. Starting from easier negatives, the model progressively strengthens its ability to identify the mismatching information (e.g., semantic incoherence) in the response candidate. The following gives a detailed description of the proposed approach. <ref type="figure" target="#fig_1">Figure 1</ref>: An illustration of the proposed learning framework: On the left part, two training context-response pairs with different difficulty levels are presented (the upper one is more difficult than the lower one, and P denotes the positive response). For each training instance, we show three associated negative responses (N1, N2 and N3) with increasing difficulty from the bottom to the top. In the negative responses, the words that also appear in the dialogue context are marked as italic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Corpus-Level Curriculum</head><formula xml:id="formula_2">Given the dataset D = {(c i , r i )} |D| i=1</formula><p>, the corpuslevel curriculum (CC) arranges the ordering of different training context-response pairs. The model first learns to find easier matching clues from the pairs with lower difficulty. As the training progresses, harder cases are presented to the model to learn less obvious matching signals. Two examples are shown in the left part of <ref type="figure" target="#fig_1">Figure 1</ref>. For the easier pair, the context and the positive response are semantically coherent as well as lexically overlapped (e.g., TV series and Game of Thrones) with each other and such matching clues are simple for the model to learn. As for the harder case, the positive response can only be identified via numerical reasoning, which makes it harder to learn.</p><p>Difficulty Function. To measure the difficulty of each training context-response pair (c i , r i ), we adopt a pre-trained ranking model G(?, ?) ( ?3.4) to calculate its relevance score as G(c i , r i ). Here, a higher score of G(c i , r i ) corresponds to a higher relevance between c i and r i and vice versa. Then, for each pair (c i , r i ) ? D, its corpus-level difficulty d cc (c i , r i ) is defined as</p><formula xml:id="formula_3">d cc (c i , r i ) = 1.0 ? G(c i , r i ) max (c k ,r k )?D G(c k , r k ) ,<label>(2)</label></formula><formula xml:id="formula_4">where d cc (c i , r i ) is normalized to [0.0, 1.0].</formula><p>Here, a lower difficulty score indicates the pair (c i , r i ) is easier for the model to learn and vise versa.</p><p>Pacing Function. In training, to select the training context-response pairs with appropriate difficulty, we define a corpus-level pacing function, p cc (t), which controls the pace of learning from easy to hard instances. In other words, at time step t, p cc (t) represents the upper limit of difficulty and the model is only allowed to use the training instances (c i , r i ) whose corpus-level difficulty score d cc (c i , r i ) is lower than p cc (t). In this work, we propose a simple functional form for p cc (t) 1 as</p><formula xml:id="formula_5">p cc (t) = 1.0?pcc(0) T ? t + p cc (0) if t ? T, 1.0 otherwise,</formula><p>where p cc (0) is a predefined initial value. At the training warm up stage (first T steps), we learn a basic matching model with a easy subset of the training data. In this subset, the difficulty of all samples are lower than p cc (t). After p cc (t) becomes 1.0 (at time step T ), the corpus-level curriculum is completed and the model can then freely access the entire dataset. In <ref type="figure" target="#fig_0">Figure 2</ref>(a), we give an illustration of the corpus-level curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instance-Level Curriculum</head><p>As a complement of CC, the instance-level curriculum (IC) controls the difficulty of negative responses. For an arbitrary training context-response pair (c i , r i ), while its associated negative responses can be any responses r j (s.t. j = i) in the training set, the difficulties of different r j are diverse. Some examples are presented in the right part of <ref type="figure" target="#fig_1">Figure 1</ref>. We see that the negative responses with lower difficulty are always simple to spot as they are often obviously off the topic. As for the harder negatives, the model need to identify the fine-grained semantic incoherence between them and the context. The main purpose of IC is to select negative responses with appropriate difficulty based on the state of the learning process. At the beginning, the negative responses are randomly sampled from the entire training set, so that most of them are easy to distinguish. As the training evolves, IC gradually increases the difficulty of negative responses by sampling them from the responses with higher difficulty (i.e., from a harder subset of the training data). In this way, the model's ability in finding the mismatching information is progressively strengthened and will be more robust when handling those strong distractors in real-world scenarios.</p><p>Difficulty Function. Given a specific training instance (c i , r i ), we define the difficulty of an arbitrary response r j (s.t. j = i) as its rank in a sorted list of relevance score in descending order,</p><formula xml:id="formula_6">d ic (c i , r j ) = sort r j ?D,j =i (G(c i , r j )). (3)</formula><p>In this formula, the response r h with the highest relevance score, i.e., r h = max r j ?D,j =i G(c i , r j ), has a rank of 1, thus d ic (c i , r h ) = 1. For the response r l with the lowest relevance score, i.e., r l = min r j ?D,j =i G(c i , r j ), has a rank of |D|, thus d ic (c i , r l ) = |D|. Here, a smaller rank means the corresponding negative response is more relevant to the context c i , thus it is more difficult for the model to distinguish.</p><p>Pacing Function. Similar to CC, in IC, the pace of learning from easy to difficult negative responses is controlled by an instance-level pacing function, p ic (t). It adjusts the size of the sampling space (in log scale) from which the negative responses are sampled from. Given a training instance (c i , r i ), at time step t, the negative examples are sampled from the responses r j (s.t. j = i) whose rank is smaller than 10 p ic (t) (d ic (c i , r j ) ? 10 p ic (t) ), i.e., the negative responses are sampled from a subset of the training data which consists of the top-10 p ic (t) relevant responses in relation to c i . The smaller the p ic (t) is, the harder the sampled negatives will be. In this work, we define the function p ic (t) as</p><formula xml:id="formula_7">p ic (t) = (k 0 ?k T ) T ? (T ? t) + k T if t ? T, k T if t &gt; T,</formula><p>where T is the same as the one in the corpus-level pacing function p cc (t). k 0 = log |D| 10 , meaning that, at the start of training, the negative responses are sampled from the entire training set D. k T is a hyperparameter and it is smaller than k 0 . After p ic (t) becomes k T (at step T ), the instance-level curriculum is completed. For the following training steps, the size of the sampling space is fixed at 10 k T . An example of p ic (t) is depicted in <ref type="figure" target="#fig_0">Figure 2(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Curriculum Learning</head><p>Model Training. Our learning framework jointly employs the corpus-level and instance-level curriculum. For each training step, we construct a batch of training data as follows: First, we select the positive context-response pairs according to the corpus-level pacing function p cc (t). Then, for each instance in the selected batch, we sample its associated negative examples according to the instance- Invoke the trainer, T , using</p><formula xml:id="formula_8">{(c k , r k , R ? k )} |B t | k=1</formula><p>as input to optimize the model using Eq. (1). 7 end Output :Trained Matching Model level pacing function p ic (t). Details of our learning framework are presented in Algorithm 1.</p><p>Fast Ranking Model. As described in Eq. <ref type="formula" target="#formula_3">(2)</ref> and <ref type="formula">(3)</ref>, our framework requires a ranking model G(?, ?) that efficiently measures the pairwise relevance of millions of possible context-response combinations. In this work, we construct G(?, ?) as an non-interaction matching model with dualencoder structure such that we can precompute all contexts and responses offline and store them in cache. For any context-response pair (c, r), its pairwise relevance G(c, r) is defined as</p><formula xml:id="formula_9">G(c, r) = E c (c) T E r (r),<label>(4)</label></formula><p>where E c (c) and E r (r) are the dense context and response representations produced by a context encoder E c (?) and a response encoder E r (?) 2 .</p><p>Offline Index. After training the ranking model on the same response selection dataset D using the in-batch negative objective <ref type="bibr" target="#b9">(Karpukhin et al., 2020)</ref>, we compute the dense representations of all contexts and responses contained in D. Then, as described in Eq. (4), the relevance scores of all possible combinations of the contexts and responses in D can be easily computed through the dot product between their representations. After this step, we can compute the corpus-level and instance-level difficulty of all possible combinations and cache them in memory for a fast access in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Dialogue Response Selection. Early studies in this area devoted to the response selection for single-turn conversations <ref type="bibr" target="#b31">(Wang et al., 2013;</ref><ref type="bibr" target="#b26">Tan et al., 2016;</ref>. Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised <ref type="bibr" target="#b34">(Wu et al., 2017;</ref><ref type="bibr" target="#b5">Gu et al., 2019;</ref><ref type="bibr" target="#b4">Gu et al., 2020)</ref>. There is an emerging line of research studying how to improve existing matching models with better learning algorithms.  proposed to adopt a Seq2seq model as weak teacher to guide the training process. Curriculum Learning. Curriculum Learning <ref type="bibr" target="#b0">(Bengio et al., 2009</ref>) is reminiscent of the cognitive process of human being. Its core idea is first learning easier concepts and then gradually transitioning to more complex concepts based on some predefined learning schemes. Curriculum learning (CL) has demonstrated its benefits in various machine learning tasks <ref type="bibr" target="#b23">(Spitkovsky et al., 2010;</ref><ref type="bibr" target="#b7">Ilg et al., 2017;</ref><ref type="bibr" target="#b25">Svetlik et al., 2017;</ref><ref type="bibr" target="#b20">Platanios et al., 2019)</ref>. Recently, <ref type="bibr" target="#b19">Penha and Hauff (2020)</ref> employed the idea of CL to tackle the response selection task. However, they only apply curriculum learning for the positive-side response selection, while ignoring the diversity of the negative responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We test our approach on three benchmark datasets.</p><p>Douban Dataset. This dataset <ref type="bibr" target="#b34">(Wu et al., 2017)</ref> consists of multi-turn Chinese conversation data crawled from Douban group 3 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. Following previous works,  we report the results of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at Position 1 (P@1). In addition, we also report the results of R 10 @1, R 10 @2, R 10 @5, where R n @k means recall at position k in n candidates.</p><p>Ubuntu Dataset. This dataset <ref type="bibr" target="#b16">(Lowe et al., 2015)</ref> contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The training, validation and test size are 500k, 50k and 50k. Each dialogue context is paired with 10 response candidates. Following previous studies, we use R 2 @1, R 10 @1, R 10 @2 and R 10 @5 as evaluation metrics.</p><p>E-Commerce Dataset. This dataset <ref type="bibr" target="#b37">(Zhang et al., 2018)</ref> consists of Chinese conversations between customers and customer service staff from Taobao 4 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. R n @k are employed as the evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Models</head><p>In the experiments, we compare our approach with the following models that can be summarized into three categories.</p><p>Single-turn Matching Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response candidates, including RNN <ref type="bibr" target="#b16">(Lowe et al., 2015)</ref>, CNN <ref type="bibr" target="#b16">(Lowe et al., 2015)</ref>, LSTM <ref type="bibr" target="#b16">(Lowe et al., 2015)</ref>, Bi-LSTM 4 www.taobao.com <ref type="bibr" target="#b8">(Kadlec et al., 2015)</ref>, <ref type="bibr">MV-LSTM (Wan et al., 2016)</ref> and Match-LSTM <ref type="bibr" target="#b32">(Wang and Jiang, 2016)</ref>.</p><p>Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R , Multi-View , DUA <ref type="bibr" target="#b37">(Zhang et al., 2018)</ref>, DAM , MRFN <ref type="bibr" target="#b27">(Tao et al., 2019a)</ref>, IOI <ref type="bibr" target="#b28">(Tao et al., 2019b)</ref>, SMN <ref type="bibr" target="#b34">(Wu et al., 2017)</ref> and MSN <ref type="bibr" target="#b36">(Yuan et al., 2019)</ref>.</p><p>BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), <ref type="bibr" target="#b4">Gu et al. (2020)</ref> proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation Details</head><p>For all experiments, we set the value of p cc (0) in the corpus-level pacing function p cc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower than 0.3. For the instance-level pacing function p ic (t), the value of k T is set as 3, meaning that, after IC is completed, the negative responses of each training instance are sampled from the top-10 3 relevant responses. In the experiments, each matching model is trained for 40, 000 steps with a batch size of 128, and we set the T in both p cc (t) and p ic (t) as half of the total training steps, i.e., T = 20, 000. To build the context and   response encoders in the ranking model G(?, ?), we use a 3-layer transformers with a hidden size of 256. We select two representative models (SMN and MSN) along with the state-of-the-art SA-BERT to test the proposed learning framework. To better simulate the true testing environment, the number of negative responses (m in Eq. <ref type="formula" target="#formula_1">(1)</ref>) is set to be 5.</p><p>6 Result and Analysis 6.1 Main Results <ref type="table" target="#tab_3">Table 2</ref> shows the results on Douban, Ubuntu, and E-Commerce datasets, where X+HCL means training the model X with the proposed learning HCL.</p><p>We can see that HCL significantly improves the performance of all three matching models in terms of all evaluation metrics, showing the robustness and universality of our approach. We also observe that, by training with HCL, a model (MSN) without using pre-trained language model can even surpass the state-of-the-art model using pre-trained language model (SA-BERT) on Douban dataset. These results suggest that, while the training strategy is under-explored in previous studies, it could be very decisive for building a competent response selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of CC and IC</head><p>To reveal the individual effects of CC and IC, we train different models on Douban dataset by remov-ing either CC or IC. The experimental results are shown in <ref type="table" target="#tab_5">Table 3</ref>, from which we see that both CC and IC make positive contributions to the overall performance when used alone. Only utilizing IC leads to larger improvements than only using CC. This observation suggests that the ability of identifying the mismatching information is a more important factor for the model to achieve its optimal performance. However, the optimal performance is achieved when CC and IC are combined, indicating that CC and IC are complementary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Contrast to Existing Learning Strategies</head><p>Next, we compare our approach with other learning strategies proposed recently <ref type="bibr" target="#b19">Penha and Hauff, 2020;</ref><ref type="bibr" target="#b14">Lin et al., 2020)</ref>. We use Semi, CIR, and Gray to denote the approaches in , <ref type="bibr" target="#b19">Penha and Hauff (2020)</ref>, and Lin et al.</p><p>(2020) respectively, where Gray is the current state of the art. We conduct experiments on Douban and Ubuntu datasets and the experimental results of three matching models are listed in <ref type="table" target="#tab_6">Table 4</ref>. From the results, we can see that our approach consistently outperforms other learning strategies in all settings. The performance gains of our approach are even more remarkable given its simplicity; it does not require running additional generation models <ref type="bibr" target="#b14">(Lin et al., 2020)</ref> or re-scoring negative samples at different epochs . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Further Analysis on HCL</head><p>In this part, we study how the key hyper-parameters affect the performance of HCL, including the initial difficulty of CC, p cc (0), and the curriculum length of IC, k T . 5 In addition, we also investigate the effect of different ranking model choices.</p><p>Initial Difficulty of CC. We run sensitivity analysis experiments on Douban dataset with the SMN model by tuning p cc (0) in the corpus-level pacing function p cc (t). The results of P@1 and R 10 @2 in terms of p cc (0) and k T are shown in <ref type="figure" target="#fig_3">Figure  3</ref>(a). We observe that when p cc (0) is small (i.e., p cc (0) ? 0.3), the model performances are relatively similar. When p cc (0) approaches to 1.0, the results drop significantly. It concurs with our expectation that, in CC, the model should start learning with training context-response pairs of lower difficulty. Once p cc (0) becomes 1.0, the CC is disabled, resulting the lowest model performances.</p><p>Curriculum Length of IC. Similair to p cc (0), we also run sensitivity analysis experiments by tuning k T in the instance-level pacing function p ic (t) and <ref type="figure" target="#fig_3">Figure 3(b)</ref> shows the results. We observe that  a too small or too large K T results in performance degradation. When k T is too small, after IC is completed, the negative examples are only sampled from a very small subset of the training data that consists of responses with high relevance. In this case, the sampled responses might be false negatives that should be deemed as positive cases. Thus, learning to treat those responses as true negatives could harm the model performance. On the other hand, as k T increases, the effect of IC becomes less obvious. When k T = log 500k 10 (|D|= 500k), IC is completely disabled, leading to the further decrease of model performances.</p><p>Ranking Model Architecture. Lastly, we examine the effect of the choice of the ranking model architecture. We build two ranking model variants by replacing the Transformers module E c (?) and E r (?) in Eq. (4) with other modules. For the first case, we use 3-layer BiLSTM with a hidden size of 256. For the second one, we use BERT-base <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> model. Then, we train the matching models using the proposed HCL but with different ranking models as the scoring basis.</p><p>The results on Douban dataset are shown in Table 5. We first compare the performance of different ranking models by directly using them to select the best response. The results are shown in the "Ranking Model" row of <ref type="table" target="#tab_8">Table 5</ref>. Among all three variants, BERT performs the best but it is still less accurate than these sophisticated matching models. Second, we study the effect of different ranking models on the matching model performance. We see that, for different matching models, Transformers and BERT perform comparably but the results from BiLSTM are much worse. This further leads to a conclusion that, while the choice of ranking model does have impact on the overall results, the improvement of the ranking model does not necessarily lead to the improvement of matching models once the ranking model achieves certain accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we propose a novel hierarchical curriculum learning framework for training response selection models for multi-turn conversations. During training, the proposed framework simultaneously employs corpus-level and instance-level curricula to dynamically select suitable training data based on the state of the learning process. Extensive experiments and analysis on two benchmark datasets show that our approach can significantly improve the performance of various strong matching models on all evaluation metrics. Our code, models and other related resources can be found in https://github.com/yxuansu/HCL</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) Illustration of the corpus-level curriculum. At each step: (1) p cc (t) is computed based on the current step t; and (2) a batch of context-response pairs are uniformly sampled from the training instances whose corpuslevel difficulty is lower than p cc (t) (shaded area in the example). In this example, p cc (0) = 0.3 and T = 20000; (b) Illustration of the instance-level pacing function. In this example, k 0 = log |D| 10 = 6, k T = 3, and T = 20000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Hierarchical Curriculum Learning Input :Dataset, D = {(ci, ri)} |D| i=1 ; model trainer, T , that takes batches of training data as input to optimize the matching model; corpus-level difficulty and pacing function, dcc and pcc; instance-level difficulty and pacing function, dic and pic; number of negative responses, m; 1 for train step t = 1, ... do 2Uniformly sample one batch of context-response pairs, Bt, from all (ci, ri) ? D, such that dcc(ci, ri) ? pcc(t), as shown inFigure 2(a).3 for (cj, rj) in Bt do 4Sample m negative responses, R ? j , from all responses r, where r = rj, that satisfies the condition dic(cj, r) ? 10 p ic (t) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc> designed a co-teaching framework to eliminate the training noises. Similar to our work, proposed to alleviate the problem of trivial negatives by sampling stronger negatives.<ref type="bibr" target="#b14">Lin et al. (2020)</ref> attempted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Plots illustrating the effect of curriculum hyper-parameters, (a) p cc (0) and (b) k T , on the SMN model performance in Douban dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dialogue Context Between Two Speakers A and B A: Would you please recommend me a good TV series to watch during my spare time? B: Absolutely! Which kind of TV series are you most interested in? A: My favorite type is fantasy drama. B: I think both Game of Thrones and The Vampire Diaries are good choices. Awesome, I believe both of them are great TV series! I will first watch Game of Thrones. (Easy) P2: Cool! I think I find the perfect thing to kill my weekends. This restaurant is very expensive. (Easy) N2: Iain Glen played Ser Jorah Mormont in the HBO fantasy series Game of Thrones.</figDesc><table><row><cell>Positive Response</cell></row><row><cell>P1: (Difficult)</cell></row><row><cell>Negative Response</cell></row><row><cell>N1:</cell></row></table><note>(Difficult)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>An example dialogue context between speakers A and B, where P1 and P2 are easy and difficult positives; N1 and N2 are easy and difficult negatives.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of different models trained with our approach on Douban, Ubuntu, and E-Commerce datasets. All results acquired using HCL outperforms the original results with a significance level p-value &lt; 0.01.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on Douban dataset using different combinations of the proposed curriculum strategies.</figDesc><table><row><cell>Model</cell><cell>Strategy</cell><cell></cell><cell>Douban</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ubuntu</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">MAP MRR P@1 R10@1 R10@2 R2@1 R10@1 R10@2 R10@5</cell></row><row><cell></cell><cell>Semi</cell><cell>0.554</cell><cell>0.605 0.425</cell><cell>0.253</cell><cell>0.412</cell><cell>0.934</cell><cell>0.762</cell><cell>0.865</cell><cell>0.967</cell></row><row><cell>SMN</cell><cell>CIR ? Gray</cell><cell>0.561 0.564</cell><cell>0.611 0.432 0.615 0.443</cell><cell>0.267 0.271</cell><cell>0.433 0.439</cell><cell>0.935 0.938</cell><cell>0.760 0.765</cell><cell>0.870 0.873</cell><cell>0.963 0.969</cell></row><row><cell></cell><cell>HCL</cell><cell>0.575</cell><cell>0.620 0.446</cell><cell>0.281</cell><cell>0.452</cell><cell>0.947</cell><cell>0.777</cell><cell>0.885</cell><cell>0.981</cell></row><row><cell></cell><cell>Semi ?</cell><cell>0.591</cell><cell>0.638 0.473</cell><cell>0.301</cell><cell>0.461</cell><cell>0.952</cell><cell>0.804</cell><cell>0.903</cell><cell>0.983</cell></row><row><cell>MSN</cell><cell>CIR ? Gray</cell><cell>0.595 0.599</cell><cell>0.640 0.472 0.645 0.476</cell><cell>0.304 0.308</cell><cell>0.466 0.468</cell><cell>0.955 0.958</cell><cell>0.808 0.812</cell><cell>0.910 0.911</cell><cell>0.985 0.987</cell></row><row><cell></cell><cell>HCL</cell><cell>0.620</cell><cell>0.668 0.507</cell><cell>0.321</cell><cell>0.508</cell><cell>0.969</cell><cell>0.826</cell><cell>0.924</cell><cell>0.989</cell></row><row><cell></cell><cell>Semi ?</cell><cell>0.623</cell><cell>0.664 0.500</cell><cell>0.317</cell><cell>0.490</cell><cell>0.968</cell><cell>0.858</cell><cell>0.931</cell><cell>0.989</cell></row><row><cell>SA-BERT</cell><cell>CIR ? Gray ?</cell><cell>0.624 0.628</cell><cell>0.666 0.503 0.670 0.503</cell><cell>0.318 0.320</cell><cell>0.497 0.503</cell><cell>0.969 0.970</cell><cell>0.860 0.861</cell><cell>0.935 0.934</cell><cell>0.990 0.991</cell></row><row><cell></cell><cell>HCL</cell><cell>0.639</cell><cell>0.681 0.514</cell><cell>0.330</cell><cell>0.531</cell><cell>0.977</cell><cell>0.867</cell><cell>0.940</cell><cell>0.992</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on Douban and Ubuntu datasets using different training strategies on various models. Results marked with ? are from our runs with their released code.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparisons of different ranking model architectures. Best results of each matching model are bold-faced. The "Ranking Model" rows represent the performances of different ranking models.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More sophisticated designs for the function pcc(t) are possible, but we do not consider them in this work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The encoders can be any model, e.g., LSTM<ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref> and Transformers<ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.douban.com/group</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Our experiments show that other hyper-parameter settings have little impact on the model performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Jialu Xu and Sihui Wang for their insightful discussions and support. Many thanks to our anonymous reviewers for their suggestions and comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statement</head><p>We honor and support the ACL code of Ethics. Dialogue response selection aims to build a retrievalbased dialogue system which better interacts with users. The selection of the best response does not involve any bias towards to the participants. All datasets used in this work are from previously published works, and in our view, do not have any attached privacy or ethical issues.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ranking Model Training</head><p>Here we provide more details on how to train the neural ranking model G(?, ?) that serves as the scoring basis in the proposed HCL framework.</p><p>Modelling. Given a dialogue context c and a response r, their context-response relevance score is defined as</p><p>Note that, the context c is a long sequence which is acquired by concatenating all utterances in the dialogue context. The E c (c) and E r (r) are the context and response encoder. The context encoder E c (?) takes the token sequence c = c 0 , ..., c |c| and returns the context representation E c (c) by taking the output state corresponds to the last token c |c| . The same operation is applied when computing the response representation E r (r). In practice, the choice of E(?) could be any sequence model, e.g., LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>, RNN <ref type="bibr" target="#b2">(Elman, 1990)</ref>, Transformers <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, and BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>. In this work, we choose Transformers as our modelling basis.</p><p>Learning. The goal of training the ranking model is to create a vector space such that similar pair of dialogue contexts and responses have higher relevance score than the dissimilar ones.</p><p>We train the ranking model with the same response selection data set D using the in-batch negative objective <ref type="bibr" target="#b9">(Karpukhin et al., 2020)</ref>. For a sampled batch of training data {(c k , r k )} b k=1 , where b is the batch size, the sampled contexts and responses are separately encoded using Eq. (4) as E c (C) ? R b?n and E r (R) ? R b?n , where n is the output size of encoder modules. Next, the score matrix S is computed as E c (C) T E r (R) ? R b?b . The in-batch negative objective <ref type="bibr" target="#b9">(Karpukhin et al., 2020)</ref> is then defined as minimizing the negative log likelihood of positive responses</p><p>In this work, we build the context and response encoder with a 3-layer Transformers and its output size is 256. For all considered datasets, we pre-train the ranking model with a batch size b = 128 for 20, 000 steps. For optimization, we use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 2e-5. For more details, we refer the readers to the original paper <ref type="bibr" target="#b9">(Karpukhin et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-parameter Setup</head><p>In the following, we provide details on the search space for the hyperparameters. For number of negative responses m in Eq. (1), the search space is {1, 5, 10, 15, 20}, where the underline indicates the number selected based on the model performance on the validation set. The search space for the p cc (0) in corpus-level pacing function p cc (t) is {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. For the k T in instance-level pacing function p ic (t), the search space is {1, 2, 3, 4, 5, log 500k 10 }, where 500k is the size of the training set.</p><p>Each matching model is optimized with Adam optimizer (Kingma and Ba, 2015) with a learning rate of 2e-5 and a batch size of 128. The total training step is set as 40, 000. T in the corpuslevel pacing fucntion p cc (t) and the instance-level pacing function p ic (t) is set as the half of the total training steps (i.e., T = 20000).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning<address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06-14" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA; Long and Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog1402_1</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a matching model with co-teaching for multi-turn response selection in retrieval-based dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3805" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speaker-aware BERT for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412330</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19" />
			<biblScope unit="page" from="2041" to="2044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive matching network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="2321" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.179</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved deep learning baselines for ubuntu corpus dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<idno>abs/1510.03753</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The alexa meaning representation language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauren</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lambert</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradford</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-3022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
	<note>NAACL-. Industry Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sampling matters! an empirical study of negative sampling strategies for learning of matching models in retrievalbased dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1291" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple instance curriculum learning for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2017-09-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The world is not binary: Learning to rank with grayscale data for dialogue response selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curriculum learning for natural answer generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/587</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="4223" to="4229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w15-4640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL</title>
		<meeting>the SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conference</surname></persName>
		</author>
		<title level="m">The 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-04" />
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constructing interpretive spatio-temporal features for multiturn responses selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenbin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeying</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="44" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Curriculum learning strategies for IR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Penha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45439-5_46</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -42nd European Conference on IR Research</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-04-14" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="699" to="713" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Competence-based curriculum learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otilia</forename><surname>Emmanouil Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1162" to="1172" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John McIntyre Conference Centre</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">From eliza to xiaoice: Challenges and opportunities with social chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1801.01957</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How &quot;less is more&quot; in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06-02" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Prototype-to-style: Dialogue generation with style-aware editing on retrieval memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno>abs/2004.02214</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic curriculum graph generation for reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Svetlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Leonetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jivko</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="2590" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multirepresentation fusion network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="267" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One time of interaction may not be enough: Go deep with an interaction-over-interaction network for response selection in dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Match-srnn: Modeling the recursive matching structure with spatial RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA, 9</addrLine></address></meeting>
		<imprint>
			<publisher>IJCAI/AAAI Press</publisher>
			<date type="published" when="2016-07-15" />
			<biblScope unit="page" from="2922" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A dataset for research on short-text conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="935" to="945" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1170</idno>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016. The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning matching models with weak supervision for response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="420" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911542</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-hop selector network for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling multiturn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08-20" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-view response selection for humancomputer conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-turn response selection for chatbots with deep attention matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
