<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushuang</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised anomaly detection and localization is crucial to the practical application when collecting and labeling sufficient anomaly data is infeasible. Most existing representation-based approaches extract normal image features with a deep convolutional neural network and characterize the corresponding distribution through non-parametric distribution estimation methods. The anomaly score is calculated by measuring the distance between the feature of the test image and the estimated distribution. However, current methods can not effectively map image features to a tractable base distribution and ignore the relationship between local and global features which are important to identify anomalies. To this end, we propose FastFlow implemented with 2D normalizing flows and use it as the probability distribution estimator. Our FastFlow can be used as a plug-in module with arbitrary deep feature extractors such as ResNet and vision transformer for unsupervised anomaly detection and localization. In training phase, FastFlow learns to transform the input visual feature into a tractable distribution and obtains the likelihood to recognize anomalies in inference phase. Extensive experimental results on the MVTec AD dataset show that FastFlow surpasses previous state-of-the-art methods in terms of accuracy and inference efficiency with various backbone networks. Our approach achieves 99.4% AUC in anomaly detection with high inference efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The purpose of anomaly detection and localization in computer vision field is to identify abnormal images and locate abnormal areas, which is widely used in industrial defect detection <ref type="bibr">(Bergmann et al. 2019</ref><ref type="bibr">(Bergmann et al. , 2020</ref>, medical image inspection <ref type="bibr">(Philipp Seeb?ck et al. 2017)</ref>, security check (Akcay, Atapour-Abarghouei, and Breckon 2018) and other fields. However, due to the low probability density of anomalies, the normal and abnormal data usually show a serious longtail distribution, and even in some cases, no abnormal samples are available. The drawback of this reality makes it difficult to collect and annotate a large amount of abnormal data for supervised learning in practice. Unsupervised anomaly detection has been proposed to address this problem, which is also denoted as one-class classification or outof-distribution detection. That is, we can only use normal samples during training process but need to identify and locate anomalies in testing.</p><p>One promising method in unsupervised anomaly detection is using deep neural networks to obtain the features of normal images and model the distribution with some statistical methods, then detect the abnormal samples that have different distributions <ref type="bibr" target="#b2">(Bergman and Hoshen 2020;</ref><ref type="bibr" target="#b15">Rippel, Mertens, and Merhof 2021;</ref><ref type="bibr" target="#b22">Yi and Yoon 2020;</ref><ref type="bibr" target="#b6">Cohen and Hoshen 2020;</ref><ref type="bibr" target="#b7">Defard et al. 2020)</ref>. Following this methodology, there are two main components: the feature extraction module and the distribution estimation module.</p><p>To the distribution estimation module, previous approaches used the non-parametric method to model the distribution of features for normal images. For example, they estimated the multidimensional Gaussian distribution <ref type="bibr" target="#b11">(Li et al. 2021;</ref><ref type="bibr" target="#b7">Defard et al. 2020</ref>) by calculating the mean and variance for features, or used a clustering algorithm to estimate these normal features by normal clustering <ref type="bibr" target="#b12">(Reiss et al. 2021;</ref><ref type="bibr" target="#b16">Roth et al. 2021)</ref>. Recently, some works (Rudolph, Wandt, and Rosenhahn 2021; Gudovskiy, Ishizaka, and Kozuka 2021) began to use normalizing flow <ref type="bibr" target="#b10">(Kingma and Dhariwal 2018)</ref> to estimate distribution. Through a trainable process that maximizes the log-likelihood of normal image features, they embed normal image features into standard normal distribution and use the probability to identify and locate anomalies. However, original one-dimensional normalizing flow model need to flatten the two-dimensional input feature into a one-dimensional vector to estimate the distribution, which destroys the inherent spatial positional relationship of the two-dimensional image and limits the ability of flow model. In addition, these methods need to extract the features for a large number of patches in images through the sliding window method, and detect anomalies for each patch, so as to obtain anomaly location results, which leads to high complexity in inference and limits the practical value of these methods. To address above problems, we propose the FastFlow which extend the original normalizing flow to two-dimensional space. We use fully convolutional network as the subnet in our flow model and it can maintain the relative position of the space to improve the performance of anomaly detection. At the same time, it supports the end-toend inference of the whole image and directly outputs the anomaly detection and location results at once to improve the inference efficiency.</p><p>To the feature extraction module in anomaly detection, besides using CNN backbone network such as <ref type="bibr">ResNet (He et al. 2016)</ref> to obtain discriminant features, most of the existing work <ref type="bibr" target="#b7">(Defard et al. 2020;</ref><ref type="bibr" target="#b12">Reiss et al. 2021;</ref><ref type="bibr" target="#b16">Rudolph, Wandt, and Rosenhahn 2021;</ref><ref type="bibr" target="#b8">Gudovskiy, Ishizaka, and Kozuka 2021)</ref> focuses on how to reasonably use multiscale features to identify anomalies at different scales and semantic levels, and achieve pixel-level anomaly localization through sliding window method. The importance of the correlation between global information and local anomalies <ref type="bibr" target="#b21">(Yan et al. 2021;</ref><ref type="bibr" target="#b20">Wang et al. 2021)</ref> can not be fully utilized, and the sliding window method needs to test a large number of image patches with high computational complexity. To address the problems, we use FastFlow to obtain learnable modeling of global and local feature distributions through an end-to-end testing phase, instead of designing complicated multi-scale strategy and using sliding window method. We conducted experiments on two types of backbone networks: vision transformers and CNN. Compared with CNN, vision transformers can provide a global receptive field and make better use of global and local information while maintaining semantic information in different depths. Therefore, we only use the feature of one certain layer in vision transform. Replacing CNN with vision transformer seems trivial, but we found that performing this simple replacement in other methods actually degrade the performance, but our 2D flow achieve competitive results when using CNN. Our FastFlow has stronger global and local modeling capabilities, so it can better play the effectiveness of the transformer.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in our approach, we first extract visual features by the feature extractor and then input them into the FastFlow to estimate the probability density. In training stage, our FastFlow is trained with normal images to transform the original distribution to a standard normal distribution in a 2D manner. In inference, we use the probability value of each location on the two-dimensional feature as the anomaly score.</p><p>To summarize, the main contributions of this paper are:</p><p>? We propose a 2D normalizing flow denoted as FastFlow for anomaly detection and localization with fully convolutional networks and two-dimensional loss function to effectively model global and local distribution.</p><p>? We design a lightweight network structure for FastFlow with the alternate stacking of large and small convolution kernels for all steps. It adopts an end-to-end inference phase and has high efficiency.</p><p>? The proposed FastFlow model can be used as a plugin model with various different feature extractors. The experimental results in MVTec anomaly detection dataset <ref type="bibr">(Bergmann et al. 2019)</ref> show that our method outperforms the previous state-of-the-art anomaly detection methods in both accuracy and reasoning efficiency.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anomaly Detection Methods</head><p>Existing anomaly detection methods can be summarized as reconstruction-based and representation-based methods.</p><p>Reconstruction-based methods <ref type="bibr">(Bergmann et al. 2019;</ref><ref type="bibr" target="#b7">Gong et al. 2019;</ref><ref type="bibr" target="#b11">Perera, Nallapati, and Xiang 2019)</ref> typically utilize generative models like auto-encoders or generative adversarial networks to encode and reconstruct the normal data. These methods hold the insights that the anomalies can not be reconstructed since they do not exist at the training samples. Representation-based methods extract discriminative features for normal images <ref type="bibr" target="#b17">(Ruff et al. 2018;</ref><ref type="bibr" target="#b2">Bergman and Hoshen 2020;</ref><ref type="bibr" target="#b15">Rippel, Mertens, and Merhof 2021;</ref><ref type="bibr" target="#b16">Rudolph, Wandt, and Rosenhahn 2021)</ref> or normal image patches <ref type="bibr" target="#b22">(Yi and Yoon 2020;</ref><ref type="bibr" target="#b6">Cohen and Hoshen 2020;</ref><ref type="bibr" target="#b12">Reiss et al. 2021;</ref><ref type="bibr" target="#b8">Gudovskiy, Ishizaka, and Kozuka 2021)</ref> with deep convolutional neural network, and establish distribution of these normal features. Then these methods obtain the anomaly score by calculating the distance between the feature of a test image and the distribution of normal features. The distribution is typically established by modeling the Gaussian distribution with mean and variance of normal features <ref type="bibr" target="#b7">(Defard et al. 2020;</ref><ref type="bibr" target="#b11">Li et al. 2021)</ref>, or the kNN for the entire normal image embedding <ref type="bibr" target="#b12">(Reiss et al. 2021;</ref><ref type="bibr" target="#b16">Roth et al. 2021)</ref>. We follow the methodology in representationbased method which extract the visual feature from vision transformer or ResNet and establish the distribution through FastFlow model.  <ref type="bibr">(Touvron et al. 2021b</ref>) are two typical models for ViT. DeiT introduces a teacher-student strategy specific to transformers, which makes image transformers learn more efficiently and got a new state-of-the-art performance. CaiT proposes a simple yet effective architecture designed in the spirit of encoder/decoder architecture and demonstrates that transformer models offer a competitive alternative to the best convolutional neural networks. In  <ref type="figure">Figure 2</ref>: (a) the whole pipeline for unsupervised anomaly detection and localization in our method, which consists of a feature extractor and our FastFlow model. We can use an arbitrary network as the feature extractor such as CNN or vision transformer. FastFlow is alternatly stacked by the "3 ? 3" and "1 ? 1" flow. (b) one flow step for our FastFlow, the "Conv 2d" can be 3 ? 3 or 1 ? 1 convolution layer for 3 ? 3 or 1 ? 1 flow, respectively. this paper, we use various networks belonging to CNN and ViT to prove the universality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature extractors for Anomaly Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Normalizing Flow</head><p>Normalizing Flows (NF) <ref type="bibr" target="#b13">(Rezende and Mohamed 2015)</ref> are used to learn transformations between data distributions with special property that their transform process is bijective and the flow model can be used in both directions. Real-NVP (Dinh, Sohl-Dickstein, and Bengio 2016) and Glow <ref type="bibr" target="#b10">(Kingma and Dhariwal 2018)</ref> are two typical methods for NF, in which both forward and reverse processes can be processed quickly. NF is generally used to generate data from variables sampled in a specific probability distribution, such as images or audios. Recently, some work (Rudolph, Wandt, and Rosenhahn 2021; Gudovskiy, Ishizaka, and Kozuka 2021) began to use it for unsupervised anomaly detection and localization. DifferNet (Rudolph, Wandt, and Rosenhahn 2021) achieved good image level anomaly detection performance by using NF to estimate the precise likelihood of test images. Unfortunately, this work failed to obtain the exact anomaly localization results since they flattened the outputs of feature extractor. CFLOW-AD (Gudovskiy, Ishizaka, and Kozuka 2021) proposes to use hard code position embedding to leverage the distribution learned by NF, which probably underperforms at more complicated datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce the pipeline of our method and the architecture of the FastFlow, as shown in <ref type="figure">Figure</ref> 2. We first set up the problem definition of unsupervised anomaly detection, and introduce the basic methodology that uses a learnable probability density estimation model in the representation-based method. Then we describe the details of feature extractor and FastFlow models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Basic Methodology</head><p>Unsupervised anomaly detection is also denoted as oneclass classification or out-of-distribution detection which requires the model to determines whether the test image is normal or abnormal. Anomaly localization requires a more fine-grained result that gives the anomalies label for each pixel. During the training stage, only normal images were observed, but the normal images and abnormal images simultaneously appear in inference. One of the mainstream methods is representation-based method which extracts discriminative feature vectors from normal images or normal image patches to construct the distribution and calculate anomaly score by the distance between the embedding of a test image and the distribution. The distribution is typically characterized by the center of an n-sphere for the normal image, the Gaussian distribution of normal images, or the normal embedding cluster stored in the memory bank obtained from KNN. After extract the features of the training dataset D = {x 1 , x 2 , ? ? ? , x N } where x i , i = 1, 2, ? ? ? , N are samples from the distribution p X (x), a representationbased anomaly detection model P = {P ? : ? ? ?} aims to learn the parameter ? in the parameter space ? to map all x i from the raw distribution p X (x) into the same distribution p Z (z), with anomalous pixels or instances mapped out of the distribution. In our method, we follow this methodology and propose FastFlow P ? to project the high-dimensional visual features of normal images extracted from typical backbone networks into the standard normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extractor</head><p>In the whole pipeline of our method, we first extract the representative feature from the input image through ResNet or vision transformers. As mentioned in the Sec 1, one of significant challenges in the anomaly detection task is the global relation grasped to distinguish those abnormal regions from other local parts. Therefore, when using vision transformer (ViT) (Dosovitskiy et al. 2020) as the feature extractor, we only use the feature of one certain layer because ViT has stronger ability to capture the relationship between local patches and the global feature. For ResNet, we directly use the features of the last layer in the first three blocks, and put these features into three corresponding FastFlow model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">2D Flow Model</head><p>As shown in <ref type="figure">Figure 2</ref>, our 2D flow f : X ? Z is used to project the image features x ? p X (x) into the hidden variable z ? p Z (z) with a bijective invertible mapping. For this bijection function, the change of the variable formula defines the model distribution on X by:</p><formula xml:id="formula_0">p X (x) = p Z (z) det( ?z ?x )<label>(1)</label></formula><p>We can estimate the log likelihoods for image features from p Z (z) by:</p><formula xml:id="formula_1">log p X (x) = log p Z (z) + log det( ?z ?x ) = log p Z (f ? (x)) + log det( ?f ? (x) ?x ) ,<label>(2)</label></formula><p>where z ? N (o, I) and the ?f ? (x) ?x is the Jacobian of a bijective invertible flow model that z = f ? (x) and x = f ?1 ? (z), ? is parameters of the 2D flow model. In inference, the features of anomalous images should be out of distribution and hence have lower likelihoods than normal images and the likelihood can be used as the anomaly score. Specifically, we sum the two-dimensional probabilities of each channel to get the final probability map and upsample it to the input image resolution using bilinear interpolation. In actual implementation, our flow model f 2d is constructed by stacking multiple invertible transformations blocks f i in a sequence that:</p><formula xml:id="formula_2">X f1 ? ? H 1 f2 ? ? H 2 f3 ? ? ? ? ? f K ? ? ? Z, and X f ?1 1 ??? H 1 f ?1 2 ??? H 2 f ?1 3 ??? ? ? ? f ?1 K ??? Z, where the 2D flow model is f 2d = f 1 ? f 2 ? f 3 ? ? ? ? ? f K with K transformation blocks.</formula><p>Each transformation block f i consists of multiple steps. Following (Dinh, Krueger, and Bengio 2014), we employ affine coupling layers in each block, and each step is formulated as follow: </p><p>where s(y a ) and b(y a ) are outputs of two neural networks. The split(?) and concat(?) functions perform splitting and concatenation operations along the channel dimension. The two subnets s(?) and b(?) are usually implemented as fully connected networks in original normalizing flow model and need to flatten and squeeze the input visual features from 2D to 1D which destroy the spatial position relationship in the feature map. To convert the original normalizing flow to 2D manner, we adopt two-dimensional convolution layer in the default subnet to reserve spatial information in the flow model and adjust the loss function accordingly. In particular, we adopt a fully convolutional network in which 3?3 convolution and 1?1 convolution appear alternately, which reserves spatial information in the flow model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>We evaluate the proposed method on three datasets: MVTec AD <ref type="bibr">(Bergmann et al. 2019)</ref>, <ref type="bibr">BTAD (Mishra et al. 2021</ref>) and <ref type="bibr">CIFAR-10 (Krizhevsky, Hinton et al. 2009</ref>). MVTec AD and BTAD are both industrial anomaly detection datasets with pixel-level annotations, which are used for anomaly detection and localization. CIFAR-10 is built for image classification and we use it to do anomaly detection. Following the previous works, we choose one of the categories as normal, and the rest as abnormal. The anomalies in these industrial datasets are finer than those in CIFAR-10, and the anomalies in CIFAR-10 are more related to the semantic high-level information. For example, the anomalies in MVTec AD are defined as small areas, while the anomalies in CIFAR-10 dataset are defined as different object categories. Under the unsupervised setting, we train our model for each category with its respective normal images and evaluate it in test images that contain both normal and abnormal images. The performance of the proposed method and all comparable methods is measured by the area under the receiver operating characteristic curve (AUROC) at image or pixel level. For the detection task, evaluated models are required to output single score (anomaly score) for each input test image. In the localization task, methods need to output anomaly scores for every pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Complexity Analysis</head><p>We make a complexity analysis of FastFlow and other methods from aspects of inference speed, additional inference time and additional model parameters, "additional" refers to not considering the backbone itself. The hardware configuration of the machine used for testing is Intel(R) Xeon(R) CPU E5-2680 V4@2.4GHZ and NVIDIA GeForce GTX 1080Ti. SPADE and Patch Core perform KNN clustering between each test feature of each image patch and the gallery features of normal image patches, and they do not need to introduce parameters other than backbone. CFlow avoids the time-consuming k-nearest-neighbor-search process, but it still needs to perform testing phase in the form of a slice window. Our FastFlow adopts an end-to-end inference phase which has high efficiency of inference. The analysis results are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Results</head><p>MVTec AD There are 15 industrial products in MVTec AD dataset <ref type="bibr">(Bergmann et al. 2019)</ref>, with a total of 5,354 images, among which 10 are objects and the remaining 5 are textures. The training set is only composed of normal images, while the test set is a mixture of normal images and abnormal images. We compare our proposed method with the state-of-the-art anomaly detection works, including SPADE* <ref type="bibr" target="#b12">(Reiss et al. 2021</ref>  <ref type="table" target="#tab_5">Table 3</ref>. We can observe that our FastFlow achieves 97.0 pixel-wise AUC and suppresses other methods as high as 7% AUC.</p><p>CIFAR-10 dataset CIFAR-10 has 10 categories with 60000 natural images. Under the setting of anomaly detection, one category is regarded as anomaly and other categories are used as normal data. And we need to train the corresponding model for each class respectively. The AUC scores of our method and other methods are reported in <ref type="table" target="#tab_6">Table 4</ref>. Methods for comparison includes OC-SVM <ref type="bibr">(Sch?lkopf et al. 1999)</ref>, KDE <ref type="bibr" target="#b5">(Bishop 2006)</ref>, l 2 -AE <ref type="bibr" target="#b9">(Hadsell, Chopra, and LeCun 2006)</ref>, VAE <ref type="bibr" target="#b1">(An and Cho 2015)</ref>, Pixel <ref type="bibr">CNN (Oord et al. 2016)</ref>, LSA <ref type="bibr" target="#b0">(Abati et al. 2019)</ref>, AnoGAN <ref type="bibr" target="#b18">(Schlegl et al. 2017)</ref>, DSVDD <ref type="bibr" target="#b17">(Ruff et al. 2018</ref>) and OCGAN (Perera, Nallapati, and Xiang 2019). Our method outperforms these comparison methods. The results in three different datasets show that our method can adapt to different anomaly detection settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To investigate the effectiveness of the proposed FastFlow structure, we design ablation experiments about the convolution kernel selection in subnet. We compare alternately using 3 ? 3 and 1 ? 1 convolution kernel and only using 3 ? 3 kernel under the AUC and inference speed for the subnet with various backbone networks. The results are shown in    formance while reducing the amount of parameters. For the backbone network with small model capacities such as DeiT and ResNet18, only using 3 ? 3 convolution layer has higher performance. To achieve the balance of accuracy and inference speed, we use alternate convolution kernels of 3 ? 3, 1 ? 1 with DeiT, CaiT and Wide-ResNet50-2, and only use 3 ? 3 convolution layer with ResNet18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Feature Visualization and Generation.</head><p>Our FastFlow model is a bidirectional invertible probability distribution transformer. In the forward process, it takes the feature map from the backbone network as input and transforms its original distribution into a standard normal distribution in two-dimensional space. In the reverse process, the inverse of FastFlow can generate the visual feature from a specific probability sampling variable. To better understand this ability in view of our FastFlow, we visualize the forward (from visual features to probability map) and reverse (from probability map to visual features) processes. As shown in <ref type="figure">Figure 4</ref>, we extract the features of an input image belonging to the leather class and the abnormal area is indicated by  the red arrow. We forward it through the FastFlow model to obtain the probability map. Our FastFlow successfully transformed the original distribution into the standard normal distribution. Then, we add noise interference to a certain spatial area which is indicated by the yellow arrow in this probability map, and generate a leather feature tensor from the pollution probability map by using the inverse Fastflow model. In which we visualized the feature map of one channel in this feature tensor, and we can observe that new anomaly appeared in the corresponding pollution position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Results</head><p>We visualize some results of anomaly detection and localization in <ref type="figure">Figure 3</ref> with the MVTec AD dataset. The top row shows test images with ground truth masks with and without anomalies, and the anomaly localization score heatmap is shown in the bottom row. There are both normal and abnormal images and our FastFlow gives accurate anomaly localization results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Implementation Details</head><p>We provide the details of the structure of feature extractor, the selection of feature layer and the size of input image in <ref type="table" target="#tab_10">Table 6</ref>. For vision transformer, our method only uses feature maps of a specific layer, and does not need to design complicated multi-scale features manually. For ResNet18 and Wide-ResNet50-2, we directly use the features of the last layer in the first three blocks, put these features into the 2D flow model to obtain their respective anomaly detection and localization results, and finally take the average value as the final result. All these backbone are initialized with the ImageNet pre-trained weights and their parameters are frozen in the following training process. For FastFlow, we use 20-step flows in CaiT and DeiT and 8-step flows for ResNet18 and Wide-ResNet50-2. We train our model using Adam optimizer with the learning rate of 1e-3 and weight decay of 1e-5. We use a 500 epoch training schedule, and the batch size is 32.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel approach named FastFlow for unsupervised anomaly detection and localization.</p><p>Our key observation is that anomaly detection and localization requires comprehensive consideration of global and local information with a learnable distribution modeling method, and efficient inference process, which are ignored in the existing approaches. To this end, we present a 2D flow model denoted as FastFlow which has a lightweight structure and is used to project the feature distribution of normal images to the standard normal distribution in training, and use the probabilities as the anomaly score in testing.   6 More Ablation Studies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Channels of Hidden Layers in Flow Model</head><p>In the original flow model which has been used in DifferNet (Rudolph, Wandt, and Rosenhahn 2021) and CFLOW <ref type="bibr" target="#b8">(Gudovskiy, Ishizaka, and Kozuka 2021)</ref>, the number of channels of hidden layers in all subnet is set to 2? as much the input and output layer's channel. This kind of design improves the results by increasing the complexity of the model, but it reduces the efficiency of inference. In our FastFlow, we found that using 0.16? number of channels in CaiT and 1? number of channels in Wide-ResNet50-2 can achieve a balance between performance and model parameters. In addition, when we use 0.25? number of channels of Wide-ResNet50-2, we can further reduce the model parameters while still maintaining high accuracy. The results are shown in <ref type="table" target="#tab_12">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training Data Augmentation</head><p>In order to learn a more robust FastFlow model, we apply various data augmentation methods to the MVTec AD dataset during the training phase. We use random horizontal flip, vertical flip and rotation, with probabilities of 0.5, 0.3 and 0.7, respectively. It should be noted that some categories are not suitable for violent data augmentation. For example, the transistor can not be flipped upside down and rotated. The results are shown in <ref type="table" target="#tab_13">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Bad Cases and Ambiguity Label</head><p>We visualize bad cases for our method on MVTec AD dataset in <ref type="figure">Figure 5</ref> to <ref type="figure">Figure 7</ref> which are summarized into three categories. We show the missing detection cases in <ref type="figure">Figure 5</ref>, false detection cases in <ref type="figure">Figure 6</ref> and label ambiguity cases in <ref type="figure">Figure 7</ref>. In <ref type="figure">Figure 5</ref>, our method missed a few small and unobvious anomalies. In <ref type="figure">Figure 6</ref>, our method had false detection results in some background areas, such as areas with hair and dirt in the background. In <ref type="figure">Figure 7</ref>, our method found some areas belong to abnormal but not be labeled, such as the "scratch neck" for screw and the "fabric interior" for zipper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Non-aligned Disturbed MVTec AD Dataset</head><p>Considering that the MVTec AD dataset has the characteristic of sample alignment which is infrequent in practical application, we perform a series of spatial perturbations on the test data to obtain an unaligned MVTec AD dataset. In detail, we apply random zoom in/out with 0.85 ratio, random rotation with ?15 angle, random translation with 0.15 ratio to expand the original test dataset by 4? to the new test dataset. We evaluate our FastFlow (with CaiT) in this new test dataset and we obtain 99.2 image-level AUC and 98.1 pixle-level AUC. There is almost no performance loss compared with the results in original aligned MVTec AD test dataset, which proves the robustness of our method. We also give some visualization results in <ref type="figure" target="#fig_4">Figure 8</ref>. We can observe that FastFlow can still have high performance on anomaly detection and location result in this non-aligned disturbed MVTec AD dataset. <ref type="figure">Figure 7</ref>: Bad cases caused by label ambiguity. In the first two rows, there are abnormal areas localized by our method while not labeled. In the last row of hazelnut, we show the label ambiguity of the "print" subclass, in which one hazelnut print is labeled finely, while the other is labeled with a rough area. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of the proposed FastFlow. FastFlow transforms features of the input image from the original distribution to the standard normal distribution. The features of the normal area in the input image fall in the center of the distribution, while the abnormal features are far away from the center of the distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>y a , y b = split(y) y a = y a y b = s(y a ) y b + b(y a ) y = concat(y a , y b ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Anomaly localization results of MVTec AD datasets. From top to bottom, input images with ground-truth localization area labeled in red and anomaly localization heatmaps. The bidirectional invertible process for FastFlow. "FE" is the feature extractor, "FF" is our FastFlow model, "FF ?1 " is the reverse for FastFlow. The red and yellow arrows point to the original anomaly and the new anomaly introduced after the noise disturbance respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Anomaly localization results of the non-aligned disturbed MVTec AD datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, we can observe that our method is up</cell></row></table><note>Table 1: Complexity comparison in terms of inference speed (FPS), additional inference time (millisecond) and number of additional parameters (M) for various backbones. A.d. Time means the additional inference time and A.d. Parmas is the number of additional parameters compared with backbone network.to 10? faster than other methods. Compared with CFlow which also uses flow model, our method achieves 1.5? speedup and 2? parameter reduction. When using vision transformers (deit and cait) as the feature extractor, our Fast- Flow can achieve 99.4 image-level AUC for anomaly de- tection which is superior to CFlow and Patch Core. From the perspective of additional inference time, our method achieves up to 4? reduction compared to Cflow and 10? reduction compared to Patch Core. Our FastFlow can still have a competitive performance when using ResNet model as feature extractor.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell>PatchSVDD</cell><cell>SPADE*</cell><cell>DifferNet</cell><cell>PaDiM</cell><cell>Cut Paste</cell><cell>PatchCore</cell><cell>CFlow</cell><cell>FastFlow</cell></row><row><cell>carpet</cell><cell cols="2">(92.9,92.6) (98.6,97.5)</cell><cell>(84.0,-)</cell><cell>(-,99.1)</cell><cell cols="4">(100.0,98.3) (98.7,98.9) (100.0,99.3) (100.0,99.4)</cell></row><row><cell>grid</cell><cell cols="2">(94.6,96.2) (99.0,93.7)</cell><cell>(97.1,-)</cell><cell>(-,97.3)</cell><cell>(96.2,97.5)</cell><cell>(98.2,98.7)</cell><cell>(97.6,99.0)</cell><cell>(99.7,98.3)</cell></row><row><cell>leather</cell><cell cols="2">(90.9,97.4) (99.5,97.6)</cell><cell>(99.4,-)</cell><cell>(-,99.2)</cell><cell cols="4">(95.4,99.5) (100.0,99.3) (97.7,99.7) (100.0,99.5)</cell></row><row><cell>tile</cell><cell cols="2">(97.8,91.4) (89.8,87.4)</cell><cell>(92.9,-)</cell><cell>(-,94.1)</cell><cell cols="2">(100.0,90.5) (98.7,95.6)</cell><cell cols="2">(98.7,98.0) (100.0,96.3)</cell></row><row><cell>wood</cell><cell cols="2">(96.5,90.8) (95.8,88.5)</cell><cell>(99.8,-)</cell><cell>(-,94.9)</cell><cell>(99.1,95.5)</cell><cell>(99.2,95.0)</cell><cell cols="2">(99.6,96.7) (100.0,97.0)</cell></row><row><cell>bottle</cell><cell cols="2">(98.6,98.1) (98.1,98.4)</cell><cell>(99.0,-)</cell><cell>(-,98.3)</cell><cell cols="4">(99.9,97.6) (100.0,98.6) (100.0,99.0) (100.0,97.7)</cell></row><row><cell>cable</cell><cell cols="2">(90.3,96.8) (93.2,97.2)</cell><cell>(86.9,-)</cell><cell>(-,96.7)</cell><cell cols="4">(100.0,90.0) (99.5,98.4) (100.0,97.6) (100.0,98.4)</cell></row><row><cell>capsule</cell><cell cols="2">(76.7 ,95.8) (98.6,99.0)</cell><cell>(88.8,-)</cell><cell>(-,98.5)</cell><cell>(98.6 ,97.4)</cell><cell>(98.1,98.8)</cell><cell cols="2">(99.3 ,99.0) (100.0,99.1)</cell></row><row><cell>hazelnut</cell><cell cols="2">(92.0 ,97.5) (98.9,99.1)</cell><cell>(99.1,-)</cell><cell>(-,98.2)</cell><cell cols="4">(93.3 ,97.3) (100.0,98.7) (96.8 ,98.9) (100.0,99.1)</cell></row><row><cell>meta nut</cell><cell cols="2">(94.0,98.0) (96.9,98.1)</cell><cell>(95.1,-)</cell><cell>(-,97.2)</cell><cell cols="4">(86.6,93.1) (100.0,98.4) (91.9,98.6) (100.0,98.5)</cell></row><row><cell>pill</cell><cell cols="2">(86.1,95.1) (96.5,96.5)</cell><cell>(95.9,-)</cell><cell>(-,95.7)</cell><cell>(99.8,95.7)</cell><cell>(96.6,97.1)</cell><cell>(99.9,99.0)</cell><cell>(99.4,99.2)</cell></row><row><cell>screw</cell><cell cols="2">(81.3,95.7) (99.5,98.9)</cell><cell>(99.3,-)</cell><cell>(-,98.5)</cell><cell>(90.7,96.7)</cell><cell>(98.1,99.4)</cell><cell>(99.7,98.9)</cell><cell>(97.8,99.4)</cell></row><row><cell cols="3">toothbrush (100.0,98.1) (98.9,97.9)</cell><cell>(96.1,-)</cell><cell>(-,98.8)</cell><cell cols="3">(97.5 ,98.1) (100.0,98.7) (95.2,99.0)</cell><cell>(94.4 ,98.9)</cell></row><row><cell>transistor</cell><cell cols="2">(91.5,97.0) (81.0,94.1)</cell><cell>(96.3,-)</cell><cell>(-,97.5)</cell><cell cols="3">(99.8,93.0) (100.0,96.3) (99.1 ,98.0)</cell><cell>(99.8,97.3)</cell></row><row><cell>zipper</cell><cell cols="2">(97.9,95.1) (98.8,96.5)</cell><cell>(98.6,-)</cell><cell>(-,98.5)</cell><cell>(99.9,99.3)</cell><cell>(98.8,98.8)</cell><cell>(98.5 ,99.1)</cell><cell>(99.5,98.7)</cell></row><row><cell>AUCROC</cell><cell cols="2">(92.1,95.7) (96.2,96.5)</cell><cell>(94.9,-)</cell><cell cols="2">(97.9,97.5) (97.1,96.0)</cell><cell>(99.1,98.1)</cell><cell>(98.3,98.6)</cell><cell>(99.4,98.5)</cell></row></table><note>. For the backbone network with large model ca- pacities such as CaiT and Wide-ResNet50-2, alternate using 3 ? 3 and 1 ? 1 convolution layer can obtain higher per-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Anomaly detection and localization performance on MVTec AD dataset with the format (image-level AUC, pixel-level AUC). We report the detailed results for all categories.</figDesc><table><row><cell cols="5">Categories AE MSE AE MSE+SSIM VT-ADL FastFlow</cell></row><row><cell>0</cell><cell>0.49</cell><cell>0.53</cell><cell>0.99</cell><cell>0.95</cell></row><row><cell>1</cell><cell>0.92</cell><cell>0.96</cell><cell>0.94</cell><cell>0.96</cell></row><row><cell>2</cell><cell>0.95</cell><cell>0.89</cell><cell>0.77</cell><cell>0.99</cell></row><row><cell>Mean</cell><cell>0.78</cell><cell>0.79</cell><cell>0.90</cell><cell>0.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Anomaly localization results on BTAD datasts. We compare our method with convolutional auto encoders trained with MSE-loss and MSE+SSIM loss, and VT-ADL.</figDesc><table><row><cell cols="2">Method OC-SVM</cell><cell>KDE</cell><cell>l 2 -AE</cell><cell>VAE</cell><cell>Pixel CNN</cell></row><row><cell>AUC</cell><cell>58.6</cell><cell>61.0</cell><cell>53.6</cell><cell>58.3</cell><cell>55.1</cell></row><row><cell>Method</cell><cell>LSA</cell><cell cols="3">AnoGAN DSVDD OCGAN</cell><cell>FastFlow</cell></row><row><cell>AUC</cell><cell>64.1</cell><cell>61.8</cell><cell>64.8</cell><cell>65.6</cell><cell>66.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Anomaly detection results on CIFAR-10 datast.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Results of ablation experiments with various back-</cell></row><row><cell>bone networks. 3-1 means alternately using 3 ? 3 and 1 ? 1</cell></row><row><cell>convolution layers and 3-3 is only using 3 ? 3 convolution</cell></row><row><cell>layer in the subnet for FastFlow. A.d. Params is the num-</cell></row><row><cell>ber of additional model parameters compared with backbone</cell></row><row><cell>network.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>We use four different feature extractors in all experiments. The input picture size and feature size are set according to the backbone network and the block index indicates the block from which the feature is obtained..</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>FastFlow can be used in typical feature extraction networks such as ResNet and ViT in the form of plug-ins. Extensive experimental results on MVTec AD dataset show FastFlow superiority over the state-of-the art methods in terms of accuracy and reasoning efficiency. Material for FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows</figDesc><table><row><cell cols="4">Supplementary Channel Ratio Parameters (M) Image-level AUC Pixel-level AUC</cell></row><row><cell>CaiT</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.16?</cell><cell>14.8</cell><cell>99.4</cell><cell>98.5</cell></row><row><cell>0.33?</cell><cell>29.6</cell><cell>98.9</cell><cell>98.4</cell></row><row><cell>Wide-ResNet50-2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.25?</cell><cell>10.9</cell><cell>98.9</cell><cell>98.0</cell></row><row><cell>0.5?</cell><cell>20.7</cell><cell>99.1</cell><cell>98.1</cell></row><row><cell>1.0?</cell><cell>41.3</cell><cell>99.3</cell><cell>98.1</cell></row><row><cell>2.0?</cell><cell>82.6</cell><cell>99.4</cell><cell>98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Ablation study results about the hidden layer channels for CNN and vision transformer in MVTec AD dataset. Channel Ratio means the ratio of the number of channels in the hidden layer to the number of channels in the input and output layers for subnet in our FastFlow.</figDesc><table><row><cell cols="3">Data Augmentation Image-level AUC Pixel-level AUC</cell></row><row><cell>CaiT</cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>99.3</cell><cell>98.4</cell></row><row><cell>w</cell><cell>99.4</cell><cell>98.5</cell></row><row><cell>Wide-ResNet50-2</cell><cell></cell><cell></cell></row><row><cell>w/o</cell><cell>98.9</cell><cell>98.2</cell></row><row><cell>w</cell><cell>99.3</cell><cell>98.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>The effect of data augmentation on the anomaly detection and localization performance.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<title level="m">Sub-image anomaly detection with deep pyramid correspondences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08785</idno>
		<idno>arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="m">PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE/CVF International Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gudovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishizaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kozuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12571</idno>
		<title level="m">CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ocgan: Oneclass novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04015</idno>
		<idno>arXiv:1606.05328</idno>
	</analytic>
	<monogr>
		<title level="m">VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bad cases of false detection type. We give the typical results of our method in this figure</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6726" to="6733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08265</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1907" to="1916" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Towards Total Recall in Industrial Anomaly Detection</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
	<note>NIPS. Citeseer</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, 10347-10357. PMLR. Touvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and J?gou, H. 2021b. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glancing at the Patch: Anomaly Localization With Global and Local Feature Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3110" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
