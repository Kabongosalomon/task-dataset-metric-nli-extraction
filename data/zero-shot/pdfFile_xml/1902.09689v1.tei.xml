<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
							<email>bchang@stat.ubc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
							<email>minminc@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
							<email>haber@math.ubc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
							<email>edchi@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Brain Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of British Columbia Vancouver</orgName>
								<address>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Google Brain Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ANTISYMMETRICRNN: A DYNAMICAL SYSTEM VIEW ON RECURRENT NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent networks called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. Existing approaches to improving RNN trainability often incur significant computation overhead. In comparison, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory and matches the performance on tasks where short-term dependencies dominate despite being much simpler. advocate going beyond initialization and forcing the weight matrices to be orthogonal throughout the entire learning process. However, some of these approaches come with significant computational overhead and reportedly hinder representation power of these models <ref type="bibr" target="#b43">(Vorontsov et al., 2017)</ref>. Moreover, orthogonal weight matrices alone do not prevent exploding and vanishing gradients, due to the nonlinear nature of deep neural networks as shown in <ref type="figure">(Pennington et al., 2017)</ref>.</p><p>Here we offer a new perspective on the trainability of RNNs from the dynamical system viewpoint. While exploding gradient is a manifestation of the instability of the underlying dynamical system, vanishing gradient results from a lossy system, properties that have been widely studied in the dynamical system literature <ref type="bibr" target="#b17">(Haber &amp; Ruthotto, 2017;</ref><ref type="bibr" target="#b29">Laurent &amp; von Brecht, 2017)</ref>. The main contributions of the work are:</p><p>? We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recurrent neural networks (RNNs) <ref type="bibr" target="#b39">(Rumelhart et al., 1986;</ref><ref type="bibr" target="#b15">Elman, 1990)</ref> have found widespread use across a variety of domains from language modeling <ref type="bibr" target="#b33">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b27">Kiros et al., 2015;</ref><ref type="bibr" target="#b25">Jozefowicz et al., 2016)</ref> and machine translation  to speech recognition <ref type="bibr" target="#b16">(Graves et al., 2013)</ref> and recommendation systems <ref type="bibr" target="#b19">(Hidasi et al., 2015;</ref><ref type="bibr" target="#b45">Wu et al., 2017)</ref>. Modeling complex temporal dependencies in sequential data using RNNs, especially the long-term dependencies, remains an open challenge. The main difficulty arises as the error signal back-propagated through time (BPTT) suffers from exponential growth or decay, a dilemma commonly referred to as exploding or vanishing gradient <ref type="bibr" target="#b36">(Pascanu et al., 2012;</ref><ref type="bibr" target="#b5">Bengio et al., 1994)</ref>.</p><p>Vanilla RNNs as originally proposed are particularly prone to these issues and are rarely used in practice. Gated variants of RNNs, such as long short-term memory (LSTM) networks <ref type="bibr" target="#b20">(Hochreiter &amp; Schmidhuber, 1997)</ref> and gated recurrent units (GRU) ) that feature various forms of "gating" are proposed to alleviate these issues. The gates allow information to flow from inputs at any previous time steps to the end of the sequence more easily, partially addressing the vanishing gradient problem <ref type="bibr" target="#b11">(Collins et al., 2016)</ref>. In practice, these models must be paired with techniques such as normalization layers <ref type="bibr" target="#b22">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b3">Ba et al., 2016)</ref> and gradient clipping <ref type="bibr" target="#b37">(Pascanu et al., 2013)</ref> to achieve good performance.</p><p>Identity and orthogonal initialization is another proposed solution to the exploding or vanishing gradient problem of deep neural networks <ref type="bibr" target="#b30">(Le et al., 2015;</ref><ref type="bibr" target="#b35">Mishkin &amp; Matas, 2015;</ref><ref type="bibr" target="#b40">Saxe et al., 2013;</ref><ref type="bibr" target="#b8">Chen et al., 2018a)</ref>. Recently, <ref type="bibr" target="#b0">Arjovsky et al. (2016)</ref>; <ref type="bibr" target="#b44">Wisdom et al. (2016)</ref>; Hyland &amp; R?tsch ? The stability of the ODE solutions and the numerical methods for solving ODEs lead us to design a special form of RNNs, which we name AntisymmetricRNN, that can capture longterm dependencies in the inputs. The construction of the model is much simpler compared to the existing methods for improving RNN trainability.</p><p>? We conduct extensive simulations and experiments to demonstrate the benefits of this new RNN architecture. AntisymmetricRNN exhibits well-behaved dynamics and outperforms the regular LSTM model on tasks requiring long-term memory, and matches its performance on tasks where short-term dependencies dominate with much fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Trainability of RNNs. Capturing long-term dependencies using RNNs has been a long-standing research topic with various approaches proposed. The first group of approaches mitigates the exploding or vanishing gradient issues by introducing some forms of gating. Long short-term memory networks (LSTM) <ref type="bibr" target="#b20">(Hochreiter &amp; Schmidhuber, 1997)</ref> and gated recurrent units (GRU)  are the most prominent models along this line of work. Parallel efforts include designing special neural architectures, such as hierarchical RNNs <ref type="bibr" target="#b14">(El Hihi &amp; Bengio, 1996)</ref>, recursive neural networks <ref type="bibr" target="#b41">(Socher et al., 2011)</ref>, attention networks , dilated convolutions <ref type="bibr" target="#b47">(Yu &amp; Koltun, 2015)</ref>, recurrent batch normalization <ref type="bibr" target="#b12">(Cooijmans et al., 2017)</ref>, residual RNNs <ref type="bibr" target="#b48">(Yue et al., 2018)</ref>, and Fourier recurrent units <ref type="bibr" target="#b50">(Zhang et al., 2018b)</ref>. These, however, are fundamentally different networks with different tradeoffs.</p><p>Another direction is to constrain the weight matrices of an RNN so that the back-propagated error signal is well conditioned, in particular, the input-output Jacobian having unitary singular values. <ref type="bibr" target="#b30">Le et al. (2015)</ref>; <ref type="bibr" target="#b34">Mikolov et al. (2014)</ref>; <ref type="bibr" target="#b35">Mishkin &amp; Matas (2015)</ref> propose the use of identity or orthogonal matrix to initialize the recurrent weight matrix. Followup works further constrain the weight matrix throughout the entire learning process either through re-parametrization <ref type="bibr" target="#b0">(Arjovsky et al., 2016)</ref>, geodesic gradient descent on the Stiefel manifold <ref type="bibr" target="#b44">(Wisdom et al., 2016;</ref><ref type="bibr" target="#b43">Vorontsov et al., 2017)</ref>, or constraining the singular values <ref type="bibr" target="#b23">(Jose et al., 2017;</ref><ref type="bibr" target="#b26">Kanai et al., 2017;</ref><ref type="bibr" target="#b49">Zhang et al., 2018a)</ref>. It is worth noting that, orthogonal weights by themselves do not guarantee unitary Jacobians. Nonlinear activations still cause the gradients to explode or vanish. Contractive maps such as sigmoid and hyperbolic tangent lead to vanishing gradients. <ref type="bibr" target="#b8">Chen et al. (2018a)</ref> offer an initialization scheme taking into account the nonlinearity. However, the theory developed relies heavily on the random matrix assumptions, which only hold at the initialization point of training. Although it has been shown to predict trainability beyond initialization.</p><p>Dynamical systems view of recurrent networks. Connections between dynamical systems and RNNs have not been well explored. <ref type="bibr" target="#b29">Laurent &amp; von Brecht (2017)</ref> study the behavior of dynamical systems induced by recurrent networks, and show that LSTMs and GRUs exhibit chaotic dynamics in the absence of input data. They propose a simplified gated RNN named the chaos free network (CFN), that has non-chaotic dynamics and achieves comparable performance to LSTMs and GRUs on language modeling. <ref type="bibr" target="#b42">Tallec &amp; Ollivier (2018)</ref> formulate RNNs as a time-discretized version of ODE and show that time invariance leads to gate-like mechanisms in RNNs. With this formulation, the authors propose an initialization scheme by setting the initial gate bias according to the range of time dependencies to capture.</p><p>Dynamical systems view of residual networks. Another line of work that is closely related to ours is the dynamical systems view on residual networks (ResNets) <ref type="bibr" target="#b18">(He et al., 2016)</ref>. <ref type="bibr" target="#b17">Haber &amp; Ruthotto (2017)</ref>; <ref type="bibr" target="#b6">Chang et al. (2018a;</ref><ref type="bibr">b)</ref>; <ref type="bibr" target="#b32">Lu et al. (2018)</ref> propose to interpret ResNets as ordinary differential equations (ODEs), under which learning the network parameters is equivalent to solve a parameter estimation problem involving the ODE. <ref type="bibr" target="#b9">Chen et al. (2018b)</ref> parameterize the continuous dynamics of hidden units using an ODE specified by a neural network. Stable and reversible architectures are developed <ref type="bibr" target="#b17">(Haber &amp; Ruthotto, 2017;</ref><ref type="bibr" target="#b6">Chang et al., 2018a)</ref> from this viewpoint, which form the basis of our proposed recurrent networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANTISYMMETRICRNNS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ORDINARY DIFFERENTIAL EQUATIONS</head><p>We first give a brief overview of the ordinary differential equations (ODEs), a special kind of dynamical systems that involves a single variable, time t in this case. Consider the first-order ODE</p><formula xml:id="formula_0">h (t) = f (h(t)),<label>(1)</label></formula><p>for time t ? 0, where h(t) ? R n and f : R n ? R n . Together with a given initial condition h(0), the problem of solving for the function h(t) is called the initial value problem. For most ODEs, it is impossible to find an analytic solution. Instead, numerical methods relying on discretization are commonly used to approximate the solution. The forward Euler method is probably the best known and simplest numerical method for approximation. One way to derive the forward Euler method is to approximate the derivative on the left-hand side of Equation 1 by a finite difference, and evaluate the right-hand side at h t?1 :</p><formula xml:id="formula_1">h t ? h t?1 = f (h t?1 ).<label>(2)</label></formula><p>Note that for the approximation to be valid, &gt; 0 should be small by the definition of the derivative. One can easily prove that the forward Euler method converges linearly w.r.t. , assuming f (h) is Lipschitz continuous on h and that the eigenvalues of the Jacobian of f have negative real parts.</p><p>Rearranging it, we have the forward Euler method for a given initial value</p><formula xml:id="formula_2">h t = h t?1 + f (h t?1 ), h 0 = h(0).<label>(3)</label></formula><p>Geometrically, each forward Euler step takes a small step along the tangential direction to the exact trajectory starting at h t?1 . As a result, is usually referred to as the step size.</p><p>As an example, consider the ODE h (t) = tanh (W h(t)) .</p><p>The forward Euler method approximates the solution to the ODE iteratively as</p><formula xml:id="formula_3">h t = h t?1 + tanh(W h t?1 ),</formula><p>which can be regarded as a recurrent network without input data. Here h t is the hidden state at the t-th step, W is a model parameter, and is a hyperparameter. This provides a general framework of designing recurrent network architectures by discretizing ODEs. As a result, we can design ODEs that possess desirable properties by exploiting the theoretical successes of dynamical systems, and the resulting recurrent networks will inherit these properties. Stability is one of the important properties to consider, which we will discuss in the next section. It is worth mentioning that the "skip connection" in this architecture resembles the residual RNN <ref type="bibr" target="#b48">(Yue et al., 2018)</ref> and the Fourier RNN <ref type="bibr" target="#b50">(Zhang et al., 2018b)</ref>, which are proposed to mitigate the vanishing and exploding gradient issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STABILITY OF ORDINARY DIFFERENTIAL EQUATIONS: ANTISYMMETRICRNNS</head><p>In numerical analysis, stability theory addresses the stability of solutions of ODEs under small perturbations of initial conditions. In this section, we are going to establish the connections between the stability of an ODE and the trainability of the RNNs by discretizing the ODE, and design a new RNN architecture that is stable and capable of capturing long-term dependencies.</p><p>An ODE solution is stable if the long-term behavior of the system does not depend significantly on the initial conditions. A formal definition is given as follows.</p><formula xml:id="formula_4">Definition 1. (Stability) A solution h(t) of the ODE in Equation 1 with initial condition h(0) is stable if for any &gt; 0, there exists a ? &gt; 0 such that any other solutionh(t) of the ODE with initial conditionh(0) satisfying |h(0) ?h(0)| ? ? also satisfies |h(t) ?h(t)| ? , for all t ? 0.</formula><p>In plain language, given a small perturbation of size ? of the initial state, the effect of the perturbation on the subsequent states is no bigger than . The eigenvalues of the Jacobian matrix play a central role in stability analysis. Let J (t) ? R n?n be the Jacobian matrix of f , and ? i (?) denotes the i-th eigenvalue.</p><formula xml:id="formula_5">Proposition 1. The solution of an ODE is stable if max i=1,2,...,n Re(? i (J (t))) ? 0, ?t ? 0,<label>(4)</label></formula><p>where Re(?) denotes the real part of a complex number.</p><p>A more precise proposition that involves the kinematic eigenvalues of J (t) is given in Ascher et al. <ref type="bibr">(1994)</ref>. Stability alone, however, does not suffice to capture long-term dependencies. As argued in <ref type="bibr" target="#b17">Haber &amp; Ruthotto (2017)</ref>, Re(? i (J (t))) 0 results in a lossy system; the energy or signal in the initial state is dissipated over time. Using such an ODE as the underlying dynamical system of a recurrent network will lead to catastrophic forgetting of the past inputs during the forward propagation. Ideally, Re(? i (J (t))) ? 0, ?i = 1, 2, . . . , n, (5) a condition we referred to as the critical criterion. Under this condition, the system preserves the long-term dependencies of the inputs while being stable.</p><p>Stability and Trainability. Here we connect the stability of the ODE to the trainability of the RNN produced by discretization. Inherently, the stability analysis studies the sensitivity of a solution, i.e., how much a solution of the ODE would change w.r.t. changes in the initial condition. Differentiating Equation 1 with respect to the initial state h(0) on both sides, we have the following sensitivity analysis (with chain rules):</p><p>d dt</p><formula xml:id="formula_6">?h(t) ?h(0) = J (t) ?h(t) ?h(0) .<label>(6)</label></formula><p>For notational simplicity, let us define A(t) = ?h(t)/?h(0), then we have</p><formula xml:id="formula_7">dA(t) dt = J (t)A(t), A(0) = I.<label>(7)</label></formula><p>Note that this is a linear ODE with solution A(t) = e J?t = P e ?(J)t P ?1 , assuming the Jacobian J does not vary or vary slowly over time (We will later show this is a valid assumption). Here ?(J ) denotes the eigenvalues of J , and the columns of P are the corresponding eigenvectors. See Appendix A for a more detailed derivation. In the language of RNNs, A(t) is the Jacobian of a hidden state h t with respect to the initial hidden state h 0 . When the critical criterion is met, i.e., Re(?(J )) ? 0, the magnitude of A(t) is approximately constant in time, thus no exploding or vanishing gradient problems.</p><p>With the connection established, we next design ODEs that satisfy the critical criterion. An antisymmetric matrix is a square matrix whose transpose equals its negative; i.e., a matrix M ? R n?n is antisymmetric if M T = ?M . An interesting property of an antisymmetric matrix M is that, the eigenvalues of M are all imaginary:</p><p>Re(? i (M )) = 0, ?i = 1, 2, . . . , n, making antisymmetric matrices a suitable building block of a stable recurrent architecture.</p><p>Consider the following ODE</p><formula xml:id="formula_8">h (t) = tanh (W h ? W T h )h(t) + V h x(t) + b h ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">h(t) ? R n , x(t) ? R m , W h ? R n?n , V h ? R n?m and b h ? R n . Note that W h ? W T h</formula><p>is an antisymmetric matrix. The Jacobian matrix of the right hand side is</p><formula xml:id="formula_10">J (t) = diag tanh (W h ? W T h )h(t) + V h x(t) + b (W h ? W T h ),<label>(9)</label></formula><p>whose eigenvalues are all imaginary, i.e., Re(? i (J (t))) = 0, ?i = 1, 2, . . . , n. In other words, it satisfies the critical criterion in Equation 5. See Appendix B for a proof. The entries of the diagonal matrix in Equation 9 are the derivatives of the activation function, which are bounded in [0, 1] for sigmoid and hyperbolic tangent. In other words, the Jacobian matrix J (t) changes smoothly over time. Furthermore, since the input and bias term only affect the bounded diagonal matrix, their effect on the stability of the ODE is insignificant compared with the antisymmetric matrix.</p><p>A naive forward Euler discretization of the ODE in Equation 8 leads to the following recurrent network we refer to as the AntisymmetricRNN.</p><formula xml:id="formula_11">h t = h t?1 + tanh (W h ? W T h )h t?1 + V h x t + b h ,<label>(10)</label></formula><p>where h t ? R n is the hidden state at time t; x t ? R m is the input at time t; W h ? R n?n , V h ? R n?m and b h ? R n are the parameters of the network; &gt; 0 is a hyperparameter that represents the step size.</p><p>Note that the antisymmetric matrix W h ? W T h only has n(n ? 1)/2 degrees of freedom. When implementing the model, W h can be parameterized as a strictly upper triangular matrix, i.e., an upper triangular matrix of which the diagonal entries are all zero. This makes the proposed model more parameter efficient than an unstructured RNN model of the same size of hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">STABILITY OF THE FORWARD EULER METHOD: DIFFUSION</head><p>Given a stable ODE, its forward Euler discretization can still be unstable, as illustrated in Section 4. The stability condition of the forward Euler method has been well studied and summarized in the following proposition. </p><formula xml:id="formula_12">if max i=1,2,...,n |1 + ? i (J t )| ? 1,<label>(11)</label></formula><p>where | ? | denote the absolute value or modulus of a complex number and J t is the Jacobian matrix evaluated at h t .</p><p>See Ascher &amp; Petzold (1998) for a proof. The ODE as defined in Equation 8 is however incompatible with the stability condition of the forward Euler method. Since ? i (J t ), the eigenvalues of the Jacobian matrix, are all imaginary, |1 + ? i (J t )| is always greater than 1, which makes the AntisymmetricRNN defined in Equation 10 unstable when solved using forward Euler.</p><p>One easy way to fix it is to add diffusion to the system by subtracting a small number ? &gt; 0 from the diagonal elements of the transition matrix. The model thus becomes</p><formula xml:id="formula_13">h t = h t?1 + tanh (W h ? W T h ? ?I)h t?1 + V h x t + b h ,<label>(12)</label></formula><p>where I is the identity matrix of size n and ? &gt; 0 is a hyperparameter that controls the strength of diffusion. By doing so, the eigenvalues of the Jacobian have slightly negative real parts. This modification improves the stability of the numerical method as demonstrated in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GATING MECHANISM</head><p>Gating is commonly employed in RNNs. Each gate is often modeled as a single layer network taking the previous hidden state h t?1 and data x t as inputs, followed by a sigmoid activation.</p><p>As an example, LSTM cells make use of three gates, a forget gate, an input gate, and an output gate. A systematic ablation study suggests that some of the gates are crucial to the performance of LSTM <ref type="bibr" target="#b24">(Jozefowicz et al., 2015)</ref>.</p><p>Gating can be incorporated into AntisymmetricRNN as well. However, it should be done carefully so that the critical condition in Equation 5 still holds. We propose the following modification to AntisymmetricRNN, which adds an input gate z t to control the flow of information into the hidden states:</p><formula xml:id="formula_14">z t = ? (W h ? W T h ? ?I)h t?1 + V z x t + b z , h t = h t?1 + z t ? tanh (W h ? W T h ? ?I)h t?1 + V h x t + b h ,<label>(13)</label></formula><p>where ? denotes the sigmoid function and ? denotes the Hadamard product.</p><p>The effect of z t resembles the input gate in LSTM and the update gate in GRU. By sharing the antisymmetric weight matrix, the number of model parameters only increases slightly, instead of being doubled. More importantly, the Jacobian matrix of this gated model has a similar form as that in <ref type="figure">Equation 9</ref>, that is, a diagonal matrix multiplied by an antisymmetric matrix (ignoring diffusion). As a result, the real parts of the eigenvalues of the Jacobian matrix are still close to zero, and the critical criterion remains satisfied.  Adopting the visualization technique used by <ref type="bibr" target="#b29">Laurent &amp; von Brecht (2017)</ref> and <ref type="bibr" target="#b17">Haber &amp; Ruthotto (2017)</ref>, we study the behavior of two-dimensional vanilla RNNs (left) and RNNs with feedback (right) in the absence of input data and bias:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SIMULATION</head><formula xml:id="formula_15">vanilla: h t = tanh(W h t?1 ), feedback: h t = h t?1 + tanh(W h t?1 ).</formula><p>Here h t ? R 2 and 1 ? t ? T . We arbitrarily choose three initial states: (0, 0.5), (?0.5, ?0.5) and (0.5, ?0.75), and apply the corresponding RNN recurrence. <ref type="figure" target="#fig_2">Figure 1</ref> plots the progression of the states h t of vanilla RNNs (first row) and RNNs with feedback (second row) parameterized by different transition matrices W . In each figure, more translucent points represent earlier time steps. The number of total time steps T = 50.</p><p>Figure 1(a) corresponds to a random weight matrix, where the entries are independent and standard Gaussian. The three initial points (shown in stars) converge to two fixed points on the boundary. Since the weight matrix is unstructured, the behavior is unpredictable as expected. More examples with different weight matrices are shown in Appendix D. <ref type="figure" target="#fig_2">Figure 1(b)</ref> shows the behavior of the identity weight matrix. Since tanh(?) is a contractive mapping, the origin is the unique fixed point. <ref type="figure" target="#fig_2">Figure 1</ref>(c) and (d) correspond to orthogonal weight matrices that represent reflection and rotation transforms respectively. A two-dimensional orthogonal matrix is either a reflection or a rotation transform; the determinant of the matrix is 1 or ?1 respectively. Even though the weight matrix is orthogonal, the states all converge to the origin because of the derivative of the activation function 0 ? tanh (W h t?1 ) ? 1.</p><p>In the case of RNNs with feedback, the trajectory of the hidden states is predictable based on the eigenvalues of the weight matrix W . We consider the following four weight matrices that correspond to <ref type="figure" target="#fig_2">Figure 1</ref>(e)-(f) respectively:</p><formula xml:id="formula_16">W + = 2 ?2 0 2 , W ? = ?2 2 0 ?2 , W 0 = 0 ?2 2 0 , W diff = ?0.15 ?2 2 ?0.15 .</formula><p>We also overlay the vector field that represents the underlying ODE. The step size is set to = 0.1.</p><p>For <ref type="figure" target="#fig_2">Figure 1</ref>(e) and (f), the eigenvalues are ? 1 (W + ) = ? 2 (W + ) = 2 and ? 1 (W ? ) = ? 2 (W ? ) = ?2. As a result, the hidden states are moving away from the origin and towards the origin respectively. <ref type="figure" target="#fig_2">Figure 1(g)</ref> corresponds to the antisymmetric weight matrix W 0 , whose eigenvalues are purely imaginary: ? 1 (W 0 ) = 2i, ? 2 (W 0 ) = ?2i. In this case, the vector field is circular; a state moves around the origin without exponentially increasing or decreasing its norm. However, on closer inspection, the trajectories are actually outward spirals. This is because, at each time step, the state moves along the tangential direction by a small step, which increases the distance from the origin and leads to numerical instability. It is the behavior characterized by Proposition 2. This issue can be mitigated by subtracting a small diffusion term ? from the diagonal elements of the weight matrix. We choose ? = 0.15 and the weight matrix W diff has eigenvalues of <ref type="figure" target="#fig_2">Figure 1(h)</ref> shows the effect of the diffusion terms. The vector field is slightly tilting toward the origin and the trajectory maintains a constant distance from the origin.</p><formula xml:id="formula_17">? 1 (W diff ) = ?0.15 + 2i, ? 2 (W diff ) = ?0.15 ? 2i.</formula><p>These simulations show that the hidden states of an AntisymmetricRNN <ref type="figure" target="#fig_2">(Figure 1(g)</ref> and (h)) have predictable dynamics. It achieves the desirable behavior, without the complication of maintaining an orthogonal or unitary matrix as in <ref type="figure" target="#fig_2">Figure 1(d)</ref>, which still suffers from vanishing gradients due to the contraction of the activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>The performance of the proposed antisymmetric networks is evaluated on four image classification tasks with long-range dependencies. The classification is done by feeding pixels of the images as a sequence to RNNs and sending the last hidden state h T of the RNNs into a fully-connected layer and a softmax function. We use the cross-entropy loss and SGD with momentum and Adagrad <ref type="bibr" target="#b13">(Duchi et al., 2011)</ref> as optimizers. More experimental details can be found in Appendix C. In this section, AntisymmetricRNN denotes the model with diffusion in Equation 12, and AntisymmetricRNN w/ gating represents the model in Equation 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PIXEL-BY-PIXEL MNIST</head><p>In the first task, we learn to classify the MNIST digits by pixels <ref type="bibr" target="#b31">(LeCun et al., 1998)</ref>. This task was proposed by <ref type="bibr" target="#b30">Le et al. (2015)</ref> and used as a benchmark for learning long term dependencies. MNIST images are grayscale with 28 ? 28 pixels. The 784 pixels are presented sequentially to the recurrent net, one pixel at a time in scanline order (starting at the top left corner of the image and ending at the bottom right corner). In other words, the input dimension m = 1 and number of time steps T = 784. The pixel-by-pixel MNIST task is to predict the digit of the MNIST image after seeing all 784 pixels. As a result, the network has to be able to learn the long-range dependencies in order to correctly classify the digit. To make the task even harder, the MNIST pixels are shuffled using a fixed random permutation. It creates non-local long-range dependencies among pixels in an image. This task is referred to as the permuted pixel-by-pixel MNIST.  <ref type="bibr" target="#b44">(Wisdom et al., 2016)</ref> 92.8% 92.1% 116 16k FC uRNN <ref type="bibr" target="#b44">(Wisdom et al., 2016)</ref> 96.9% 94.1% 512 270k Soft orthogonal <ref type="bibr" target="#b43">(Vorontsov et al., 2017)</ref> 94.1% 91.4% 128 18k KRU <ref type="bibr" target="#b23">(Jose et al., 2017)</ref> 96  <ref type="table">Table 1</ref> summarizes the performance of our methods and the existing methods. On both tasks, the proposed AntisymmetricRNNs outperform the regular LSTM model using only 1/7 of parameters. Imposing orthogonal weights <ref type="bibr" target="#b0">(Arjovsky et al., 2016)</ref> produces worse results, which corroborates with the existing study showing that such constraints restrict the capacity of the learned model. Softening the orthogonal weights constraints <ref type="bibr" target="#b44">(Wisdom et al., 2016;</ref><ref type="bibr" target="#b43">Vorontsov et al., 2017;</ref><ref type="bibr" target="#b23">Jose et al., 2017)</ref> leads to slightly improved performance. AntisymmetricRNNs outperform these methods by a large margin, without the computational overhead to enforce orthogonality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">PIXEL-BY-PIXEL CIFAR-10</head><p>To test our methods on a larger dataset, we conduct experiments on pixel-by-pixel CIFAR-10. The CIFAR-10 dataset contains 32 ? 32 colour images in 10 classes <ref type="bibr" target="#b28">(Krizhevsky &amp; Hinton, 2009</ref>). Similar to pixel-by-pixel MNIST, we feed the three channels of a pixel into the model at each time step. The input dimension m = 3 and number of time steps T = 1024.</p><p>The results are shown in <ref type="table">Table 2</ref>. The AntisymmetricRNN performance is on par with the LSTM, and AntisymmetricRNN with gating is slightly better than LSTM, both using only about half of the parameters of the LSTM model. Further investigation shows that the task is mostly dominated by short term dependencies. LSTM can achieve about 48.3% classification accuracy by only seeing the last 8 rows of CIFAR-10 images 2 .  <ref type="table">Table 2</ref>: Evaluation accuracy on pixel-by-pixel CIFAR-10 and noise padded CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NOISE PADDED CIFAR-10</head><p>To introduce more long-range dependencies to the pixel-by-pixel CIFAR-10 task, we define a more challenging task call the noise padded CIFAR-10, inspired by the noise padded experiments in <ref type="bibr" target="#b8">Chen et al. (2018a)</ref>. Instead of feeding in one pixel at one time, we input each row of a CIFAR-10 image at every time step. After the first 32 time steps, we input independent standard Gaussian noise for the remaining time steps. Since a CIFAR-10 image is of size 32 with three RGB channels, the input dimension is m = 96. The total number of time steps is set to T = 1000. In other words, only the first 32 time steps of input contain salient information, all remaining 968 time steps are merely random noise. For a model to correctly classify an input image, it has to remember the information from a long time ago. This task is conceptually more difficult than the pixel-by-pixel CIFAR-10, although the total amount of signal in the input sequence is the same. The results are shown in <ref type="table">Table 2</ref>. LSTM fails to train at all on this task while our proposed methods perform reasonably well with fewer parameters. To verify that AntisymmetricRNNs indeed mitigate the exploding/vanishing gradient issues, we conduct an additional set of experiments varying the length of noise padding so that the total time steps T ? {100, 200, 400, 800}. LSTMs and AntisymmetricRNNs with different diffusion constants ? ? {0.001, 0.01, 0.1, 1, 5, 10} are trained on these tasks. <ref type="figure" target="#fig_3">Figure 2</ref> visualizes the mean and standard deviation of the eigenvalues of the end-to-end Jacobian matrices for these networks. Unitary eigenvalues, i.e., mean close to 1 and standard deviation close to 0, indicate non-exploding and non-vanishing gradients. As shown in the figure, the eigenvalues for LSTMs quickly approaches zero as time steps increase, indicating vanishing gradients during back-propagation. This explains why LSTMs fail to train at all on this task. AntisymmetricRNNs with a broad range of diffusion constants ?, on the other hand, have eigenvalues centered around 1. It is worth noting though as the diffusion constant increases to large values, AntisymmetricRNNs run into vanishing gradients as well. The diffusion constant ? plays an important role in striking a balance between the stability of discretization and capturing long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">ABLATION STUDY</head><p>For the CIFAR-10 experiments, we have also conducted an ablation study to further demonstrate the effect of antisymmetric weight matrix. The ablation model in <ref type="table">Table 2</ref> refers to the model that replaces the antisymmetric weight matrices in Equation 13 with unstructured weight matrices. As shown in <ref type="table">Table 2</ref>, without the antisymmetric parametrization, the performance on both pixel-bypixel and noise padded CIFAR-10 is worse.</p><p>It is worth mentioning that the antisymmetric formulation is a sufficient condition of stability, not necessary. There are possibly other conditions that lead to stability as well as suggested in <ref type="bibr" target="#b8">Chen et al. (2018a)</ref>, which could explain the modest degradation in the ablation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present a new perspective on the trainability of RNNs from the dynamical system viewpoint. We draw connections between RNNs and the ordinary differential equation theory and design new recurrent architectures by discretizing ODEs. This new view opens up possibilities to exploit the computational and theoretical success from dynamical systems to understand and improve the trainability of RNNs. We also propose the AntisymmetricRNN, which is a discretization of ODEs that satisfy the critical criterion. Besides its appealing theoretical properties, our models have demonstrated competitive performance over strong recurrent baselines on a comprehensive set of benchmark tasks.</p><p>By establishing a link between recurrent networks and ordinary differential equations, we anticipate that this work will inspire future research in both communities. An important item of future work is to investigate other stable ODEs and numerical methods that lead to novel and well-conditioned recurrent architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A AN OVERVIEW OF STABILITY THEORY</head><p>In this section, we provide a brief overview of the stability theory by examples. Most of the materials are adapted from <ref type="bibr" target="#b1">Ascher &amp; Petzold (1998)</ref>.</p><p>Consider the simple scalar ODE, often referred to as the test equation: y (t) = ?y(t), where ? ? C is a constant. We allow ? to be complex because it later represents an eigenvalue of a system's matrix. The solution to this initial value problem is y(t) = e ?t y(0). If y(t) and?(t) are two solutions of the test equation, then their difference at any time t is |y(t) ??(t)| = |e ?t (y(0) ??(0))| = e Re(?)t |y(0) ??(0)|.</p><p>If Re(?) &gt; 0, then a small perturbation of the initial states would cause an exponentially exploding difference. If Re(?) &lt; 0, the system is stable, but the perturbation decays exponentially. The perturbation is preserved in the system only if Re(?) = 0.</p><p>We now consider the extension of the test equation to a matrix ODE y (t) = Ay(t). The solution is y(t) = e At y(0). To simplify the analysis, we assume A is diagonalizable, i.e., P ?1 AP = ?, where ? is a diagonal matrix of the eigenvalues of A and the columns of P are the corresponding eigenvectors. If we define w(t) = P ?1 y(t), then w (t) = ?w(t). The system for w(t) is decoupled: for each component w i (t) of w(t), we have a test equation w i (t) = ? i w i (t). Therefore, the stability for w(t), hence also for y(t), is determined by the eigenvalues ? i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOF OF A PROPOSITION</head><p>In this section, we provide a proof of a proposition which implies that the AntisymmetricRNN and AntisymmetricRNN with gating satisfy the critical criterion, i.e., the eigenvalues of the Jacobian matrix are imaginary. The proof is adapted from <ref type="bibr" target="#b6">Chang et al. (2018a)</ref>. Proposition 3. If W ? R n?n is an antisymmetric matrix and D ? R n?n is an invertible diagonal matrix, then the eigenvalues of DW are imaginary.</p><p>Re(? i (DW )) = 0, ?i = 1, 2, . . . , n.</p><p>Proof. Let ? and v be a pair of eigenvalue and eigenvector of DW , then</p><formula xml:id="formula_19">DW v = ?v, W v = ?D ?1 v, v * W v = ?(v * D ?1 v),</formula><p>On one hand, v * D ?1 v is real. On the other hand,</p><formula xml:id="formula_20">(v * W v) * = v * W * v = ?v * W v,</formula><p>where * represents conjugate transpose. It implies that v * W v is imaginary. Therefore, ? has to be imaginary. As a result, all eigenvalues of DW are imaginary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS</head><p>Let m be the input dimension and n be the number of hidden units. The input to hidden matrices are initialized to N (0, 1/m). The hidden to hidden matrices are initialized to N (0, ? 2 w /n), where ? w is chosen from ? w ? {0, 1, 2, 4, 8, 16}. The bias terms are initialized to zero, except the forget gate bias of LSTM is initialized to 1, as suggested by <ref type="bibr" target="#b24">Jozefowicz et al. (2015)</ref>. For AntisymmetricRNNs, the step size ? {0.01, 0.1, 1} and diffusion ? ? {0.001, 0.01, 0.1, 1.0}. We use SGD with momentum and Adagrad <ref type="bibr" target="#b13">(Duchi et al., 2011)</ref> as optimizers, with batch size of 128 and learning rate chosen from {0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1}. On MNIST and pixel-by-pixel CIFAR-10, all the models are trained for 50,000 iterations. On noise padded CIFAR-10, models are trained for 10,000 iterations. We use the standard train/test split of MNIST and CIFAR-10. The performance measure is the classification accuracy evaluated on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL VISUALIZATIONS</head><p>In this section, we present additional visualizations that are related to the simulation study in Section 4. <ref type="figure">Figure 3</ref> and 4 show the dynamics of vanilla RNNs and AntisymmetricRNNs with standard Gaussian random weights using different seeds. These visualizations further illustrate the random behavior of a vanilla RNN and the predictable dynamics of an AntisymmetricRNN. <ref type="figure" target="#fig_5">Figure 5</ref> shows the dynamics of AntisymmetricRNNs with independent standard Gaussian input. This shows that the dynamics become noisier compared to <ref type="figure" target="#fig_2">Figure 1</ref>, but the trend remains the same.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proposition 2. (Stability of the forward Euler method) The forward propagation in Equation 10 is stable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of the dynamics of RNNs and RNNs with feedback using different weight matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Mean and standard deviation of eigenvalues of the end-to-end Jacobian matrix in Antisym-metricRNNs with different diffusion constants and LSTMs, trained on the noise padded CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Visualization of the dynamics of vanilla RNNs with standard Gaussian random weights, using seeds from 1 to 16. Visualization of the dynamics of RNN with feedback with standard Gaussian random weights, using seeds from 1 to 16, diffusion strength ? = 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the dynamics of RNN with feedback with independent standard Gaussian input.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"><ref type="bibr" target="#b12">Cooijmans et al. (2017)</ref> also report the performance of the LSTM as a baseline: 98.9% on MNIST and 90.2% on pMNIST. We decide to use the LSTM baseline reported by<ref type="bibr" target="#b0">Arjovsky et al. (2016)</ref> because it has a higher accuracy on the more challenging pMNIST task than that in<ref type="bibr" target="#b12">Cooijmans et al. (2017)</ref> (92.6% vs 90.2%).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Last one row: 33.6%, last two rows: 35.6%, last four rows: 39.6%, last eight rows: 48.3%.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Computer methods for ordinary differential equations and differential-algebraic equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">R</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petzold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">61</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Numerical solution of boundary value problems for ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert D</forename><surname>Mattheij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">13</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reversible architectures for arbitrarily deep residual neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Holtham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-level residual networks from dynamical systems view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Begert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schoenholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07366</idno>
		<title level="m">Neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Capacity and trainability in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmine</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Aglar G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<title level="m">Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning unitary operators with help from u (n)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2050" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cijo</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustpaha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10142</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Kronecker recurrent units. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Preventing gradient explosions in gated recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekitoshi</forename><surname>Kanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotetsu</forename><surname>Iwamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A recurrent neural network without chaos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James Von Brecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxiao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3276" to="3285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7753</idno>
		<title level="m">Learning longer memory in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06422</idno>
		<title level="m">All you need is a good init</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem. CoRR, abs/1211</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5063</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page">533</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Can recurrent neural networks warp time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On orthogonality and learning recurrent networks with long term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3570" to="3578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recurrent recommender networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">How</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01827</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Residual recurrent neural networks for learning sequential representations. Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxuan</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stabilizing gradients for deep neural networks via efficient SVD parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5806" to="5814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via Fourier recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5815" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
