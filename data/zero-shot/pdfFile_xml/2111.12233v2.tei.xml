<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Up Vision-Language Pre-training for Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
							<email>xiaowei.hu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
							<email>zhengyang@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
							<email>zliu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
							<email>yumaolu@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
							<email>lijuanw@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename></persName>
						</author>
						<title level="a" type="main">Scaling Up Vision-Language Pre-training for Image Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, we have witnessed significant performance boost in the image captioning task based on visionlanguage pre-training (VLP). Scale is believed to be an important factor for this advance. However, most existing work only focuses on pre-training transformers with moderate sizes (e.g., 12 or 24 layers) on roughly 4 million images. In this paper, we present LEMON , a LargE-scale iMage captiONer, and provide the first empirical study on the scaling behavior of VLP for image captioning. We use the state-of-the-art VinVL model as our reference model, which consists of an image feature extractor and a transformer model, and scale the transformer both up and down, with model sizes ranging from 13 to 675 million parameters. In terms of data, we conduct experiments with up to 200 million image-text pairs which are automatically collected from web based on the alt attribute of the image (dubbed as ALT200M ). Extensive analysis helps to characterize the performance trend as the model size and the pre-training data size increase. We also compare different training recipes, especially for training on large-scale noisy data. As a result, LEMON achieves new state of the arts on several major image captioning benchmarks, including COCO Caption, nocaps, and Conceptual Captions. We also show LEMON can generate captions with long-tail visual concepts when used in a zero-shot manner.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in image captioning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40]</ref> can be largely attributed to vision-language pre-training (VLP) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, the current prevailing training paradigm for vision-language (VL) research. VLP <ref type="bibr" target="#b5">[6]</ref> is usually conducted on a combined image-text dataset comprising of several or tens of millions images in total, e.g., Visual Genome <ref type="bibr" target="#b22">[23]</ref>, SBU <ref type="bibr" target="#b35">[36]</ref> and Conceptual Captions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>. While previous studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref>    <ref type="table" target="#tab_1">Table 2</ref>. Increasing the model size is not significantly beneficial at small pre-training dataset scales. However, when we use sufficiently large datasets, we see strong performance boost from a larger model. training dataset would impact the performance, and how it correlates with different model settings. Along the journey of pushing the limit of VLP, it becomes increasingly important to answer this question.</p><p>Scale is believed to be an important ingredient in attaining excellent performance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>. Recent work has investigated the Pareto frontier of training transformer models, often referred to as the neural scaling law, in the domains of natural language processing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b45">46]</ref> and computer vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b54">55]</ref>, via unsupervised or weakly-supervised learning methods. These studies have observed consistent benefits of increasing the model size to billions of parameters, given billion magnitude of pre-training data available.</p><p>More recently, contrastive image-text pre-training <ref type="bibr">[20,</ref>  The models are first pre-trained, then finetuned on COCO caption training split. Note that x-axis is plotted in a logarithmic scale. <ref type="bibr" target="#b37">38</ref>] has also been scaled up to 400 million and 1.8 billion data sizes for image representation learning and image-text retrieval. Both CLIP <ref type="bibr" target="#b37">[38]</ref> and ALIGN <ref type="bibr" target="#b19">[20]</ref> employ two individual networks to encode the image and the text separately for alignment, which well fits the image-text retrieval task, but little is known about the scaling properties when it comes to image captioning.</p><p>To study the characteristics of this scaling trend on the captioning task, we first construct a large-scale image-text dataset (dubbled as ALT200M), consisting of up to 200 million image-text pairs from web based on the alt attribute of the images. Then, we conduct extensive experiments to scale VLP for image captioning from both the data and model perspectives, and name our model as LEMON , short for a LargE-scale iMage captiONer. To simulate the process of data scaling, we create multiple subsets of ALT200M, ranging from 3 to 200 million. In terms of model, we use the state-of-the-art image captioning model VinVL <ref type="bibr" target="#b55">[56]</ref> as our reference model, composed of an image feature extractor and a transformer model. We adapt the pre-training task to be consistent with the captioning task, and then scale the width and depth of the transformer model with the number of parameters ranging from 13 (i.e., tiny) to 675 (i.e., huge) millions. Combining different models and pre-training data sizes, we summarize our results in Figure 1 and 2, which characterize the linear-logarithmic scaling trend. Larger models tend to benefit more when we have more than 10 million data for pre-training. However, with only 3 million data, the performance starts to saturate early as the model size increases. Moreover, we also investigate other design choices of VLP, e.g., model architectures and training objectives.</p><p>Our contributions are summarized as follows.</p><p>? We present the VLP scaling rule for image captioning. Not only does this prove the effectiveness of learning from large-scale noisy data, but it also sheds lights on how performance can be efficiently improved by increasing the model and pre-training data sizes together to avoid a saturation plateau.</p><p>? We achieve new state-of-the-art results for image captioning across several major benchmarks, including COCO Caption, nocaps, and Conceptual Captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vision-Language Pre-training. Since the birth of ViL-BERT <ref type="bibr" target="#b33">[34]</ref> and LXMERT <ref type="bibr" target="#b44">[45]</ref>, we have witnessed a boom of methods for vision-language pre-training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref>. Prominent examples include UNITER <ref type="bibr" target="#b5">[6]</ref>, VL-BERT <ref type="bibr" target="#b41">[42]</ref>, OSCAR <ref type="bibr" target="#b32">[33]</ref>, UNIMO <ref type="bibr" target="#b31">[32]</ref>, and VinVL <ref type="bibr" target="#b55">[56]</ref>. Along the journey of VLP, researchers have investigated different training strategies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35]</ref>, robustness <ref type="bibr" target="#b28">[29]</ref>, compression <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b46">47]</ref>, probing analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, and the extension to video-text modeling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref>. More recently, instead of using object detectors for image feature extraction, end-to-end VLP based on convolution networks and transformers are becoming popular <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49]</ref>. However, as another important factor in achieving superior performance, the scaling behavior of VLP is less studied. While most works pre-train transformer of base/large sizes on no more than 4M images, we train models from tiny to huge, on up to 200M images. CLIP <ref type="bibr" target="#b37">[38]</ref> and ALIGN <ref type="bibr" target="#b19">[20]</ref> scaled up contrastive pre-training to 400M and 1.8B images, and SimVLM <ref type="bibr" target="#b47">[48]</ref>   <ref type="table">Table 1</ref>. Statistics of existing and our collected datasets. The number of images in CC3M and CC12M are calculated for valid RGB images at the time we downloaded them. The unigrams are counted and sorted by occurrences from large to small to form a distribution curve for each dataset. Our dataset features much more long-tail concepts, as indicated by the number of unigrams included in the 0.1% distribution tail. The datasets used in CLIP <ref type="bibr" target="#b37">[38]</ref> and ALIGN <ref type="bibr" target="#b19">[20]</ref> are not included, since we do not know the corresponding statistics.</p><p>its scaling behavior w.r.t. pre-training data sizes. Compared with them, we focus on image captioning, provide a more comprehensive study on the scaling behavior via altering data and model sizes, and show that by using 200M images, we can outperform SimVLM on image captioning.</p><p>Scaling Law. With the success of large-scale pre-trained models in both the language and vision domains, there has been a surging research interest in discovering the empirical scaling law of these models. <ref type="bibr" target="#b20">[21]</ref> presented that the language model performance scales as power-law across many orders of magnitude with dataset size, model size, and computation used in training. <ref type="bibr" target="#b12">[13]</ref> further studied the scaling of autoregressive generative modeling. Aside from the model size, <ref type="bibr" target="#b45">[46]</ref> showed that the model shape also matters for efficient transfer from upstream pre-training to downstream finetuning. In the vision domain, <ref type="bibr" target="#b54">[55]</ref> scaled a series of vision transformer models evaluated on image classification tasks. While the scaling protocols have been investigated for many NLP and vision tasks, we are the first to study the scaling behavior of VLP for image captioning, and push multimodal transformer pre-training to a much larger scale.</p><p>In Appendix, we also provide a detailed related work review on non-pretraining-based image captioning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present the pre-training dataset in Section 3.1, the model structure in Section 3.2, and training objective in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pre-training Dataset</head><p>We construct a data collection pipeline to crawl the images from the Internet and the associated alt attribute, which usually provides the description of the image content. In order to scale up easily, we follow the natural distribution of images without re-balancing, and apply only minimal rulebased filtering. We keep images with the longer side more than 200 pixels and aspect ratio smaller than 3. As some alttexts are too long, we split them up by punctuation marks, such as period and exclamation mark, and select the longest part. To filter out some rare or misspelled words, we build a vocabulary of unigrams with English Wikipedia titles and body text. We remove unigrams that are present less than 5 times, resulting in approximately 250 million unique unigrams. We remove the alt-text if any of its unigrams cannot be found in the vocabulary. Afterwards, we count the frequency of all the remaining sentences, and filter out some boilerplate sentences that are too generic, e.g., stock image, 3D illustration, vector photo. For the sake of privacy, we use a Named Entity Recognition model spaCy 1 to identify person and location names, and replace them with special tokens ?PERSON?, ?LOC?, respectively. At last, we perform duplication check on all the collected images to ensure that they do not overlap with existing test sets, such as COCO, nocaps, and Conceptual Captions.</p><p>The final dataset, named as ALT200M, contains more than 200 million images, each corresponding to one alt-text. The word cloud of 200 most frequent words is visualized in <ref type="figure" target="#fig_3">Figure 3</ref>. As shown in <ref type="table">Table 1</ref>, compared to CC12M, ALT200M has nearly 16? more images. The vocabulary is almost doubled. We observe that 56% of unigrams sum up to only 0.1% of total occurrences, characterizing an extremely long tail of rarely occurring unigrams. The average length of the captions is 13.01, more than that of the COCO caption dataset <ref type="bibr">(10.44)</ref>. We also observe that our dataset contains much more shorter captions with only 2 or 3 unigrams. This indicates a shift in the distribution of captions from pre-training to finetuning.</p><p>Besides CC12M, there also exist some other large- scale image-text datasets, such as WIT <ref type="bibr" target="#b40">[41]</ref>, WenLan <ref type="bibr" target="#b18">[19]</ref>, LAION-400M <ref type="bibr" target="#b38">[39]</ref>, and the datasets used in CLIP <ref type="bibr" target="#b37">[38]</ref> and ALIGN <ref type="bibr" target="#b19">[20]</ref>. More detailed discussions on them are provided in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VLP Model for Captioning</head><p>We use the pre-trained Faster R-CNN detector from [56] to extract image region features, which are concatenated with scaled bounding boxes as position encoding. Following <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b55">56]</ref>, we also add the detected object tags as input. The text input, including the caption and objects tags, are tokenized by WordPiece, with a vocabulary of 30522 tokens. A multi-layer transformer model is used for multimodal fusion, which consists of a stack of encoder layers, each of which has a multi-head self-attention (MSA) layer followed by a feed-forward layer. To enable text generation with the encoder layers, we use the sequence-tosequence attention mask <ref type="bibr" target="#b57">[58]</ref> in each self-attention layer for the captioning module. Specifically, the input consists</p><formula xml:id="formula_0">of image embeddings V = {v i } N i=1 , object tag embed- dings T = {t j } M j=1</formula><p>, and token embeddings for the caption W = {w k } L k=1 , where N, M, L are the number of image regions, tags, and caption tokens, respectively. The corresponding outputs are:</p><formula xml:id="formula_1">R vi := MSA(v i , V ? T) ,<label>(1)</label></formula><formula xml:id="formula_2">R tj := MSA(t j , V ? T) ,<label>(2)</label></formula><formula xml:id="formula_3">R w k := MSA(w k , V ? T ? {w l } k l=1 ) ,<label>(3)</label></formula><p>where MSA(x, Y) is the MSA layer with x mapped to query, and Y mapped to key/value. ? means concatenation of matrices, and the index of R vi denotes the position corresponding to v i . The output representation is fed into the next layer, or used for prediction at the end. In this way, during inference, the model can decode the token from left to right in an auto-regressive manner. To study the scaling trend, we experiment with 8 model configurations, ranging from "tiny" of 13M parameters to "huge" of 674M parameters, detailed in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Objective</head><p>While bidirectional Masked Language Modeling (MLM) has been widely used in both language and vision-language pre-training, its bidirectional nature makes it sub-optimal for text generation. In contrast to VLP works that are mostly evaluated on VL understanding tasks, we use sequenceto-sequence MLM for generation tasks. During training, we randomly mask out 15% of caption tokens following BERT <ref type="bibr" target="#b8">[9]</ref> to form a "corrupted" caption W = {w k } L k=1 , wherew k is either equal to w k , or replaced with [MASK] token or another token sampled from vocabulary. The training loss is defined as:</p><formula xml:id="formula_4">L(W, V, T) = k?D CE(w k , Rw k ) (4) = k?D (? log p(w k |V, T, {w l } k l=1 )) ,</formula><p>where CE(?, ?) is the cross-entropy loss with softmax, D is the subset of masked positions. avoid duplicate computation, thereby making the generation efficient. We also experimented with other model structures and training objectives, such as predicting the next token with language modeling, as shown in <ref type="figure" target="#fig_4">Figure 4</ref> and will be detailed later in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first present our experimental setup in Section 4.1, and then detail our results in Section 4.2, followed by comprehensive analysis in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. To measure the progress brought about by large-scale pre-training, we aim to evaluate the model's capability of describing varieties of (long-tail) visual concepts, which is essential for captioning in the wild. For this purpose, we choose nocaps <ref type="bibr" target="#b0">[1]</ref> as the evaluation benchmark, which is developed to evaluate object captioning at scale. The dataset consists of 15100 images from Open Images, and covers more than 600 object categories, of which nearly 400 of them are unseen from the training set in COCO <ref type="bibr" target="#b4">[5]</ref>. Based on whether the image contains novel objects unseen in the COCO training set, the nocaps images are divided into three domains: "in", "near", and "out". None of the objects in the out-domain are seen in COCO. This discrepancy raises the importance of learning from external resources for recognizing novel objects, rather than relying on the clean and fully annotated captioning training data. As the external training resources may vary for different methods, in <ref type="table" target="#tab_2">Table 3</ref>, we only compare our model with other models that also use extra image-caption pairs, and take the pre-training dataset size into account.</p><p>Implementation details. To study the scaling trend, we experiment with 8 model configurations and 5 pre-training data sizes. We train all the models from scratch if not otherwise specified. In the pre-training, we do not include COCO or Visual Genome data, to exclude the possible impact of data quality when plotting the scaling trend, as these datasets are manually annotated instead of web collected. To create pre-training dataset of different sizes, we randomly sample from ALT200M at different data scales. Note that the larger dataset is a superset of the smaller ones.</p><p>We use AdamW optimizer with linearly decaying learning rate. During pre-training, the batch size is 8192. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Captioning Results</head><p>Results on nocaps validation and test sets are shown in <ref type="table" target="#tab_2">Table 3</ref>. By leveraging large-scale pre-training on the automatically collected alt-texts, LEMON has achieved remarkable improvement, especially for out-of-domain images. Compared to the baseline trained on COCO only (row 8), after pre-training on ALT200M (row 12), the CIDEr score is improved by 16.3 for the in-domain part, and 45.3 for the out-of-domain part. This evidences that large-scale pre-training improves the model's ability to recognize a wide range of long-tailed visual objects. We also present results of models pre-trained on CC3M and CC12M. Compared to the best reported results on these datasets (row 1, 2), our evaluated CIDEr scores (row 9, 10) are increased by 18.4 and 13.0, respectively. This demonstrates the performance improvement in our captioning results brought about by the proposed training scheme when the pre-training dataset is the same. On the leaderboard 2 test set, our large and huge models (row 19, 20) both surpassed the top-ranking model (row 18) that is pre-trained on 1.8B image-text pairs, creating the new state-of-the-art of 114.3 in CIDEr. We also achieve the state of the art on other image captioning benchmarks, including COCO Caption and Conceptual Captions, as summarized in <ref type="table" target="#tab_3">Table 4</ref>   <ref type="table">Table 5</ref>. Results on the Conceptual Captions (CC3M) dev set.</p><p>All the models are finetuned on CC3M with cross-entropy loss only. We compare with the best results reported on the dev set with and without pre-training. PT: pre-training.</p><p>Large-scale pre-training not only benefits VL representation learning, but also equips the model with the capability to zero-shot generalization. We use the pre-trained model to generate captions directly without further finetuning. The prefix "a picture of" is added as prompt to improve the quality of generated captions. Some examples are illustrated in <ref type="figure">Figure 5</ref>. The pre-trained model demonstrates strong ability in recognizing various long-tail visual concepts. Compared to the model trained only on small clean set, it shows the knowledge of many fine-grained categories (e.g., "metal instrument" vs. "tuba"), which are learned from the largescale noisy supervision of alt-texts from web. We also notice that our pre-trained model tends to generate very short descriptions when used in a zero-shot manner, but this is mitigated after finetuning on COCO. We posit that the reason for this is the relatively large proportion of short alttexts in our pre-training datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation and Analysis</head><p>Scaling law: influence of data and model sizes. We conduct comprehensive experiments to understand how much gain can be obtained in the downstream tasks by scaling up pre-training. <ref type="figure" target="#fig_2">Figure 2</ref> shows the relationship between the number of images used in pre-training and the CIDEr scores evaluated in the downstream captioning tasks. All  <ref type="figure">Figure 5</ref>. Examples of generated captions on nocaps validation set. B: the baseline model trained on COCO caption only without pre-training. F: the model finetuned on COCO after pre-training on ALT200M. Z: the pre-trained model without finetuning, where we add the prefix "a picture of" during inference as the prompt to improve the quality of zero-shot generation following <ref type="bibr" target="#b47">[48]</ref>. the models are pre-trained from scratch, and then finetuned on COCO. While all the models can be improved after pretraining with more data, the improvement is obviously less significant for the smaller models than for the larger models. On COCO, the gap between "small" and "large" models is negligible at 3M scale, but it becomes large as the data size increases. Moreover, when evaluating on nocaps, the gap in out-of-domain set is consistently larger than that in in-domain. This implies the advantage of large models in transferring knowledge from pre-training to downstream tasks, especially when the finetuning data are too limited to cover all test scenarios.</p><p>Besides, we observe that the model capacity becomes the performance bottleneck as the amount of available data increases. <ref type="figure" target="#fig_1">Figure 1</ref> plots the scaling trend w.r.t. the number of model parameters. When pre-training with 3M data, the "base" size appears to be sufficient, and there is no significant benefit to using larger models. However, with more than 40M data, the larger models start to outperform the smaller ones by a significant margin. When the data magnitude reaches hundreds of millions, and if the observed trend from "base" to "huge" can be kept, there is promise in training an even larger model to push the limits of VLP for captioning tasks.</p><p>At last, to have a better understanding of the data quality, we perform pre-training with the same settings on CC12M and the 12M subset of ALT200M. With the only difference in pre-training data source, the models yield fairly similar results (0.1 to 0.3 differences in CIDEr) on COCO and nocaps. This indicates that our data quality is comparable to that of CC12M. The observed performance improvement should be attributed to the pre-training scale. Sample efficiency. We examine the improvement of learned representations along with the progress of pretraining. Progress is measured quantitatively by the number of image-text paired samples seen in pre-training, i.e., the effective batch size multiplied by the training steps. In <ref type="figure" target="#fig_6">Figure 6</ref>, we report the results on COCO Caption after finetuning intermediate pre-trained checkpoints. We also evaluate the finetuned models on nocaps, indicating the ability of generalization under domain shift. We present two models in the figure, one with "base" size, the other with "huge" size. Both models are pre-trained on ALT200M. We observe that both models continue to improve after seeing more samples in pre-training, but the larger model learns much "faster". To achieve similar results in the downstream COCO captioning task, the base model must see more than 2 to 8 times more samples in pretraining. This factor is even greater when evaluating on the nocaps out-of-domain images. The result of the "base" model seeing 19 billion samples is still slightly worse than that of the "huge" model seeing 0.8 billion samples. This demonstrates the efficiency of large models in learning from large-scale data, as well as the robustness in generalization.</p><p>Further ablation. We compare with other common model structures and training objectives, such as the encoder-decoder transformer model and unidirectional language modeling (LM). Experiments are conducted with models of "base" size as specified in <ref type="table" target="#tab_1">Table 2</ref>. For the encoder-decoder structure, we use 6 encoder layers (with self-attention) followed by 6 decoder layers (with crossattention after self-attention), while other model configurations remain unchanged. The training objectives are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. For each experiment setting, we sweep the hyperparameters, e.g., pre-training epochs from 40 to 200, finetuning epochs from 10 to 60, and learning rates from 1 ? 10 ?6 to 3 ? 10 ?5 . The results of the best hyperparameters are reported.</p><p>We train the models under 4 different settings on COCO and CC3M, respectively. Results are summarized in <ref type="table" target="#tab_5">Table 6</ref>. On COCO, the differences among the 4 settings are small (1.41% relative change in CIDEr), with the worst being 119.2 from encoder+LM, and the best being 120.9 from encoder-decoder+LM. In contrast, on CC3M, the difference is much larger (9.10% relative change in CIDEr). The worst is 94.9 from encoder-decoder+LM, while the best is 104.4 from encoder+MLM. As CC3M is collected over the Internet and contains much more noise, we assume that the model that tends to overfit is prone to error when data quality is low, even though it performs well given wellannotated data.</p><p>Moreover, to compare training objectives, we first pretrain the models on CC12M, using s2s-MLM or LM, then finetune the intermediate checkpoints on COCO. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, we observe that although the model trained with LM converges faster at the beginning, it enters saturation early, and does not achieve scores as high as the model using s2s-MLM. We also find that training with LM is very sensitive to learning rates. Given the above results, we choose the s2s-MLM model and the encoder structure to scale up with the noisy pre-training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we study the scaling behavior of VLP models for image captioning, and construct our own largescale dataset ALT200M. Our experiments show that scaling up pre-training leads to remarkable improvement for the downstream captioning tasks. Our model LEMON has achieved new state-of-the-arts on multiple benchmarks, including COCO Caption, nocaps, and Conceptual Captions. LEMON also has impressive capability of recognizing a wide range of long-tail visual objects, even in the zero-shot manner. Moreover, our study on large transformer models indicates that with orders of magnitude larger training data available, the model capacity tends to be the bottleneck. It is a promising direction to train a substantially larger model to take more advantage from the large amounts of alt-text data widely circulated on the Internet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work on Image Captioning</head><p>There is a rich literature on image captioning studying different model structures and learning approaches. Recent works have proposed enhanced attention-based models to improve the performance, such as ORT <ref type="bibr" target="#b13">[14]</ref>, AoANet <ref type="bibr" target="#b15">[16]</ref>, M 2 Transformer <ref type="bibr" target="#b7">[8]</ref>, X-LAN <ref type="bibr" target="#b36">[37]</ref>, and RSTNet <ref type="bibr" target="#b56">[57]</ref>. Besides, researchers have explored to leverage semantic attributes <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b52">53]</ref>, scene graphs <ref type="bibr" target="#b49">[50]</ref>, and graph convolutional networks <ref type="bibr" target="#b51">[52]</ref> for captioning. While those methods focus on learning from well-annotated captions in relatively small datasets, such as COCO Caption, we investigate the impact of data scales with a much larger and noisier dataset. This allows the model to learn more diverse visual concepts, and makes one step further towards in-the-wild captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison of Image-Text Datasets</head><p>Besides CC3M <ref type="bibr" target="#b39">[40]</ref> and CC12M <ref type="bibr" target="#b3">[4]</ref>, we also compare with other web-crawled image-text datasets as described in the following.</p><p>? The dataset used in CLIP <ref type="bibr" target="#b37">[38]</ref> has 400 million imagetext pairs. Unlike our dataset crawled from web without re-balancing, their dataset is built upon a set of 500, 000 queries. The queries include all the words occurring at least 100 times in the English version of Wikipedia and are augmented with bi-grams. The image-text pairs are searched such that the text includes one of the queries.</p><p>The final results are also balanced to include up to 20, 000 image-text pairs per query.</p><p>? The dataset used in ALIGN [20] has 1.8 billion imagetext pairs. A later work SimVLM <ref type="bibr" target="#b47">[48]</ref> also uses this dataset. The data collection pipeline is similar to that used in Conceptual Captions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, but relaxes most cleaning steps. Only some rule-based filters are applied, such as image size, alt-text frequencies, and rare words.</p><p>? The Wikipedia-based Image Text Dataset (WIT) <ref type="bibr" target="#b40">[41]</ref> is composed of 11.5 million unique images and 37.6 million texts. Different from all other listed datasets, it features multilingual texts across 108 languages. The images and texts are collected from the Wikipedia content pages. It provides texts from multiple sources, such as reference, attribution and alt-texts, and texts in different languages for the same image.</p><p>? WenLan <ref type="bibr" target="#b18">[19]</ref> has 30 million image-text pairs. The webcollected pairs have gone through an elaborate cleaning process. For each data source, they also use topic models to extract topic words, and analyze the topic distribution to help with selecting desired contents.</p><p>? LAION-400M <ref type="bibr" target="#b38">[39]</ref> has 400 million image-text pairs, and is recently released to public. Instead of applying human   <ref type="bibr" target="#b37">[38]</ref> itself is also trained on noisy image-text pairs, it is yet unknown about the quality of this dataset compared to others cleaned by heuristic-based pipelines.</p><p>We summarize the characteristics of existing image-text datasets from three perspectives:</p><p>? Accessibility: The datasets CC3M <ref type="bibr" target="#b39">[40]</ref>, CC12M <ref type="bibr" target="#b3">[4]</ref>, WIT <ref type="bibr" target="#b40">[41]</ref> and LAION-400M <ref type="bibr" target="#b38">[39]</ref> are released to public with the image URL and associated meta files. Other datasets are proprietary.</p><p>? Scale: Except LAION-400M <ref type="bibr" target="#b38">[39]</ref>, the other released datasets have tens of millions of images, which are not enough for the scaling purpose.</p><p>? Collection pipeline: LAION-400M <ref type="bibr" target="#b38">[39]</ref> is mainly filtered by the CLIP <ref type="bibr" target="#b37">[38]</ref> model, while all the other datasets are filtered by a series of rules including expert designed heuristics and/or complicated models. As to the data sources, WIT <ref type="bibr" target="#b40">[41]</ref> is collected from Wikipedia. The other datasets are crawled from Internet.</p><p>In a nutshell, we construct ALT200M to study the scaling behavior, as no such large-scale datasets are available (LAION-400M <ref type="bibr" target="#b38">[39]</ref> is concurrent with ours). Following prior works, we crawl images and the alt attributes from Internet, and apply minimal rule-based filters to retain as many images as possible. Our experiments show that the web-collected data can help to substantially improve the performance of captioning models.  <ref type="figure" target="#fig_1">Figure 1</ref>. The x-axis plots the number of parameters for each model size (e.g., tiny, small, huge) in a logarithmic scale. The definition of model sizes is detailed in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detailed Hyperparameters</head><p>We include a table of important hyperparameters in <ref type="table" target="#tab_7">Table 7</ref> for all model sizes as defined in <ref type="table" target="#tab_1">Table 2</ref>. All the models are pre-trained for 60 epochs on subsets of ALT200M, and finetuned for 40 epochs on COCO with batch size 512. During pre-training, the learning rate is warmed up for the first 2% steps to the peak value, then linearly decaying to 0. During finetuning, the learning rate linearly decays from the initial value to 0 without warm up. We use AdamW optimizer with weight decay 0.05. The cross-entropy loss is calculated with label smoothing 0.1.</p><p>The evaluation results on COCO "Karpathy" test split and nocaps validation set are plotted in <ref type="figure" target="#fig_1">Figure 1</ref>, 2, and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Qualitative Examples</head><p>In this section, we provide more qualitative examples of our web-crawled dataset ALT200M and the captioning results of our LEMON model. <ref type="figure" target="#fig_11">Figure 9</ref> shows some examples of the image-text pairs in ALT200M. While some of the alt attributes are descriptive sentences that can serve as good training targets, e.g., <ref type="figure" target="#fig_11">Figure 9</ref> (7), (8), <ref type="bibr" target="#b8">(9)</ref>, it is noted that some texts are not semantically well formed, e.g., <ref type="figure" target="#fig_11">Figure 9</ref> (10). Some texts are very short phrases containing only 2 to 4 words, e.g., <ref type="figure" target="#fig_11">Figure 9</ref> (1) - <ref type="bibr" target="#b5">(6)</ref>. We also observe that some texts do not precisely describe what content is shown in the image, but mention external knowledge or information. For example, <ref type="figure" target="#fig_11">Figure 9</ref> (12) shows a woman pointing at the camera, but the text is "I got you". And the text of <ref type="figure" target="#fig_11">Figure 9</ref> (11) are likely to be extracted from news. These might put challenges for the model to learn from noisy text supervision. However, there are indeed a large variety of (fine-grained) visual objects present in the images and texts, such as burdock leaf, karate, mahjong, and great blue heron. Compared to human-annotated datasets, these web-collected data provide much richer training resources, especially for the longtailed concepts.</p><p>After training on the ALT200M dataset, our LEMON model has achieved impressive results, even in zero-shot manner. We present more examples of generated captions in <ref type="figure" target="#fig_1">Figure 10</ref>, in addition to <ref type="figure">Figure 5</ref> in the main paper. Compared to the baseline model trained on COCO only, the LEMON model can recognize much more fine-grained objects, as highlighted in red.</p><p>(1) Massive pylon.   . ALT200M examples of images and the associated alt attributes. The image-text pairs cover rich visual concepts, but some texts are not well formed (e.g., <ref type="figure" target="#fig_1">Figure (10)</ref>), or do not directly reflect the image content (e.g., <ref type="figure" target="#fig_1">Figure (11) -(14)</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>have analyzed various choices of pre-training objectives and model architectures, it remains unclear to what extent the pre-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Image captioning performance on COCO when upscaling model for each dataset size. The x-axis plots the number of parameters for each model size (e.g., tiny, small, huge) in a logarithmic scale. The definition of model sizes is detailed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>(a) finetuned and evaluated on COCO (b) finetuned on COCO, evaluated on nocaps Image captioning performance in data upscaling for each model size. The x-axis shows the number of image-text pairs used in pre-training. The y-axis shows the evaluation score (CIDEr) on COCO "Karpathy" test split and nocaps validation set, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Word cloud of the top 200 words in our pre-training dataset ALT200M, excluding the stop words, e.g., a, the, of, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of training objectives. (Top) Language Modeling (LM), to predict the next token at each position. (Bottom) Masked Language Modeling (MLM), to predict the masked and/or possibly polluted tokens at the masked positions. Both use causal masking for model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>B</head><label></label><figDesc>: a woman holding a pink umbrella over her head. F: a woman in a kimono holding a purple umbrella. Z: a picture of a geisha B: a close up of a tree branch F: a close up of a dinosaur skull with a black background Z: a picture of a dinosaur skeleton B: a picture of a cat with a red tail. F: a black and white image of a killer whale. Z: a picture of a killer whale B: a man wearing a hat and a straw hat standing in front of a large metal instrument. F: a man in a green hat playing a tuba. Z: a picture of a bavarian musician playing a tuba B: a collection of wooden tools on a white background. F: a collection of swords on display in a museum. Z: a picture of ancient swords</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>(a) pre-training accuracy (b) finetuned/evaluated on COCO (c) finetuned on COCO, evaluated on nocaps Comparison of sample efficiency for different model sizes. Figure (a) shows the learning curve in pre-training, measured by the accuracy of cross-entropy loss for masked token prediction. Figures (b) and (c) show the results of finetuned intermediate checkpoints, evaluated on COCO "Karpathy" test set and nocaps validation set, respectively. The larger model can consistently achieve better results in downstream tasks with far fewer pre-training epochs, especially for out-of-domain data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Comparison of different training objectives by pretraining on CC12M and finetuning on COCO. The models are finetuned from intermediate checkpoints using the same objective as used in pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Image captioning performance on nocaps when upscaling model for each dataset size. The results of the same models evaluated on COCO are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>of a great blue heron standing on a tree branch with one leg tucked up inside. A great blue heron stands on a tree bough on one leg against a the first COVID-19 positive case was detected, the residents of a number of locations in &lt;LOC&gt; town sealed their areas. After the first COVID-19 Novel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9</head><label>9</label><figDesc>Figure 9. ALT200M examples of images and the associated alt attributes. The image-text pairs cover rich visual concepts, but some texts are not well formed (e.g., Figure (10)), or do not directly reflect the image content (e.g., Figure (11) -(14)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>further use 1.8B images for prefix language modeling pre-training. However, CLIP and ALIGN focus on image-text retrieval, while SimVLM did not study</figDesc><table><row><cell>Dataset</cell><cell cols="2">#images (M) #cap./image</cell><cell cols="4">Unigram #unique #unique in 0.1% tail mean ? std Caption lengths P5%/50%/95%</cell></row><row><cell>COCO Caption [5]</cell><cell>0.1</cell><cell>5</cell><cell>19, 264</cell><cell>1, 184</cell><cell>10.44 ? 2.24</cell><cell>8/10/14</cell></row><row><cell>CC3M [40]</cell><cell>3.1</cell><cell>1</cell><cell>49, 638</cell><cell>22, 677</cell><cell>10.25 ? 4.64</cell><cell>5/9/19</cell></row><row><cell>CC12M [4]</cell><cell>12.2</cell><cell>1</cell><cell>1, 319, 284</cell><cell>193, 368</cell><cell>17.17 ? 12.76</cell><cell>6/13/43</cell></row><row><cell>ALT200M (Ours)</cell><cell>203.4</cell><cell>1</cell><cell>2, 067, 401</cell><cell>1, 167, 304</cell><cell>13.01 ? 8.85</cell><cell>2/11/27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Details of model architecture. FLOPs are calculated via taking 50 image region features and 35 text tokens as input in one forward pass. The dimension of image region feature is 2054, which is mapped to the transformer width via a linear layer.</figDesc><table><row><cell>Model</cell><cell>Layers</cell><cell>Width</cell><cell>MLP</cell><cell>Heads</cell><cell>Param (M)</cell><cell>FLOPs</cell></row><row><cell>tiny</cell><cell>6</cell><cell>256</cell><cell>1024</cell><cell>4</cell><cell>13.4</cell><cell>1.1</cell></row><row><cell>tiny12</cell><cell>12</cell><cell>256</cell><cell>1024</cell><cell>4</cell><cell>18.1</cell><cell>1.5</cell></row><row><cell>small</cell><cell>12</cell><cell>384</cell><cell>1536</cell><cell>6</cell><cell>34.3</cell><cell>2.9</cell></row><row><cell cols="2">small24 24</cell><cell>384</cell><cell>1536</cell><cell>6</cell><cell>55.6</cell><cell>4.8</cell></row><row><cell>base</cell><cell>12</cell><cell>768</cell><cell cols="3">3072 12 111.7</cell><cell>9.5</cell></row><row><cell>base24</cell><cell>24</cell><cell>768</cell><cell cols="4">3072 12 196.7 16.8</cell></row><row><cell>large</cell><cell cols="6">24 1024 4096 16 338.3 28.9</cell></row><row><cell>huge</cell><cell cols="6">32 1280 5120 16 675.4 57.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The loss for the recovery of the possibly polluted tokens by intuition fits into the scenario of training with noisy captions. Note that we use the same loss in pre-training and finetuning. During inference, at step s, given the previous predicted tokens {? k } s?1 k=1 , we setw s to [MASK], andw k =? k for k &lt; s. Therefore, the generation process simulates recovering the [MASK] token at the end in each step. Since the representations of caption tokens do not depend on the subsequent tokens, the intermediate representations of predicted tokens can be saved to Results on nocaps validation and test sets. All our models are trained with cross-entropy loss only, without CIDEr optimization. The VinVL model with * is not pre-trained, but use SCST+CBS as reported in the paper. The VinVL results with ? are reproduced by us via finetuning from the released checkpoints, which are pre-trained on the combined datasets including 5.65M images, 2.5M QAs, 4.68M captions and 1.67M pseudo-captions. The numbers with ? are copied from the nocaps leaderboard.</figDesc><table><row><cell>#</cell><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell>Pre-training data</cell><cell cols="6">in-domain CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE near-domain out-of-domain overall</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Validation Set</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="5">Encoder-Decoder [4] CC3M [40]</cell><cell cols="2">81.8 11.6</cell><cell cols="2">73.7 11.1</cell><cell cols="2">65.3 10.1</cell><cell>73.2 11.0</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CC12M [4]</cell><cell cols="2">88.3 12.3</cell><cell cols="2">86.0 11.8</cell><cell cols="2">91.3 11.2</cell><cell>87.4 11.8</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CC3M+CC12M</cell><cell cols="2">92.6 12.5</cell><cell cols="2">88.3 12.1</cell><cell cols="2">94.5 11.9</cell><cell>90.2 12.1</cell></row><row><cell>4</cell><cell>VinVL base</cell><cell cols="3">*  [56]</cell><cell>N/A</cell><cell cols="2">96.8 13.5</cell><cell cols="2">90.7 13.1</cell><cell cols="2">87.4 11.6</cell><cell>90.9 12.8</cell></row><row><cell>5</cell><cell>VinVL base</cell><cell cols="2">?</cell><cell></cell><cell>5.65M combined</cell><cell cols="2">103.1 14.2</cell><cell cols="2">96.1 13.8</cell><cell cols="2">88.3 12.1</cell><cell>95.5 13.5</cell></row><row><cell>6</cell><cell cols="2">VinVL large</cell><cell>?</cell><cell></cell><cell>5.65M combined</cell><cell cols="2">106.3 14.5</cell><cell cols="2">98.0 14.0</cell><cell cols="2">88.8 12.6</cell><cell>97.3 13.8</cell></row><row><cell>7</cell><cell cols="4">SimVLM huge [48]</cell><cell>1.8B</cell><cell>113.7</cell><cell>-</cell><cell>110.9</cell><cell>-</cell><cell>115.2</cell><cell>-</cell><cell>112.2</cell><cell>-</cell></row><row><cell>8</cell><cell cols="3">LEMON base</cell><cell></cell><cell>N/A</cell><cell cols="2">91.4 13.3</cell><cell cols="2">81.4 12.5</cell><cell cols="2">62.6 10.6</cell><cell>79.0 12.3</cell></row><row><cell>9</cell><cell cols="3">LEMON base</cell><cell></cell><cell>CC3M</cell><cell cols="2">96.0 13.8</cell><cell cols="2">91.7 13.2</cell><cell cols="2">88.1 11.8</cell><cell>91.6 13.0</cell></row><row><cell cols="4">10 LEMON base</cell><cell></cell><cell>CC12M</cell><cell cols="2">104.5 14.6</cell><cell cols="2">100.7 14.0</cell><cell cols="2">96.7 12.4</cell><cell>100.4 13.8</cell></row><row><cell cols="4">11 LEMON large</cell><cell></cell><cell>CC12M</cell><cell cols="2">103.6 14.4</cell><cell cols="2">101.1 13.8</cell><cell cols="2">102.7 12.6</cell><cell>101.8 13.6</cell></row><row><cell cols="4">12 LEMON base</cell><cell></cell><cell>ALT200M</cell><cell cols="2">107.7 14.7</cell><cell cols="2">106.2 14.3</cell><cell cols="2">107.9 13.1</cell><cell>106.8 14.1</cell></row><row><cell cols="4">13 LEMON large</cell><cell></cell><cell>ALT200M</cell><cell cols="2">116.9 15.8</cell><cell cols="2">113.3 15.1</cell><cell cols="2">111.3 14.0</cell><cell>113.4 15.0</cell></row><row><cell cols="4">14 LEMON huge</cell><cell></cell><cell>ALT200M</cell><cell cols="2">118.0 15.4</cell><cell cols="2">116.3 15.1</cell><cell cols="2">120.2 14.5</cell><cell>117.3 15.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">15 Human</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">80.6 15.0</cell><cell cols="2">84.6 14.7</cell><cell cols="2">91.6 14.2</cell><cell>85.3 14.6</cell></row><row><cell cols="4">16 SimVLM base</cell><cell></cell><cell>1.8B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.8 13.1</cell></row><row><cell cols="5">17 SimVLM large</cell><cell>1.8B</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>108.5 14.2</cell></row><row><cell cols="4">18 SimVLM huge</cell><cell>?</cell><cell>1.8B</cell><cell cols="2">109.0 14.6</cell><cell cols="2">110.8 14.6</cell><cell cols="2">109.5 13.9</cell><cell>110.3 14.5</cell></row><row><cell cols="4">19 LEMON large</cell><cell></cell><cell>ALT200M</cell><cell cols="2">111.2 15.6</cell><cell cols="2">112.3 15.2</cell><cell cols="2">105.0 13.6</cell><cell>110.9 15.0</cell></row><row><cell cols="4">20 LEMON huge</cell><cell></cell><cell>ALT200M</cell><cell cols="2">112.8 15.2</cell><cell cols="2">115.5 15.1</cell><cell cols="2">110.1 13.7</cell><cell>114.3 14.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results (single model) on COCO "Karpathy" test split. B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE.</figDesc><table><row><cell>The</cell></row></table><note>initial learning rate is set to 2 ? 10 ?4 for the base and large model, and to 1 ? 10 ?4 for the huge model. The mod- els are trained for 60 epochs. The maximum length of im- age regions, tags and caption tokens are 50, 15, 20, respec- tively. During finetuning, the model is trained for 40 epochs with batch size 512. The initial learning rate is 1 ? 10 ?5 , 1 ? 10 ?6 , and 8 ? 10 ?7 for the base, large, and huge mod- els, respectively. During inference, the caption is generated with beam search and the beam size is 5. The generation ends when the ?EOS? token is predicted, or the maximum length of 20 tokens is reached. More training details are provided in Appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation of models with different architectures, and trained with different objectives. Results are reported on COCO Caption "Karpathy" test split and Conceptual Captions val split. All the models are trained from scratch. s2s-MLM indicates sequence-to-sequence MLM as described in Sec. 3.3.</figDesc><table><row><cell>Arch.</cell><cell>Obj.</cell><cell cols="2">COCO CIDEr SPICE CIDEr SPICE CC3M</cell></row><row><cell>Enc-Dec</cell><cell>LM s2s-MLM</cell><cell>120.9 21.8 120.4 22.1</cell><cell>94.9 18.1 99.9 18.9</cell></row><row><cell>Encoder</cell><cell>LM s2s-MLM</cell><cell>119.2 21.5 119.9 21.9</cell><cell>96.1 18.0 104.4 19.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>? 10 ?3 2 ? 10 ?4 2 ? 10 ?4 tiny12 32768 1 ? 10 ?3 1 ? 10 ?4 5 ? 10 ?5 small 16384 5 ? 10 ?4 8 ? 10 ?5 5 ? 10 ?5</figDesc><table><row><cell>Model</cell><cell cols="2">Pre-training BS LR</cell><cell cols="2">Finetuning LR (3M) LR (&gt; 3M)</cell></row><row><cell cols="4">tiny 32768 2 small24 8192 2 ? 10 ?4 5 ? 10 ?5</cell><cell>3 ? 10 ?5</cell></row><row><cell>base</cell><cell>8192</cell><cell cols="2">2 ? 10 ?4 3 ? 10 ?5</cell><cell>1 ? 10 ?5</cell></row><row><cell>base24</cell><cell>8192</cell><cell cols="2">2 ? 10 ?4 1 ? 10 ?5</cell><cell>5 ? 10 ?6</cell></row><row><cell>large</cell><cell>8192</cell><cell cols="2">2 ? 10 ?4 5 ? 10 ?6</cell><cell>1 ? 10 ?6</cell></row><row><cell>huge</cell><cell>8192</cell><cell>1 ? 10 ?4</cell><cell>-</cell><cell>8 ? 10 ?7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Hyperparameters used for all model sizes. BS: effective batch size. LR: learning rate. LR(3M) is used for finetuning checkpoints pre-trained on the 3M subset. LR(&gt; 3M) is used for other finetuning. designed heuristics in data cleaning, this dataset relies on the CLIP<ref type="bibr" target="#b37">[38]</ref> model to filter image-text pairs, where the cosine similarity scores between image and text embeddings are calculated and filtered by 0.3. As CLIP</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/explosion/spaCy</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(1) B: a group of women riding roller blades on a track. F: a group of roller derby players on a track. Z: a picture of a roller derby game <ref type="bibr" target="#b1">(2)</ref> B: a man in a white shirt and black pants and a white shirt and a black tie and F: a group of people lifting dumbbells in a gym. Z: a picture of a man lifting weights (3) B: a bunch of vegetables that are sitting on the ground. F: a close up of a group of squash and zucchini. Z: a picture of summer squash (4) B: a small furry animal with a long beak and a green plant F: a small hedgehog is eating some leaves on the ground. Z: a picture of a hedgehog (5) B: a small bird is sitting on a green leaf F: a ladybug is sitting on a green leaf. Z: a picture of a ladybug (6) B: a shark is swimming in the water with a fish. F: a large stingray is swimming in the water. Z: a picture of a stingray  <ref type="figure">Figure 10</ref>. More examples of captions generated by our LEMON model in a zero-shot manner (Z) or after finetuning on COCO (F). The notation is the same as described in <ref type="figure">Figure 5</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">nocaps: novel object captioning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Behind the scene: Revealing the secrets of pre-trained vision-and-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meshed-memory transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressing visuallinguistic model via knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Playing lottery tickets with vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11832</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Large-scale adversarial training for visionand-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scaling laws for autoregressive generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14701</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simao</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Soares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05963</idno>
		<title level="m">Image captioning: Transforming objects into words</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing out of the box: End-toend pre-training for vision-language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<title level="m">Bridging vision and language by large-scale multi-modal pre-training</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Entangled transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A closer look at the robustness of vision-and-language pre-trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What does bert with vision look at</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">X-linear attention networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01913</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scale efficiently: Insights from pre-training and fine-tuning transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10686</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06946</idno>
		<title level="m">Minivlm: A smaller and faster vision-language model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probing intermodality: Visual parsing with self-attention for visionlanguage pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tap: Text-aware pre-training for text-vqa and textcaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Scaling vision transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rstnet: Captioning with adaptive attention on visual and non-visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unified vision-language pretraining for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
