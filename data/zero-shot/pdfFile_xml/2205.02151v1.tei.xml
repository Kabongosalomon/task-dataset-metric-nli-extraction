<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Zhu</surname></persName>
							<email>haowei.zhu@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Ke</surname></persName>
							<email>wenjing.ke@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
							<email>d.li@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
							<email>lu.tian@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shan</surname></persName>
							<email>yi.shan@amd.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Advanced Micro Devices, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, self-attention mechanisms have shown impressive performance in various NLP and CV tasks, which can help capture sequential characteristics and derive global information. In this work, we explore how to extend selfattention modules to better learn subtle feature embeddings for recognizing fine-grained objects, e.g., different bird species or person identities. To this end, we propose a dual cross-attention learning (DCAL) algorithm to coordinate with self-attention learning. First, we propose global-local cross-attention (GLCA) to enhance the interactions between global images and local high-response regions, which can help reinforce the spatial-wise discriminative clues for recognition. Second, we propose pairwise cross-attention (PWCA) to establish the interactions between image pairs. PWCA can regularize the attention learning of an image by treating another image as distractor and will be removed during inference. We observe that DCAL can reduce misleading attentions and diffuse the attention response to discover more complementary parts for recognition. We conduct extensive evaluations on finegrained visual categorization and object re-identification. Experiments demonstrate that DCAL performs on par with state-of-the-art methods and consistently improves multiple self-attention baselines, e.g., surpassing DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-attention is an attention mechanism that can relate different positions of a single sequence and draw global dependencies. It is originally applied in natural language processing (NLP) tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref> and exhibits the outstanding performance. Recently, Transformer with self-attention learning has also been explored for various vision tasks (e.g., image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref> and object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b69">70]</ref>) as an alternative of convolutional neu-* Equal contribution. ral network (CNN). For general image classification, selfattention has been proved to work well for recognizing 2D images by viewing image patches as words and flattening them as sequences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>In this work, we investigate how to extend self-attention modules to better learn subtle feature embeddings for recognizing fine-grained objects, e.g., different bird species or person identities. Fine-grained recognition is more challenging than general image classification owing to the subtle visual variations among different sub-classes. Most of existing approaches build upon CNN to predict class probabilities or measure feature distances. To address the subtle appearance variations, local characteristics are often captured by learning spatial attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref> or explicitly localizing semantic objects / parts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63]</ref>. We adopt a different way to incorporate local information based on vision Transformer. To this end, we propose global-local cross-attention (GLCA) to enhance the interactions between global images and local high-response regions. Specifically, we compute the cross-attention between a selected subset of query vectors and the entire set of key-value vectors. By coordinating with self-attention learning, GLCA can help reinforce the spatial-wise discriminative clues to recognize fine-grained objects.</p><p>Apart from incorporating local information, another solution to distinguish the sutble visual differences is pairwise learning. The intuition is that one can identify the subtle variations by comparing image pairs. Exiting CNNbased methods design dedicated network architectures to enable pair-wise feature interaction <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b70">71]</ref>. A contrastive loss <ref type="bibr" target="#b15">[16]</ref> or score ranking loss <ref type="bibr" target="#b70">[71]</ref> is used for feature learning. Motivated by this, we also employ a pair-wise learning scheme to establish the interactions between image pairs. Different from optimizing the feature distance, we propose pair-wise cross-attention (PWCA) to regularize the attention learning of an image by treating another image as distractor. Specifically, we compute the cross-attention between query of an image and combined key-value from both images. By introducing confusion in key and value vectors, the attention scores are diffused to another image so that the difficulty of the attention learning of the current image increases. Such regularization allows the network to discover more discriminative regions and alleviate overfitting to sample-specific features. It is noted that PWCA is only used for training and thus does not introduce extra computation cost during inference.</p><p>The proposed two types of cross-attention are easy-toimplement and compatible with self-attention learning. We conduct extensive evaluations on both fine-grained visual categorization (FGVC) and object re-identification (Re-ID). Experiments demonstrate that DCAL performs on par with state-of-the-art methods and consistently improves multiple self-attention baselines. Particularly, for FGVC, DCAL improves DeiT-Tiny by 2.5% and reaches 92.0% top-1 accuracy with the larger R50-ViT-Base backbone on CUB-200-2011. For Re-ID, DCAL improves DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.</p><p>Our main contributions can be summarized as follows. (1) We propose global-local cross-attention to enhance the interactions between global images and local high-response regions for reinforcing the spatial-wise discriminative clues.</p><p>(2) We propose pair-wise cross-attention to establish the interactions between image pairs by regularizing the attention learning. (3) The proposed dual cross-attention learning can complement the self-attention learning and achieves consistent performance improvements over multiple vision Transformer baselines on various FGVC and Re-ID benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-Attention Mechanism</head><p>The self-attention mechanism is originally proposed to relate distinct positions in a sequence and draw global dependencies. Transformer carrying forward this mechanism has dominated in various sequence-to-sequence NLP tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref>. Transformer usually consists of multiple encoder and decoder modules. Each encoder / decoder includes a multi-head self-attention (MSA) layer and a feedforward network (FFN) layer. A decoder also has an extra MSA layer to handle the output of encoder. Besides, layer normalization (LN) and residual connection are used in each MSA or FFN layer. Recent work has applied Transformers to various vision tasks (e.g., image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b52">53]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b69">70]</ref>, semantic segmentation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref> and low-level tasks <ref type="bibr" target="#b3">[4]</ref>) and shown competitive performance compared to the stateof-the-art CNNs. For general image classification, iGPT <ref type="bibr" target="#b4">[5]</ref> first uses auto-regressive and BERT <ref type="bibr" target="#b9">[10]</ref> objectives for selfsupervised pre-training and then fine-tunes for classification tasks. ViT <ref type="bibr" target="#b11">[12]</ref> reshapes an image into a sequence of flattened fixed-size patches for training Transformer encoders only. Attempts have also been made to improve ViT by knowledge distillation <ref type="bibr" target="#b45">[46]</ref> and progressive tokeniza-tion <ref type="bibr" target="#b58">[59]</ref>. Fine-grained recognition is more challenging than general image classification owing to the sutble visual variations among different sub-classes. In this work, we extend self-attention to better recognize fine-grained objects with two types of cross-attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Fine-Grained Visual Categorization</head><p>Fine-grained visual categorization (FGVC) is a special case of image classification, which aims to identify those highly-confused categories with fine differences. Prior CNN-based methods address this task by mining effective information from multi-level features <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b59">60]</ref>, adopting multi-granularity training strategies <ref type="bibr" target="#b12">[13]</ref>, locating discriminative objects or parts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b62">63]</ref> and exploring feature interaction in pair-wise learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b70">71]</ref>. Recently, a few Transformer-based methods address FGVC by feature fusion on multi-level Transformer layers <ref type="bibr" target="#b53">[54]</ref> and part selection <ref type="bibr" target="#b16">[17]</ref>. Our motivation is similar with <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">54]</ref> in the aspects of aggregating multi-level attention and selecting patch tokens. However, they are based on self-attention only while we design two cross-attention modules for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Object Re-Identification</head><p>Similar to FGVC, object re-identification also aims to distinguish different person / vehicle identities with subtle inter-class differences. Mainstream Re-ID methods are based on the CNN structure and metric learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. Local information is crucial for Re-ID and many different approaches have been presented by encoding discriminative part-level features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref>. Transformer with selfattention structure has recently been applied to Re-ID by introducing part tokens <ref type="bibr" target="#b68">[69]</ref>, shuffling patch embeddings <ref type="bibr" target="#b16">[17]</ref>, and learning disentangled features <ref type="bibr" target="#b23">[24]</ref>. Our work differs from the most related methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b68">69]</ref> in the following aspects. First, we adopt a different way to encode the local information by GLCA, while <ref type="bibr" target="#b16">[17]</ref> does not explicitly mine part regions and <ref type="bibr" target="#b68">[69]</ref> computes the attention between a part token and its associated subset of patch embeddings by online clustering. Second, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b68">69]</ref> uses a single image for training while we employ image pairs for PWCA. Third, <ref type="bibr" target="#b16">[17]</ref> requires side information (e.g., camera IDs and viewpoint labels) while our method only takes images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>3.1. Revisit Self-Attention <ref type="bibr" target="#b47">[48]</ref> originally proposes the self-attention mechanism to address NLP tasks by calculating the correlation between each word and all the other words in the sentence. <ref type="bibr" target="#b11">[12]</ref> inherits the idea by taking each patch in the image / feature map as a word for general image classification. In gen- eral, a self-attention function can be depicted as mapping a query vector and a set of key and value vectors to an output. The output is computed as a weighted sum of value vectors, where the weight assigned to each value is computed by a scaled inner product of the query with the corresponding key. Specifically, a query q ? R 1?d is first matched against N key vectors (K = [k 1 ; k 2 ; ? ? ? ; k N ], where each k i ? R 1?d ) using inner product. The products are then scaled and normalized by a softmax function to obtain N attention weights. The final output is the weighted sum of N value vectors (V = [v 1 ; v 2 ; ? ? ? ; v N ], where each v i ? R 1?d ). By packing N query vector into a matrix Q = [q 1 ; q 2 ; ? ? ? ; q N ], the output matrix of self-attention (SA) can be represented as:</p><formula xml:id="formula_0">f SA (Q, K, V ) = softmax( QK T ? d )V = SV<label>(1)</label></formula><p>where 1 ? d is a scaling factor. Query, key and value matrices are computed from the same input embedding X ? R N ?D with different linear transformations: Q = XW Q , K = XW K , V = XW V , respectively. S ? R N ?N denotes the attention weight matrix.</p><p>To jointly attend to information from different representation subspaces at different positions, multi-head selfattention (MSA) is defined by considering multiple attention heads. The process of MSA can be computed as linear transformation on the concatenations of self-attention blocks with subembeddings. To encode positional information, fixed / learnable position embeddings are added to patch embeddings and then fed to the network. To predict the class, an extra class embedding? LS ? R 1?d is prepended to the input embedding X throughout the network, and finally projected with a linear classifer layer for prediction. Thus, the input embeddings as well as query, key and value matrices become (N + 1) ? d and the self-attention function (Eq. 1) allows to spread information between patch and class embeddings.</p><p>Based on self-attention, a Transformer encoder block can be constructed by an MSA layer and a feed forward network (FFN). FFN consists of two linear transformation with a GELU activation. Layer normalization (LN) is put prior to each MSA and FFN layer and residual connections are used for both layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global-Local Cross-Attention</head><p>Self-attention treats each query equally to compute global attention scores according to Eq. 1. In other words, each local position of image is interacted with all the positions in the same manner. For recognizing fine-grained objects, we expect to mine discriminative local information to facilitate the learning of subtle features. To this end, we propose global-local cross-attention to emphasize the interaction between global images and local high-response regions. First, we follow attention rollout <ref type="bibr" target="#b0">[1]</ref> to calculate the accumulated attention scores for i-th block:</p><formula xml:id="formula_1">S i =S i ?S i?1 ? ? ? ?S 1<label>(2)</label></formula><p>whereS = 0.5S + 0.5E means the re-normalized attention weights using an identity matrix E to consider residual connections, ? means the matrix multiplication operation. In this way, we track down the information propagated from the input layer to a higher layer. Then, we use the aggregated attention map to mine the high-response regions. According to Eq. 2, the first row of? i = [? i,j ] (N +1)?(N +1) means the accumulated weights of class embedding? LS.</p><p>We select top R query vectors from Q i that correspond to the top R highest responses in the accumulated weights of? LS to construct a new query matrix Q l , representing the most attentive local embeddings. Finally, we compute the cross attention between the selected local query and the global set of key-value pairs as below.</p><formula xml:id="formula_2">f GLCA (Q l , K g , V g ) = softmax( Q l K g T ? d )V g (3)</formula><p>In self-attention (Eq. 1), all the query vectors will be interacted with the key-value vectors. In our GLCA (Eq. 3), only a subset of query vectors will be interacted with the key-value vectors. We observe that GLCA can help reinforce the spatial-wise discriminative clues to promote recognition of fine-grained classes. Another possible choice is to compute the self-attention between local query Q l and local key-value vectors (K l , V l ). However, through establishing the interaction between local query and global key-value vectors, we can relate the high-response regions with not only themselves but also with other context outside of them. <ref type="figure" target="#fig_0">Figure 1</ref> (a) illustrates the proposed globallocal cross-attention and we use M = 1 GLCA block in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pair-Wise Cross-Attention</head><p>The scale of fine-grained recognition datasets is usually not as large as that of general image classification, e.g., Im-ageNet <ref type="bibr" target="#b8">[9]</ref> contains over 1 million images of 1,000 classes while CUB <ref type="bibr" target="#b48">[49]</ref> contains only 5,994 images of 200 classes for training. Moreover, smaller visual differences between classes exist in FGVC and Re-ID compared to large-scale classification tasks. Fewer samples per class may lead to network overfitting to sample-specific features for distinguishing visually confusing classes in order to minimize the training error.</p><p>To alleviate the problem, we propose pair-wise cross attention to establish the interactions between image pairs. PWCA can be viewed as a novel regularization method to regularize the attention learning. Specifically, we randomly sample two images (I 1 , I 2 ) from the same training set to construct the pair. The query, key and value vectors are separately computed for both images of a pair. For training I 1 , we concatenate the key and value matrices of both images, and then compute the attention between the query of the target image and the combined key-value pairs as follows:</p><formula xml:id="formula_3">f PWCA (Q 1 , K c , V c ) = softmax( Q 1 K T c ? d )V c<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">K c = [K 1 ; K 2 ] ? R (2N +2)?d and V c = [V 1 ; V 2 ] ? R (2N +2)?d .</formula><p>For a specific query from I 1 , we compute N +1 self-attention scores within itself and N + 1 cross-attention scores with I 2 according to Eq. 4. All the 2N + 2 attention scores are normalized by the softmax function together and thereby contaminated attention scores for the target image I 1 are learned. Optimizing this noisy attention output increases the difficulty of network training and reduces the overfitting to sample-specific features. <ref type="figure" target="#fig_0">Figure 1</ref> (b) illustrates the proposed pair-wise cross-attention and we use T = 12 PWCA blocks in our method. Note that PWCA is only used for training and will be removed for inference without consuming extra computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Datasets. We conduct extensive experiments on two fine-grained recognition tasks: fine-grained visual categorization (FGVC) and object re-identification (Re-ID). For FGVC, we use three standard benchmarks for evaluations: CUB-200-2011 <ref type="bibr" target="#b48">[49]</ref>, Stanford Cars <ref type="bibr" target="#b26">[27]</ref>, FGVC-Aircraft <ref type="bibr" target="#b35">[36]</ref>. For Re-ID, we use four standard benchmarks: Mar-ket1501 <ref type="bibr" target="#b63">[64]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b55">[56]</ref>, MSMT17 <ref type="bibr" target="#b54">[55]</ref> for Person Re-ID and VeRi-776 <ref type="bibr" target="#b65">[66]</ref> for Vehicle Re-ID. In all experiments, we use the official train and validation splits for evaluation.</p><p>Baselines. We use DeiT and ViT as our self-attention baselines. In detail, ViT backbones are pre-trained on ImageNet-21k <ref type="bibr" target="#b8">[9]</ref> and DeiT backbones are pre-trained on ImageNet-1k <ref type="bibr" target="#b8">[9]</ref>. We use multiple architectures of DeiT-T/16, DeiT-S/16, DeiT-B/16, ViT-B/16, R50-ViT-B/16 with L = 12 SA blocks for evaluation.</p><p>Implementation Details. We coordinate the proposed two types of cross-attention with self-attention in the form of multi-task learning. We build L = 12 SA blocks, M = 1 GLCA blocks and T = 12 PWCA blocks as the overall architecture for training. The PWCA branch shares weights with the SA branch while GLCA does not share weights with SA. We follow <ref type="bibr" target="#b60">[61]</ref> to adopt dynamic loss weights for collaborative optimization, avoiding exhausting manual hyper-parameter search. The PWCA branch has the same GT target as the SA branch since we treat another image as distractor.</p><p>For FGVC, we resize the original image into 550?550 and randomly crop to 448?448 for training. The sequence length of input embeddings for self-attention baseline is 28 ? 28 = 784. We select input embeddings with top R = 10% highest attention responses as local queries. We apply stochastic depth <ref type="bibr" target="#b20">[21]</ref> and use Adam optimizer with weight decay of 0.05 for training. The learning rate is initialized as lr scaled = 5e?4 512 ? batchsize and decayed with a cosine policy. We train the network for 100 epochs with batch size of 16 using the standard cross-entropy loss.</p><p>For Re-ID, we resize the image into 256?128 for pedestrian datasets, and 256?256 for vehicle datasets. We select input embeddings with top R = 30% highest attention responses as local queries. We use SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. The batch size is set to 64 with 4 images per ID. The learning rate is initialized as 0.008 and decayed with a cosine policy. We train the network for 120 epochs using the cross-entropy and triplet losses.</p><p>All of our experiments are conducted on PyTorch with Nvidia Tesla V100 GPUs. Our method costs 3.8 hours with DeiT-Tiny backbone for training using 4 GPUs on CUB, and 9.5 hours with ViT-Base for training using 1 GPU on MSMT17. During inference, we remove all the PWCA modules and only use the SA and GLCA modules. We add class probabilities output by classifiers of SA and GLCA for prediction for FGVC, and concat two final class tokens of SA and GLCA for prediction for Re-ID. A single image with the same input size as training is used for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Fine-Grained Visual Categorization</head><p>We evaluate our method on three standard FGVC benchmarks and compare with the state-of-the-art approaches in <ref type="table" target="#tab_0">Table 1</ref>. Our method achieves competitive performance compared to the prior CNN-based and Transformer-based methods. Particularly, with the R50-ViT-Base backbone, DCAL reaches 92.0%, 95.3% and 93.3% top-1 accuracy on CUB-200-2011, Stanford Cars and FGVC-Aircraft benchmarks, respectively. <ref type="table" target="#tab_0">Table 1</ref> also shows our method can consistently improve different vision Transformer baselines on all the three benchmarks, e.g., surpassing the pure Transformer (DeiT-Tiny) by 2.2% and the hybrid structure of CNN and Transformer (R50-ViT-Base) by 1.3% on Stanford Cars. The results validate the compatibility of our method to different Transformer architectures.</p><p>Comparisons to Transformer-based Methods. Our method performs on par with the recent Transformer variants on FGVC: TransFG <ref type="bibr" target="#b16">[17]</ref>, RAMS-Trans <ref type="bibr" target="#b19">[20]</ref>, FFVT <ref type="bibr" target="#b53">[54]</ref>. These existing methods also select tokens based on aggregated attention responses. Differently, they continue to model the selected tokens by self-attention while we perform cross-attention between local query and global keyvalue vectors. Compared to self-attention in selected tokens, we can relate the high-response regions with not only themselves but also with other context outside of them. Besides, TransFG <ref type="bibr" target="#b16">[17]</ref> uses overlapping patches and will largely increase training time and computation overhead, while we adopt the standard non-overlapping patch split method.</p><p>Comparisons to CNN-based Methods. (1) Existing region-based methods can be divided to two categories. Explicit localization methods (e.g, RACNN <ref type="bibr" target="#b14">[15]</ref>, MA-CNN <ref type="bibr" target="#b61">[62]</ref>, NTS-Net <ref type="bibr" target="#b57">[58]</ref>, MGE-CNN <ref type="bibr" target="#b59">[60]</ref>) utilize attention / localization sub-network with ranking losses to mine object regions. Implicit localization methods (e.g., S3N <ref type="bibr" target="#b10">[11]</ref>, TASN <ref type="bibr" target="#b62">[63]</ref>) use class activation map and Gaussian sampling to amplify object regions in the original image. Our GLCA adopts a different scheme to incorporate the local information with higher performance, e.g., +3.5% over MGE-CNN on CUB. (2) Pair-wise learning is also applied for FGVC by interacting features (CIN <ref type="bibr" target="#b15">[16]</ref>, API-Net <ref type="bibr" target="#b70">[71]</ref>) or introducing confusion (PC <ref type="bibr" target="#b13">[14]</ref>, SPS <ref type="bibr" target="#b21">[22]</ref>) between image pairs during training. Our motivation of PWCA is similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref> but we implement a different regularization method to alleviate overfitting. Our method surpasses these related pair-wise learning methods, e.g., +3.9% over CIN and +5.1% over PC on CUB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Object Re-ID</head><p>We evaluate our method on four standard Re-ID benchmarks in <ref type="table" target="#tab_1">Table 2</ref>  Comparisons to Transformer-based Methods. Our method performs on par with the recent Transformer variants on Re-ID: DRL-Net <ref type="bibr" target="#b23">[24]</ref>, AAformer <ref type="bibr" target="#b68">[69]</ref>, TransReID <ref type="bibr" target="#b17">[18]</ref>. DRL-Net <ref type="bibr" target="#b23">[24]</ref> imposes decorrelation constraints on Transformer decoder to disentangle ID relevant and irrelevant features, while we only employ Transformer encoder and extend self-attention to cross-attention. Both of existing methods (TransReID <ref type="bibr" target="#b17">[18]</ref>, AAformer <ref type="bibr" target="#b68">[69]</ref>) and our methods incorporate local information for recognition but adopt different manners. TransReID <ref type="bibr" target="#b17">[18]</ref> designs a jigsaw patch module to shuffle the patch embeddings for learning robust features. AAformer <ref type="bibr" target="#b68">[69]</ref> computes the attention between a part token and its associated subset of patch embeddings by online clustering. Differently, we proposes global-local cross-attention to enhance the interactions between global images and local regions.</p><p>Comparisons to CNN-based Methods. (1) Many prior approaches have been presented to encode discriminative part-level features for recognition. Typical part-based ReID methods include SPReID <ref type="bibr" target="#b25">[26]</ref> and PCB <ref type="bibr" target="#b43">[44]</ref>. SPReID <ref type="bibr" target="#b25">[26]</ref> utilizes a parsing model to generate human part masks to compute reliable part representations, which consumes extra computation overhead in segmentation part. PCB <ref type="bibr" target="#b43">[44]</ref> utilizes a refined part pooling to retrieve the body part information. Our method does not aim to mine precise object parts but establish the interactions between global images and high-response local regions. (2) Image pairs or triplets are widely used in Re-ID for metric learning. Recent Re-ID methods also introduce pair-wise spatial transformer to match the holistic and partial image pairs <ref type="bibr" target="#b33">[34]</ref> or design pair-wise loss to learn fine-grained features for recognition <ref type="bibr" target="#b56">[57]</ref>. Our pair-wise cross-attention is a new practice in Re-ID in contrast to previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Contributions from Algorithmic Components. We examine the contributions from the two types of crossattention modules using different vision Transformer baselines in <ref type="table" target="#tab_2">Table 3</ref>. We use DeiT-Tiny for FGVC and ViT-Base for Re-ID. With either GLCA or PWCA alone, our method can obtain higher performance than the baselines. With both cross-attention modules, we can further improve the results. We note that PWCA will be removed for inference so that it does not introduce extra parameters or FLOPs. We</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB-200-2011</head><p>VeRi-776 MSMT17 Params FLOPs Acc Params FLOPs mAP R1 Params FLOPs mAP R1 uses one GLCA module in our method, which only requires a small increase of parameters or FLOPs compared to the baseline.</p><p>Ablation Study on GLCA.</p><p>(1) Cross-ViT <ref type="bibr" target="#b2">[3]</ref> is a most recent method based on cross-attention for general image classification. It constructs two Transformer branches to handle image tokens of different sizes and uses the class token from one branch to interact with patch tokens from another branch. We implement this idea using the same selected local queries and the same DeiT-Tiny backbone. The cross-token strategy obtains 82.1% accuracy on CUB, which is worse than our GLCA by 1%. (2) Another possible baseline to incorporate local information is computing the self-attention for the high-response local regions (i.e., local query, key and value vectors). This local self-attention baseline obtains 82.6% accuracy on CUB using the DeiT-Tiny backbone, which is also worse than our GLCA (83.1%). <ref type="bibr" target="#b2">(3)</ref> We conduct more ablation experiments to examine the effect of GLCA. We obtain 82.6% accuracy on CUB by selecting local query randomly and obtain 82.8% by selecting local query based on the penultimate layer only. Our GLCA outperforms both baselines, validating that mining high-response local query with aggregated attention map is effective for our cross-attention learning.</p><p>Ablation Study on PWCA. We compare PWCA with different regularization strategies in <ref type="table">Table 4</ref> by taking I 1 as the target image. The results show that adding image noise or label noise without cross-attention causes degraded performance compared to the self-attention learning baseline. As the extra image I 2 used in PWCA can be viewed as distractor, we also test replacing the key and value embeddings of I 2 with Gaussian noise. Such method performs better than adding image / label noise, but still worse than our method. Moreover, sampling I 2 from a different dataset (i.e., COCO), sampling intra-class / inter-class pair only, or sampling intra-class &amp; inter-class pairs with equal probability performs worse than PWCA. We assume that the randomly sampled image pairs from the same dataset (i.e., natural distribution of the dataset) can regularize our crossattention learning well. Amount of Cross-Attention Blocks. <ref type="figure">Figure 2</ref> presents the ablation experiments on the amount of our crossattention blocks using DeiT-Tiny for CUB and ViT-Base for MSMT17. For GLCA, the results show that M = 1 performs best. We analyze that the deeper Transformer encoder can produce more accurate accumulated attention scores as the attention flow is propagated from the input layer to higher layer. Moreover, using one GLCA block only introduces small extra Parameters and FLOPs for inference. For PWCA, the results show that T = 12 performs best. It implies that adding I 2 throughout all the encoders can sufficiently regularize the network as our self-attention baseline has L = 12 blocks in total. Note that PWCA is only used for training and will be removed for inference without consuming extra computation cost.  <ref type="figure" target="#fig_4">Figure 4</ref> (a) visualize the generated attention map using <ref type="bibr" target="#b0">[1]</ref> and the selected high-response patches. We observe that self-attention tend to highlight the most discriminative regions in the image. Thanks to GLCA, our method can reduce misleading attention and encourage the network to discover more discriminative clues for recognition. <ref type="figure" target="#fig_1">Figure 5</ref> (b) and <ref type="figure" target="#fig_4">Figure 4</ref> (b) visualize the generated attention map using <ref type="bibr" target="#b0">[1]</ref> for self-attention and PWCA. We observe that PWCA can diffuse the attention responses to explore more complementary parts of objects compared to self-attention. We also visualize the attention map on the distractor image and the blue gauze on it indicates that little attention is derived. It is accordance with our expectation that the attention weights will dominate on the target image as we compute the cross-attention between the query of target image and the combined key-value vectors (Eq. 4).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Limitations</head><p>Compared to the self-attention learning baseline, our method may take longer time for network convergence as we perform joint training of self-attention and the proposed two types of cross-attention. For example, the self-attention baseline costs 2.1 hours while our method costs 3.8 hours for training on CUB with the same DeiT-backbone and same epochs of 100. However, it is noted that fine-grained recognition datasets are much smaller than the large-scale image classification benchmark and thereby our training time in practice is still acceptable.</p><p>Another limitation is that GLCA will increase small computation cost compared to the self-attention baseline. For example, <ref type="table" target="#tab_2">Table 3</ref> shows that GLCA increases 9% Params and 2% FLOPs for DeiT-Tiny on CUB and increases 8% Params and 3% FLOPs for ViT-Base on VeRi-776. We also test removing both GLCA and PWCA blocks for maintaining the same computation cost with the selfattention baseline, and the performance slightly drops, e.g, 84.3% vs. 84.6% (Ours) accuracy on CUB and 80.1% vs. 80.2% (Ours) mAP on VeRi-776.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce two types of cross-attention mechanisms to better learn subtle feature embeddings for recognizing fine-grained objects. GLCA can help reinforce the spatial-wise discriminative clues by modeling the interactions between global images and local regions. PWCA can establish the interactions between image pairs and can be viewed as a regularization strategy to alleviate overfitting. Our cross-attention design is easy-to-implement and compatible to different vision Transformer baselines. Extensive experiments on seven benchmarks have demonstrated the effectiveness of our method on FGVC and Re-ID tasks. We expect that our method can inspire new insights for the self-attention learning regime in Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary material, we present more experimental results and analysis.</p><p>? We test different inference architectures.</p><p>? We provide additional ablation study on effect of ratio of local query selection.</p><p>? We show more visualization results of generated attention maps on different benchmarks.</p><p>? We conduct experiments on more Transformer baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Different Inference Architectures</head><p>Our default inference architecture is that all the PWCA modules are removed and only SA and GLCA modules are used. For FGVC, we add class probabilities output by classifiers of SA and GLCA for prediction. For Re-ID, we concat two final class tokens of SA and GLCA as the output feature for prediction. We also test two different inference architectures: (1) "SA": using the last SA module for inference. (2) "GLCA": using the GLCA module for inference. <ref type="table" target="#tab_4">Table 5</ref> and 6 present the detailed performance with different baselines on all the FGVC and Re-ID benchmarks, respectively. The results show that only using the SA or GLCA module can obtain similar performance with our default setting. It is also noted that "SA" has the same inference architecture with the baseline by removing all the PWCA and GLCA modules for inference, which does not introduce extra computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study on Effect of R</head><p>We test different choices of the ratios of selecting highresponse regions as local query. <ref type="figure" target="#fig_0">Figure 11</ref> shows that different choices of R can obtain similar performance. We set R = 10% for all the FGVC benchmarks and set R = 30% for all the Re-ID benchmarks as default in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Visualization Results</head><p>We show more visualization results by comparing selfattention and our cross-attention method. <ref type="figure" target="#fig_1">Figure 5</ref>, 6, 7 present the generated attention maps on different FGVC benchmarks. <ref type="figure">Figure 8, 9</ref>, 10 present the generated attention maps on different Re-ID benchmarks. The results show that our DCAL can reduce misleading attentions and diffuse the attention response to discover more complementary parts for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Transformer Baselines</head><p>We conduct two more experiments on CaiT <ref type="bibr" target="#b46">[47]</ref> and Swin Transformer <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr">CaiT</ref>   <ref type="table">Table 6</ref>. Ablation study on different inference architectures for object Re-ID in terms of mAP and rank-1 accuracy. SA: using SA as the last layer to output final feature. GLCA: using GLCA as the last layer to output final feature. SA+GLCA: combine the output of SA and GLCA for inference.   <ref type="figure" target="#fig_0">Figure 10</ref>. Visualization of the generated attention map for selfattention learning and our cross-attention learning on VeRi-776. <ref type="figure" target="#fig_0">Figure 11</ref>. Effect on the ratio of local query selection. DeiT-Tiny is used for CUB and ViT-base is used for MSMT17. We set R = 10% for all the FGVC benchmarks and set R = 30% for all the Re-ID benchmarks as default in our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Global-Local Cross-Attention (GLCA) (b) Pair-Wise Cross-Attention (PWCA) Overview of the proposed two types of cross-attention mechanisms. We stack L self-attention, M global-local cross-attention, T pair-wise cross-attention modules in our network. See Section 3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 (</head><label>5</label><figDesc>a) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Effect on the amount of cross-attention blocks. DeiT-Tiny is used for CUB and ViT-base ise used for MSMT17. For all the backbones and all the datasets, we build the same M = 1 GLCA block and same T = 12 PWCA blocks in our method.(a) SA vs. GLCA (b) SA vs. PWCA Visualization of the generated attention map for selfattention learning and our cross-attention learning on CUB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) SA vs. GLCA (b) SA vs. PWCA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of the generated attention map for selfattention learning and our cross-attention learning on MSMT17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visualization of the generated attention map for selfattention learning and our cross-attention learning on CUB-200-2011. (a) SA vs. GLCA (b) SA vs. PWCA Visualization of the generated attention map for selfattention learning and our cross-attention learning on Stanford-Cars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .Figure 9 .</head><label>789</label><figDesc>Visualization of the generated attention map for selfattention learning and our cross-attention learning on FGVC-Aircraft. (a) SA vs. GLCA (b) SA vs. PWCA Visualization of the generated attention map for selfattention learning and our cross-attention learning on Market-1501. (a) SA vs. GLCA (b) SA vs. PWCA Visualization of the generated attention map for self-attention learning and our cross-attention learning on DukeMTMC-ReID. (a) SA vs. GLCA (b) SA vs. PWCA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">Accuracy (%) CUB CAR AIR</cell></row><row><cell>RA-CNN [15]</cell><cell>VGG19</cell><cell>85.3</cell><cell cols="2">92.5 88.4</cell></row><row><cell>MA-CNN [62]</cell><cell>VGG19</cell><cell>86.5</cell><cell cols="2">92.8 89.9</cell></row><row><cell>MAMC [41]</cell><cell>ResNet101</cell><cell>86.5</cell><cell>93.0</cell><cell>-</cell></row><row><cell>PC [14]</cell><cell>DenseNet161</cell><cell>86.9</cell><cell cols="2">92.9 89.2</cell></row><row><cell>FDL [29]</cell><cell>DenseNet161</cell><cell>89.1</cell><cell>94.0</cell><cell>-</cell></row><row><cell>NTS-Net [58]</cell><cell>ResNet50</cell><cell>87.5</cell><cell cols="2">93.9 91.4</cell></row><row><cell>Cross-X [35]</cell><cell>ResNet50</cell><cell>87.7</cell><cell>94.6</cell><cell>-</cell></row><row><cell>S3N [11]</cell><cell>ResNet50</cell><cell>88.5</cell><cell cols="2">94.7 92.8</cell></row><row><cell>MGE-CNN [60]</cell><cell>ResNet50</cell><cell>88.5</cell><cell>93.9</cell><cell>-</cell></row><row><cell>DCL [8]</cell><cell>ResNet50</cell><cell>87.8</cell><cell cols="2">94.5 93.0</cell></row><row><cell>TASN [63]</cell><cell>Resnet50</cell><cell>87.9</cell><cell>93.8</cell><cell>-</cell></row><row><cell>PMG [13]</cell><cell>ResNet50</cell><cell>89.6</cell><cell cols="2">95.1 93.4</cell></row><row><cell>CIN [16]</cell><cell>ResNet50</cell><cell>88.1</cell><cell cols="2">94.5 92.8</cell></row><row><cell>API-Net [71]</cell><cell>DenseNet161</cell><cell>90.0</cell><cell cols="2">95.3 93.9</cell></row><row><cell>LIO [67]</cell><cell>ResNet50</cell><cell>88.0</cell><cell cols="2">94.5 92.7</cell></row><row><cell>SPS [22]</cell><cell>ResNet50</cell><cell>88.7</cell><cell cols="2">94.9 92.7</cell></row><row><cell>CAL [39]</cell><cell>ResNet101</cell><cell>90.6</cell><cell cols="2">95.5 94.2</cell></row><row><cell>TransFG [17]</cell><cell>ViT-Base</cell><cell>91.7</cell><cell>94.8</cell><cell>-</cell></row><row><cell>RAMS-Trans [20]</cell><cell>ViT-Base</cell><cell>91.3</cell><cell>-</cell><cell>-</cell></row><row><cell>FFVT [54]</cell><cell>ViT-Base</cell><cell>91.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>DeiT-Tiny</cell><cell>82.1</cell><cell cols="2">87.2 84.7</cell></row><row><cell>Baseline + DCAL</cell><cell>DeiT-Tiny</cell><cell>84.6</cell><cell cols="2">89.4 87.4</cell></row><row><cell>Baseline</cell><cell>DeiT-Small</cell><cell>85.8</cell><cell cols="2">90.7 88.1</cell></row><row><cell>Baseline + DCAL</cell><cell>DeiT-Small</cell><cell>87.6</cell><cell cols="2">92.3 90.0</cell></row><row><cell>Baseline</cell><cell>DeiT-Base</cell><cell>88.0</cell><cell cols="2">92.9 90.3</cell></row><row><cell>Baseline + DCAL</cell><cell>DeiT-Base</cell><cell>88.8</cell><cell cols="2">93.8 92.6</cell></row><row><cell>Baseline</cell><cell>ViT-Base</cell><cell>90.8</cell><cell cols="2">92.5 90.0</cell></row><row><cell>Baseline + DCAL</cell><cell>ViT-Base</cell><cell>91.4</cell><cell cols="2">93.4 91.5</cell></row><row><cell>Baseline</cell><cell cols="2">R50-ViT-Base 91.3</cell><cell cols="2">94.0 92.4</cell></row><row><cell cols="3">Baseline + DCAL R50-ViT-Base 92.0</cell><cell cols="2">95.3 93.3</cell></row></table><note>. Performance comparisons in terms of top-1 accuracy on three standard FGVC benchmarks: CUB-200-2011, Stanford Cars and FGVC-Aircraft.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>and achieve competitive performance compared to the state-of-the-art methods on both Person Re-ID and Vehicle Re-ID tasks. Particularly, with the ViT-Base backbone, DCAL reaches 80.2%, 64.0%, 87.5%, 80.1% mAP on VeRi-776, MSMT17, Market1501, DukeMTMC, respectively. Similar to FGVC, our method can consistently improve different vision Transformer baselines, e.g., surpassing the light-weight Transformer (DeiT-Tiny) by</figDesc><table><row><cell>Method</cell><cell cols="8">VeRi-776 mAP (%) R1 (%) mAP (%) R1 (%) mAP (%) R1 (%) mAP (%) R1 (%) MSMT17 Market1501 DukeMTMC</cell></row><row><cell>SPReID [26]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.4</cell><cell>93.7</cell><cell>73.3</cell><cell>86.0</cell></row><row><cell>PCB [44]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.6</cell><cell>93.8</cell><cell>69.2</cell><cell>83.3</cell></row><row><cell>MGN [51]</cell><cell>-</cell><cell>-</cell><cell>52.1</cell><cell>76.9</cell><cell>86.9</cell><cell>95.7</cell><cell>78.4</cell><cell>88.7</cell></row><row><cell>SAN [25]</cell><cell>72.5</cell><cell>93.3</cell><cell>55.7</cell><cell>79.2</cell><cell>88.0</cell><cell>96.1</cell><cell>75.7</cell><cell>87.9</cell></row><row><cell>ABDNet [6]</cell><cell>-</cell><cell>-</cell><cell>60.8</cell><cell>82.3</cell><cell>88.3</cell><cell>95.6</cell><cell>78.6</cell><cell>89.0</cell></row><row><cell>HOReID [50]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.9</cell><cell>94.2</cell><cell>75.6</cell><cell>86.9</cell></row><row><cell>ISP [68]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>88.6</cell><cell>95.3</cell><cell>80.0</cell><cell>89.6</cell></row><row><cell>STNReID [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.9</cell><cell>93.8</cell><cell>-</cell><cell>-</cell></row><row><cell>CDNet [28]</cell><cell>-</cell><cell>-</cell><cell>54.7</cell><cell>78.9</cell><cell>86.0</cell><cell>95.1</cell><cell>76.8</cell><cell>88.6</cell></row><row><cell>FIDI [57]</cell><cell>77.6</cell><cell>95.7</cell><cell>-</cell><cell>-</cell><cell>86.8</cell><cell>94.5</cell><cell>77.5</cell><cell>88.1</cell></row><row><cell>SPAN [7]</cell><cell>68.9</cell><cell>94.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PVEN [37]</cell><cell>79.5</cell><cell>95.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CAL (ResNet50) [39]</cell><cell>74.3</cell><cell>95.4</cell><cell>56.2</cell><cell>79.5</cell><cell>87.0</cell><cell>94.5</cell><cell>76.4</cell><cell>87.2</cell></row><row><cell>DRL-Net [24]</cell><cell>-</cell><cell>-</cell><cell>55.3</cell><cell>78.4</cell><cell>86.9</cell><cell>94.7</cell><cell>76.6</cell><cell>88.1</cell></row><row><cell>AAformer [69]</cell><cell>-</cell><cell>-</cell><cell>63.2</cell><cell>83.6</cell><cell>87.7</cell><cell>95.4</cell><cell>80.0</cell><cell>90.1</cell></row><row><cell>TransReID* (ViT-Base) [18]</cell><cell>79.2</cell><cell>96.9</cell><cell>63.6</cell><cell>82.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeiT-Tiny</cell><cell>71.3</cell><cell>94.3</cell><cell>42.1</cell><cell>63.9</cell><cell>77.9</cell><cell>90.3</cell><cell>69.5</cell><cell>82.9</cell></row><row><cell>DeiT-Tiny + DCAL (Ours)</cell><cell>74.1</cell><cell>94.7</cell><cell>44.9</cell><cell>68.2</cell><cell>79.8</cell><cell>91.8</cell><cell>71.7</cell><cell>84.9</cell></row><row><cell>DeiT-Small</cell><cell>76.7</cell><cell>95.5</cell><cell>53.3</cell><cell>75.0</cell><cell>84.3</cell><cell>93.7</cell><cell>75.7</cell><cell>87.6</cell></row><row><cell>DeiT-Small + DCAL (Ours)</cell><cell>78.1</cell><cell>95.9</cell><cell>55.1</cell><cell>77.3</cell><cell>85.3</cell><cell>94.0</cell><cell>77.4</cell><cell>87.9</cell></row><row><cell>DeiT-Base</cell><cell>78.3</cell><cell>95.9</cell><cell>60.5</cell><cell>81.6</cell><cell>86.6</cell><cell>94.4</cell><cell>79.1</cell><cell>88.7</cell></row><row><cell>DeiT-Base + DCAL (Ours)</cell><cell>80.0</cell><cell>96.5</cell><cell>62.3</cell><cell>83.1</cell><cell>87.2</cell><cell>94.5</cell><cell>80.2</cell><cell>89.6</cell></row><row><cell>ViT-Base</cell><cell>78.1</cell><cell>96.0</cell><cell>61.6</cell><cell>81.4</cell><cell>87.1</cell><cell>94.3</cell><cell>78.9</cell><cell>89.4</cell></row><row><cell>ViT-Base + DCAL (Ours)</cell><cell>80.2</cell><cell>96.9</cell><cell>64.0</cell><cell>83.1</cell><cell>87.5</cell><cell>94.7</cell><cell>80.1</cell><cell>89.0</cell></row></table><note>. Performance comparisons on four Re-ID benchmarks: VeRi-776, MSMT17, Market1501, DukeMTMC. The input size is 256?128 for pedestrian datasets and 256?256 for vehicle datasets. * means results without side information for fair comparison.2.8% and the larger Transformer (ViT-Base) by 2.4% on MSMT17.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effect of the proposed two types of cross-attention learning on CUB-200-2011, VeRi-776 and MSMT17. We use DeiT-Tiny for CUB, ViT-Base for VeRi-776 and MSMT17 as baselines in this ablation experiment.</figDesc><table><row><cell>Baseline</cell><cell>5.5M</cell><cell>8.6G</cell><cell cols="2">82.1 81.6M</cell><cell>41.1G</cell><cell>78.1 96.0 81.6M</cell><cell>20.5G</cell><cell>61.6 81.4</cell></row><row><cell>+ GLCA</cell><cell>6.0M</cell><cell>8.8G</cell><cell cols="2">83.1 88.4M</cell><cell>42.4G</cell><cell>79.5 96.5 88.4M</cell><cell>21.3G</cell><cell>63.7 83.0</cell></row><row><cell>+ PWCA</cell><cell>5.5M</cell><cell>8.6G</cell><cell cols="2">83.1 81.6M</cell><cell>41.1G</cell><cell>79.2 96.5 81.6M</cell><cell>20.5G</cell><cell>62.8 82.3</cell></row><row><cell>Ours</cell><cell>6.0M</cell><cell>8.8G</cell><cell cols="2">84.6 88.4M</cell><cell>42.4G</cell><cell>80.2 96.9 88.4M</cell><cell>21.3G</cell><cell>64.0 83.1</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">CUB MSMT17 Acc mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell>82.1</cell><cell>61.6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ PWCA</cell><cell></cell><cell></cell><cell>83.1</cell><cell>62.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ Adding noise in I 1</cell><cell></cell><cell>77.3</cell><cell>56.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ Adding noise in label of I 1</cell><cell></cell><cell>81.6</cell><cell>60.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ I 2 from noise</cell><cell></cell><cell></cell><cell>82.1</cell><cell>62.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ I 2 from COCO</cell><cell></cell><cell></cell><cell>82.5</cell><cell>62.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ I 2 from intra-class only</cell><cell></cell><cell>81.7</cell><cell>62.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ I 2 from inter-class only</cell><cell></cell><cell>83.0</cell><cell>62.7</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">+ I 2 from intra-&amp; inter-class (1:1) 83.0</cell><cell>62.5</cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 4. Comparisons of different regularization methods. DeiT- Tiny is used for CUB and ViT-Base is used for MSMT17.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>-XS24 obtains 88.5% while our method obtains 89.7% top-1 accuracy on CUB. Swin-T obtains 84.9% while our method obtains 85.8% top-1 accuracy on CUB. For Re-ID on MSMT, Swin-T achieves 55.7% while we achieve 56.7% mAP. As locality has been incorporated by windows in Swin Transformer, we only apply PWCA into it.</figDesc><table><row><cell>Model</cell><cell cols="3">SA (%) GLCA (%) SA+GLCA (%)</cell></row><row><cell>DeiT-Tiny</cell><cell>84.4</cell><cell>83.6</cell><cell>84.6</cell></row><row><cell>DeiT-Small</cell><cell>87.6</cell><cell>87.4</cell><cell>87.6</cell></row><row><cell>DeiT-Base</cell><cell>88.7</cell><cell>88.5</cell><cell>88.8</cell></row><row><cell>ViT-Base</cell><cell>91.3</cell><cell>91.4</cell><cell>91.4</cell></row><row><cell>R50-ViT-Base</cell><cell>91.5</cell><cell>91.9</cell><cell>92.0</cell></row><row><cell></cell><cell cols="2">(a) CUB-200-2011</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">SA (%) GLCA (%) SA+GLCA (%)</cell></row><row><cell>DeiT-Tiny</cell><cell>89.2</cell><cell>87.8</cell><cell>89.4</cell></row><row><cell>DeiT-Small</cell><cell>92.4</cell><cell>91.8</cell><cell>92.3</cell></row><row><cell>DeiT-Base</cell><cell>93.9</cell><cell>93.5</cell><cell>93.8</cell></row><row><cell>ViT-Base</cell><cell>93.5</cell><cell>92.9</cell><cell>93.4</cell></row><row><cell>R50-ViT-Base</cell><cell>95.3</cell><cell>94.8</cell><cell>95.3</cell></row><row><cell></cell><cell cols="2">(b) Stanford-Cars</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">SA (%) GLCA (%) SA+GLCA (%)</cell></row><row><cell>DeiT-Tiny</cell><cell>86.9</cell><cell>86.7</cell><cell>87.4</cell></row><row><cell>DeiT-Small</cell><cell>90.1</cell><cell>89.8</cell><cell>90.0</cell></row><row><cell>DeiT-Base</cell><cell>92.5</cell><cell>92.3</cell><cell>92.6</cell></row><row><cell>ViT-Base</cell><cell>91.4</cell><cell>91.1</cell><cell>91.5</cell></row><row><cell>R50-ViT-Base</cell><cell>93.3</cell><cell>93.1</cell><cell>93.3</cell></row><row><cell></cell><cell cols="2">(c) FGVC-Aircraft</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on different inference architectures for FGVC in terms of accuracy. SA: using SA as the last layer to output class probabilities. GLCA: using GLCA as the last layer to output class probabilities. SA+GLCA: combine the output of SA and GLCA for inference.</figDesc><table><row><cell>Model</cell><cell>SA (%)</cell><cell cols="2">GLCA (%) SA+GLCA (%)</cell></row><row><cell cols="3">DeiT-Tiny 44.8 / 68.1 44.8 / 68.1</cell><cell>44.9 / 68.2</cell></row><row><cell cols="3">DeiT-Small 54.9 / 77.4 55.1 / 77.2</cell><cell>55.1 / 77.3</cell></row><row><cell cols="3">DeiT-Base 62.2 / 83.1 62.3 / 83.1</cell><cell>62.3 / 83.1</cell></row><row><cell>ViT-Base</cell><cell cols="2">63.9 / 83.2 63.9 / 83.1</cell><cell>64.0 / 83.1</cell></row><row><cell></cell><cell cols="2">(a) MSMT17</cell></row><row><cell>Model</cell><cell>SA (%)</cell><cell cols="2">GLCA (%) SA+GLCA (%)</cell></row><row><cell cols="3">DeiT-Tiny 71.6 / 85.1 71.7 / 84.9</cell><cell>71.7 / 84.9</cell></row><row><cell cols="3">DeiT-Small 77.4 / 88.0 77.4 / 87.8</cell><cell>77.4 / 87.9</cell></row><row><cell cols="3">DeiT-Base 80.2 / 89.9 80.2 / 89.6</cell><cell>80.2 / 89.6</cell></row><row><cell>ViT-Base</cell><cell cols="2">80.1 / 89.1 80.1 / 89.0</cell><cell>80.1 / 89.0</cell></row><row><cell></cell><cell cols="2">(b) DukeMTMC-ReID</cell></row><row><cell>Model</cell><cell>SA (%)</cell><cell cols="2">GLCA (%) SA+GLCA (%)</cell></row><row><cell cols="3">DeiT-Tiny 79.7 / 91.8 79.7 / 91.8</cell><cell>79.8 / 91.8</cell></row><row><cell cols="3">DeiT-Small 85.2 / 94.1 85.2 / 94.0</cell><cell>85.3 / 94.0</cell></row><row><cell cols="3">DeiT-Base 87.2 / 94.5 87.2 / 94.4</cell><cell>87.2 / 94.5</cell></row><row><cell>ViT-Base</cell><cell cols="2">87.5 / 94.8 87.5 / 94.7</cell><cell>87.5 / 94.7</cell></row><row><cell></cell><cell cols="2">(c) Market1501</cell></row><row><cell>Model</cell><cell>SA (%)</cell><cell cols="2">GLCA (%) SA+GLCA (%)</cell></row><row><cell cols="3">DeiT-Tiny 74.1 / 94.6 74.0 / 94.6</cell><cell>74.1 / 94.7</cell></row><row><cell cols="3">DeiT-Small 78.0 / 95.9 78.0 / 95.9</cell><cell>78.1 / 95.9</cell></row><row><cell cols="3">DeiT-Base 79.9 / 96.6 80.0 / 96.6</cell><cell>80.0 / 96.5</cell></row><row><cell>ViT-Base</cell><cell cols="2">80.1 / 96.9 80.2 / 96.9</cell><cell>80.2 / 96.9</cell></row><row><cell></cell><cell cols="2">(d) VeRi-776</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) SA vs. GLCA (b) SA vs. PWCA</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Orientation-aware vehicle re-identification with semantics-guided part attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ting</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="330" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Kumar Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise confusion for finegrained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Channel interaction networks for finegrained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Neng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transreid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04378</idno>
		<title level="m">Transformer-based object reidentification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rams-trans: Recurrent attention multi-scale transformer for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4239" to="4248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic partial swap: Enhanced model generalization and interpretability for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning disentangled representation implicitly via transformer for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02380</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantics-aligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11173" to="11180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combined depth space based architecture search for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6729" to="6738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Filtration and distillation: Enhancing region attention for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end comparative attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meibin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3492" to="3506" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond the parts: Learning multi-view cross-part correlation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stnreid: Deep convolutional networks with pairwise spatial transformer networks for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TMM</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2905" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjie</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing-based view-aware embedding network for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dechao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rethinking transformer-based set prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10881</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<idno>birds-200-2011 dataset. 2011. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High-order information matters: Learning relation and topology for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Guan&amp;apos;an Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Feature fusion vision transformer for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02341</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep coattention-based comparator for relative representation learning in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE T NEUR NET LEAR</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: person re-identification with fine-grained difference-aware pairwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fairmot: On the fairness of detection and reidentification in multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Vehiclenet: Learning robust visual representation for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Identity-guided human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="346" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Aaformer: Auto-aligned transformer for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaopan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00921</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
