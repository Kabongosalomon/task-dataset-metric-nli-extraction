<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Language Modeling with Sparse all-MLP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Efficient Language Modeling with Sparse all-MLP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks.</p><p>In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2? improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers have been the state-of-the-art architecture for natural language processing (NLP) tasks <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b25">Radford et al., 2018;</ref><ref type="bibr" target="#b27">Raffel et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020a)</ref>. Recently, architectures using solely MLPs (multi-layer perceptrons) have shown to be competitive, especially in computer vision tasks <ref type="bibr" target="#b33">(Tolstikhin et al., 2021;</ref><ref type="bibr" target="#b18">Liu et al., 2021b;</ref><ref type="bibr" target="#b13">Lee-Thorp et al., 2021;</ref><ref type="bibr" target="#b11">Hou et al., 2021;</ref><ref type="bibr" target="#b20">Lou et al., 2021)</ref>. In the field of NLP, recent work, like gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>, shows that all-MLP architecture can match Transformers <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> in language modeling perplexity, but there is still a gap in downstream performance.</p><p>In this paper, we aim to push the performance of all-MLPs in large-scale NLP pretraining <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b25">Radford et al., 2018;</ref><ref type="bibr" target="#b27">Raffel et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020a)</ref>. We analyze current all-MLP approaches, and find limitations in their expressiveness. Drawing on the recent success of Transformer-based Mixture-of-Experts (MoEs) <ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021;</ref><ref type="bibr" target="#b37">Yang et al., 2021)</ref>, we propose sparsely-activated all-MLPs as an alternative to address this limitations. Our proposed architecture -sparse MLP (sMLP for short) replaces major dense blocks in gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref> with sparse blocks. As a result, it improves the expressive power of gMLP while keeping the compute cost constant.</p><p>Specifically, we apply sparsity to the two fundamental operations that are common in NLP models:</p><p>? Hidden dimension operation: We adopt the experts in feed-forward layers as used in recent Transformerbased MoEs like the Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> and Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref>.</p><p>? Token-mixing operations: This is implemented as self-attention in Transformers and the Spatial Gating Unit in gMLP. We observe that naive extensions in the Transformer architecture, such as turning the selfattention module into experts, performs poorly. We design a new MoE module, named sMoE, to address this challenge, which chunks the hidden representations of the input through the hidden dimension, sends chunked vectors to different expert, and performs spatial projection inside each expert.</p><p>We provide an in-depth analysis of the routing mechanisms in the sMoE module, where we point out that a less careful design can easily lead to information leaking when routing weights are learnt from future tokens. In the context of language modeling, the router makes use of future tokens during training, while at the inference time, the model needs to predict the tokens in an autoregressive fashion without looking into the future. To solve this problem, we propose two routing strategies : (1) Deterministic Routing: instead of learning the routing strategy, we directly send hidden vectors to each expert by cutting the hidden dimension. It arXiv:2203.06850v3 [cs.CL] 31 May 2022</p><p>has an indirect connection with multi-head attention mechanism <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref>, where each expert serves as one head;</p><p>(2) Partial Prediction: instead of learning the routing strategy from the whole sentence, we only use the first 20% of tokens for learning the routing strategy and use it to predict the remaining 80%.</p><p>To summarize, our contributions are as follows:</p><p>? We propose a sparsely activated all-MLP architecture, named sMLP. To the best of our knowledge, this is the first NLP work combining all-MLP-based models with MoEs. We provide an in-depth analysis of why the MLP architecture lags behind Transformers in terms of expressiveness and identify two core challenges in turning MLPs into sparsely activated MoEs. The proposed sMLP architecture addresses these challenges with a novel sMoE module and two routing strategies.</p><p>? We empirically evaluate its performance on language modeling and compare with strong baselines of both sparse Transformer-based MoEs such as Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref>, Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref>, Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> and HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> as well as dense models including Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> and gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>. Our sMLP outperforms these models in terms of valid perplexities while obtaining up to 2 ? improvement in pretraining speed with the same compute budget (shown in <ref type="figure">Fig. 1</ref>). In addition, sMLP demonstrates good scalability where it still outperforms sparse Transformers counterparts when we scale sMLP to 10B parameters with a large pretraining corpus of 100B tokens.</p><p>? Finally, we evaluate its zero-shot priming performance after pretraining on language modeling. Through head-to-head comparison, the proposed sparsely-activated all-MLP language model outperforms sparsely-activated Transformers on six downstream tasks ranging from natural language commonsense reasoning to QA tasks. Compared with dense Transformers such as GPT-3 <ref type="bibr" target="#b4">(Brown et al., 2020b</ref>), our sMLP model achieves similar zero-shot performance despite being pretrained on a much smaller dataset (3 ? smaller than the dataset used by GPT-3) with less computing resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Token-wise Operations</head><p>Transformers and all-MLPs perform token-mixing operations in different ways. The former uses self-attention <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref>, while the latter uses Spatial Gating Unit <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>.  <ref type="figure">Figure 1</ref>. The proposed sparse all-MLP architecture (sMLP) achieves 2? training efficiency improvement compared to state-ofthe-art sparse Transformer-based MoEs: Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref>, Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref>, Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref>, HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref>. Full comparison is provided in <ref type="figure" target="#fig_1">Fig. 5</ref>.</p><p>Given the input token representation X ? R T ?H , where T represents the sequence length and H represents the hidden dimension. We set h to be the total head number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self attention module</head><p>The multi-head self attention module is a concatenation of token-mixing operations from each head:</p><formula xml:id="formula_0">Y = Concat (M 1 , . . . , M h ) with M i = Softmax(XW Q i XW K i )XW V i (1)</formula><p>where Y is the output, W Q i , W K i , W V i ? R H?d are projection matrices. d is the hidden dimension for input to a single head d = H/h. Spatial Gating Unit gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref> designs the Spatial Gating Unit to replace the self attention module. Within each head,</p><formula xml:id="formula_1">Y = W s X<label>(2)</label></formula><p>, where W s ? R T ?T . Note that, W s corresponds to the attention score instead of W V in Equation (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sparse Expert Models</head><p>Sparse expert models provide a highly efficient way to scale neural network training. Compared to the standard dense models that require extremely high computational cost in training, sparse expert models are shown to be able to deal with a much larger scale of data and converge significantly faster. In sparse models, model weights are distributed to different workers according to the MoE mechanism <ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021;</ref><ref type="bibr" target="#b37">Yang et al., 2021</ref> which leverages a routing algorithm to dispatch each token to k sub feed-forward modules. These sub feed-forward modules are allocated independently on different devices, which are also called experts. MoE allows processing input tokens in parallel specified by a gating function to achieve maximum computational efficiency.</p><p>MoE Routing Let {E i (x)} N i=1 be a set of N experts for a token representation x. <ref type="bibr" target="#b32">Shazeer et al. (2017)</ref> proposed a MoE layer that learns to dispatch the token representation to the best determined top-k experts, selected from a set {E i (x)} N i=1 of N experts. The router variable W r produces logits h(x) = x ? W r which are normalized via a softmax distribution over the available N experts at that layer. The gate-value for expert i is given by,</p><formula xml:id="formula_2">p i (x) = e h(x)i N j=1 e h(x)j<label>(3)</label></formula><p>The top-k gate values are selected for routing the token x.</p><p>If T is the set of selected top-k indices then the output computation of the layer is the linearly weighted combination of each expert's computation on the token by the gate value,</p><formula xml:id="formula_3">y = i?T p i (x)E i (x)<label>(4)</label></formula><p>Given that each token will be sent to different devices (experts), unbalanced loading adds heavy affects the efficiency of the model. The Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> designed a differential load balancing loss to reduce communication latency of sending tokens to different devices. Base</p><p>Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> further simplify the framework by eliminating the balancing loss, adapting the balancing assignment algorithm <ref type="bibr" target="#b1">(Bertsekas, 1992)</ref> to send tokens from each batch to different devices equally. Instead of learning the routing weight W r , HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> use a random hashing function as the routing gate. In order to distinguish this standard Transformer-MoE routing <ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021;</ref><ref type="bibr" target="#b37">Yang et al., 2021)</ref> from ours, we call this method tMoE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>A straightforward extension of gMLP to MoE structure hurts performance, which we provide a detailed analysis in Appendix A. Instead of sending tokens to different experts, in Section 3.1 we propose a novel sMoE module to send hidden vectors to different experts. We further propose two corresponding routing strategies in Section 3.2.1 and Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sparsely-activated all-MLP</head><p>The overall architecture of sMLP is illustrated in <ref type="figure">Fig. 2</ref>. Our sMLP model contains N 1 dense blocks and N 2 sparse blocks. Both N 1 and N 2 are hyper-parameters. In each sparse block, it contains two modules: (i) tMoE module: we adopt the MoE from Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> to replace the FFN module in dense Transformers <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref>; (ii) sMoE module: we design this sMoE module to replace the self attention module in the Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> and the Spatial Gating Unit in gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>;</p><p>Both tMoE and sMoE blocks contain two elements:</p><p>Expert Modules These are the modules that process the input. The tMoE module contains an FFN in each expert. For our sMoE module, each expert contains the Spatial Gating Unit as is shown in <ref type="figure">Fig. 6</ref> </p><formula xml:id="formula_4">(Right) in Appendix A.</formula><p>Gating Function This is the module that decides which expert should process each part of the input. tMoE uses standard token-wise routing (described in Section 2.2). However, naively applying token-wise routing to gMLP is flawed, as it sends tokens from the same sentence to different experts. Tokens can only attend to previous tokens in the same device, which means that as the number of experts increases, the number of previous tokens that each word can attend will decrease. Refer to Appendix A for more details. For that reason, we have to design a distinct routing method to extend the MoE structure to the feature dimension. x ij denotes the value of the j th hidden dimension in the i th tokens. The tMoE sends these four tokens to these three experts at the FFN layer using a learnt gating function described in Eq.(3) parameterized by W r ? R 4?3 .</p><p>Unlike these existing MoEs, in sparse all-MLP architecture we propose to chunk hidden representation along the hidden dimension and send chunked vectors to different experts, as is shown in <ref type="figure" target="#fig_0">Fig. 3</ref> (Right). We next discuss the design of the gating function for this new routing mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Routing in Feature Space</head><p>Compared to routing tokens, routing hidden dimensions faces a unique challenge in autoregressive models, with information leaking from looking ahead at future tokens if done naively (more details can be found in Appendix B.). Furthermore, unlike Transformers-based MoEs with selfattention, appropriate masking cannot be directly applied to prevent information leaking. Due to the aforementioned problems, we cannot adopt existing routing methods in Transformers-based MoEs for language modeling. We propose and compare the following two solutions: deterministic routing and partial prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">DETERMINISTIC ROUTING</head><p>In order to decide where to send different hidden vectors, we need to learn a gating weight W r ? R T ?N . This gating weight inevitably needs to multiply the hidden vector v   <ref type="figure" target="#fig_0">Figure 3</ref>. Left: tMoE Gating: sending 4 tokens to 3 experts; Right: sMoE Gating: sending 4 hidden vectors to 3 experts.</p><formula xml:id="formula_5">x x x x x x x x x x x x x x x x router gpu0 SL2 gpu1 FFN1 gpu2 FFN3 SL1 gpu2 SL3 router ff1</formula><formula xml:id="formula_6">x x x x x x x x x x x x x x x x router gpu0 SL2 gpu1 FFN1 gpu2 FFN3 SL1 gpu2 SL3</formula><p>to obtain the routing probability p i (v). This vector is one specific hidden dimension of the whole sentence, including future tokens.</p><p>In order to prevent the gating weights from exploiting future token information, which is not available at inference time, our first approach removes W r directly. Instead, we chunk the vector in a hidden dimension and send hidden vectors to experts deterministically. In this case, if we want to send the hidden vector V ? R H?T to N expert modules, we will chuck h(v) i ? R H/N ? T from hidden vector V . If i equals to 0, the gating will send the first to H/N hidden vectors to E 1 (v).</p><p>Unlike the Spatial Gating Unit, our attention-free sMoEdeterministic module splits the hidden dimensions among multiple experts. In each expert module, we insert one Spatial Gating Unit, similar to multi-head attention, where each expert serves as one head. The total number of parameters is N * T * T where N is the number of experts/heads. Moreover, with each head being fitted to one device, the computation is parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">PARTIAL PREDICTION</head><p>Another way to prevent the gating weight W r from using future token information is partial prediction. Instead of predicting all the tokens in the sentence, we condition on the first 20% of tokens, which are used to decide the routing, and predict the remaining 80% of tokens.</p><p>Given the input token representation X ? R T ?H , we divide it into two parts in the token dimension: the first 20% of tokens X 1 ? R 0.2T ?H , and the remaining 80% of tokens X 2 ? R 0.8T ?H . Instead of training the language model on the whole sequence length T , we only train it to predict X 2 . We use X 1 to learn the gating weights W r . In sMoE, we transpose theinput</p><formula xml:id="formula_7">X 1 to V 1 ? R H?0.2T , input X 2 to V 2 ? R H ? 0.2T . The router variable W r ? R 0.2T ?N produces h(V 1 ) = V 1 * W r . V 1 contains H hidden vectors v i ? R 0.2T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines Model</head><p>Dense Baselines Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref> Sparse Baselines</p><p>Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020</ref>) Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref>    <ref type="table">Table 3</ref>. Token-wise Operations Comparison: we compare our method with the previous two token-wise operations regarding parameters and FLOPs by each operation.</p><p>The probability for sending i th hidden vector is learned by</p><formula xml:id="formula_8">p i (v) = e h(v)i N j e h(v)j<label>(5)</label></formula><p>Different from the previous method, after learning the probability p i (v), this partial prediction method sends hidden vectors v i ? R 0.8T from V 2 to the expert module instead of hidden vectors from V 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Baselines We compare to strong baselines consisting of state-of-the-art dense and sparse models, as is summarized in <ref type="table" target="#tab_4">Table 1</ref>. We train all our baselines (except GPT3 from paper) and our model in PyTorch <ref type="bibr" target="#b24">(Paszke et al., 2017)</ref> using FAIRSEQ . Our experimental settings are summarized in <ref type="table" target="#tab_5">Table 2</ref>. (i) we compare to all baselines in a regular setting. In order to control for FLOPs, we change the number of layers in the model. More concretely, since Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref> and Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> have larger FLOPs values for the same model structure, we reduce their number of layers while keeping the same hidden dimension to achieve the same FLOPs. (ii) we also conduct experiments at larger scale and data size to test its scalability. We use pretraining datasets from RoBERTa  and the English portion of CC100 <ref type="bibr" target="#b6">(Conneau et al., 2019)</ref>. More details can be found in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings and Dataset</head><p>Metrics <ref type="formula">(</ref> Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref> 3.48B 19.26 27.54 136k Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> 3.48B 15.31 20.67 185k Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> 3.60B 18.16 21.13 178k HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> 3 Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref> 9.03B 14.5 198 115k Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> 9.03B 10.45 120 139k Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> 10.31B 20.36 (divergence) 115.13 135k HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> 10   Implementation Our sMLP model contains N 1 dense blocks and N 2 sparse blocks. We follow Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> setting: insert sparse blocks after the (N1+N2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N2+1</head><p>. . . N2(N1+N2) N2+1 th dense layers. For sparse blocks, it contains one tMoE module and one sMoE module. We adopt tMoE modules from Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref>. For our sMoE module, in each expert module, we insert one Spatial Gating Unit. Among our baselines, Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> and HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> adopt the same approach to arranging sparse layers. In contrast, Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref> and Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> insert a sparse module at every other layer. More details of model architectures and implementation details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Token-wise operations comparison</head><p>In this section, we compare our model with two dense models: Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> and gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>. The main difference between the all-MLP-based and the Transformer-based models is the token-wise operation. We compare these three kinds of token-wise operations: the self-attention module in Transformers, the Spatial Gating Unit in gMLP, and the sMoE module in our model.</p><p>In <ref type="table">Table 3</ref>, we compare these three token-wise operations and their respective head mechanisms. To extend multi-head mechanism into gMLP mode, instead of using one W s in Equation <ref type="formula" target="#formula_1">(2)</ref>, we chunk the input X into h parts along hidden dimension, and each part is multiplied by a separate W s . We set head number h to be 16 to compare Spatial Gating Unit from gMLP with Self-attention module from Transformer. Then the total number of parameters is h * T * T . Compared to Transformer self attention, a shortcoming of the Spatial Gating Unit is that the number of parameters increases linearly with the number of heads h and quadratically with sequence length T . For comparison, our s-MoE module set one expert on each device, and each expert only contains one W s , which is equal to heads number 1. In <ref type="figure">Fig. 4</ref>, we compare our model with the dense model with different heads number. The Transformer model greatly benefits from the multi-head mechanism. However, although the gMLP model increases the parameter amount, it does not improve performance through the multi-head mechanism. Our model can also be seen as a solution for gMLP's multi-head. Our model dramatically improves the performance of the all-MLP-based model and also outperforms the Transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Sparse MLP</head><p>We report quality (valid perplexity) and training efficiency in <ref type="figure" target="#fig_1">Fig. 5</ref>, measured by number of training steps (Top) and training time (Bottom). We found that sMLP with both variants of routing strategies outperforms state-of-the-art Transformer-based MoE models with roughly the same amount of FLOPs. We point out that sample efficiency observed on a step basis does not necessarily translate into better model quality as measured by the wall clock due to two reasons. First, our model has additional all2all communication cost for sMoE module. Second, Base Layers, HASH Layers, and our model send tokens/vectors to the expert module in a balanced manner. In contrast, Gshard and Switch Transformer leverage a loss function to balance the assignment, which is not an equally balanced assignment. Although they have the same FLOPs, unbalanced loading can add extra computing time. <ref type="figure" target="#fig_1">Fig. 5 (Bottom)</ref> shows that given the same training time, our model achieves the best results (lowest valid perplexity), indicating that our sparse all-MLP models improved training efficiency over state-of-the-art Transformer-based MoE models on language modeling.</p><p>Table 4 (Top) summarizes the detailed comparison in the main experiments. We control the FLOPs of all models to be roughly 0.8T. Except that the number of model layers is different, their embedding dimension is 1024, and the hidden dimension is 4096. We can see that our model achieves the best generalization at 25k training steps and in the meanwhile achieves the highest training speed. HASH Layers has the best performance among all Transformer baselines and requires the least time, which is consistent with results reported in <ref type="bibr" target="#b30">Roller et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on Scalability</head><p>In order to test the scalability of our model, we increase the model size training for 2.0 TFLOPs.  <ref type="table" target="#tab_10">Table 4</ref> (Top), we enlarged all of the models, changing the embedding from 1024 to 2048, and adjusting the hidden dimension from 4096 to 8192 as reported in <ref type="table" target="#tab_11">Table 5</ref>. We also increased the pretraining data size as described in <ref type="table" target="#tab_5">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared to models in</head><p>In this setting, we found Switch Transformer to remain a strong baseline. Although Base Layers and HASH Layers perform well when trained on small models and datasets, we find stability issues on large-scale training. Although previous papers <ref type="bibr" target="#b8">(Fedus et al., 2021;</ref><ref type="bibr" target="#b14">Lepikhin et al., 2020)</ref> have controlled the FLOPs for model comparison, we found the FLOPs-match Switch Transformer has fewer parameters than our model. Therefore, to provide a more fair comparison, we also trained an enlarged version of Switch Transformer (by adding one additional sparse layer and dense layer) with 10.31B parameters which also increases its FLOPs to 2.3T. Our sMLP still outperforms Switch Transformer-Enlarge despite the latter having more FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Zero-shot Priming</head><p>The previous experiments demonstrate sMLP's strong performance in language modeling. In this section, we evaluate whether these gains in pretraining translate to improved generalization in downstream tasks. Specifically, we measure zero-shot in-context learning capability as is shown in GPT-3 <ref type="bibr" target="#b4">(Brown et al., 2020b)</ref>.  <ref type="table">Table 6</ref>. Zero-shot priming evaluation: we provide head-to-head comparison of our sMLP model with FLOPs-matched state-of-the-art sparse Transformers on six representative NLP tasks evaluated in GPT-3 in-context learning <ref type="bibr" target="#b4">(Brown et al., 2020b)</ref>. Specifically, we control for training data, batch size, learning rate and training steps for all these models. As a reference point, we also compare to the performance of a FLOPs-matched GPT3 model (dense Transformer) reported in <ref type="bibr" target="#b4">Brown et al. (2020b)</ref>. Note that the GPT3 model from the paper uses larger training dataset (about 3 ? larger).</p><p>Baselines Our baselines are Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref>, Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref>, Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> and HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref> with roughly 2.0 TFLOPs from Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks and datasets</head><p>We select six representative NLP tasks which probes commonsense reasoning and question answering, including COPA <ref type="bibr" target="#b29">(Roemmele et al., 2011)</ref>, PIQA <ref type="bibr" target="#b2">(Bisk et al., 2020)</ref>, StoryCloze <ref type="bibr" target="#b21">(Mostafazadeh et al., 2016)</ref>, Winogrande <ref type="bibr" target="#b15">(Levesque et al., 2012)</ref>, HellaSwag <ref type="bibr" target="#b38">(Zellers et al., 2019)</ref>, ReCoRD <ref type="bibr" target="#b39">(Zhang et al., 2018)</ref>. We use a language model to separately score each label choice using the same templates, and pick the one with the highest score as <ref type="bibr" target="#b0">Artetxe et al. (2021)</ref>. More details about tasks and datasets can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>As is shown in <ref type="table">Table 6</ref>, sMLP outperforms all sparse Transformers in terms of average accuracy. Notable improvements come from commonsense reasoning tasks such as COPA, StoryCloze and HellaSwag. We also compared to a FLOPs-matched dense Transformer reported in the GPT-3 paper <ref type="bibr" target="#b4">(Brown et al., 2020b)</ref>, which served as the strongest dense baseline. It is worth noting that the GPT-3 model was trained with more pre-training data (GPT-3 used 300 billion tokens for pre-training and our pre-training data contains 100 billion tokens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Mixture of Experts (MoE) was shown to be effective in <ref type="bibr" target="#b32">Shazeer et al. (2017)</ref>, where an MoE layer was stacked between LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> layers. More recently, Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref> extended this idea to Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> feed-forward layers and provided a way for parallel computing, which scaled up multilingual neural machine translation with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic parameter sharding.</p><p>Several recent work improves the routing strategy in Transformer-based MoEs. Switch Transformer <ref type="bibr" target="#b8">(Fedus et al., 2021)</ref> shows the design can be simplified by routing tokens to only one single expert (top-1 routing). In addition, they design a differentiable load balancing loss to reduce communication latency of sending tokens to different devices. Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> further simplifies the framework by eliminating the balancing function, adapting the balancing assignment algorithm <ref type="bibr" target="#b1">(Bertsekas, 1992)</ref> to send tokens from each batch to different devices equally. Instead of learning the gating weights for token assignment, HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021</ref>) design a Hash function to replace the learnable gating weights. <ref type="bibr" target="#b12">(Jaszczur et al., 2021)</ref> is the first one successfully extending sparsity to attention layers. However, their results could just match dense transformer and did not get improvements in training speed. Different from all of the above routing methods, our sMoEdeterministic method (described in Section 3.2.1) chunk vectors by deterministic assignment.</p><p>There is also a growing body of work studying the overall design and scaling of Transformer-based MoE models. <ref type="bibr" target="#b37">Yang et al. (2021)</ref> explores key factors inside sparse expert models and investigates how they affect the model quality and computational efficiency. <ref type="bibr" target="#b5">Clark et al. (2022)</ref> studied the scaling laws of MoE models in terms of effective parameter count and compared three routing strategies. <ref type="bibr" target="#b41">Zoph et al. (2022)</ref> investigated several training details which affects training stability and finetuning performance. In addition to NLP, <ref type="bibr" target="#b28">Riquelme et al. (2021)</ref> firstly applied MoEs in the Computer Vision field.</p><p>A closely related work is <ref type="bibr" target="#b20">Lou et al. (2021)</ref>, which developed all-MLP-based MoEs for computer vision. When applied to NLP, there are additional challenges in designing the sparsely activated token-mixing operations by preserving the sequential nature of the input, which would otherwise lead to information leaking as is shown in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed sMLP, extending the recent gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref> model with sparsely activated conditional computation using mixture-of-experts (MoEs). Different from Transformer-based MoEs, which only contains sparse feed-forward layers while still keeping dense computation for self-attention, the proposed architecture is fully sparse. We analyzed the challenges in designing routing strategies of sparse all-MLP architecture for language modeling and proposed two solutions. Through extensive evaluations on language modeling, we show that sMLP outperforms stateof-the-art sparse Transformer-based MoE models in terms of generalization and 2? improvement in training efficiency. Besides gains in pretraining, sMLP also achieves higher accuracy in zero-shot in-context learning of six representative NLP tasks, closing the gap of all-MLP architecture with Transformers as was observed in gMLP. It is worth noting that all our discussions about routing challenges and solutions are based on autoregressive language modeling. In the future, we hope to try all-MLP based MoE method with encoder-only models with bidirectional attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Direct applying token-wise routing to gMLP</head><p>The most straightforward way to extend gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref> to mixture-of-experts (MoEs) is to apply the same token-routing used in Transformer-based MoEs <ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021)</ref>, as is illustrated in <ref type="figure">Fig. 6</ref> in the case of two experts (GPUs).  <ref type="figure">Figure 6</ref>. Left: one layer of gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>; Right: directly extend gMLP to token-level conditional computation commonly used in Transformer MoEs <ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021)</ref> with two experts (GPUs).  <ref type="table">Table 7</ref>. Extending gMLP to token-based MoEs hurts performance compared to dense gMLP. Furthermore, increasing the number of experts worsens the performance. <ref type="table">Table 7</ref> shows that this approach, despite being simple and straightforward, has poor generalization. Since the Spatial Gating Unit is inserted into each device, and the tokens from the same sentence will be sent to different devices. That means tokens can only attend to previous tokens which were sent to the same device. This problem will be more severe as we increase expert (device) numbers. As the number of experts increases, the probability of sending tokens to the same expert from one sentence will decrease, so the number of tokens that each word can attend will decrease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gating analysis for the sMLP module</head><p>MoE layers take token representation X ? R T ?H , where T represents the sequence length and H represents the hidden dimension. In the sMoE, we transpose input to the hidden vector V ? R H * T . A gating function routes this to the best top-k experts, selected from a set {E i (v)} N i=1 of N experts. The router variable W r ? R T ?N produces h(V ) = V W r which are normalized via a softmax distribution over the available N experts at that layer. V contains H hidden vector v i ? R T . The probability for sending i th hidden vector is given by</p><formula xml:id="formula_9">p i (v) = e h(v)i N j e h(v)j<label>(6)</label></formula><p>The top-k gate values are selected for routing the hidden vector v. If T is the set of selected top-k indices then the output computation of the layer is the linearly weighted combination of each expert's computation on the token by the gate value,</p><formula xml:id="formula_10">y = i?N p i (v)E i (v)<label>(7)</label></formula><p>Through learning the gating weight W r , the MoE model routes hidden vectors to different experts. <ref type="figure">Fig. 7</ref> shows the results by this method (sMLP in the <ref type="figure">figure)</ref>.</p><p>We can see that this method achieves the lowest perplexity. However, this is due to information leaking from future tokens in learning W r . Specifically, the gating weight for sMoE is W r ? R T ?N , which is trained after seeing all the tokens in the sentence, especially looking ahead at future tokens. In this way, this gating can making "smartest" decision. This results in information leaking for autoregressive language models. Unlike self attention model, where a diagonal mask could be applied to attention scores A ? R T ?T to prevent information leaking from future tokens, gating weights W r ? R T ?N cannot use a valid mask. Due to the aforementioned problems, we cannot adopt existing MoE gating methods for routing hidden vectors in autoregressive language models. Therefore, we propose two improved methods sMLP -deterministic (in Section 3.2.1) and sMLP -partial (in Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More implementation details C.1. Choice of baselines</head><p>The dense counterpart is gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a)</ref>. We adapted the balanced assignment routing method proposed in Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref> for the tMoE implementation. Thus, the closest Transformer-based sparse models is Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref>. We also compared to the dense Transformer <ref type="bibr" target="#b36">(Vaswani et al., 2017b)</ref> and other state-of-the-art Transformer MoE models (Gshard <ref type="bibr" target="#b14">(Lepikhin et al., 2020)</ref>, Base Layers <ref type="bibr" target="#b16">(Lewis et al., 2021)</ref>, HASH Layers <ref type="bibr" target="#b30">(Roller et al., 2021)</ref>) as listed in <ref type="table" target="#tab_4">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Layers HASH Layers</head><p>Gshard Switch Transformer sMLP sMLP --deterministic sMLP --partial <ref type="figure">Figure 7</ref>. Compared sMLP method with sMLP -deterministic (Section 3.2.1) and sMLP -partial (Section 3.2.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Training</head><p>Small Model Configurations   Large model configuration  <ref type="table" target="#tab_10">Table 4</ref>.</p><p>Dense model configuration <ref type="table" target="#tab_17">Table 9</ref> shows the model architecture we use in Section 4.2.</p><p>All the models are trained on 32 Nvidia 32G V100 GPUs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Datasets</head><p>The RoBERTa contains five English-language corpora of varying sizes and domains:</p><p>? BOOKCORPUS <ref type="bibr" target="#b40">(Zhu et al., 2015)</ref> plus English WIKIPEDIA. This is the original data used to train BERT. (16GB).</p><p>? CC-NEWS, which collected from the English portion of the CommonCrawl News dataset <ref type="bibr" target="#b22">(Nagel, 2016)</ref>. The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering)</p><p>? OPENWEBTEXT <ref type="bibr" target="#b9">(Gokaslan &amp; Cohen, 2019)</ref>, an open-source recreation of the WebText corpus described in <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).</p><p>? STORIES, a dataset introduced in <ref type="bibr" target="#b34">(Trinh &amp; Le, 2018)</ref> containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transformer vs. gMLP</head><p>We compared the Transformer <ref type="bibr" target="#b35">(Vaswani et al., 2017a)</ref> with the gMLP <ref type="bibr" target="#b17">(Liu et al., 2021a</ref>) model architecture in <ref type="figure" target="#fig_3">Fig. 8</ref>.</p><p>When comparing these two model structures, the FFN module is the same. The differences exist in the token-wise operations. The Transformer leverages the self attention module to calculate the relations between different tokens. However, the gMLP model leverages the Spatial Gating Unit to replace the self attention module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Downstream Task Descriptions</head><p>We evaluate on six representative tasks covering commonsense reasoning and question answering, including:</p><p>? COPA (Choice of Plausible Alternatives) <ref type="bibr" target="#b29">(Roemmele et al., 2011</ref>) is a causal reasoning task in which a system is given a premise sentence and must determine either the cause or effect of the premise from two possible choices. All examples are handcrafted and focus on blogs and photography-related encyclopedia topics.</p><p>? PIQA (PhysicalQA) <ref type="bibr" target="#b2">(Bisk et al., 2020</ref>) is a question answering task which asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world.</p><p>? StoryCloze <ref type="bibr" target="#b21">(Mostafazadeh et al., 2016)</ref> involves selecting the correct ending sentence for five-sentence long stories.</p><p>? Winogrande The Winograd Schemas Challenge <ref type="bibr" target="#b15">(Levesque et al., 2012)</ref> is a classical task in NLP that involves determining which word a pronoun refers to when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently finetuned language models have achieved near-human performance on the original Winograd dataset, but more complex versions adversarially-mined Winogrande dataset <ref type="bibr" target="#b31">(Sakaguchi et al., 2020)</ref> still significantly lag human performance. We test our performance on Winogrande.</p><p>? HellaSwag dataset <ref type="bibr" target="#b38">(Zellers et al., 2019)</ref> involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models.</p><p>? ReCoRD (Reading Comprehension with Commonsense Reasoning Dataset) <ref type="bibr" target="#b39">(Zhang et al., 2018</ref>) is a multiple-choice QA task. Each example consists of a news article and a Cloze-style question about the article in which one entity is masked out. The system must predict the masked out entity from a list of possible entities in the provided passage, where the same entity may be expressed with multiple different surface forms, which are all considered correct. Articles are from CNN and Daily Mail.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 (</head><label>3</label><figDesc>Left) shows an example of the gating function from existing Transformer-based MoEs<ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b16">Lewis et al., 2021;</ref><ref type="bibr" target="#b30">Roller et al., 2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Top: Valid Perplexity v.s. Training Steps; Bottom: Valid Perplexity v.s. Training Time. We compare our two methods (sMLP-deterministic and sMLP-partial) with sparse Transformer MoEs with 0.8T FLOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Model architecture comparison: Transformer v.s. gMLP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Baselines: we compare our sMLP with dense Transformers and MLPs as well as sparse Transformer-based MoEs.</figDesc><table><row><cell cols="3">Experiments FLOPs Model Size</cell><cell>Datasets</cell></row><row><cell>Main Experiments</cell><cell>0.8T</cell><cell>3.50B</cell><cell>1/10 data from RoBERTa dataset (Liu et al., 2019)</cell></row><row><cell>Scalability Experiments</cell><cell>2.0T</cell><cell>9.41B</cell><cell>? 100B tokens. RoBERTa and the English subset of the CC100 corpus (Conneau et al., 2019)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Experimental Settings: In Section 4.3, we run our small model (3.50B parameters) with FLOPs per batch tokens 0.8T on the small dataset (1/10 of the RoBERTa dataset); In Section 4.4, we run our large model (9.41B parameters) with FLOPs per batch tokens 2.0T on the whole RoBERTa dataset plus CC100 corpus.</figDesc><table><row><cell>Modules</cell><cell cols="3">heads params.(M) FLOPs (G)</cell></row><row><cell>Self-attention</cell><cell>16</cell><cell>4.198</cell><cell>12.885</cell></row><row><cell>Self-attention</cell><cell>1</cell><cell>4.198</cell><cell>12.885</cell></row><row><cell>Spatial Gating Unit</cell><cell>16</cell><cell>16.798</cell><cell>4.316</cell></row><row><cell>Spatial Gating Unit</cell><cell>1</cell><cell>1.054</cell><cell>4.316</cell></row><row><cell>s-MoE</cell><cell>1</cell><cell>1.058</cell><cell>4.387</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>A sparsely activated model splits unique weights on different devices. Therefore, the weights of the model increase with the number Comparison with Dense Models: We compare our model (the orange dashed line) with dense models (solid lines), including both Transformer (Vaswani et al., 2017a) and gMLP (Liu et al., 2021a), with different heads. Our model (sMLP) only has one head activated on each device.of devices, all while maintaining a manageable memory and computational footprint on each device. Previous work<ref type="bibr" target="#b14">(Lepikhin et al., 2020;</ref><ref type="bibr" target="#b8">Fedus et al., 2021;</ref><ref type="bibr" target="#b37">Yang et al., 2021)</ref> controls the floating point operations (FLOPs) per example for model comparison. We utilize the fvcore library 1 to measure the floating point operations (FLOPs) per batch tokens during evaluation for model comparison. Since all models contain exactly the same tokens in each batch, we actually controls FLOPs per token.</figDesc><table><row><cell></cell><cell cols="3">Transformer 1 head</cell><cell cols="4">Transformer 16 heads</cell></row><row><cell></cell><cell>gMLP 1 head</cell><cell cols="3">gMLP 16 heads</cell><cell cols="3">sMLP-determinstic</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Valid Perplexity</cell><cell>16 18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>steps (k)</cell><cell></cell><cell></cell></row><row><cell>Figure 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1). Language modeling perplexity: We report validation perplexity as the indicator of language modeling performance.(2). Training efficiency: For measuring training speed and efficiency, we compare by both number of updates and word-per-second (WPS). (3). Accuracy: we report accuracy on downstream tasks to measure performance of zero-shot priming.</figDesc><table><row><cell>Models</cell><cell>Model Size (Parameters)</cell><cell>Quality after 25k steps (Perplexity ?)</cell><cell>Time to Quality (hours ?)</cell><cell>Speed (world per second ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Head-to-head comparison measures per step and per time benefits of the sMLP (our model) over baselines. We report perplexity (lower better) and time to reach (lower better) as quality and training efficiency measures. All models are trained with the same amount of computation</figDesc><table><row><cell>Models</cell><cell>FLOPs (T)</cell><cell>Model Size (M)</cell><cell>Embedding Dim</cell><cell>Hidden Dim</cell><cell>Sparse Layers</cell><cell>Dense Layers</cell><cell>Num of Heads</cell><cell>Num of Experts</cell><cell>Num of GPUs</cell></row><row><cell>Gshard</cell><cell>2.0</cell><cell>9.03</cell><cell>2048</cell><cell>8192</cell><cell>7</cell><cell>7</cell><cell>16</cell><cell>64</cell><cell>32</cell></row><row><cell>Switch Transformer</cell><cell>2.0</cell><cell>9.03</cell><cell>2048</cell><cell>8192</cell><cell>7</cell><cell>7</cell><cell>16</cell><cell>32</cell><cell>32</cell></row><row><cell>Base Layers</cell><cell>2.0</cell><cell>10.31</cell><cell>2048</cell><cell>8192</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>32</cell></row><row><cell>HASH Layers</cell><cell>2.0</cell><cell>10.29</cell><cell>2048</cell><cell>8192</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>32</cell></row><row><cell>Switch Transformer -Enlarge</cell><cell>2.3</cell><cell>10.31</cell><cell>2048</cell><cell>8192</cell><cell>8</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>32</cell></row><row><cell>sMLP -deterministic (our model)</cell><cell>2.0</cell><cell>9.41</cell><cell>2048</cell><cell>8192</cell><cell>7</cell><cell>22</cell><cell>1</cell><cell>32</cell><cell>32</cell></row></table><note>and hardware. In order to test parameter-matched models, we train an enlarged Switch Transformer with 2.0T FLOPs. Top: Small models (FLOPs 0.8T). Bottom: Scaled-up models (FLOPs 2.0T). More model details are described in Table 5.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Scaled-up Model details: We control FLOPs to the same level by adjusting the number of layers of the model. Since our model is based on gMLP (Liu et al., 2021a), more dense layers are needed, and only one head is needed for each device. While controlling FLOPs value, we found that only the Switch Transformer (Fedus et al., 2021) can be compared with our sMLP among all the baselines. With the same amount of FLOPs value, our model has more parameters than the Switch Transformer; then, we train a Switch Transformer -Enlarge model for comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 (</head><label>4</label><figDesc>Bottom) summarizes the results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Table 8shows the the detailed model architecture used in the main experiments. Through observation, we noticed that all the models (except for Gshard) drop negligibly after 15k. In addition, there is a small amount of fluctuation for the Base Layers after 25k. Then we choose 25k for model comparison inTable 4.</figDesc><table><row><cell>Models</cell><cell cols="2">Base Layers sMLP-deterministic</cell></row><row><cell>Expert Parameters</cell><cell>100.75 M</cell><cell>113.44 M</cell></row><row><cell>Non Expert Parameters</cell><cell>241.79 M</cell><cell>258.73 M</cell></row><row><cell>FLOPs</cell><cell>0.83 T</cell><cell>0.83 T</cell></row><row><cell>Expert Modules per MoE layer</cell><cell>32</cell><cell>32</cell></row><row><cell>Number of MoE layers</cell><cell>12</cell><cell>12</cell></row><row><cell>FFNs per Expert Module</cell><cell>1</cell><cell>1</cell></row><row><cell>Spatial Gating Unit per Expert Module</cell><cell>1</cell><cell>1</cell></row><row><cell>Embedding Size</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Hidden Dimension</cell><cell>4096</cell><cell>4096</cell></row><row><cell>Heads</cell><cell>16</cell><cell>1</cell></row><row><cell>Number of Dense Layers</cell><cell>12</cell><cell>28</cell></row><row><cell>Context Length</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Batch Size</cell><cell>4</cell><cell>4</cell></row><row><cell>Gradient Accumulation</cell><cell>4</cell><cell>4</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>ddp-backend</cell><cell>no c10d</cell><cell>no c10d</cell></row><row><cell>optimizer</cell><cell>adam</cell><cell>adam</cell></row><row><cell>adam-betas</cell><cell>(0.9, 0.98)</cell><cell>(0.9, 0.98)</cell></row><row><cell>weight-decay</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>lr</cell><cell>5e-4</cell><cell>5e-4</cell></row><row><cell>lr-scheduler</cell><cell>inverse sqrt</cell><cell>inverse sqrt</cell></row><row><cell>warmup-updates</cell><cell>4000</cell><cell>4000</cell></row><row><cell>warmup-init-lr</cell><cell>1e-7</cell><cell>1e-7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8 .</head><label>8</label><figDesc>Model architectures used in the main experiments in Section 4.3.</figDesc><table><row><cell>Models</cell><cell>Transformer</cell><cell>gMLP</cell><cell>sMLP-deterministic</cell></row><row><cell>Expert Parameters</cell><cell>0 M</cell><cell>0 M</cell><cell>258.73M</cell></row><row><cell>Non Expert Parameters</cell><cell>354.76 M</cell><cell>353.87 M</cell><cell>113.44M</cell></row><row><cell>FLOPs</cell><cell>0.83 T</cell><cell>0.83 T</cell><cell>0.83 T</cell></row><row><cell>Sparse Layers</cell><cell>0</cell><cell>0</cell><cell>12</cell></row><row><cell>Dense Layers</cell><cell>24</cell><cell>41</cell><cell>28</cell></row><row><cell>Embedding Size</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Hidden Dimension</cell><cell>4096</cell><cell>4096</cell><cell>4096</cell></row><row><cell>Heads</cell><cell>16</cell><cell>1</cell><cell>1</cell></row><row><cell>Context Length</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Batch Size</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Gradient Accumulation</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>Dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>ddp-backend</cell><cell>no c10d</cell><cell>no c10d</cell><cell>no c10d</cell></row><row><cell>optimizer</cell><cell>adam</cell><cell>adam</cell><cell></cell></row><row><cell>adam-betas</cell><cell>(0.9, 0.98)</cell><cell>(0.9, 0.98)</cell><cell>(0.9, 0.98)</cell></row><row><cell>weight-decay</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>lr</cell><cell>5e-4</cell><cell>5e-4</cell><cell>5e-4</cell></row><row><cell>lr-scheduler</cell><cell cols="2">inverse sqrt inverse sqrt</cell><cell>inverse sqrt</cell></row><row><cell>warmup-updates</cell><cell>4000</cell><cell>4000</cell><cell>4000</cell></row><row><cell>warmup-init-lr</cell><cell>1e-7</cell><cell>1e-7</cell><cell>1e-7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 .</head><label>9</label><figDesc>Small dense model architectures used in Section 4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>shows the the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 10 .</head><label>10</label><figDesc>Large model architectures used in the scalability experiments in Section 4.4 and Section 4.5.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">FLOPs are calculated for the forward pass in https:// github.com/facebookresearch/fvcore</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficient large scale language modeling with mixtures of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pasunuru</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10684</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auction algorithms for network flow problems: A tutorial introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational optimization and applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="66" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reasoning about physical commonsense in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unified scaling laws for routed language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01169</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openwebtext</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corpus</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Vision permutator: A permutable mlplike architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12368</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse is enough in scaling transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaszczur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanerva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9895" to="9907" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Mixing tokens with fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gshard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16668</idno>
		<title level="m">Scaling giant models with conditional computation and automatic sharding</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Base layers: Simplifying training of large, sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16716</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02008</idno>
		<title level="m">Sparse-mlp: A fullymlp architecture with conditional computation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A corpus and evaluation framework for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01696</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cc-News</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://commoncrawl.org/2016/10/newsdataset-available." />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairseq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<title level="m">A fast, extensible toolkit for sequence modeling</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scaling vision with sparse mixture of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05974</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hash layers for large sparse models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04426</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An adversarial winograd schema challenge at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winogrande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8732" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploring sparse expert models and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15082</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hellaswag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<title level="m">Can a machine really finish your sentence? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<title level="m">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Designing effective sparse expert models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08906</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
