<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relation Modeling in Spatio-Temporal Action Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">THUIBCS</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
							<email>jianwen.jjw@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
							<email>qingzhiwu.qzw@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
							<email>mingqian.tmq@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<email>gaoyue@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">THUIBCS</orgName>
								<address>
									<settlement>BNRist</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relation Modeling in Spatio-Temporal Action Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our solution to the AVA-Kinetics Crossover Challenge of ActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of relation modeling methods for spatio-temporal action detection and adopts a training strategy to integrate multiple relation modeling in end-to-end training over the two large-scale video datasets. Learning with memory bank and finetuning for long-tailed distribution are also investigated to further improve the performance. In this paper, we detail the implementations of our solution and provide experiments results and corresponding discussions. We finally achieve 40.67 mAP on the test set of AVA-Kinetics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Spatio-temporal action localization aims to localize atomic actions of people in videos with 3D bounding boxes, which has attract large efforts in recent years <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref>. Generally, there are two main factors showing fundamental influence on the performance of this task, i.e. video backbones and relation modeling. The design of video networks has been widely studied <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> and greatly enhance the performance of downstream tasks. Besides, pretraining such networks on large-scale networks is also demonstrated to be effective <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16]</ref>, e.g. pretrain on Ki-netics700 <ref type="bibr" target="#b1">[2]</ref>. And there are also multiple ways to perform the pretraining, such as supervised pre-training <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref> as used in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>, and unsupervised ones <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>. For relation modeling, different approaches has been studied in the fields of computer vision <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>, social networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> and nature language processing <ref type="bibr" target="#b21">[22]</ref>. Specifically, transformed-based relation modeling has been proved for improving the spatio-temporal localization task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this paper, we investigate multiple types of relation modeling methods for spatio-temporal action localization. Inspired by previous works, an off-the-shelf person detector are employed first to generate all human bounding boxes in the videos. Then we adopt a backbone model to extract visual features and build a relation module upon the feature maps of each person via roi align <ref type="bibr" target="#b6">[7]</ref>. After relation module, an action predictor is used to generate score for each action category. The whole pipeline of our solution is shown in 1.</p><p>In following sections, we first detail the implementation of our method. Then the experimental results and corresponding discussions are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>In this section, we present our approach for relation modeling in spatio-temporal action localization. Firstly, we introduce our overall pipeline for this task. Then relation modeling module is presented with transformer-based architectures to capture the relations among persons in the spatial and temporal dimensions. Furthermore, we adapt memory bank for storing person features along the temporal context of video clips to model long-range relations. Different strategies of online or offline maintaining memory bank on the AVA-Kinetics Crossover are studied. Finally, we investigate learning approaches for the lone-tailed category distribution in the AVA-Kinetics dataset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overall Pipeline</head><p>Our designed pipeline is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Given the input video clip, the key frame of this clip is extracted and fed into a 2D person detector to generate bounding boxes of persons inside this clip. The whole video clip is sampled into frames in specified interval and encoded with an video backbone, e.g. SlowFast <ref type="bibr" target="#b3">[4]</ref> and CSN <ref type="bibr" target="#b20">[21]</ref>, to output a 3D video feature map. Then the 2D person boxes are inflated along the temporal dimension and used for extracting person features from the feature map with 3D RoI-Align <ref type="bibr" target="#b6">[7]</ref>. The pooled person features are further fused together with the video feature map via channel-wise concatenation and convolution layers. To model the hidden relations among persons inside the same video clip for improving the effectiveness of action prediction, we fed the person features into our relation modeling module with transformerbased blocks. To specify the spatial and temporal relations, we select features along the same spatial or temporal dimension from different persons. The selected features are flatten to a sequence of tokens and fed into a transformer encoder block to model their relations via attention mechanism. Finally, the output tokens of all block in spatial or temporal dimensions are globally averaged and fed into full-connected layers to predict the action classes for each detected person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Person Relation Modeling</head><p>The input person features to relation modeling module are 3D feature maps denoted as P i ? R T ?H?W , where i is the person index. Such person features are firstly transferred into sequential tokens as the input of transformer encoder block. To effectively model the relations along spatial and temporal dimension while maintain low computation cost, two types of relation head, i.e. S-only and T-only, are proposed to extract relations separately on each dimension. For S-only head, we generate HW sequences of input tokens. The sequence corresponding to spatial posi-</p><formula xml:id="formula_0">tion (h, w) is {agg T t=1 (P i t,h,w ) | i ? I},</formula><p>where agg(?) indicates an aggreagation function, e.g. average pooling or max pooling, and I is the index set of persons. Similarly, T-only head generate T sequences of input tokens, where the sequence corresponding to temporal position t</p><formula xml:id="formula_1">is {agg H h=1 W w=1 (P i t,h,w ) | i ? I}.</formula><p>Each sequence of tokens are then fed into a transformer encoder block for relation modeling. It is noted that transformer blocks for different spatial or temporal positions share the same weights. The output of transformer blocks are then averaged along different spatial or temporal positions into the final representation of persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Memory Bank</head><p>Maintaining feature memory bank for storing and utilizing representations along long term context has been demonstrated to be effective strategies for this task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref>. We also adapt the feature bank, which saves our pooled feature features and provides previously stored person features of timestamps within a long-range of current video clip. The loaded features are concatenated and fed into the relation modeling module.</p><p>Existing methods use online maintaining strategy of memory bank, which continually updates the stored features of current video during the training stage. However, for the AVA-Kinetics Crossover, such an online strategy is hard to be implemented since there is only one officially annotated clip of each video in Kinetics and the remaining clips will not be reached in the training stage. To address this issue, we design a two-stage training strategy. In the first stage, the non-annotated clips in Kinetics are not considered. We either train only on the AVA dataset or maintain an empty memory bank for the Kinetics dataset. In the second stage, we extract and store features of all clips in Kinetics, freeze the weights of backbone and finetune the relation head and classifier on both AVA and Kinetics. Besides, we also investigate the strategy that training without memory bank in the first stage, and retrain another head with memory bank in the second stage. Comparison of different strategies are shown in experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Long-tailed Learning</head><p>There exists obvious long-tailed category distribution in the original AVA dataset, which leads to the challenge of learning those classes with less number of samples. With the join of Kinetics annotated data, the long-tailed distribution still remains a large problem for this task.</p><p>To perform a more suitable training, we consider the decoupling strategy from <ref type="bibr" target="#b10">[11]</ref>. The training process is decoupled into two stages. Stage one follows the normal training strategy with randomly sampled data. While in stage two, we freeze all the models except the final classifier and train with class-balanced data sampling. Such a strategy helps to improve the performance on small classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Settings</head><p>Person Detector. We adopt GFocalV2 <ref type="bibr" target="#b13">[14]</ref> as the person detector. We first train the model on the subset of person category from COCO Dataset <ref type="bibr" target="#b14">[15]</ref>. Then we continue training the model on the AVA-Kinetics dataset from COCO-Person pre-training.</p><p>Backbone. We adopt CSN152 <ref type="bibr" target="#b20">[21]</ref>, slowfast101 <ref type="bibr" target="#b3">[4]</ref>, slowfast152 <ref type="bibr" target="#b3">[4]</ref> as the visual feature extractors. We first train the model on the training set of Kinetics700 <ref type="bibr" target="#b1">[2]</ref> dataset and then use the weight to initial the backbone part of our pipeline.</p><p>Heads. We train with four types of heads to predict actions. The linear head simply use full-connected layers as the baseline head. For relation heads, considering the size of features from S-only head will make it changeable to maintain memory bank , we train S-only without memory bank and T-only with/without memory bank.</p><p>Training and Inference. During the training process, we concatenate the data list of AVA and Kinetics for mixed learning. We train with SGD optimizer, initial learning rate as 1e-2, batch size as 64, weight decay as 1e-7 and total training iterations as 30k. All the batchnorm layers are frozen in training. We do linear warmup in the first 1.5k iterations, drop the learning rate by 0.66 at iteration 13.5k, 18k, 22.5k and 27k. The input videos are resized with minimum side of 256 and maximum side of 464. Color jitter and box jitter are used for data augmentation. During the inference process, we test with three scales {256, 288 and 320} and horizontal flips. <ref type="table">Table 1</ref> shows our main results on AVA-Kinetics. We report results of models using different backbones, pretrained datasets, input formats relation heads and memory banks. Our best single model achieves 37.95 mAP and 35.26 mAP on AVA and Kinetics, respectively, and 38.43 mAP on AVA-Kinetics Crossover. For backbones, ir-CSN-152 achieves better performance compared with SlowFast-152, which may attribute to that ir-CSN-152 is pretrained with smaller  downscale of 1/16, while SlowFast-152 is 1/32. For the pretrained dataset, pretraining on Kinetics700 dataset generally improves 1 mAP compared with those pretrained on Kinetics400. We also investigate results with different temporal resolutions, and it is shown that increasing the temporal resolution ?2 could improve around 0.6 mAP. For S-only and T-only relation heads, they both achieve great improvement compared with linear head and similar results to each other. T-only head with memory bank increase mAP of about 0.8 mAP. Among those models, we select 15 models and ensemble their results with average voting. The ensembled result achieves 40.97 mAP on the validation set of AVA-Kinetics, and 40.67 mAP on the test set. It is noted that even without training on the validation set, we only drop 0.3 mAP compared with the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Studies and Discussions</head><p>Influence of memory bank strategies. We train the three different strategies of maintain memory bank on AVA-Kinetics. As shown in <ref type="table">Table 2</ref>, only training on AVA in the first stage and finetuning on AVA-Kinetics could achieve satisfying performance on AVA, but the results on Kinetics will be poor since the backbones are not fully trained on this dataset. Training head without memory bank on both AVA and Kinetics in the first stage could improve the result on Kinetics in a large margin. However, this strategy must re-initialize the weights of head in the second stage and may lose some valuable training efforts. The final strategy trains on AVA-Kinetics on two stages and maintain empty bank in the first stage, which achieve the best performance and and used in our final version.</p><p>Influence of decoupled learning. We compare the results of all classes before and after the decoupled classbalanced finetuning. The classes are ranked by their number of samples, and we list the averaged difference of the top-20 and bottom-20 classes. As shown in <ref type="table" target="#tab_2">Table 3</ref>, there is an improvement of 1.05 mAP of those small classes. At the same time, the performance of the top-20 classes almost do not drop. We also report classes with largest changes after decoupled training. It is noted that some of the small classes, e.g. cut, could get great improvement, while there also ex-classes mAP@0.5 diff.  <ref type="table">Table 4</ref>. Influence of person detector on AVA and Kinetics validation sets. The column of AP@0.5 shows the detection results of different detectors. Results using GT bounding boxes are listed for reference. All the results are trained with ir-CSN-152 backbone and T-only head, and are tested with 3 scales and horizontal flips. "*" notes the results of det-v3, which has similar AP@0.5 to det-v2 but achieves better performance when measuring AP@0.5:0.95.</p><p>ists classes with dropped performance. This may attribute to the overfitting of those classes with increased training times, which could be further improved.</p><p>Influence of human detector. Here we have also investigate how much the improvement of the detector per-formance will gain the final action detection performance. Three detection boxes with 77.0%, 81.7%, 82.3% AP on Kinetics subset of AVA-Kinetics, respectively, are applied in our pipeline. As <ref type="table">Table 4</ref> shown, when the performance of the detector increases from 77% to 81.7%, the final performance can obtain significant improvement of about 1 mAP on both AVA and Kinetics parts of the dataset. If the GT boxes are applied, the performance of action detector will reach 43.5 mAP and 48.5 mAP respectively on the two subset. Moreover, even the improvements of human detector are similar (95.1 v.s. 95.1 and 81.7 v.s. 82.3 on AVA and Kinetics subset respectively), the final action detector also obtains corresponding gains of performance. The results demonstrate that human detector is one of bottleneck modules in the action detection pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Pipeline of our approach for spatio-temporal action localization. 'S' and 'T' indicate operation along the spatial and temporal dimensions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Comparison of different training strategies for memory bank on AVA-Kinetics. All models are trained with SlowFast-152 backbone and T-only head. Results are tested with 3 scales and horizontal flips. "+M(A)" and "+M(K)" indicate maintaining online memory bank of AVA and Kinetics during the training stage. "A" indicates training with AVA only, and "A+K" indicates training jointly with AVA-Kinetics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Influence of applying decoupled training for lone-tail learning. Results of ir-CSN-152+T-only on AVA dataset are used for comparison. Classes are ranked by their numbers of labeled samples. Averaged results of the top-20 and bottom-20 classes, top-3 improved classes and bottom-3 improved classes are listed.</figDesc><table><row><cell></cell><cell></cell><cell>top-20 -0.16</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">bottom-20 + 1.05</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>all-60 + 0.14</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">cut + 22.57</cell><cell></cell></row><row><cell></cell><cell></cell><cell>write + 4.93</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">text on cellphone + 3.66</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>get up -3.93</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">give/serve to -4.12</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">work on computer -8.85</cell><cell></cell><cell></cell></row><row><cell>detector</cell><cell cols="4">AVA AP@0.5 mAP@0.5 AP@0.5 mAP@0.5 Kinetics</cell></row><row><cell>det-v1</cell><cell>94.5</cell><cell>35.72</cell><cell>77.0</cell><cell>32.70</cell></row><row><cell>det-v2</cell><cell>95.1</cell><cell>36.16</cell><cell>81.7</cell><cell>33.84</cell></row><row><cell>det-v3</cell><cell>95.1*</cell><cell>36.41</cell><cell>82.3</cell><cell>34.80</cell></row><row><cell>GT</cell><cell>-</cell><cell>43.53</cell><cell>-</cell><cell>48.49</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by Alibaba Group through Alibaba Innovative Research Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<title level="m">Multiscale vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6047" to="6056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Self-supervised motion learning from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Ang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00240</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human centric spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei Zhang4 Yunkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ActivityNet Workshop on CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic hypergraph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2635" to="2641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The ava-kinetics localized human actions video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghana</forename><surname>Thotakuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/2005.00214, 2020. 1</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12885,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Actor-context-actor relation network for spatiotemporal action localization. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13141</idno>
		<title level="m">Changxin Gao, and Nong Sang. Temporal context aggregation network for temporal action proposal refinement</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tacnet: Transition-aware context network for spatio-temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11987" to="11995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asynchronous interaction aggregation for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019-10-27" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cbr-net: Cascade boundary refinement network for action detection: Submission to activitynet challenge 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiteng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongpeng</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07526</idno>
	</analytic>
	<monogr>
		<title level="m">Shiwei Zhang, and Nong Sang</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Longterm feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
