<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProCAN: Progressive Growing Channel Attentive Non- Local Network for Lung Nodule Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mundher</forename><surname>Al-Shabi</surname></persName>
							<email>mundher.al-shabi@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Systems Engineering Discipline</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Monash University Malaysia</orgName>
								<address>
									<postCode>47500</postCode>
									<settlement>Bandar Sunway</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Shak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Systems Engineering Discipline</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Monash University Malaysia</orgName>
								<address>
									<postCode>47500</postCode>
									<settlement>Bandar Sunway</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical and Computer Systems Engineering Discipline</orgName>
								<orgName type="department" key="dep2">School of Engineering</orgName>
								<orgName type="institution">Monash University Malaysia</orgName>
								<address>
									<postCode>47500</postCode>
									<settlement>Bandar Sunway</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Oklahoma</orgName>
								<address>
									<postCode>73019</postCode>
									<settlement>Norman</settlement>
									<region>OK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mundher</forename><surname>Al-Shabi</surname></persName>
						</author>
						<title level="a" type="main">ProCAN: Progressive Growing Channel Attentive Non- Local Network for Lung Nodule Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method. lung nodules are very small and benign and malignant nodules sometimes look similar and hard to distinguish from one another.</p><p>In recent years, deep learning methods have been shown to significantly outperform the previous hand-crafted feature based approaches due to their ability to extract useful features automatically without relying on hand-crafted features <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. However, their performance for lung nodule classification is still not at the same level as their performance on general classification tasks (e.g., ImageNet).</p><p>A main challenge of the lung nodule classification task is that nodule sizes vary greatly, i.e., between 3 to 30 millimeters (mm) in size <ref type="bibr" target="#b6">[7]</ref>. This huge size variation combined with the heterogeneity of the nodules makes the task of classifying them <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref> a very challenging one. A recent study suggested using patches of multiple sizes and implementing multiple feature extractors for each nodule size <ref type="bibr" target="#b10">[10]</ref>. However, this solution increases the number of weights and parameters, which is problematic when we train the parameters on small public datasets, such as LIDC-IDRI <ref type="bibr" target="#b6">[7]</ref>.</p><p>Instead of varying the patch sizes [10], we suggested using a Non-Local Network [11] to capture global features that are suitable for middle to big-sized nodules combined with Resnet blocks [12] to capture local features that are suitable for smaller nodules. The combination of Non-Local layers and Resnet blocks produces better results as both global and local features are extracted for the nodule classification task <ref type="bibr" target="#b9">[9]</ref>. However, the Non-Local operator does not provide channel-wise attention, which is crucial [13] for feature extraction. Therefore, in this study, we propose a new Channel Attentive Non-Local (CAN) network, which generalizes the Non-Local operator to provide channel attention as well as spatial attention.</p><p>Another challenge is the trade-off between training a deep network to extract high-level features and the difficulties of training deep networks, such as the vanishing gradient problem. Moreover, the bigger/deeper the network, the higher the required memory and training time. In a recent study, <ref type="bibr" target="#b14">[14]</ref> proposed a method called ProGAN to grow the deep network gradually, whereby the network learns low-level features first before discovering high-level features. In this way, the network will need less memory and will converge faster as it is kept small for most training cycles. However, adding new layers during training disturbs the process, at the same time slowing network convergence <ref type="bibr" target="#b14">[14]</ref>. Thus, in this paper, we propose to progressively introduce new layers using a novel blending algorithm called the Blending algorithm.</p><p>We also observed in our previous study [8] that middle-sized nodules (i.e., between 5 to 12 mm) are very challenging to classify. On the other hand, smaller and bigger nodules are easier to classify as benign or malignant <ref type="bibr" target="#b8">[8]</ref>. Therefore, to help the deep learning network to learn more effectively, we can gradually and progressively increase the difficulty of the tasks as we grow the network using a method called Curriculum Learning [15]. This method is suitable for lung nodule classification as there are nodules that are easy to classify even by non-complex models, whereas other nodules require complex models to classify correctly <ref type="bibr" target="#b8">[8]</ref>. Thus, we first train the network on easier tasks (i.e., easy nodules) using a shallow network; then, we increase the Curriculum's difficulty by training the network on difficult tasks (i.e., hard nodules). Once the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cancer is one of the most deadly diseases, whereby one in every six deaths is due to cancer <ref type="bibr" target="#b0">[1]</ref>. Most cancer deaths were caused by lung cancer (i.e., <ref type="bibr" target="#b18">18</ref>.4% of all deaths). Furthermore, lung cancer is the most frequently diagnosed cancer (i.e., 11.6% of all cancer cases) <ref type="bibr" target="#b0">[1]</ref>. Recent studies <ref type="bibr" target="#b1">[2]</ref> show that we can reduce lung cancer mortality by examining patients with Low Dosage Computed Tomography (LDCT) in early stages <ref type="bibr" target="#b2">[3]</ref> to capture the cancer heterogeneity in a non-invasive way. However, the task of manually diagnosing patients is extremely timeconsuming for the radiologist as every CT scan consists of multiple slices. Furthermore, many Curriculum gets harder, we increase the network capability by increasing its depth using the new Blending algorithm.</p><p>To summarize, in this work, we propose a new Progressively Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification, whereby we add channel attention to the Non-Local Network and propose a new method to increase the depth of the network and the difficulty of the examples gradually. Our main contributions are summarized as follows:</p><p>1) The proposed ProCAN model generalizes the Non-Local operator to provide channel attention and spatial attention and integrates them into one block.</p><p>2) We enhance the ProGAN gradually growing algorithm by introducing a new matrix in the blending algorithm.</p><p>3) We use Curriculum Learning with the nodule diameter and radiologist' ratings criteria, whereby we train the network on easy nodules first before the hard ones.</p><p>4) The proposed ProCAN model evaluated on the public LIDC-IDRI and LUNGx datasets achieved state-of-the-art performances for both datasets in benign/malignant lung nodule classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The Non-Local network was first introduced by <ref type="bibr" target="#b11">[11]</ref> for video classification and its purpose is to increase the receptive field of the Convolutional Neural Network (CNN) <ref type="bibr" target="#b16">[16]</ref> to as large as the input image by considering a weighted sum of all generated features in the whole video. <ref type="bibr" target="#b17">[17]</ref> proposed the Transformer, which is based on self-attention operations. The Non-Local operator is closely related to the Transformer <ref type="bibr" target="#b17">[17]</ref>, which is utilized for language models, but in a twodimensional (2D) state. Both Transformers and Non-Local networks compute dot-products between the features (see Equations 1-6 in Section 3.1). However, they differ in how these features are represented. For example, in Transformers, the attention is between different words, whereas for Non-Local networks, it is between pixels.</p><p>In recent studies, <ref type="bibr" target="#b9">[9]</ref> suggested using regular CNN alongside the Non-Local network as the former is suitable for extracting local features only. Therefore, combining Non-Local networks to extract global features combined with CNNs achieves better performance than just regular convolutional models. The Squeeze and Excitation network presented by <ref type="bibr" target="#b13">[13]</ref> proposed aggregating the feature maps (Squeeze), followed by a Fully-Connected layer and multiplying the aggregated features to the input (Excitation).</p><p>Fu et. al <ref type="bibr" target="#b18">[18]</ref> presented their Dual Attention Network that extends the non-local design paradigm for channel attention to spatial attention. The Dual-Attention Network uses two separate and independent attention blocks for channel and spatial attention. Although both Dual Attention and CAN use channel attention, there are three main differences between the two designs: First, Dual Attention uses two separate and independent attention modules, whereas in CAN, the channel attention depends on the spatial attention. Second, CAN uses fewer attention maps to perform the same task as Dual Attention, as shown in <ref type="table" target="#tab_0">Table 1</ref>. CAN's parameter-efficient design is important for tasks like lung nodule classification, whereby labeled data is hard to acquire. Third, CAN uses CNNs to solve the permutation equivariant problem in the Non-Local design. With nodule classification being the final stage of CAD schemes for lung cancer, it is not surprising that research is being performed extensively in this field. This might be one of the most important steps in the end-to-end automated schemes for lung cancer prediction in CT scans to classify whether an extracted nodule is malignant or benign.</p><p>With the popularity of the non-local paradigm rising, several attempts have been made in the literature incorporating non-local networks. In our recent studies <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, we observed that the main difficulty in classifying lung nodules as benign or malignant is due to the large variation in nodule sizes. Therefore, we presented a new method <ref type="bibr" target="#b9">[9]</ref> to address this challenge by using nonlocal mechanisms to extract global features with the help of ResNet blocks to extract local features, and showed that this improved the classification task significantly. In our recent study <ref type="bibr" target="#b8">[8]</ref>, we proposed another model that revolves around a CNN <ref type="bibr" target="#b16">[16]</ref> that utilizes two different dilations and a novel gating sub-network to guide the features between the two dilations.</p><p>Other authors such as <ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref> have incorporated models to learn 3D lung nodule characteristics. In a recent study, <ref type="bibr" target="#b19">[19]</ref> suggested using a 3D version of the Non-Local network with a Dual-Path network and an ensemble of all the models. <ref type="bibr" target="#b20">[20]</ref> performed similar methods of implementing a 3D Dual Path Network to extract feature maps, which then go through the ensembling method of a Gradient Boosting Machine. By considering nine different views of nodule patches, <ref type="bibr" target="#b21">[21]</ref> constructed knowledge-based collaborative submodels for each of the views to enhance their classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Channel attentive non-local network</head><p>In this section, we introduce a new Channel Attentive Non-Local (CAN) Network design block as shown in <ref type="figure" target="#fig_0">Figure 1</ref> and describe its operation. Firstly, given an input ? ? ? ? where ? ? denote channel, height, and width sizes respectively, we reshaped it to ? ? ? whereby is the product of and . Then we linearly transform into three feature spaces ? ? ? ? , ? ? ? ? and ? ? ? as shown in equations (1) to (3) below:</p><formula xml:id="formula_0">, = , ,<label>(1)</label></formula><formula xml:id="formula_1">, = , ,<label>(2)</label></formula><formula xml:id="formula_2">, = , ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">? ? ? ? , ? ? ? ?</formula><p>and ? ? ? are learnable parameters and ? is the number of channels in and . The operations in equations (1) to (3) are simple matrix multiplications between matrices and , which can be implemented efficiently using convolution with a kernel size of one. Then, we create a spatial attention matrix ? ? ? by applying the function to the product of and as shown in equations (4) and (5) below: ? denotes matrix multiplication, * denotes convolution and ? denotes a multiplication between a vector and a matrix as described in Equation <ref type="bibr" target="#b6">(7)</ref>.</p><formula xml:id="formula_4">, = , ,<label>(4)</label></formula><formula xml:id="formula_5">, = ( , )<label>(5)</label></formula><p>For every spatial features in we have attention values whereby , attends to the ith spatial feature for the jth output location. Finally, we multiplied the attention matrix with the feature space in equation <ref type="bibr" target="#b5">(6)</ref> as follows:</p><formula xml:id="formula_6">, = , ,<label>(6)</label></formula><p>where A = (a c1 , a c2 , ? , a C in ) and V = (v c1 , v c2 , ? , v C in ). Every value in the a c ? ? has been calculated from all values in v c ? ? , while represents the spatial attention of each value. Note that the same values in are applied over all channels in V, namely the attention mechanism does not change from one channel to another. Hence, the attention mechanism here is called spatial attention as the attention varies across different positions (or pixels) in the image, but the same attention is applied across all channels. Compared with the conventional convolutional neural network (CNN) <ref type="bibr" target="#b16">[16]</ref>, whereby the receptive field is limited to the kernel size (typically, 3?3, 5?5, 7?7, etc.), the Non-Local Network's receptive field has the same dimensions as the input . Thus, we applied the concept of the Non-Local Network in our CAN, which captures long-range dependencies and computes the attention (or response) at a pixel as a weighted sum of the features at all pixels.</p><p>The Non-Local Network only considers spatial (or position) based attention to the image. Thus, to extend the attention mechanism to the channels of the image, we introduced and applied a novel Channel Attentive operation on A with residual connection coming from :</p><formula xml:id="formula_7">, = , + ,<label>(7)</label></formula><p>In contrast with B, the spatial attention matrix, ? ? is the attention vector for channels where is the attention value for the channel in A. Similar to the spatial attention, we used the Non-Local paradigm to generate . To do so, we first created a feature space ? ? by multiplying with a learnable vector ? ? :</p><formula xml:id="formula_8">= ,<label>(8)</label></formula><p>We then applied the function to the product of and , to limit the values to range between zero and one as follows:</p><formula xml:id="formula_9">= ( , )<label>(9)</label></formula><p>The attention mechanism here is a novel channel attention mechanism that we propose in this work.</p><p>is the attention mechanism for different channels and is an extension of the Non-Local spatial attention paradigm applied to channel attention.</p><p>The channel attention, is multiplied with the spatial attention matrix, . Then, at the end of the CAN layer, we add this result to the input, to obtain matrix, . The matrix is reshaped from ? ? to ? ? ? and passed through a 3x3 convolution with a activation function:</p><formula xml:id="formula_10">= ( * )<label>(10)</label></formula><p>where ? ? ? ? ? and is the number of output channels. Applying the twodimensional (2D) 3x3 convolution allows us to control the number of the output channels and the spatial dimension. Moreover, adding a local operator like the 2D convolution to the Non-Local design block increases the accuracy of the CAN network, which has been shown and experimented in multiple works <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b22">22]</ref>. By incorporating both local and global (i.e., non-local) layers to the network architecture, both local and global features can be extracted in an image, which have been shown to be very useful for lung nodule classification in our previous study <ref type="bibr" target="#b9">[9]</ref>. This is mainly because lung nodule classification is a very challenging task as lung nodules have diverse shapes and sizes <ref type="bibr" target="#b23">[23]</ref>. Global feature extraction through the Non-Local Network is required to extract features that can describe the nodule shape and size, whereas local features are important to pay attention to details including the nodule density and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Curriculum learning method for hard and easy-to-classify nodules</head><p>Inspired by humans and animals, in curriculum learning, the training examples are ordered in a meaningful way <ref type="bibr" target="#b15">[15]</ref>. Remarkably, the examples are ordered from easy to hard where the model is trained first on easy examples before the hard ones. Such a strategy helps the optimizer to find a better local minima and converge faster <ref type="bibr" target="#b15">[15]</ref>.</p><p>We propose to train the model on the easy examples first, and we stop the training if the accuracy of the next epoch is lower than the accuracy of the last three epochs:</p><formula xml:id="formula_11">+ &lt; ( , ? , ? )<label>(11)</label></formula><p>where is the validation accuracy for the epoch. The validation set is independent of the training and testing sets, whereby we sampled 10% of the training set for the validation set.</p><p>For our lung nodule classification task, there are two criteria we can use to classify an example as either easy or hard. The first criterion is based on the median score from the four radiologists (see Section 4.1.1), whereby on a scale of 1 to 5, 1 indicates that a nodule is benign and 5 that it is malignant. The easy examples are the ones that are clearly benign (rating 1) or clearly malignant (rating 5) [24]:</p><formula xml:id="formula_12">= { , = = ,<label>(12)</label></formula><p>where is the median rating of the training example. This criterion assumes that what is not clearly benign or malignant from the radiologists' perspective would be hard for Artificial Intelligence (AI) to classify. However, this assumption can be challenged by many examples in the real-world, whereby AI can solve many tasks easily that have been recognized as hard by domain experts and vice versa <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b26">26]</ref>.</p><p>The second criterion is based on the diameter of the nodule. Usually, small nodules (i.e., &lt;5mm) are benign, whereas big nodules (i.e., &gt;12mm) are malignant <ref type="bibr" target="#b8">[8]</ref>. The classifier can be easily trained to distinguish these nodules (in fact, by applying a simple thresholding operator, one can likely obtain a reasonable nodule classification score <ref type="bibr" target="#b8">[8]</ref>). However, the nodule sizes between 5 to 12 mm represent the mid-range nodules that are "difficult" to classify and there is no straightforward way to classify these nodules <ref type="bibr" target="#b8">[8]</ref>:</p><formula xml:id="formula_13">= { , ? ? ,<label>(13)</label></formula><p>where is the diameter of the training example. In contrast with the first criterion, we applied this criterion from the perspective of AI instead of the radiologists' perspective. Namely, we observed in our previous experiments <ref type="bibr" target="#b8">[8]</ref> that AI performs really well in classifying nodules &lt;5 mm (as benign) and &gt;12 mm (as malignant); however, its performance is poor in classifying the mid-range nodules between 5 to 12 mm. <ref type="figure">Figure 2</ref>: Progressive Growing States from start state, transition state to final state. The dashed connection is temporary and used during the transition state. Once we reach the final state, we may remove the temporary connection. The Channel Attentive Non-Local (CAN) block with gray color is the new CAN block, and we refer to it in the text (Section 3.3) as while the CAN block with white background is a part of the base feature extractor, which we refer to in the text as . The red arrows denote multiplications while the black arrows are just simple connectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Progressive growing method with a two-dimensional Bernoulli matrix</head><p>In this section, we propose a new Progressive Growing method to help increase or progressively grow our network architecture in steps. The performance of a deep learning network is limited by its size/architecture <ref type="bibr" target="#b12">[12]</ref>; therefore, to ensure that the deep learning network performs optimally, we need to gradually increase its size and progressively grow it in a "safe" environment. As we increase the difficulty of the task using Curriculum Learning (in Section 3.2), the network performance reaches its limit, and the accuracy cannot be improved further on the current network architecture. Therefore, we propose to add new CAN blocks on the fly during the training procedure.</p><p>Our network consists of two main functions: base feature extractors, (CANs) and a classifier (i.e., a fully-connected layer). The feature extractor consists of one or more CAN blocks, as follows:</p><formula xml:id="formula_14">= ( ? (? , ( )))<label>(14)</label></formula><p>where is the number of CAN blocks, and is the input image. The straightforward implementation would be connecting the feature extractor with a fully-connected layer directly as ( ( )). To expand the base feature extractor , we can simply attach a new CAN block to the end of .</p><p>( ) = ( ( )) <ref type="bibr" target="#b15">(15)</ref> where is the CAN block that is being added during the training to ( ). Nevertheless, the process of adding a new block during the training will disturb the training procedure, and the optimizer may take longer to converge or it may be stuck at bad local minima <ref type="bibr" target="#b14">[14]</ref>. Therefore, for the Progressively Growing GAN (ProGAN) architecture <ref type="bibr" target="#b14">[14]</ref>, gradual growing was proposed by introducing a scalar value ? ? to control the flow of the information from the base features and the new CAN block as follows:</p><formula xml:id="formula_15">( ) = ( ( )) + ( ? ) ( )<label>(16)</label></formula><p>The gradual growing process passes through three states: (1) start state, (2) transition state, and (3) final state. In the start state, the value is set to = to block all the features from the new block ; namely, in the start state, it is as though there is no new block added to the network architecture. Then, in the transition state, the values are set to = { . , . , . } in that order. For example, in the first epoch, = .</p><p>; then, in the second epoch = . , and so on. This allows the new features that come from to slowly blend with the old features that come from . Namely, the network architecture is increased in a "safe" environment to allow the network to gradually adapt to prescribed changes in the architecture. In the final state, the value is set to = to allow all features from to pass through, as depicted in <ref type="figure">Figure 2</ref>.</p><p>In the ProGAN paper <ref type="bibr" target="#b14">[14]</ref>, it was shown that gradually introducing improves the network's performance. However, the main shortcoming of this approach is that multiplying with the values modifies/destroys every single value coming from . As we modify and ( ) in every epoch, the following fully-connected layer, has to adapt to this shift of the feature values in every single epoch. For example, when = . , this means every value coming from is divided by two. The multiplication with modifies the feature maps which leads to losing the information gained by or ( ). To overcome this problem and to avoid modifying the feature values, we propose to use a 2D matrix with a probability of instead of just multiplying with the scalar values, as follows:</p><formula xml:id="formula_16">? ( )<label>(17)</label></formula><p>where ? ? ? is a 2D matrix that has the same spatial size as the output features from ( ) ? ? ? ? .</p><p>Every value in the matrix = { , , ? , , } is either zero or one and when we multiply it with , the matrix either allows the feature to pass through completely (i.e., = ) or blocks it completely (i.e., = ), without modifying the feature values in any way. In this way, applying the matrix does not modify the output values unlike multiplying with the values in the ProGAN paper <ref type="bibr" target="#b14">[14]</ref>:</p><formula xml:id="formula_17">( ) = ( ( )) + ( ? ) ( )<label>(18)</label></formula><p>Finally, we connect to the last layer, which is a fully-connected layer, to obtain the final output of our new ProCAN network:</p><formula xml:id="formula_18">?( ) = ( ( ))<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Full algorithm and neural network architecture</head><p>The pseudocode of our ProCAN method is given in Algorithm 1. We first train the network on easy training data, followed by hard training data in the Curriculum Learning framework (i.e., Section 3.2) by minimizing the Binary Cross Entropy (BCE) loss function. We then progressively grow the network by adding new CAN blocks using our new Progressive Growing approach (i.e., Section 3.3). Finally, the number of CAN blocks, that gives the best result and its corresponding output, ? are returned at the end of Algorithm 1. The final network structure of our ProCAN method consists of seven blocks of CANs followed by a Global Average Pooling (GAP) layer and a Fully-Connected layer, as tabulated in <ref type="table" target="#tab_1">Table 2</ref> (see Ablation study results in Section 5.3 for a full analysis of how seven CAN blocks were obtained for the final network architecture). The first four layers are a part of the base feature extractors (see the first part of the Ablation study results in Section 5.3 for a full analysis of how 4 base feature extractors was obtained), whereas the other three are added during the training (see the second part of the Ablation study results in Section 5.3 for a full analysis of how 3 extended feature extractors was obtained). The input to the first base feature extractor is a 3D volume of size 32?32?32 mm 3 at the nodule center. The Stride in <ref type="table" target="#tab_1">Table 2</ref> refers to the stride size of the last convolution operation in the CAN block (i.e., Eq. 10). We also set the size of the intermediate channels, ? (in matrices and in Eqs. 1 and 2, respectively) of the CAN blocks to one (see the second part of the Ablation study results in Section 5.2 for a full analysis of how the optimal number of intermediate channels, ? = was obtained). </p><formula xml:id="formula_19">256?8?8 1 GAP 256?8?8 256?1?1 ------ Fully-Connected 256?1?1 1 ------</formula><p>We also placed a batch normalization layer <ref type="bibr" target="#b27">[27]</ref> at the end of every CAN block to normalize the feature maps. Moreover, we applied dropout regularization <ref type="bibr" target="#b28">[28]</ref> with a probability of 0.5 before the Fully-Connected layer to prevent overfitting and to improve network generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Preprocessing</head><p>Our first preprocessing step was to normalize the CT scans using trilinear interpolation. This results in isotropic resolution in all three (x, y, and z) dimensions. Then we cropped a volume of size 32?32?32 mm 3 around the nodule, which is enough to fit the largest nodule (see Section 4.1 and the largest diameter of nodules in the LIDC-IDRI dataset is 30 mm). Third, we clamped the Hounsfield unit (HU) values of the scans that were less than -1000 or larger than 400, which is a common practice employed in the literature to filter out the air and bone regions <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10]</ref>, as follows:</p><formula xml:id="formula_20">= ( ( , ? ), )<label>(20)</label></formula><p>Finally, we normalized the nodules to have zero mean and unit variance according to standard practice in the deep learning literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Data augmentation</head><p>A method that is frequently used in the literature to avoid or prevent overfitting of a deep learning model is augmenting the training data. In this study, we augmented the nodules in our dataset by rotating each nodule around the three (x, y, and z) axes. For each axis (x, y, and z), we rotated each nodule in seven directions (i.e., Although data augmentation regulates the model and prevents it from overfitting, it also introduces a gap in the data distribution between the training and test data <ref type="bibr" target="#b29">[29]</ref>. Therefore, we experimented with refining the model at the end of the training procedure by training the model on non-augmented data; that is, we trained the model on augmented data first, and then refined the trained model without any data augmentation. Unless explicitly mentioned in the manuscript, all proposed methods were trained on 9 folds with data augmentation and tested on the remaining one fold without data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">LIDC-IDRI dataset</head><p>The Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) database <ref type="bibr" target="#b6">[7]</ref> is the largest publicly-available lung nodule dataset. It contains 1,018 computed tomography (CT) scans from 1,010 patients altogether collated from seven academic centers across the United States (US). The slice thicknesses of the CT scans range from 0.45 to 5.0 mm, as the scans were collated from different institutions and devices.  <ref type="bibr" target="#b6">[7]</ref> and LUNGx <ref type="bibr" target="#b30">[30]</ref> datasets that were used in this study. LIDC-IDRI and LUNGx are highly-popular and LIDC-IDRI is the biggest publicly-available dataset for lung cancer classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Public dataset Number of malignant cases Number of benign cases</head><p>LIDC-IDRI 406 442 LUNGx <ref type="bibr">36 37</ref> Each CT scan was annotated by four experienced thoracic radiologists altogether. In this study, we follow a rigorous approach as <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b23">23]</ref>, whereby we only used the nodules that were annotated by at least three out of four radiologists to ensure that the majority of radiologists agreed on the presence of a nodule. The four radiologists also rated each nodule on a scale of one to five, whereby one indicates that a nodule is clearly benign and five, clearly malignant. Therefore, to aggregate all of the malignancy ratings by all four radiologists, similar to previous studies <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b32">32]</ref>, we took the median of the ratings of all four radiologists as the ground truth in our experiments. We excluded nodules with a median rating of 3 as we could not assign them to either benign or malignant groups. In this way, we obtained 848 nodules altogether, of which 442 were benign and 406 malignant as shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">LUNGx dataset</head><p>The LUNGx Challenge dataset associated with the 2015 SPIE Medical Imaging Symposium challenge <ref type="bibr" target="#b30">[30]</ref> consists of 70 CT scans -10 for calibration and 60 for testing. The 60 test scans contain 73 nodules, of which 37 are benign, and 36 are malignant as shown in <ref type="table" target="#tab_2">Table 3</ref>. Considering there is no training set associated with the LUNGx dataset, we used the LIDC-IDRI dataset for training purposes, as suggested in the challenge guidelines <ref type="bibr" target="#b30">[30]</ref>. Similar to other studies <ref type="bibr" target="#b30">[30]</ref>, we only examined the performance of our method on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental setup</head><p>Similar to our previous studies <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref> and other studies <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref> conducted in the literature, we evaluated our proposed ProCAN model on the LIDC-IDRI dataset using a 10-fold cross-validation method, in which 9 folds were used for training and one fold for testing. Moreover, we sampled 10% of the training data as a validation dataset.</p><p>For the second dataset, namely LUNGx, no training dataset was provided. Therefore, similar to previous studies that utilized this dataset <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b33">33]</ref>, we directly tested our trained ProCAN model on the 73 nodules in the testing dataset. To validate our model on the LUNGx dataset, we used the same validation method utilized by the organizers of the LUNGx Challenge <ref type="bibr" target="#b30">[30]</ref>, namely bootstrapping with 1000 iterations.</p><p>Our ProCAN network implementation was based on PyTorch 1.4 <ref type="bibr" target="#b34">[34]</ref>. We ran our experiments using a NVIDIA RTX 2080Ti GPU. The following parameters apply for our experimental design: We applied a weight decay of 0.0001 for the fully-connected layer. We also set the weight decay and dropout values to 0.5 to regularize the fully-connected layer and prevent it from overfitting. For the learning rate, we first initialized it to 0.001 and then to 0.0001 at the 20 th epoch. All the models were optimized using the Adam optimizer <ref type="bibr" target="#b35">[35]</ref> for 60 epochs with a batch size of 256. At the 51 st epoch, the model was refined by training it without augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation criteria</head><p>The performance of the lung nodule classification scheme was assessed using the standard evaluation criteria used in the literature, namely accuracy, sensitivity, precision, and AUC. These performance metrics were computed using equations <ref type="bibr" target="#b21">(21)</ref> to <ref type="formula" target="#formula_1">(25)</ref>  </p><p>where , , , and represent the number of true positive, true negative, false negative, and false positive nodules, respectively. In a ROC curve, the false positive rate (i.e., ) axis ranges from 0 to 1 and the true positive rate (i. e. , ) is a function of the ; + and ? represent the confidence/probability scores for a positive and negative sample, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>In this section, we compare our proposed model with state-of-the-art models in the literature on both popular datasets of LIDC-IDRI and LUNGx. We first compare our method with other methods on the LIDC-IDRI database, then we perform the same analysis on the LUNGx database. In the first 6 rows of <ref type="table" target="#tab_3">Table 4</ref>, we analyze the performance of our ProCAN model against other methods on the LIDC-IDRI database. Moreover, we designed an ensemble version of ProCAN (i.e., Ensemble ProCAN) to compare with recent ensemble models in the literature <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b33">33]</ref>. We constructed our ensemble model by taking a simple average sum of three different variations of ProCAN: ProCAN-6, ProCAN-7, and ProCAN-8, where 6, 7, and 8 denote the number of the blocks used in the ProCAN network architecture. The individual performances results of ProCAN-6, ProCAN-7, and ProCAN-8 are given in Section 5.3.</p><p>From <ref type="table" target="#tab_3">Table 4</ref>, we observe that both our ProCAN and Ensemble ProCAN models outperform the state-of-the-art models in the literature on all evaluation criteria. Also, our Ensemble ProCAN model outperforms ProCAN on all the evaluation criteria, which is consistent with the results in the literature that show that ensemble methods generally outperform non-ensemble ones <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref>. Furthermore, our non-ensemble ProCAN model marginally outperforms all the non-ensemble and ensemble methods in <ref type="table" target="#tab_3">Table 4</ref>, excluding Ensemble ProCAN, whereas Ensemble ProCAN considerably outperforms all other methods in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>We also compare the performance of our ProCAN and Ensemble ProCAN models with other state-of-the-art models on the LUNGx database. With the LUNGx database, no training dataset is provided; therefore, similar with other methods that use this database, we only evaluate our proposed model on a provided testing dataset. Thus, the evaluation of our proposed model on this dataset measures the robustness of the model on changes in the distribution of the applied dataset, namely it evaluates the generalizability of our model on completely independent/unseen datasets. The results and comparisons with other state-of-the art methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref> are tabulated in <ref type="table" target="#tab_4">Table 5</ref>. We observe in <ref type="table" target="#tab_4">Table 5</ref>, that our ProCAN and Ensemble ProCAN models achieve the highest AUC values compared to standard machine learning and state-of-the-art methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref>. In particular, Ensemble ProCAN achieves AUC = 80.1?4.6, which to the best of our knowledge is a result that surpasses all other reported methods in the literature on this challenging dataset to date. These results also demonstrate that ProCAN and Ensemble ProCAN are generalizable to unseen datasets and their performance is robust to variability in the distribution of the dataset at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we analyze the efficacy of our proposed model, namely each proposed component of our ProCAN network architecture. We perform extensive studies and compare the performance of our methods with other established state-of-the-art methods in the literature. We also conduct extensive ablation studies to examine each applied step and component described in the Methods section of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Analysis of different data augmentation methods</head><p>We analyze our proposed algorithm under different data augmentation settings in this section. First, we examine the performance of our method without any data augmentation to verify that applying data augmentation improves the results. Next, we analyze our method's performance with different rotation angles (i.e., seven and four rotation angles). Finally, we examine the application of the refinement technique described by <ref type="bibr" target="#b29">[29]</ref> and explained in Section 3.6. We tabulated all the obtained results in <ref type="table" target="#tab_5">Table 6</ref>.</p><p>Applying the refinement technique <ref type="bibr" target="#b29">[29]</ref> was beneficial for a data augmentation with both four and seven angles. Especially for data augmentation with seven angles, the refinement method improved the results of all analyzed performance metrics; for data augmentation with four angles, improvements were observed for all performance metrics except the sensitivity, which declined marginally by 0.49%. Overall, the best result was obtained for data augmentation with seven angles with refinement. We observe in <ref type="table" target="#tab_5">Table 6</ref> that the performance drops significantly when we do not apply any data augmentation. The results for seven and four rotation angles are quite close, whereby the augmentation with seven angles improves the accuracy by ~1% and improves all the other performance metrics except the sensitivity result, which declines marginally by 0.14%. Furthermore, we tested the augmentation at test time as a replacement for refinement. First, we conducted an experiment, namely "7 Angles + Test Aug", whereby we augmented the test data 27 times (7 angles ? 3 views) and averaged the results. As shown in <ref type="table" target="#tab_5">Table 6</ref>, it performs slightly worse than applying augmentation and refinement ("7 Angles + Refine"). Besides the marginal gain in AUC and Accuracy when using refinement rather than augmenting the test data, refinement is 27 times faster as we just pass the image once during test time. We also combined all methods "7 Angles + Test Aug + Refine", but the result is even worse than using refinement or augmenting the test data, since in the last 10 epochs of the refinement, we tune the model on one angle and one view, but test it on different angles and views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Design of CAN block</head><p>In this section, we analyze the importance of channel attention and our CAN design block in an extensive ablation study. First, we remove the channel attention from the CAN block to examine the effectiveness of our new channel attention mechanism described in Section 3.1. In doing so, we are left with the Non-Local network (i.e., the blue dashed box in <ref type="figure" target="#fig_0">Figure 1</ref>) and the last convolutional layer (i.e., equation 10) in our network architecture. This constitutes the first result obtained in the first row of <ref type="table" target="#tab_6">Table 7</ref> (i.e., the "Non-Local" model).</p><p>To examine the efficacy of our new CAN block even further, we also examine two different state-of-the-art and popular channel attention mechanisms: (1) Squeeze and Excitation (SE) Networks <ref type="bibr" target="#b13">[13]</ref> and (2) Dual Attention Networks <ref type="bibr" target="#b18">[18]</ref>. The SE Network weights is a highly popular network that weights each channel to a single numeric value using average pooling. Dual Attention Networks on the other hand, use the Non-Local paradigm for spatial and channel attention, but with two separate blocks.</p><p>We ran the experiments and compared all three modifications of channel attention with our CAN block and tabulated the results in <ref type="table" target="#tab_6">Table 7</ref>. From the results, we observe that adding the channel attention in the form of the SE block combined with the Non-Local network improves the performance across all performance metrics, as expected. The Dual Attention Network performs better than the Non-Local Network individually, but its performance is poorer than Non-Local combined with SE. This is due to the redundancy in the Dual Attention Network design block, whereby there are two independent skip connections in the block (see <ref type="figure">Fig. 2</ref> of the <ref type="bibr" target="#b18">[18]</ref> reference). Another shortcoming of the Dual Attention Network is the size of the intermediate channels ? = , which not only slows the training but also increases the number of learnable parameters (please refer to the analysis of the results of <ref type="table" target="#tab_7">Table 8</ref> below where we performed the ablation study of the number of learnable parameters). We observe in <ref type="table" target="#tab_6">Table 7</ref> that apart from the sensitivity result (which is marginally below Non-Local plus SE by 0.48%), our new CAN block outperforms all the other methods across all performance metrics. We also studied the effects of varying the number of intermediate channels, ? described in Section 3.1 on the overall results. ? is the number of channels in the and matrices described in equations (1) and <ref type="bibr" target="#b1">(2)</ref>, respectively in Section 3.1. We examined three different ? values altogether; first, ? = as prescribed by <ref type="bibr" target="#b18">[18]</ref>, which simply means that the number of intermediate channels should be equal to the number of input channels. Second, <ref type="bibr" target="#b38">[38]</ref> proposed to set ? = / , which means that ? is eight times smaller than , thus reducing computational requirements and the overall number of learnable parameters in the process. The third value of ? that we tried was just ? = , which is the minimum value of ? that can be used. We tabulated the obtained results in <ref type="table" target="#tab_7">Table 8</ref>. As shown in <ref type="table" target="#tab_7">Table 8</ref>, as ? gets smaller, the overall performance increases. The best overall result was obtained for ? = except the sensitivity result, which was marginally less than ? = / by 0.72%. Over-parameterization (i.e., in this case too many intermediate channels) leads to overfitting on the training set and a subsequent performance decline on the testing set. Additionally, the LIDC-IDRI dataset used in this study is much smaller than the generic datasets used by <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b38">38]</ref>, which could further worsen the effects of the overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The influence of the number of base and extended CAN blocks</head><p>In this section, we first conduct an ablation study to examine the optimal number of base feature extractors or base CAN blocks, (see Section 3.3) to use in our final network architecture. We also perform a second (similar) ablation study to analyze the optimal number of extended CAN blocks, added through our new Progressive Growing method proposed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Effects of varying the number of base CAN blocks, in the network architecture</head><p>First, to find the optimal number of base CAN blocks, , we tested our CAN network without using the Progressive Growing algorithm and without Curriculum Learning. The obtained AUC and accuracy results are plotted in <ref type="figure">Figure 3</ref>. We can observe that the accuracy reaches the peak performance when we use four base CAN blocks and the performance degrades as we increase the number of blocks beyond four.</p><p>Next, we used the four base CAN blocks as the base feature extractor, and tried a different number of extended blocks, using our Progressive Growing and training method described in Algorithm 1 (see Section 3.4). We examined the results of adding one to four CAN blocks on the fly using the training procedure described in Algorithm 1. The AUC and accuracy results of our ablation study are plotted in <ref type="figure">Figure 4</ref>. From <ref type="figure">Figure 4</ref>, we observe that the best results were obtained using 3 extended blocks. We also observe that using our Progressive Growing method to extend the CAN blocks to our ProCAN network architecture is very effective as all the (AUC and accuracy) results in <ref type="figure">Figure 4</ref> outperform the best result of AUC = 95.86% and accuracy = 91.98% obtained for the number of base CAN blocks ( = ) in <ref type="figure">Figure 3</ref>. This result demonstrates that our new Progressive Growing method using the matrix instead of just the scalar effectively enables our network architecture to progressively grow it in a "safe" environment (see also Progressive Growing Ablation study results in Section 5.5 below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effectiveness of curriculum learning method</head><p>We also conducted an ablation study to analyze the effects of applying different curriculum learning criteria as explained in Section 3.2. The first criterion was based on the malignancy rating of the radiologists and the second criterion on the nodule diameter. We tabulate the results of the ablation study in <ref type="table" target="#tab_9">Table 9</ref>.</p><p>The ablation study results show that incorporating both the radiologists' ratings and nodule diameter criteria improves the performance over not using Curriculum learning at all. The experiments also show that curriculum learning using the diameter criteria is better than using radiologists' ratings on all performance metrics except the precision. This indicates that the nodule diameter criterion carries more relevant information than the radiologists' ratings for the classification task, which was also observed in our previous study <ref type="bibr" target="#b8">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Effectiveness of progressive growing method</head><p>In the last ablation study, we conducted experiments to show the effect of different blending methods while growing the network as described in Section 3.3 on Progressive Growing. Specifically, we analyzed the results with no blending strategy as in equation <ref type="bibr" target="#b15">(15)</ref>. We also examined the performances of blended learning using the scalar value, as in the ProGAN method <ref type="bibr" target="#b14">[14]</ref> and the newly proposed 2D matrix (see Section 3.3). We tabulated all the results in <ref type="table" target="#tab_0">Table 10</ref>. We observe from <ref type="table" target="#tab_0">Table 10</ref> that not applying any blending strategy results in performance dropping significantly/ drastically. As we discussed in Section 3.3, suddenly adding a new layer during the training procedure significantly affects the overall network performance. The results also show that using the new 2D matrix instead of just the scalar value, improves the results significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Effectiveness of batch size</head><p>Finally, we conducted experiments to show the effect of different batch sizes on ProCAN's performance. In <ref type="table" target="#tab_0">Table 11</ref>, we observe that 256 is the best batch size across all performance metrics excluding sensitivity, followed by 128 and 64. Our proposed method uses Batch Normalization <ref type="bibr" target="#b27">[27]</ref>, which is very sensitive to batch sizes. Usually, Batch Normalization works better with larger batch sizes as when the batch size increases, the sample mean and standard deviation in Batch Normalization approximates that of the population more closely <ref type="bibr" target="#b27">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this study, we proposed a new Progressive Growing Channel Attentive Non-Local Network (ProCAN) network for lung nodule classification with several novel characteristics. First, we added a new channel-wise attention capability to the highly popular Non-Local network called the CAN block. The CAN block gives our ProCAN network the ability to detect sizeinvariant nodules with both spatial and channel-wise attention. Second, we developed a Curriculum Learning method based on the nodule diameter and radiologists' ratings, and found that we can improve model performance if we train it on easy examples (i.e., small or big nodules in the nodule size category and clearly benign or clearly malignant in the radiologist ratings' category) before the difficult examples (i.e., mid-sized nodules in the nodule size category and probably benign or probably malignant in the radiologist ratings' category). Third, we found that progressively growing the network during the training is crucial and improves the results compared with using fixed number of blocks in the overall network architecture. Moreover, we propose a new strategy to blend the new blocks gradually with a matrix sampled from a 2D distribution and show that this improves the overall accuracy and the AUC. We analyzed the effectiveness of all these new components/contributions in extensive ablation studies and in comparisons with other state-of the-art methods. The overall experimental results show that our proposed ProCAN model achieves state-of-the-art results in the literature. We will use ProCAN principles to design an end-to-end early-stage lung cancer detection and classification model in future work, and examine its performance on bigger, more diverse datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Block diagram of our new Channel Attentive Non-Local Network (CAN). The block diagram shows the relationship between the standard Non-Local Network (blue dashed box) and our new Channel Attentive operation. Symbols in red are intermediate symbols.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>ProCAN training procedure Requirements:{ , }: easy training data, { , }: easy and hard training data, 0 : base feature extractor, : fully connected layer, : number of extended CAN blocks minimize ( ( 0 ( )) , ) until Eq. 11 satisfied minimize ( ( 0 ( )), ) until Eq. 11 satisfied for ? {1, ? , } do = for ? {0.25,0.5,0.75,1} do ? ( ) ( ) = ( ?1 ( )) + (1 ? ) ?1 ( ) ?( ) = ( ( )) minimize (?( ), ) for one epoch end for remove unnecessary temporary connectors from ? minimize (?( ), ) until Eq. 11 satisfied end for minimize (?( ), ) for remaining epochs Return: ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0 o , 45 o , 90 o , 135 o , 180 o , 225 o , 270 o ). This means that every training example was augmented 21 times (i.e., 3?7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary and characteristics of related work in spatial and channel attention. denotes the spatial size; the number of channels. MM, GAP and FC represent Matrix Multiplications, Global Average Pooling and Fully-Connected, respectively.</figDesc><table><row><cell>Method</cell><cell cols="2">Spatial Attention Channel Attention</cell><cell>Attention Mechanism</cell><cell>Permutation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Equivariant</cell></row><row><cell>SE [13]</cell><cell>None</cell><cell>?</cell><cell>GAP with FC</cell><cell>No</cell></row><row><cell>Non-Local [11]</cell><cell>?</cell><cell>None</cell><cell>MM</cell><cell>Yes</cell></row><row><cell>Transformer [17]</cell><cell>?</cell><cell>None</cell><cell>MM</cell><cell>No</cell></row><row><cell>Dual-Attention [18]</cell><cell>?</cell><cell>?</cell><cell>Two Independent MM</cell><cell>Yes</cell></row><row><cell>Local-Global [9]</cell><cell>?</cell><cell>None</cell><cell>MM</cell><cell>Yes</cell></row><row><cell>3D Dual-Path [19]</cell><cell>?</cell><cell>None</cell><cell>MM</cell><cell>No</cell></row><row><cell>CAN (ours)</cell><cell>?</cell><cell>?</cell><cell>Two Dependent MM</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Network Architecture of ProCAN</figDesc><table><row><cell>Layer</cell><cell>Input</cell><cell>Output</cell><cell>Stride</cell></row><row><cell></cell><cell>? ?</cell><cell>? ?</cell><cell></cell></row><row><cell>Base CAN 1</cell><cell>32?32?32</cell><cell>32?32?32</cell><cell>1</cell></row><row><cell>Base CAN 2</cell><cell>32?32?32</cell><cell>64?16?16</cell><cell>2</cell></row><row><cell>Base CAN 3</cell><cell>64?16?16</cell><cell>128?8?8</cell><cell>2</cell></row><row><cell>Base CAN 4</cell><cell>128?8?8</cell><cell>256?8?8</cell><cell>1</cell></row><row><cell>Extended CAN 1</cell><cell>256?8?8</cell><cell>256?8?8</cell><cell>1</cell></row><row><cell>Extended CAN 2</cell><cell>256?8?8</cell><cell>256?8?8</cell><cell>1</cell></row><row><cell>Extended CAN 3</cell><cell>256?8?8</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of distribution of CT scans/patients in the two publicly-available LIDC-IDRI</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our ProCAN and Ensemble ProCAN methods with other state-of-the-art methods in theliterature on the LIDC-IDRI database.</figDesc><table><row><cell>Model</cell><cell cols="2">Ensemble? AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell cols="2">Sensitivity F1-Score</cell></row><row><cell>Multi-Crop [32]</cell><cell>No</cell><cell>93.0</cell><cell>87.14</cell><cell>---</cell><cell>77.0</cell><cell>---</cell></row><row><cell>HSCNN [36]</cell><cell>No</cell><cell>85.6?2.6</cell><cell>84.2?2.5</cell><cell>---</cell><cell>70.5?4.5</cell><cell>---</cell></row><row><cell>Local-Global [9]</cell><cell>No</cell><cell cols="5">95.62?0.02 88.46?0.04 87.38?0.07 88.66?0.06 88.37?0.04</cell></row><row><cell>Gated-Dilated [8]</cell><cell>No</cell><cell cols="5">95.14?0.03 92.57?0.03 91.85?0.05 92.21?0.04 92.60?0.03</cell></row><row><cell>Swarm [37]</cell><cell>No</cell><cell>---</cell><cell>93.71</cell><cell>93.53</cell><cell>92.96</cell><cell>---</cell></row><row><cell>ProCAN (ours)</cell><cell>No</cell><cell cols="5">97.13?0.02 94.11?0.03 94.54?0.04 93.12?0.05 93.81?0.03</cell></row><row><cell>3D DPN(Ensemble)[19]</cell><cell>Yes</cell><cell>---</cell><cell>90.24</cell><cell>---</cell><cell>92.04</cell><cell>90.45</cell></row><row><cell>MV-KBC [21]</cell><cell>Yes</cell><cell cols="5">95.70?0.24 91.60?0.15 87.75?0.24 86.52?0.25 87.13?0.16</cell></row><row><cell>MSCS-DeepLN [10]</cell><cell>Yes</cell><cell cols="5">94.00?0.25 92.65?0.26 90.39?0.93 85.58?0.94 87.91?0.43</cell></row><row><cell>MK-SSAC [33]</cell><cell>Yes</cell><cell cols="3">95.81?0.19 92.53?0.05 ---</cell><cell cols="2">84.94?0.17 ---</cell></row><row><cell cols="2">Ensemble ProCAN (ours) Yes</cell><cell cols="5">98.05?0.02 95.28?0.02 95.75?0.04 94.33?0.04 95.04?0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of our ProCAN method with other state-of-the-art and best-performing methods on the LUNGx Challenge dataset. We have incorporated these results from the relevant published papers<ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b30">30]</ref>. .</figDesc><table><row><cell>Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of different data augmentation methods. All these models were trained for 60 epochs. 'Refine' denotes training the model with augmented data for 50 epochs, then training for an additional 10 epochs without data augmentation. '7 Angles + Test Aug' denotes applying 7 augmentation angles during training and testing.'None' denotes that no augmentation nor refinement was applied.</figDesc><table><row><cell>Augmentation Method</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Sensitivity</cell><cell>F1-Score</cell></row><row><cell>7 Angles + Refine</cell><cell>97.13</cell><cell>94.11</cell><cell>94.54</cell><cell>93.12</cell><cell>93.81</cell></row><row><cell>7 Angles</cell><cell>96.54</cell><cell>93.87</cell><cell>94.25</cell><cell>92.86</cell><cell>93.55</cell></row><row><cell>4 Angles + Refine</cell><cell>96.37</cell><cell>92.92</cell><cell>92.61</cell><cell>92.61</cell><cell>92.61</cell></row><row><cell>4 Angles</cell><cell>96.19</cell><cell>92.81</cell><cell>91.97</cell><cell>93.1</cell><cell>92.53</cell></row><row><cell>7 Angles + Test Aug</cell><cell>97.10</cell><cell>93.99</cell><cell>94.26</cell><cell>93.10</cell><cell>93.68</cell></row><row><cell>7 Angles + Test Aug + Refine</cell><cell>96.92</cell><cell>93.87</cell><cell>94.25</cell><cell>92.86</cell><cell>93.55</cell></row><row><cell>None</cell><cell>93.31</cell><cell>88.56</cell><cell>89.31</cell><cell>86.45</cell><cell>87.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="6">Performance comparisons of our novel CAN block design with other popular or state-of-the-art methods in</cell></row><row><cell></cell><cell></cell><cell cols="2">the literature</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Sensitivity</cell><cell>F1-Score</cell></row><row><cell>Non-Local</cell><cell>96.4</cell><cell>92.81</cell><cell>93.02</cell><cell>91.87</cell><cell>92.44</cell></row><row><cell>Non-Local + SE</cell><cell>96.56</cell><cell>93.75</cell><cell>93.37</cell><cell>93.6</cell><cell>93.48</cell></row><row><cell>Dual Attention</cell><cell>96.48</cell><cell>93.16</cell><cell>93.07</cell><cell>92.61</cell><cell>92.84</cell></row><row><cell>CAN</cell><cell>97.13</cell><cell>94.11</cell><cell>94.54</cell><cell>93.12</cell><cell>93.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Effects of varying the number of intermediate channels</figDesc><table><row><cell>Number of</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Sensitivity</cell><cell>F1-Score</cell></row><row><cell>intermediate channels,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>97.13</cell><cell>94.11</cell><cell>94.54</cell><cell>93.12</cell><cell>93.81</cell></row><row><cell>/</cell><cell>96.84</cell><cell>93.87</cell><cell>93.38</cell><cell>93.84</cell><cell>93.61</cell></row><row><cell></cell><cell>96.85</cell><cell>92.92</cell><cell>93.03</cell><cell>92.12</cell><cell>92.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Effects of applying different curriculum learning criteria on ProCAN's performance. None indicates that no curriculum learning was applied.</figDesc><table><row><cell>Criteria</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Sensitivity</cell><cell>F1-Score</cell></row><row><cell>None</cell><cell>95.32</cell><cell>92.92</cell><cell>92.61</cell><cell>92.61</cell><cell>92.61</cell></row><row><cell>Rating</cell><cell>96.54</cell><cell>93.63</cell><cell>94.67</cell><cell>91.87</cell><cell>93.25</cell></row><row><cell>Diameter</cell><cell>97.13</cell><cell>94.11</cell><cell>94.54</cell><cell>93.12</cell><cell>93.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Effects of different blending methods on ProCAN's performance. None means no blending was applied as</figDesc><table><row><cell>in equation (15)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Effects of different batch sizes on ProCAN's performance</figDesc><table><row><cell>Batch size</cell><cell>AUC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Sensitivity</cell><cell>F1-Score</cell></row><row><cell>64</cell><cell>95.95</cell><cell>93.75</cell><cell>93.37</cell><cell>93.60</cell><cell>93.48</cell></row><row><cell>128</cell><cell>96.45</cell><cell>93.99</cell><cell>93.40</cell><cell>94.09</cell><cell>93.74</cell></row><row><cell>256</cell><cell>97.13</cell><cell>94.11</cell><cell>94.54</cell><cell>93.12</cell><cell>93.81</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">WHO report on cancer: setting priorities, investing wisely and providing care for all, World Health Organization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>WHO</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Radiomics: Extracting more information from medical images using advanced feature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lambin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rios-Velazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leijenaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G P M</forename><surname>Van Stiphout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Granton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M L</forename><surname>Zegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boellard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dekker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J W L</forename><surname>Aerts</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejca.2011.11.036</idno>
		<ptr target="https://doi.org/10.1016/j.ejca.2011.11.036" />
	</analytic>
	<monogr>
		<title level="j">Eur. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="441" to="446" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reduced Lung-Cancer Mortality with Low-Dose Computed Tomographic Screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Fagerstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Gareen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatsonis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R D</forename><surname>Sicks</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMoa1102873</idno>
		<ptr target="https://doi.org/10.1056/NEJMoa1102873" />
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="page" from="395" to="409" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing two classes of end-to-end machine-learning models in lung nodule detection and classification: MTANNs vs. CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.09.029</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.09.029" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="476" to="486" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Learning for Image-based Cancer Detection and Diagnosis -A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.05.014</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2018.05.014" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated pulmonary nodule detection in CT images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.PATCOG.2018.07.031</idno>
		<ptr target="https://doi.org/10.1016/J.PATCOG.2018.07.031" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bidaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Mcnitt-Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Henschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Kazerooni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J R</forename><surname>Van Beek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yankelevitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Biancardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Bland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Laderach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Pais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P Y</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Starkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caligiuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Gladish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Jude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Munden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Petkovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Quint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fenimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vande Casteele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Sallam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dharaiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fryd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Salganicoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shreter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Vastagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.3528204</idno>
		<ptr target="https://doi.org/10.1118/1.3528204" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gated-Dilated Networks for Lung Nodule Classification in CT Scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2958663</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2019.2958663" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="178827" to="178838" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lung nodule classification using deep Local-Global networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Shabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-01981-7</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01981-7" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1815" to="1819" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating lung nodule malignancy using multi-scale cost-sensitive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mscs-Deepln</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101772</idno>
		<ptr target="https://doi.org/10.1016/j.media.2020.101772" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101772</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00813</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00813" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality, Stability, and Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hk99zCeAb" />
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553380</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553380" />
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Annu. Int. Conf. Mach. Learn. -ICML &apos;09</title>
		<meeting>26th Annu. Int. Conf. Mach. Learn. -ICML &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Handwritten Digit Recognition with a Back-Propagation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5076" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="404" />
			<date type="published" when="1990-05-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<editor>I. Guyon, U. V Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, R. Garnett</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
	<note>Attention is All you Need</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual Attention Network for Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.02983" />
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="page" from="3141" to="3149" />
			<date type="published" when="2018-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attentive and ensemble 3D dual path networks for pulmonary nodules classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.03.103</idno>
		<ptr target="https://doi.org/10.1016/j.neucom.2019.03.103" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page" from="422" to="430" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DeepLung: Deep 3D Dual Path Nets for Automated Pulmonary Nodule Detection and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2018.00079</idno>
		<ptr target="https://doi.org/10.1109/WACV.2018.00079" />
	</analytic>
	<monogr>
		<title level="j">IEEE Winter Conf. Appl. Comput. Vis</title>
		<imprint>
			<biblScope unit="page" from="673" to="681" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge-based Collaborative Deep Learning for Benign-Malignant Lung Nodule Classification on Chest CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2876510</idno>
		<ptr target="https://doi.org/10.1109/TMI.2018.2876510" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="991" to="1004" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stand-Alone Self-Attention in Vision Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8302-stand-alone-self-attention-in-vision-models.pdf" />
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch?-Buc, E. Fox, R. Garnett</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A novel computer-aided lung nodule detection system for CT images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deklerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornelis</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.3633941</idno>
		<ptr target="https://doi.org/10.1118/1.3633941" />
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="5630" to="5645" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pulmonary nodule classification with deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wollersheim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-017-1605-6</idno>
		<ptr target="https://doi.org/10.1007/s11548-017-1605-6" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1799" to="1808" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep Learning: A Critical Appraisal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.00631" />
		<imprint>
			<date type="published" when="2018-10-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">From System 1 Deep Learning to System 2 Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13398-014-0173-7.2</idno>
		<ptr target="https://doi.org/10.1007/s13398-014-0173-7.2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.1214/12-AOS1000</idno>
		<ptr target="https://doi.org/10.1214/12-AOS1000" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.09148" />
		<title level="m">Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LUNGx Challenge for computerized lung nodule classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Drukker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hadjiiski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Tourassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Redmond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.3.4.044506</idno>
		<ptr target="https://doi.org/10.1117/1.JMI.3.4.044506" />
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">44506</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Lung Image Database Consortium (LIDC) Nodule Size Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biancardi</surname></persName>
		</author>
		<ptr target="http://www.via.cornell.edu/lidc/" />
		<imprint>
			<date type="published" when="2019-01-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-crop Convolutional Neural Networks for lung nodule malignancy suspiciousness classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2016.05.029</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2016.05.029" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="663" to="673" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised adversarial model for benign-malignant lung nodule classification on chest CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.07.004</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.07.004" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd Int. Conf. Learn. Represent. ICLR 2015</title>
		<editor>Y. Bengio, Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Conf. Track Proc.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.01.048</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2019.01.048" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detection and classification of pulmonary nodules using deep learning and swarm intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>De Pinho Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nedjah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Macedo Mourelle</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-019-7473-z</idno>
		<ptr target="https://doi.org/10.1007/s11042-019-7473-z" />
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="15437" to="15465" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-Attention Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/zhang19d.html" />
		<editor>K. Chaudhuri, R. Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7354" to="7363" />
			<pubPlace>Long Beach, California, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
