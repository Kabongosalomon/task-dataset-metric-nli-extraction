<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual training for So ware Engineering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-03">3 Feb 2022 May 21-29, 2022,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toufique</forename><surname>Ahmed</surname></persName>
							<email>tfahmed@ucdavis.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
							<email>ptdevanbu@ucdavis.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Davis Davis</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Davis Davis</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual training for So ware Engineering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-03">3 Feb 2022 May 21-29, 2022,</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3510003.3510049</idno>
					<note>ACM Reference Format: Toufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for Software Engineering. In 44th International Conference on Software Engi-neering (ICSE &apos;22), May 21-29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 13 pages. https:// Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICSE &apos;22,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>code summarization</term>
					<term>code search</term>
					<term>method name prediction</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this dataaugmenting approach is broadly compatible with different tasks, languages, and machine-learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Software and its engineering ? Software notations and tools; ? Computing methodologies ? Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Researchers in the NLP area have reported that multilingual training is beneficial for low-resource language <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b64">63]</ref>. Several papers show that multilingual-trained models show better performance <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b63">62]</ref> and are more practical to deploy <ref type="bibr" target="#b8">[9]</ref>. However, this is observed in two situations: 1) for low-resource languages and 2) when the languages are related. We find that programs in different languages solving the same problem use more similar identifiers; furthermore different languages sometimes have similar keywords and operators. High capacity deep learning models are capable of learning interlingua: shared semantic representation between languages <ref type="bibr" target="#b34">[34]</ref>. Moreover, with tasks like summarization, or method naming, we are dealing with a simplified, many-to-one setting: translating multiple source languages to a single target language), which is believed to be easier than multi-way task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b77">76]</ref>. We begin by introducing the code summarization task, which we use to motivate multilingual training.</p><p>Developers often rely heavily on comments, to gain a quick (even if approximate) understanding of the specification and design of code they are working on. An actual example of a comment is shown in <ref type="figure">Figure 1</ref>. Such comments help a developer gain a quick mental preview of what the proximate code does, and how it might go about it; this helps the developer know what to look for in the code. Knowing that such comments are useful to others (or even later to oneself) incentivizes developers to create comments that explain the code; however the resulting redundancy (viz., code that does something, and some nearby English text that describes just what the code does), with the same concept expressed in two languages results in a bit of extra work for the original coder. This extra work, of creating aligned comments explaining the code, can be fruitfully viewed <ref type="bibr" target="#b20">[21]</ref> as a task related to natural language translation (NLT) (e.g., translating English to German). The mature &amp; powerful technology of NLT becomes applicable for comment synthesis; ML approaches developed for the former can be used for the latter. An effective comment synthesizer could help developers: by saving them the trouble of writing comments; and perhaps even be used on-demand in the IDE to create descriptions of selected bits of code.</p><p>Comment synthesis is now an active research area, including many projects such as CodeNN <ref type="bibr" target="#b30">[30]</ref>, DeepCom <ref type="bibr" target="#b26">[26]</ref>, Astattgru <ref type="bibr" target="#b40">[40]</ref>, C BERT <ref type="bibr" target="#b17">[18]</ref>, Rencos <ref type="bibr" target="#b75">[74]</ref>, SecNN <ref type="bibr" target="#b43">[42]</ref>, PLBART <ref type="bibr" target="#b0">[1]</ref>, CoTexT <ref type="bibr" target="#b55">[54]</ref>, ProphetNet-X <ref type="bibr" target="#b56">[55]</ref>, NCS <ref type="bibr" target="#b1">[2]</ref>, Code2seq <ref type="bibr" target="#b6">[7]</ref>, Re 2 Com <ref type="bibr" target="#b72">[71]</ref>, and many more <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b67">66,</ref><ref type="bibr" target="#b68">67,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b73">72,</ref><ref type="bibr" target="#b74">73]</ref>. All these approaches rely on datasets of aligned code-comment pairs. Typically, these datasets are then used to train complex deep learning models to model a probabilistic distribution of the form p(comments | code) ; one can sample from these (usually generative) models to create candidate comments for a given a piece of code. Given a dataset of code-comment pairs in a specific language, e.g., Java, or Python, or PHP, or Ruby, one can train models to translate code in that language to comments. The quality of the translation will depend largely upon the inductive power of the model, and quality and diversity of the code-comment dataset. / / R e t u r n s t h e t e x t c o n t e n t o f / / t h i s n ode and i t s d e s c e n d a n t s . p u b l i c S t r i n g g e t T e x t C o n t e n t ( ) { S t r i n g B u i l d e r sb =new S t r i n g B u i l d e r ( g e t C h i l d N o d e s C o u n t ( ) + 1 ) ; a p p e n d T e x t C o n t e n t ( sb ) ; ret u rn sb . t o S t r i n g ( ) ; } <ref type="figure">Figure 1</ref>: Example for code comment generation task Of late, given the power of GPUs, and the capacity of the models, the limitations largely arise from dataset quality and diversity, especially in languages for which limited, or rather specialized data is available. For instance, C XGLUE <ref type="bibr" target="#b48">[47]</ref> dataset consists of six languages (i.e., Ruby, Java, JavaScript, Go, Php, Python). Most languages have well over 100,000 training examples, covering a wide set of application domains. Some languages, particularly Ruby and Javascript, have far fewer examples, and cover a narrower range of application domains. As a result, state-of-the-art models perform less well for these two languages. This is a well-known problem for natural language translation: while training data for language pairs like ? ? ? is abundant, resources may be lacking for less-used languages like ? or . In such cases, a common technique is adapt ML models to learn useful statistics from abundant data in other, perhaps related languages <ref type="bibr" target="#b52">[51]</ref>. This works well when languages often have similar grammars, and share common word etymologies.</p><p>We propose an analogous approach to improve the diversity and quality of training data for software-engineering tasks, exploiting an interesting property of source code that human beings write. It's generally agreed that variable names help code comprehension <ref type="bibr" target="#b37">[37]</ref>. Developers know this, and typically choose descriptive variable names (reflective of code logic and purpose) regardless of the language they are coding in. Thus, one could expect that developers coding the same functionality, using similar algorithms, even in different languages, will use similar variable names. This suggests that machine-learning approaches could sometimes leverage corpora in different programming languages. This paper a) shows that this expectation actually has a sound empirical basis, and then b) demonstrates that this approach in fact works not just for code summarization, but also for several other tasks. We make the following contributions.</p><p>(1) Using the R C dataset, we provide evidence that programs solving the same problem in different languages are more likely to use the same or similar identifier names.</p><p>(2) We show evidence suggesting that cross-language training (e.g., train on Python, test on Ruby) can sometimes lead to better performance than same-language training. (3) We study the relative value of identifiers and syntax, using ablation, and find that identifier names may matter more. (4) We show that pooled multilingual training data improves performance on several tasks, but especially for languages lacking in diverse and abundant data. We top a leaderboard for code-comment synthesis 1 . (5) We show that multilingual training helps for two other tasks:</p><p>code retrieval, and method name prediction. (6) Finally, we evaluate a few different design choices for multilingual training, and discuss threats to our findings.</p><p>Overall, this paper a) shows that multilingual training is yet another useful technique in the general arsenal of ML approaches to exploit the naturalness of code, b) shows why it is useful, and c) shows how to take good advantage of it.</p><p>Note: Technical details follow, but precisely: what we study here is multilingual training in the fine-tuning stage of "foundation models" <ref type="bibr" target="#b11">[12]</ref>. Foundation models for code, like C BERT, G C BERT <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">22]</ref> already use multilingual data for pre-training. While pre-training is self-supervised and is done with unlabeled corpora, task-specific fine-tuning is usually supervised, using clean, hardwon labeled data; multilingual pooling can be useful here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND &amp; MOTIVATION</head><p>We now present some motivating evidence suggesting the value of multilingual training data for deep-learning applications to software tasks. We begin the argument focused on code summarization.</p><p>Deep learning models have been widely applied to code summarization, with papers reporting substantial gains in performance over recent years <ref type="bibr">[1, 2, 7, 18, 19, 24-28, 30, 38-42, 50, 54, 55, 66, 67, 69-74]</ref>. We focus here on what information in the code ML models leverage for summarization (while we use summarization to motivate the approach, we evaluate later on 3 different tasks). Does every token in the program under consideration matter, for the code summarization task? Or, are the function and variable names used in the programs most important? Since identifiers carry much information about the program, this may be a reasonable assumption.</p><p>Considering the content words 2 in the example in <ref type="figure">Figure 1</ref> there are four major terms (i.e., Returns, text content, node, and descendants) used in the summary. The first 3 directly occur as tokens or subtokens in the code. Though the word "descendants" is missing in the program, high capacity neural models like BERT <ref type="bibr" target="#b16">[17]</ref> can learn to statistically connect, e.g., "descendant" with the identifier subtoken "child". This suggests that, perhaps, comments are recoverable primarily from identifiers. If this is so, and identifiers matter more for comments than the exact syntax of the programming language, that may actually be very good news indeed. If developers choose identifiers in the same way across different languages (viz., problem-dependent, rather than language dependent) perhaps we can improve the diversity and quality of dataset by pooling training set across may languages. Pooled data sets may allow us to fine-tune using multilingual data, and improve performance, especially for low-resource languages (e.g., Ruby and JavaScript from C XGLUE <ref type="bibr" target="#b48">[47]</ref>). Since this is a core theoretical background for our work, we start off with two basic research questions to empirically gauge the possibility and promise of multilingual fine-tuning.</p><p>RQ1 What role do identifiers play in for code summarization? RQ2 Do programs that solve the same problem in different languages tend to use similar identifier names?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RQ1: Role played by identifiers</head><p>We first examine the importance of identifiers for code summarization; specifically, we compare the relative value of identifier tokens and other tokens. We use the C XGLUE dataset and pre-trained C BERT embeddings for the task <ref type="bibr" target="#b17">[18]</ref>. We begin with a brief backgrounder on C BERT <ref type="bibr" target="#b17">[18]</ref> &amp; BERT <ref type="bibr" target="#b16">[17]</ref>. C BERT uses the pre-training + fine-tuning strategy of BERT, RoBERTa etc <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">45]</ref>. This approach begins with a self-supervised "pre-training" step, to learn textual patterns from a large, unlabeled, corpus using just the content; in the next step, "fine-tuning", task-specific labeled data is used to provide task-related supervised training. This approach is known to achieve state-of-the-art performance in both natural language processing, and software-related tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b51">50,</ref><ref type="bibr" target="#b76">75]</ref>.</p><p>We study the effect of identifiers in several steps. For the pretraining step, we start with the available C BERT model, which is pre-trained on a large, multilingual corpus of code. For the finetuning step, for this task, we use the C XGLUE benchmark dataset (see <ref type="table" target="#tab_4">table 4</ref> for languages and dataset sizes); we start with the original set of code-comment pairs, and apply two different treatments to create overall three different fine-tuning training datasets-1) base case leaving code as is, 2) a treatment to emphasize identifiers, and 3) a treatment to de-emphasize them. First, to emphasize identifiers we abstract out the program's keywords, separators, and operators by replacing those with three generic tokens (i.e., "key", "sep", and "opt"), thus forcing the model (during fine-tuning) to rely more on the identifiers, for the task. Next, to assess the importance of keywords, separators, and operators, we abstract out the identifiers with a generic token "id". We fine-tune the model separately after each of these abstraction steps, thus yielding 3 fine-tuned models: the baseline, keyword-abstracted, and identifier-abstracted. We compare the results (smoothed BLEU-4) across all three.</p><p>If a fine-tuned model's performance is relatively unaffected by an abstraction, one may infer that the model relies less on the abstracted tokens. We perform these experiments with two languages with low-resource (i.e., Ruby and JavaScript, See table 4) and two languages with high-resource (i.e., Java and Python ). We train, validate, and test with the same dataset in each case. For each test instance, we have one value from the complete program and another one from each of the two abstracted versions. We compared these values, using two distinct pair-wise Wilcoxon tests: 1) Alternative Hypothesis (AH): complete program &gt; identifier deemphasis &amp; 2) AH: complete program &gt; identifier emphasis. We also perform the same test with the keyword-abstracted and identifierabstracted versions (AH: identifier emphasis &gt; identifier de-emphasis).</p><p>The data (table 1) suggests that abstracting the keyword, separator, and operator has a smaller impact on the performance: the BLEU-4 scores are rather similar (with effect size ranging from 0.002 to 0.033) to those from the unabstracted code. On the other hand,  The results in table 1 suggests that syntax is less relevant that identifier names. In all the prior works, the training and testing were done in the same language. Since syntax is less important, could we train and test with different languages? The C XGLUE dataset enables just such an experiment. Using six different languages, we apply a C BERT model fine-tuned in each language, to a test set in another language. <ref type="table" target="#tab_1">Table 2</ref> shows that for high-resource languages (i.e., Java, go, PHP, and Python), we achieve the best result (diagonal) when training and test data are from the same language. However, the performance does not degrade to a very large extent when trained with one language and tested on a different one. Surprisingly we observe that for Ruby and JavaScript, we actually achieve higher performance while trained with Java, PHP, and Python than the language itself. That indicates that code summarization is not completely dependent on syntax (perhaps it relies more on identifier similarity, which we shall explore next) Finding 1. Code summarization sometimes appears to train quite well with data sets from other languages, even if the syntax is different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RQ2: Identifier similarity across Languages</head><p>Here, we evaluate RQ2: given a problem, do developers choose similar, descriptive identifiers, regardless of the programming language? Based on the findings in the previous section: if identifiers were indeed used in similar ways, perhaps code-comment pairs from any programming language could help train a code summarization model, for any other language. As an example, <ref type="figure" target="#fig_1">Figure 2</ref> presents that all the "indexOf" functions implemented in Java, PHP and JavaScript use very similar identifiers "needle" and "haystack".</p><p>Quantitatively evaluating this hypothesis requires multiple implementations of the same problem in different programming languages, where we could compare identifier names. Luckily, R C provides just such a dataset. R C currently consists of 1,110 tasks, 305 draft tasks and includes 838 languages <ref type="bibr" target="#b2">3</ref> . We collect the mined data <ref type="bibr" target="#b3">4</ref> and study the same six languages (i.e., Ruby, JavaScript, Java, Go, PHP, and Python) in the C XGLUE dataset. We get 15 cross-language pairs from six languages and measure identifier similarity between pairs of programs which solve the same problem in each language (e.g., programs for graph diameter problem in Java and Ruby). For baselining, we also compare with a random pair (solving different problems) for the same two languages (e.g. graph diameter in Java, and SHA-hashing in Ruby). Fortunately, we found sufficient sample sizes for all our language pairs in R C . For example, for Java &amp; Python we find 544 matched program pairs solving the same problem in both languages. We then take the 544 Java programs and randomly pair them with 544 other Python programs. Therefore, we have two groups of programs (i.e., same program implemented in different languages and different programs implemented in different languages), and we check the similarity level between the two groups. Note that size-unrestricted random pairing may yield misleading results. Suppose we have a Java &amp; Python program matched pair with 100 Java subtokens and 40 Python subtokens. Now, if we replace the matched python program with a random, bigger program (e.g., 500 subtokens), we may have more chance of finding matched identifiers. Therefore, while choosing the random program, we try to ensure it has a similar length to the program it is replacing in the pair. We randomly select a program having the subtoken counts within a 5% length range (e.g., 38-42 subtokens for a 40 subtoken program) of the removed one. Fortunately, in 99.25% cases, we get at least one example within the 5% range. On the remaining instances, we select the program with the nearest subtoken count.</p><p>We measure identifier similarity thus:</p><p>(1) Remove all keywords, operators, and separators from the programs. (2) Break all CamelCase and snake_case identifiers and keep only one copy of each sub token. (3) Discard too-small programs with less than 5 sub-tokens. (4) Calculate the mean Szymkiewicz-Simpson coefficient (overlap coefficient) <ref type="bibr" target="#b66">[65]</ref> for both groups (i.e., same program pair and random pair) of programs. (5) Repeat this process across all 15 language pairs, for all program pairs. <ref type="table" target="#tab_3">Table 3</ref> shows the common program pairs have 89%-235% additional identifier overlap compared to random program pairs. We compare the matched and random pair overlaps using the nonparametric Wilcoxon signed-rank test (AH: random has less overlap than matched). We observe that the null hypothesis is rejected, and Szymkiewicz-Simpson Overlap coefficient 5 is significantly higher the sizes of the programs are quite different. It's calculated as</p><formula xml:id="formula_0">| ? | (| |,| |) .</formula><p>for the common program pairs in all the cases. That indicates programs solving the same problem (even in different languages) are much more likely to use the same or similar identifier names.  We also calculate each pair's Jaccard index <ref type="bibr" target="#b31">[31]</ref> (similarity coefficient) and find 112%-309% more similarity between common pairs than random ones, thus, giving essentially the same result. However, we prefer to report the detailed result using the overlap coefficient because Jaccard index can be affected by the differing verbosity of languages. For example, on average, Java, Python, and Ruby programs in R C have 29.45, 17.93, and 17.63 identifier subtokens. Java has higher subtokens compared to Python and Ruby because of the import statements, package naming etc. Therefore, Jaccard index between Java and Python will be lower than that of Python and Ruby even if the programs use very similar identifiers.</p><p>Finding 2. For a given problem, developers are likely to choose similar identifiers, even if coding in different languages.</p><p>In this section, we have presented evidence suggesting that a) identifiers are important for code summarization, that b) crosslanguage training is promising, and also that c) identifiers tend to be used in similar ways across languages. Taken together, these findings present a strong argument to try multilingual fine-tuning for SE tasks. Note that it is already well established that multilingual pre-training is helpful, and most BERT-style SE pre-trained models are multilingual <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b56">55]</ref>. However, pre-training data are unsupervised and easy to collect. Preparing clean data for the supervised fine-tuning phase requires more time and attention. In this paper, our aim is to prove that multilingual training is not only effective in pre-training stage but also in fine-tuning stage for SE models, which is already found to be beneficial for natural language models <ref type="bibr" target="#b64">[63]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BENCHMARK DATASETS AND TASKS</head><p>We evaluate the benefits of multilingual training in the context of several tasks, and associated datasets. In this section, we discuss the models and tasks used for our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Models</head><p>For our study of multilingual training, we adopt the BERT, or "foundation model" paradigm. Foundation models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b57">56]</ref> have two stages: i) unsupervised pre-training with corpora at vast scale and ii) fine-tuning with a smaller volume of supervised data for the actual task. Foundation models currently hold state-of-theart performance for a great many NLP tasks. BERT <ref type="bibr" target="#b16">[17]</ref> style models have also been adapted for code, pre-trained on a huge, multilingual, corpora, and made available: C BERT and G C BERT are both freely available: both source code and pre-trained model parameters. While these models for code have thus far generally been fine-tuned monolingually, they provide an excellent platform for training experiments like ours, to measure the gains of multilingual fine-tuning. C</p><p>BERT &amp; G C BERT use a multi-layer bidirectional Transformer-based <ref type="bibr" target="#b65">[64]</ref> architecture, and it is exactly as same as the RoBERTa <ref type="bibr" target="#b46">[45]</ref>, with 125M parameters; we explain them further below. Pre-training The C BERT <ref type="bibr" target="#b17">[18]</ref> dataset, has two parts: a matchedpairs part with 2.1M pairs of function and associated comment (NL-PL pairs) and 6.4M samples with just code. The code includes several programming languages. It was created by Hussain et al. <ref type="bibr" target="#b29">[29]</ref>. C BERT model is pre-trained with two objectives (i.e., Masked Language Modeling and Replaced Token Detection) on both parts. Mask language Modeling (MLM) is a widely applied and effective <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">45]</ref> training objective where a certain number of (15%) tokens are masked out, and the model is asked to find those tokens. For C BERT training, Feng et al. apply this first objective only to bimodal data <ref type="bibr" target="#b17">[18]</ref>. The second objective, Replaced Token Detection (RTD) <ref type="bibr" target="#b13">[14]</ref>, is a binary classification problem that is applied to both unimodal and bimodal data. Two data generators (i.e., NL and PL) generate plausible alternatives for a set of randomly masked positions, and a discriminator is trained to determine whether a word is the original one or not. We note that C BERT pre-training is all about representation-learning: by learning to perform the task well, the model learns a good way to encode the text, which is helpful during the next, fine-tuning stage. The pre-training took about 12 hours on a machine with 16 NVIDIA V100 cards, and would have taken us very much longer, so we were grateful to be able to just download the estimated parameters. Pre-training G C BERT G C BERT augments sourcecode with data flow, during pre-training. It uses a simple data flow graph (DFG) encoding a where-the-value-comes-from relation between variables <ref type="bibr" target="#b22">[22]</ref>. The DFG nodes are variable occurrences, edges are value flow. G C BERT pretraining learns a joint representation of 1) the DFG structure, 2) DFG alignment with source code, and 3) the source code token sequences. G C BERT is therefore pre-trained with three training objectives (i.e., Edge Prediction, Node Alignment, and MLM) on 2.3M functions (PL-NL pairs) from CodeSearchNet <ref type="bibr" target="#b29">[29]</ref> dataset. For details see <ref type="bibr" target="#b22">[22]</ref>.</p><p>The pre-training+fine-tuning approach relies on VERY high capacity models, and are pre-trained over a large, multilingual corpus. Thus, even before fine-tuning, the models already know a lot about each language. Thus, fine-tuning on many languages should not negatively impact what the model knows about any one language. Thus we find that multilingual fine-tuning improves on monolingual fine-tuning in most cases. We believe our proposed approach would still consider the context surrounding the individual programming language even after multilingual training because these models have sufficient capacity to do so.</p><p>We now describe our tasks: in each, we describe the task, the dataset, and the multilingual fine-tuning approach (if applicable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Code Summarization</head><p>The Task: as described earlier, the goal is to generate a NL summary given code in some PL. The Dataset: There are several different code summarization datasets; we chose C XGLUE 6 <ref type="bibr" target="#b48">[47]</ref>, for two main reasons: (1) C XGLUE is carefully de-duplicated <ref type="bibr" target="#b61">[60]</ref>. Prior datasets like TL-CodeSum <ref type="bibr" target="#b28">[28]</ref> have duplicates <ref type="bibr" target="#b61">[60]</ref> in training, testing, and validation partitions. Duplication can inflate measured performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b61">60]</ref>.</p><p>(2) We need a multilingual dataset to prove the effectiveness of multilingual fine-tuning. None of the existing datasets <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b40">40]</ref> is multilingual.  encoder is all ready well-trained in the pre-training stage; for finetuning, the encoder is primed with weights from pre-training. Now, the transformer model is given the input code token sequence and asked to generate the comment, as in the Neural Machine Translation (NMT) problem. We fine-tune using the C XGLUE paired samples. During fine-tuning, the decoder is trained auto-regressively, using next-token cross-entropy loss. Feng et al. use smooth BLEU-4 <ref type="bibr" target="#b45">[44]</ref> for the evaluations of the models. Subsequently, We replace the pre-trained C BERT with pre-trained G C BERT in the encoder while evaluating the effectiveness of multilingual finetuning with G C BERT. Why baseline with C BERT for code summarization? Feng et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>compare C</head><p>BERT with other popular encoder-decoder based (e.g., LSTM <ref type="bibr" target="#b62">[61]</ref>, Transformer <ref type="bibr" target="#b65">[64]</ref>, RoBERTa <ref type="bibr" target="#b46">[45]</ref>) models; C BERT handily beats all of them <ref type="bibr" target="#b17">[18]</ref>. Thus, C</p><p>BERT is a good baseline to measure the value of multilingual finetuning. C BERT also does very well on prior datasets: using smoothed Sentence BLEU-4, we found that C BERT reaches 44.89 on TL-Codesum <ref type="bibr" target="#b28">[28]</ref>, and 32.92 on Funcom [40] 7 . TL-Codesum has high degree of duplicates; we found that Funcom also does, but just in the comments. C XGLUE has very little duplication, which makes it more challenging, and also more reliable. Note that G C BERT does not report any performance on the code summarization task, and so we had to measure it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Code Search</head><p>The Task Given a natural language query, find the semantically closest code sample from a large set of candidates. Vector-based information retrieval methods can be used here along with BERTstyle encoders. C BERT was shown to perform quite well; the best published performance is reported by G C BERT <ref type="bibr" target="#b22">[22]</ref> (C BERT augmented with graph representations). We study the value of multilingual fine-tuning for both C BERT and G C BERT (pre-training of both models was discussed earlier in Section 3.1). The Dataset: Guo et al. adapt the same CodeSearchNet <ref type="bibr" target="#b29">[29]</ref> dataset, with some additional data for candidate codes <ref type="bibr" target="#b22">[22]</ref>. Note that it is basically the same dataset we used for code summarization except the candidate codes.</p><p>Model &amp; Fine-tuning We use Guo et al.'s G C BERT model, which at the time of submission is the best performing model with code and parameters available, and so is fine-tunable. The finetuning data is code (PL) matched with (NL) comments, from C XGLUE. The pre-trained G C BERT embedding vector is calculated for each PL and NL part. During fine-tuning, Guo et al. take a minibatch of (say ) NL query vector, along with (correct answers) PL answer vectors. 2 dot products are calculated; the embedding vectors are then full-stack trained to give "1" normalized dot product for the matches, and "0" for the mis-matches. For the actual retrieval, G C BERT calculates the vector embedding of a given query, and simply retrieves candidates ranked by the dot-product distance from the query vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Method Name Prediction</head><p>The Task as introduced by Allamanis et al. <ref type="bibr" target="#b5">[6]</ref> as the "extreme summarization" problem, the task is to predict the function name given the body. The Dataset: We adapt the C XGLUE dataset by extracting the function name and asking the model to find the name given the function body. Following <ref type="bibr" target="#b5">[6]</ref>, the function names are broken into subtokens using BPE <ref type="bibr" target="#b60">[59]</ref> (we've used BPE tokenization for all tasks). This problem then becomes very similar to code summarization.</p><p>Model &amp; Fine-tuning Previously Code2Seq <ref type="bibr" target="#b6">[7]</ref> and Code2Vec <ref type="bibr" target="#b7">[8]</ref> have worked on this problem. All prior works <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> use a monolingual datasets, which are not suitable for our experiment. We use the same model we used for summarization, except we now learn to sequentially generate the method name, subtoken by subtoken. We use F1-score for the evaluation. For example, the function name "createLocal" is broken into two sub tokens (i.e., create and Local), and the model predicts only "create". Hence, the precision, recall, and F1-score are 1.0, 0.5, and 0.66, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>In this section, we evaluate multilingual fine-tuning for the baselines for the tasks enumerated above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Code Summarization</head><p>We apply multilingual fine-tuning on the C XGLUE dataset. We first replicate the summarization task by (monolingually) fine-tuning the available pre-trained C BERT model for six languages <ref type="bibr" target="#b7">8</ref> . We replicate the fine-tuning stage for 2 reasons:</p><p>(1) We want to account for any hardware or environmental bias (e.g., we have a different set of GPUs than the original paper. We fine-tune with NVIDIA TITAN RTX, while Feng et al. <ref type="bibr" target="#b17">[18]</ref> use NVIDIA Tesla V100). (2) We use a pairwise two-sample statistical test (as described in <ref type="bibr" target="#b59">[58]</ref>, it is more precise than just comparing test-set summary statistics) to gauge differences. This requires a performance measurement for each test sample, which the repository did not include.</p><p>Our BLEU-4 numbers for monolingual training were close to reported numbers, with some differences; but we do obtain the same overall score (17.83) (table 5, leftmost 2 columns).</p><p>We use the same, per-language test sets to compare monolingual and multilingual fine-tuning. The validation set, however, is a single multilingual one combining all the monolingual validation sets. <ref type="table" target="#tab_7">Table 5</ref> shows that multilingual fine-tuning improves performance, even for high-resource languages (with more than 100K training instances). With C BERT, multilingual fine-tuning gains 2.5%-17.5% over monolingual fine-tuning, for all languages, yielding a 6.90% overall improvement (4.48% weighted improvement) <ref type="bibr" target="#b8">9</ref> . With the more advanced G C BERT, we see smaller gains, although the relative gains span a wide range.</p><p>We use a one-sided (AH: monolingual &lt; multilingual) pairwise Wilcoxon signed-rank test (thus avoiding the corpus-level measurement pitfalls noted in <ref type="bibr" target="#b59">[58]</ref>). Null hypothesis is rejected for all six languages, for C BERT. For G C BERT, it's rejected overall, and for every language; except for Javascript, where the p-value is 0.014 (all after B-H correction).</p><p>Thus our measurement indicates that multilingual fine-tuning provides a statistically significant improvement over monolingual training. We find rather low effect sizes using Cliff's Delta <ref type="bibr" target="#b49">[48]</ref>. While we report the effect size for the sake of completeness, this is not a major concern: we note that all gains are statistically highly significant. We also emphasize that even the minor improvements provided here by multilingual training (which is broadly compatible with a range of settings) constitute a relevant and potentially widely useful result. Roy et al <ref type="bibr" target="#b59">[58]</ref> have previously noted that small gains in BLEU-4 may not be perceptible to humans as increased text quality; nevertheless, we note that natural language translation (which is now widely used) attained high performance levels based on decades of incremental progress; this result and others below provide evidence that multilingual training could be an important step in the progress towards more useful automated tools. Finally, we note that BLEU-4 gains are higher for low-resource language (e.g., 17.7% for Ruby), and lower for high-resource languages (e.g., 2.5% for Python), as expected.</p><p>Comparing multi-lingual C BERT with other models Code summarization is widely studied-there are many models for this task; our specific focus here is to understand if multilingual fine-tuning provides benefits, using a high-quality token-sequence model and dataset. So we focus comparisons on the papers which report performance on C XGLUE dataset, and use a token-sequence inductive bias: comparing against all models is beyond the scope of this paper. We compare multi-lingual C BERT (P C BERT) and G C BERT (P G C BERT) with other models that have been published in peer-reviewed venues; among them, four apply pre-training strategies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b56">55]</ref>. We achieve the best overall performance (table 6), outperforming all the models, and for four specific languages (i.e., Ruby, Java, Go and PHP).</p><p>There is one other system, CoTexT <ref type="bibr" target="#b55">[54]</ref> which claims (in an unpublished, non-peer-reviewed report) better performance than us for just Python <ref type="bibr" target="#b55">[54]</ref>, but is worse overall. We will include it for comparison once it is published in a peer-reviewed venue.</p><p>This table also provides evidence supporting the effectiveness of multilingual fine-tuning. <ref type="bibr" target="#b8">9</ref> The C BERT paper simply averages the BLEU across languages to report the "overall" number; our weighted average weights each BLEU by the number of samples in that language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Code Search</head><p>We study the gains from multilingual fine-tuning using two pretrained models (i.e.,C</p><p>BERT &amp; G C BERT). We multilingually fine-tune both models using the publicly available code &amp; dataset <ref type="bibr" target="#b9">10</ref> . As we did for code summarization, we re-trained the baseline models, to get performance numbers for each case in the test set (to enable pairwise two-sample testing). We use the same test sets for both monolingual and multilingual training to evaluate our approach. During the training, G C BERT uses a matrix of dimension | | * | _ |. We could not use the full merged validation set (as we did for the code summarization task) because that makes the query and candidate code sets too large; the resulting matrix could not fit on our GPU server. We used a down-sampled validation set comprising six monolingual validation sets with 10K query and 50K candidate codes each. However, we did not face any issue while testing because we did not merge the test sets.</p><p>We report both the published values, and our replication; we need the replication to measure pairwise gains. Though C BERT and G C BERT both work on sequence of code tokens, G C BERT creates a rudimentary data-flow graph, once it's told the programming language. <ref type="table" target="#tab_11">Table 7</ref> shows that multilingual fine-tuning improves the mean reciprocal rank for all languages except Go with C BERT. The improvement for Ruby, JavaScript, and Java are statistically significant. We found similar results for G C BERT exhibiting improvement for Ruby, JavaScript, Java, and Python; but with G C BERT both Go and PHP showed performance declines. However, overall, both showed statistically signficant improvements (p &lt; 0.001); but the improvement for G C BERT (1.54%) is lower than C BERT (2.74%). Finally, we note that our numbers for C BERT differ from the performance reported for on the C XGLUE leaderboard. This is because C XGLUE benchmark uses only Python, and is based on a restricted setting where identifier names are left out. C XGLUE team argues that this abstraction enables them to stress-test the generalization ability of a model; however, here we consider an unmodified setting where someone gives an natural language query and wishes to find "natural" code with variable names intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Method Name Prediction</head><p>As for the previous two tasks, we try multilingual fine-tuning for method name prediction for C BERT. Here, too, we find evidence supporting the conclusion that multilingual training provides improvement for all the languages <ref type="table">(Table 8</ref>). Non-parametric pairwise improvements are significant for Ruby, JavaScript, and Java. We also note observe relatively greater effect size for Ruby and JavaScript. Note that we achieve highest improvement for JavaScript because many functions therein are anonymous lambdas, since these functions have no names, they are not useful, and this diminishes available the JavaScript training set relative to other tasks (lambdas still have summaries, and can be used for other tasks). Therefore, multilingual fine-tuning increases the dataset diversity and boosts JavaScript method name prediction performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Two Illustrative Examples</head><p>We used the same dataset for all tasks; for illustration, we show ( <ref type="table" target="#tab_13">Table 9</ref>) two test instances where all the tasks show improved performance from multilingual fine-tuning. In code summarization task, the monolingual fine-tuning scores 25 BLEU-4 in Example 1. C BERT produces a semantically wrong comment where multilingual fine-tuning generates the semantically correct solution. Note that the BLEU-4 is 84 for the second example because of the missing period in the gold standard (BLEU-4 is case-insensitive). Multilingual fine-tuning also helps the code search problem by increasing the MRR from 0.33 (Rank:3) to 1.00 (Rank:1). We also observe performance improvement from the method name prediction task. The gold standard consists of two sub tokens (i.e., set and Values), and mono-lingual fine-tuning generates three (i.e., set, Array, and Value), one of them is exact match. On the other hand, multilingual fine-tuning removes the extra "Array" subtoken and produces two subtokens(i.e., set and Value) resulting in the F-score 0.50. We observe a similar result in example 2. Note that like BLEU-4, our method name prediction metric is also case-insensitive.</p><p>Finding 3. Multilingual fine-tuning is likely to increase diversity and help the models perform better than those trained with smaller mono-lingual datasets, especially for low-resource languages, irrespective of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">INTERPRETING RESULTS, AND THREATS</head><p>In this section we consider several issues that are relevant to the observed performance of multilingual training, such as model choice, dataset duplication, performance metrics, generalization, and different training strategies for the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Overall Ruby JavaScript Go Python Java PHP  There are several models, including CoTexT <ref type="bibr" target="#b55">[54]</ref>, ProphetNet-X <ref type="bibr" target="#b56">[55]</ref>, and PLBART <ref type="bibr" target="#b0">[1]</ref> which report higher performance than C BERT [18] model for the code summarization task. The models for all these tasks were fine-tuned using monolingual datasets, so we might expect that multilingual fine-tuning should improve performance. These experiments would require a substantial investment of compute energy and is left for future work. We focused on C BERT (and also G C BERT on some tasks). We did some preliminary experiments with multilingual fine-tuning on PLBART. In our preliminary study, did see the same gains for low-resource language (Ruby, 5% gain). However, we found a 0.55% overall loss, which is inconsistent with what we observe with P C BERT (6.90% overall improvement) &amp; P G C BERT (5.64% overall improvement). More study is needed.</p><p>Finding 4. Multilingual fine-tuning could benefit a broad range of models. We find gains for C BERT and G C BERT, but more data is required for other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Threats: Risk of data duplication?</head><p>Data duplication can lead to poor-quality estimates of performance, especially when data is duplicated across training &amp; test; even duplication just within test data risks higher variance in the estimates. Allamanis finds that performance metrics are highly inflated when test data has duplicates, and advocates de-duplicating datasets, for more robust results <ref type="bibr" target="#b4">[5]</ref>. Shi et al. also discusses the impact of duplication in code summarization task <ref type="bibr" target="#b61">[60]</ref>.</p><p>Sadly, there is a large amount of copied code on GitHUB <ref type="bibr" target="#b47">[46]</ref>; inattentively combining different datasets harvested from GitHUB can lead to undesirable levels of duplication in the merged dataset. Fortnuately, C XGLUE is atually a carefully de-duplicated dataset; performance estimates therein are thus more robust. Combining multilingual data is unlikely to introduce the same kind of exact duplication in the dataset, because of syntax differences; There is a possibility of cross-language clones <ref type="bibr" target="#b54">[53]</ref>; the study of this is left for future work.</p><p>Finding 5. Combining multilingual datasets is unlikely to cause exact duplication, because of syntax differences. More study is needed to study the effect of cross-language clones.   <ref type="table">Table 8</ref>: Effectiveness of multi-lingual fine-tuning for method naming task. Note that p-values are adjusted using Benjamini-Hochberg</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Threats: Other metrics?</head><p>Following C XGLUE benchmark recommendation, we evaluate the code summarization task with smooth sentence BLEU-4 <ref type="bibr" target="#b45">[44]</ref> throughout this paper. However, other recognized metrics are are available (e.g., ROUGE-L <ref type="bibr" target="#b44">[43]</ref>, METEOR <ref type="bibr" target="#b9">[10]</ref>). Prior works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b61">60]</ref> provide a careful analysis of the metrics, baselines, evaluations for code summarization task. <ref type="table" target="#tab_15">Table 10</ref> shows ROUGE-L and ME-TEOR data; we find that multilingual fine-tuning increases the overall performance by 4.89% and 5.61% in ROUGE-L and METEOR, respectively. As with BLEU-4, we find that multilingual fine-tuning shows similar performance gains with these metrics. We find 0.3%-14.1% improvement in ROUGE-L and 1.2%-22.5% gains in METEOR (except for PHP, were we see a 0.17% decline, not statistically significant). We also see that Python shows the smallest improvement, not as strongly statistically significant. These metrics also indicate strong gains from multilingual training for low-resource and narrow-domain languages (i.e., Ruby and JavaScript). Finding 6. We observe performance improvement in all code summarization metrics with multilingual fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Monolingual minibatches? or multilingual?</head><p>While training deep neural networks with stochastic gradient descent, gradients (multivariate derivatives of loss w.r.t learnable parameters) are estimated over mini-batches, rather than calculating loss gradients over the entire training set; these estimates are used to adjust the weights in the network. Better choices of mini-batches could improve convergence behavior. With multilingual training, a natural question arises: is it better to sequentially interperse monolingual mini-batches (e.g., first a Java minibatch, then Ruby minibatch and so on, before going back to Java?) or should we make each minibatch per se multilingual?  In the previous experiments, we had randomly sort the dataset; hence, our mini-batches are also multilingual. So we deliberately tried sequentially monolingual minibatching during multilingual fine-tuning. We find that sequentially monolingual minibatch training appears to somewhat degrade performance: we observe the overall performance goes down by 1.05%. However, the change is not statistically significant for any language. We omit the actual numerical details, for space reasons, since we didn't find any strong results in either direction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Multilingual model as pre-trained model</head><p>Our findings provide evidence supporting the claim that a multilingual fine-tuned model is effective for code summarization task, which outperforms all the models trained with monolingual datasets. Could this this improved multilingual model further benefit from a secondary, monolingual fine-tuning exercise, where it receives specialized fine-tuning for each language separately? To evaluate this intriguing and promising idea, we load the model with the weights from multilingual fine-tuning, and fine-tune it, again, for each individual language. <ref type="table" target="#tab_17">Table 11</ref> shows that We found some minor performance improvement for JavaScript and Python. However, the performance goes down for other languages. We do not find evidence that a secondary, monolingual fine-tuning is helpful; further work is needed to understand why, and perhaps develop other ways this idea might yield further improvement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Code summarization: Code summarization has recently been a hot topic. More than 30 papers have been published in the last five years that follow some form of encoder-decoder architecture <ref type="bibr" target="#b59">[58]</ref>. Several works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b59">58,</ref><ref type="bibr" target="#b61">60]</ref> discuss the evaluations, metrics, and baselining. Roy et al. show that metric improvements of less than 2 points do not guarantee systematic improvements in summarization and are not reliable as proxies of human evaluation <ref type="bibr" target="#b59">[58]</ref>. We find more than 2 points of improvement for Ruby and almost 2 points of improvement for JavaScript. We observe less than 2 points in other languages. It should also be noted that we don't use the corpus-level metrics which Roy et al. show is problematic; we use pairwise comparisons on the test-sets. Finally, we note that progress in both code &amp; NLP occurs in small steps over decades, and innovations (especially ones that could cumulate with others) such as ours can be an important part of research community's long-term pursuit of practically relevant performance improvements.</p><p>Pre-trained models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b55">54,</ref><ref type="bibr" target="#b56">55]</ref> are proven to be more effective than prior models. Different pre-trained models are trained with the different pre-trained objectives even though fine-tuning steps are almost similar for all the models. As discussed earlier in Section 3.1, C</p><p>BERT is an encoder model, pre-trained with MLM and Repace Token Detection objectives. Unlike C BERT, PLBART <ref type="bibr" target="#b0">[1]</ref> is an encoder-decoder model which is trained as a denoising auto-encoder. Though all the models are pre-trained with different training objectives, there is one thing common among all the models: using Transformers as core architecture.</p><p>Parvez et al. very recently present an approach that augments training data using relevant code or summaries retrieved from a database (e.g., GitHub, Stack Overflow) <ref type="bibr" target="#b53">[52]</ref>. They apply this approach on monolingual Java and Python datasets from C XGLUE and claim gains over P C BERT &amp; P G C BERT for code summarization. Prima facie, multilingual fine-tuning is complementary to their approach; this needs to be studied. Code retrieval and method name prediction: Code retrieval is also getting attention recently. There are multiple datasets for this task. C XGLUE introduces a monolingual python dataset (taken initially from CodeSearchNet) abstracting the function names and variables. Guo et al. modify the multilingual CodeSearchNet dataset and achieve state-of-the-art performance on this task. However, using multilingual training, we show that both C BERT and G C BERT can be improved. There is one other very recent paper, CLSEBERT <ref type="bibr" target="#b69">[68]</ref> which reports (in an unpublished, nonpeer-reviewed report) better performance than us in all languages except Ruby. We could not show the effectiveness of multilingual training on CLSEBERT because the authors have not released the code implementation yet. Note that like code summarization, we focus only on the work using CodeSearchNet multilingual dataset.</p><p>CodeSearchNet dataset can be easily adapted to method name prediction task. Several earlier works address method name prediction, in a Java-only such as Code2Seq <ref type="bibr" target="#b6">[7]</ref>, Allamanis <ref type="bibr" target="#b5">[6]</ref>. They all use a conventional single-stage machine-learning approach (no pre-training + fine-tuning). Our goal here is to simply demonstrate that multilingual fine-tuning improves upon monolingual fine-tuning for the method-naming task, so we demonstrate using C BERT. Our numbers are roughly comparable with previously reported results, but we cannot make a precise comparison because of differences in subtokenization, and because our datasets are multilingual whereas previous work has largely been monolingual. We are simply arguing here our data suggests that multilingual finetuning is broadly beneficial in different tasks.</p><p>It would certainly be interesting to use same-domain data for fine-tuning. For example, summarizing methods in Android apps might work better if trained on Android app corpora; however curated, domain-specific datasets for each domain are needed, and may not always be possible, depending on the domain. However, cross-language data is already available, and we show that it does indeed help improve performance! The effect of domain-specific corpora is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We began this paper with three synergistic observations: First, when solving the same problem, even in different programming languages, programmers are more likely to use similar identifiers (than when solving different problems). Second, identifiers appear to be relatively much more important than syntax markers when training machine-learning models to perform code summarization. Third, we find that quite often a model trained in one programming language achieves surprisingly good performance on a test set in a different language, sometimes even surpassing a model trained on the same language as the test set! Taken together, these findings suggest that pooling data across languages, thus creating multilingual training sets, could improve performance for any language, particularly perhaps languages with limited resources, as has been found in Natural-language processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b64">63]</ref>. We test this theory, using two BERT-style models, C BERT, and G C BERT, with encouraging results.</p><p>Foundation models <ref type="bibr" target="#b11">[12]</ref> are currently achieving best-in-class performance for a wide range of tasks in both natural language and code. The models work in 2 stages, first "pre-training" to learn statistics of language (or code) construction from very large-scale corpora in a self-supervised fashion, and then using smaller labeled datasets to "fine-tune" for specific tasks. We adopt the multilingual C XGLUE dataset, and the pre-trained C BERT and G C BERT models, and study the value of multilingual fine-tuning for a variety of tasks. We find evidence suggesting that multilingual fine-tuning is broadly beneficial in many settings. Our findings suggest that multilingual training could provide added value in broad set of settings, and merits further study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>p u b l i c s t a t i c i n t i n d e x O f ( B y t e B u f n e e d l e , B y t e B u f h a y s t a c k ) { / / TODO : maybe u s e B o y e r Moor e f o r e f f i c i e n c y . i n t a t t e m p t s = h a y s t a c k . r e a d a b l e B y t e s ( ) ? n e e d l e . r e a d a b l e B y t e s ( ) + 1 ; for ( i n t i = 0 ; i &lt; a t t e m p t s ; i ++) { i f ( e q u a l s ( n e e d l e , n e e d l e . r e a d e r I n d e x ( ) , h a y s t a c k , h a y s t a c k . r e a d e r I n d e x ( ) + i , n e e d l e . r e a d a b l e B y t e s ( ) ) ) { ret u rn h a y s t a c k . r e a d e r I n d e x ( ) + i ; } } ret u rn ?1; } (a) Java p u b l i c s t a t i c f u n c t i o n i n d e x O f ( s t r i n g $ h a y s t a c k , s t r i n g $ n e e d l e , i n t $ o f f s e t = 0 ) : i n t { $p os = s e l f : : s t r p o s ( $ h a y s t a c k , $ n e e d l e , $ o f f s e t ) ; ret u rn i s _ i n t ( $p os ) ? $p os : ? 1 ; } (b) PHP f u n c t i o n i n d e x O f ( h a y s t a c k , n e e d l e ) { i f ( t y p e o f h a y s t a c k === ' s t r i n g ' ) ret u rn h a y s t a c k . i n d e x O f ( n e e d l e ) ; for ( l e t i = 0 , j = 0 , l = h a y s t a c k . l e n g t h , n= n e e d l e . l e n g t h ; i &lt; l ; i ++) { i f ( h a y s t a c k [ i ]=== n e e d l e [ j ] ) { j + + ; i f ( j ===n ) ret u rn i ? j + 1 ; } e l s e { j = 0 ; } } ret u rn ?1; } (c) JavaScript</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Usage of similar identifiers ( e.g., needle, haystack) in "in-dexOf" function in different programming languages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Intra and inter language training and testing</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Cross-language identifier similarity, when functionality is preserved</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 presents</head><label>4</label><figDesc>the number of training, testing and validation instances for each language. in C XGLUE.</figDesc><table><row><cell>Programming language</cell><cell>Training</cell><cell>Dev</cell><cell>Test</cell><cell>Candidate codes*</cell></row><row><cell>Ruby</cell><cell>24,927</cell><cell>1,400</cell><cell>1,261</cell><cell>4,360</cell></row><row><cell>JavaScript</cell><cell>58,025</cell><cell>3,885</cell><cell>3,291</cell><cell>13,981</cell></row><row><cell>Java</cell><cell>164,923</cell><cell>5,183</cell><cell>10,955</cell><cell>40,347</cell></row><row><cell>Go</cell><cell>167,288</cell><cell>7,325</cell><cell>8,122</cell><cell>28,120</cell></row><row><cell>PHP</cell><cell>241,241</cell><cell>12,982</cell><cell>14,014</cell><cell>52,660</cell></row><row><cell>Python</cell><cell>251,820</cell><cell>13,914</cell><cell>14,918</cell><cell>43,827</cell></row></table><note>Model &amp; Fine-tuning Feng et al. use a transformer-based encode- decoder architecture for the code summarization task [18]. The*Candidate codes are only used for code retrieval task</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>C XGLUE dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness of multi-lingual fine-tuning for code summarization task. Note that p-values are B-H corrected</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison to existing models, on C XGLUE dataset</figDesc><table><row><cell>5.1 Does multilingual fine-tuning help with</cell></row><row><cell>other models?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Effectiveness of multi-lingual fine-tuning for code search task. Note that p-values are BH-corrected</figDesc><table><row><cell>Language</cell><cell cols="6">C Precision Recall F-Score Precision Recall F-Score BERT P C BERT</cell><cell>F-Score Improvement</cell><cell>Effect Size</cell><cell>p-value (adjusted)</cell></row><row><cell>Ruby</cell><cell>0.44</cell><cell>0.40</cell><cell>0.41</cell><cell>0.53</cell><cell>0.49</cell><cell>0.49</cell><cell>20.59%</cell><cell>0.112</cell><cell>&lt;0.001</cell></row><row><cell>JavaScript</cell><cell>0.30</cell><cell>0.24</cell><cell>0.26</cell><cell>0.45</cell><cell>0.40</cell><cell>0.41</cell><cell>59.00%</cell><cell>0.215</cell><cell>&lt;0.001</cell></row><row><cell>Java</cell><cell>0.54</cell><cell>0.51</cell><cell>0.51</cell><cell>0.56</cell><cell>0.52</cell><cell>0.52</cell><cell>2.22%</cell><cell>0.016</cell><cell>&lt;0.001</cell></row><row><cell>Go</cell><cell>0.54</cell><cell>0.52</cell><cell>0.52</cell><cell>0.56</cell><cell>0.53</cell><cell>0.52</cell><cell>1.67%</cell><cell>0.015</cell><cell>0.004</cell></row><row><cell>PHP</cell><cell>0.56</cell><cell>0.53</cell><cell>0.52</cell><cell>0.57</cell><cell>0.53</cell><cell>0.53</cell><cell>1.30%</cell><cell>0.009</cell><cell>0.004</cell></row><row><cell>Python</cell><cell>0.49</cell><cell>0.45</cell><cell>0.45</cell><cell>0.50</cell><cell>0.45</cell><cell>0.46</cell><cell>1.60%</cell><cell>0.011</cell><cell>0.002</cell></row><row><cell>Overall Overall (weighted)</cell><cell>0. 48 0. 52</cell><cell>0.44 0.48</cell><cell>0.44 0.48</cell><cell>0.53 0.54</cell><cell>0.49 0.50</cell><cell>0.49 0.50</cell><cell>10.09% 3.37%</cell><cell>0.024</cell><cell>&lt;0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Examples exhibiting the effectiveness of multilingual training</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Performance improvement in ROUGE-L and METEOR for</figDesc><table><row><cell>code summarization task</cell></row><row><cell>Finding 7. We don't find any clear difference between multilin-</cell></row><row><cell>gual mini batches and (interspersed) monolingual mini batches.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Multilingual model as pre-trained model Finding 8. We don't find evidence that applying a secondary, mono-lingual fine-tuning provides benefits for all languages.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This claim is based on publicly available evidence. Please check https://microsoft.github.io/CodeXGLUE/ 2 "Content" words in linguistics, are words that carry meaning, as contrasted with function words, such as prepositions, pronouns, and conjunctions, which denote grammatical relationships. See https://en.wikipedia.org/wiki/Content_word. In code, we consider function words to be keywords, operators and punctuations, and content words to be identifiers (functions, variables, types, etc)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Last Accessed August, 2021 4 https://github.com/acmeism/RosettaCodeData 5 This is a measure of similarity like the Jaccard index; we use it here since sometimes</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CodeSearchNet<ref type="bibr" target="#b29">[29]</ref> dataset is a standard benchmark, which has been incorporated into C XGLUE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">As reported in<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">60]</ref>, measurement approaches vary across papers, and these numbers may differ from prior results: we use smoothed sentence BLEU-4 everywhere in our paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We use the publicly available C BERT implementation and dataset, https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/microsoft/CodeBERT/tree/master/GraphCodeBERT/codesearch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified Pre-training for Program Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2021.naacl-main.211" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2655" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Transformer-based Approach for Source Code Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to Find Usage of Library Functions in Optimized Binaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toufique</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand Ashok</forename><surname>Sawant</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSE.2021.3106572</idno>
		<ptr target="https://doi.org/10.1109/TSE.2021.3106572" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toufique</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">Rose</forename><surname>Ledesma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14671</idno>
		<title level="m">SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>cs.SE</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The adverse effects of code duplication in machine learning models of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</title>
		<meeting>the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="143" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A convolutional attention network for extreme summarization of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating Sequences from Structured Representations of Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gKYo09tX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>POPL</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Achieving Reliable Sentiment Analysis in the Software Engineering Domain using BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eeshita</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><forename type="middle">Efruz</forename><surname>Karabulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the Opportunities and Risks of Foundation Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7059" to="7069" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1536" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzheng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09340</idno>
		<title level="m">Lun Yiu Nie, and Xin Xia. 2021. Code Structure Guided Transformer for Source Code Summarization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ensemble learning for multisource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Garmash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1409" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Code to Comment ?Translation?: Data, Metrics, Baselining &amp; Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hariharan</forename><surname>Sezhiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="746" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GraphCode-BERT: Pre-training Code Representations with Data Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shuo Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Liu Shujie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Toward multilingual neural machine translation with universal encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04798</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved automatic summarization of subroutines via attention to file context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakib</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Mining Software Repositories</title>
		<meeting>the 17th International Conference on Mining Software Repositories</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masum</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanveer</forename><surname>Muttaqueen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><forename type="middle">Al</forename><surname>Ishtiaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Sajeed Mehrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahim</forename><surname>Anjum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahmid</forename><surname>Hasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14220</idno>
		<title level="m">Wasi Uddin Ahmad, Anindya Iqbal, and Rifat Shahriyar. 2021. CoDesc: A Large Code-Description Parallel Dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep code comment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep code comment generation with hybrid lexical and syntactical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2179" to="2217" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Summarizing source code with transferred api knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">?tude comparative de la distribution florale dans une portion des Alpes et des Jura</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull Soc Vaudoise Sci Nat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning type annotation: is big data enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Premkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toufique</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1483" to="1486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CURE: Code-Aware Neural Machine Translation for Automatic Program Repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaud</forename><surname>Lutellier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1161" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zeroshot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning and evaluating contextual embedding of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gogul</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5110" to="5121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A comparison of transformer and recurrent neural networks on multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surafel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Lakew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Federico</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective identifier names for comprehension and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Innovations in Systems and Software Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="303" to="318" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ensemble Models for Neural Source Code Summarization of Subroutines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakash</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11423</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved code summarization via a graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakib</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Program Comprehension</title>
		<meeting>the 28th International Conference on Program Comprehension</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A neural model for generating natural language summaries of program subroutines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<title level="m">IEEE/ACM 41st International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="795" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeepCommenter: a deep code comment generation tool with hybrid lexical and syntactical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1571" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SeCNN: A semantic CNN parser for code comment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page">111036</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Orange: a method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="501" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">D?j?Vu: a map of code duplicates on GitHub</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cristina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Maj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitesh</forename><surname>Zitny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sajnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
	<note>OOPSLA</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrosio</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">B</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<idno>abs/2102.04664</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cliff&apos;s Delta Calculator: A non-parametric effect size program for two groups of observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Macbeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenia</forename><surname>Razumiejczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rub?n Daniel</forename><surname>Ledesma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universitas Psychologica</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="545" to="555" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Raihan Islam Arnob</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junayed</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Faisal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08415</idno>
	</analytic>
	<monogr>
		<title level="m">Antonios Anastasopoulos, and Kevin Moran. 2021. Code to Comment Translation: A Comparative Study on Model Effectiveness &amp; Errors</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Studying the usage of text-to-text transfer transformer to support code-related tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Mastropaolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Scalabrino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Nader</forename><surname>Palacio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocco</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="336" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation for resource-poor languages using related resource-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1358" to="1367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md Rizwan Parvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Wasi Uddin Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baishakhi</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11601</idno>
		<title level="m">Retrieval Augmented Code Generation and Summarization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cross-language clone detection by learning over abstract syntax trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeru</forename><surname>Chiba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="518" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Peltekian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08645</idno>
		<title level="m">CoTexT: Multi-task Learning with Code-Text Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartuer</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiusheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08006</idno>
		<title level="m">ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surangika</forename><surname>Ranathunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>En-Shiun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjana</forename><forename type="middle">Prifti</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Skenduli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shekhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15115</idno>
		<title level="m">Mehreen Alam, and Rishemjit Kaur. 2021. Neural Machine Translation for Low-Resource Languages: A Survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reassessing automatic evaluation metrics for code summarization tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devjeet</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Fakhoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venera</forename><surname>Arnaoudova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<meeting>the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ensheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07112</idno>
		<title level="m">Neural Code Summarization: How Far Are We? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00401</idno>
		<title level="m">Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual translation with extensible multilingual pretraining and finetuning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A survey on similarity measures in text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Vijaymeena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Applications: An International Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improving automatic source code summarization via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering</title>
		<meeting>the 33rd ACM/IEEE International Conference on Automated Software Engineering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Reinforcement-learning-guided source code summarization via hierarchical attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on software Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04556</idno>
		<title level="m">Jin Liu, and Xin Jiang. 2021. CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ensheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.01933</idno>
		<title level="m">CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Code generation as a dual task of code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05923</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Retrieve and refine: exemplar-based neural comment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanqi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03644</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Yuchao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Moshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Junjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Qing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12938</idno>
		<title level="m">Yet Another Combination of IR-and Neural-based Comment Generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Retrieval-based neural source code summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1385" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdian</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agus</forename><surname>Stefanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haryono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00710</idno>
		<title level="m">Multi-source neural translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
