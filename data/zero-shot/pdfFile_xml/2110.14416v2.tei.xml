<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
							<email>jinwoo-kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeyoon</forename><surname>Oh</surname></persName>
							<email>saeyoon17@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
							<email>seunghoon.hong@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a generalization of Transformers to any-order permutation invariant data (sets, graphs, and hypergraphs). We begin by observing that Transformers generalize DeepSets, or first-order (set-input) permutation invariant MLPs. Then, based on recently characterized higher-order invariant MLPs, we extend the concept of self-attention to higher orders and propose higher-order Transformers for order-k data (k = 2 for graphs and k &gt; 2 for hypergraphs). Unfortunately, higher-order Transformers turn out to have prohibitive complexity O(n 2k ) to the number of input nodes n. To address this problem, we present sparse higher-order Transformers that have quadratic complexity to the number of input hyperedges, and further adopt the kernel attention approach to reduce the complexity to linear. In particular, we show that the sparse second-order Transformers with kernel attention are theoretically more expressive than message passing operations while having an asymptotically identical complexity. Our models achieve significant performance improvement over invariant MLPs and message-passing graph neural networks in large-scale graph regression and set-to-(hyper)graph prediction tasks. Our implementation is available at https://github.com/jw9730/hot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph is a universal data modality used to model social networks <ref type="bibr" target="#b27">[28]</ref>, chemical compounds <ref type="bibr" target="#b9">[10]</ref>, biological structures <ref type="bibr" target="#b8">[9]</ref>, and interactions in particle physics <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>. Recent graph neural networks (GNNs) adopt a message-passing scheme <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref>, where the node features are propagated and aggregated recurrently according to the neighborhood structure given in the input graph. Despite the simplicity, the local and recurrent nature makes them unable to discover dependency between any two nodes with a distance longer than the message-passing steps <ref type="bibr" target="#b10">[11]</ref>. The locality of message-passing is also known to be related to the over-smoothing problem that prevents scaling of GNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>An alternative approach is using a more general set of operations that involve global interactions while respecting the permutation symmetry of graphs. Such operations either produce the same output regardless of the node ordering of input graph (permutation invariance), or commute with node reordering (permutation equivariance). Message-passing is an equivariant operation, but is restricted to local neighborhoods defined by the input graph. Recently,   <ref type="bibr" target="#b25">[26]</ref> characterized the full space of invariant and equivariant linear layers. It turned out that these layers span not only the local neighborhood interactions of message-passing GNNs, but also global interactions such as the one between disconnected nodes, and even edge-to-node, or node-to-edge interactions <ref type="figure" target="#fig_1">(Figure 1</ref>). Notably, their formulation naturally extends to layers with different input &amp; output orders (e.g., edge in, node out), and higher-order layers for hypergraphs. In theory, an invariant MLP composed of these layers should be more powerful than message-passing GNNs as they can model long-range dependency between nodes <ref type="figure" target="#fig_1">(Figure 1</ref>). However, they are currently not widely adopted due to relatively low performance and high asymptotic memory complexity.  Contributions In this work, we present higher-order Transformers that address the low performance and high complexity of invariant MLPs. First, based on an observation that the renowned Transformer encoder generalizes first-order equivariant linear layers or DeepSets <ref type="bibr" target="#b34">[35]</ref>, we formulate higher-order Transformer layers that generalize equivariant linear layers by extending self-attention to higher orders ( <ref type="figure" target="#fig_1">Figure 1</ref>). Second, while Transformer layers with input and output orders k, l has O(n k+l ) asymptotic complexity, we show that by leveraging the sparsity of input hypergraphs, we can obtain O(m 2 ) complexity given input with m hyperedges. Further adopting kernel attention approaches, we propose a variant that reduces the complexity to O(m) and theoretically prove that they are more expressive than message passing networks. Finally, we test higher-order Transformers on a range of tasks, and demonstrate that they achieve significant improvements over invariant MLPs and are highly competitive in performance and scalability to the state-of-the-art GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>In this section, we first describe how (hyper)graphs can be treated as higher-order tensors. We then describe linear layers that operate on higher-order tensors while respecting node permutation symmetry. In particular, we analyze operations within linear layers for second-order tensors (graphs) and show they involve global interactions on top of local aggregation.</p><p>Let us define some notations. We denote a set as {a, ..., b}, a tuple as (a, ..., b), and denote [n] = {1, ..., n}. We denote the space of order-k tensors as R n k ?d where d is feature dimension. For an order-k tensor A ? R n k ?d , we use multi-index i = (i 1 , ..., i k ) ? [n] k to denote A i = A i1,...,i k ? R d . Let S n be the set of all permutations of [n]. ? ? S n acts on i by ?(i) = (?(i 1 ), ..., ?(i k )), and acts on an order-k tensor A by (? ? A) i = A ? ?1 (i) .</p><p>(Hyper)graphs as tensors Generally, a (hyper)graph data G can be represented as a tuple (V, A), where V is a set of n nodes and A ? R n k ?d encodes features attached to hyperedges. The type of the hypergraph is indicated by the order k of the tensor A. First-order tensor is a set of features (e.g., point cloud, bag-of-words) where A i is the feature of node i. Second-order tensor encodes edge features (e.g., adjacency) where A i1,i2 is the feature of edge (i 1 , i 2 ). Generally, an order-k tensor encodes hyperedge features (e.g., mesh) where A i1,...,i k is the feature of hyperedge (i 1 , ..., i k ).</p><p>Permutation invariance and equivariance Our problem of interest is building a functional relation f (A) ? T between tensor A and target T . If T is a single output vector, we often require that f is permutation invariant, that it satisfies f (? ? A) = f (A); if T is a tensor T = T, we often require that f is permutation equivariant, satisfying f (? ? A) = ? ? f (A), for all ? ? S n and A ? R n k ?d . In a typical design setup where a neural network f is built using linear layers and non-linear activations, the construction of f reduces to finding invariant and equivariant linear layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant and equivariant linear layers</head><p>We summarize invariant linear layers L k?0 : R n k ?d ? R d and equivariant linear layers L k?l : R n k ?d ? R n l ?d identified in   <ref type="bibr" target="#b25">[26]</ref>. Note that invariant layer is a special case of L k?l with l = 0. In summary, given an input A ? R n k ?d , the order-l output of an equivariant layer L k?l can be written with i ? [n] k , j ? [n] l as:</p><formula xml:id="formula_0">L k?l (A) j = ? i B ? i,j A i w ? + ? C ? j b ? ,<label>(1)</label></formula><p>where w ? ? R d?d , b ? ? R d are weight and bias parameters, B ? ? R n k+l and C ? ? R n l are basis tensors (will be defined), and ? and ? are equivalence classes of order-(k + l) and order-l  <ref type="figure" target="#fig_2">Figure 2</ref>: Example operations in a second-order layer L 2?2 . We illustrate how input edges (i 1 , i 2 ) = i are aggregated to an output edge (j 1 , j 2 ) = j in various equivalence classes (i, j) ? ? via basis tensor B ? i,j . Note that a loop ((i 1 , i 1 ) or (j 1 , j 1 )) represents a node.</p><formula xml:id="formula_1">? = {{i1, i2}, {j1}, {j2}} i1=i2, i1?j1, i1?j2, j1?j2 b(4)=15 ? in total B ?</formula><p>multi-indices, respectively. The equivalence classes are defined upon equivalence relation ? that, for multi-indices i, j ? [n] k , i ? j iff (i 1 , ..., i k ) = (?(j 1 ), ..., ?(j k )) for some node permutation ? ? S n . Then, a multi-index i and all members j in its equivalence class have the same (permutation-invariant) equality pattern:</p><formula xml:id="formula_2">i a = i b ? j a = j b for all a, b ? [k].</formula><p>Consequently, each equivalence class ? (or ?) is a distinct set of all order-(k + l) (or order-l) multi-indices having a specific equality pattern.</p><p>Notably, we can represent each equivalence class of order-k multi-indices as a unique partition of [k] regardless of n, where the partition specifies the equality pattern. e.g., with k = 2, we have two partitions and respective equivalent classes:</p><formula xml:id="formula_3">? 1 = {{i 1 , i 2 }} the set of all (i 1 , i 2 ) with i 1 = i 2 , and ? 2 = {{i 1 }, {i 2 }} the set of all (i 1 , i 2 ) with i 1 = i 2 . Thus, with b(k) the k-th</formula><p>Bell number or number of partitions of [k], we have b(k + l) equivalence classes ? for the weight and b(l) equivalence classes ? for the bias. We guide the readers interested in derivation to   <ref type="bibr" target="#b25">[26]</ref>.</p><p>With the equivalence classes, basis tensors are then defined as follows:</p><formula xml:id="formula_4">B ? i,j = 1 (i, j) ? ? 0 otherwise ; C ? j = 1 j ? ? 0 otherwise<label>(2)</label></formula><p>In Eq. (1), each equivalence class ? determines which (hyper)edges participate and how they interact in summation i B ? i,j A i . As an example, let us consider L 2?2 that maps input edges i = (i 1 , i 2 ) to output edges j = (j 1 , j 2 ). An equivalence class ? = {{i 1 , i 2 }, {j 1 , j 2 }} represents all (i, j) ? ? that i 1 = i 2 , j 1 = j 2 , and i 1 = j 1 . Then, due to masking by B ? , only input elements A i1,i2 with i 1 = i 2 participate in the summation and gives output L 2?2 (A) j1,j2 for j 1 = j 2 , i 1 = j 1 . Intuitively, this is analogous to computing a node feature by sum-pooling all the other nodes ( <ref type="figure" target="#fig_2">Fig. 2(b)</ref>). Different ? accounts for other interactions as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Notably, it contains a richer set of operations beyond local interactions modeled by message-passing ( <ref type="figure" target="#fig_2">Fig. 2(a)</ref>), such as global interaction across all nodes ( <ref type="figure" target="#fig_2">Fig. 2(b)</ref>), and edge-to-node ( <ref type="figure" target="#fig_2">Fig. 2(c)</ref>), node-to-edge interactions ( <ref type="figure" target="#fig_2">Fig. 2(d)</ref>).</p><p>We finish the section by writing out the first-order equivariant layer L 1?1 . As b(2) = 2, the layer has two equivalence classes ? 1 = {{i 1 , j 1 }} and ? 2 = {{i 1 }, {j 1 }}. Then, we have B ?1 = I n and B ?2 = 1 n 1 n ? I n , with 1 n ? R n vector of ones. Then, given a set of features A ? R n?d ,</p><formula xml:id="formula_5">L 1?1 (A) = I n Aw 1 + (1 n 1 n ? I n )Aw 2 + 1 n b (3) = I n Aw 1 + 1 n 1 n Aw 2 + 1 n b ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">w 1 , w 2 , w 1 , w 2 ? R d?d , b ? R d . L 1?1</formula><p>is analogous to a combination of elementwise feedforward (? 1 ) and sum-pooling of set elements (? 2 ), and is also known as a DeepSet layer <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Higher-Order Transformers</head><p>In Section 2, we introduced higher-order linear equivariant layers L k?l , and showed that they contain various global and node/edge interactions that are not covered by message-passing. In this section, we establish a connection between the first-order equivariant layer L 1?1 and self-attention of Transformer encoder layers <ref type="bibr" target="#b30">[31]</ref>. Then, we extend the relationship to higher orders by tensorizing queries and keys, and formulate higher-order Transformer layers. We finish the section by proposing a principled parameter reduction for queries and keys, which reduces a fair amount of computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformers generalize DeepSets</head><p>As shown in Section 2, first-order linear layer, or DeepSet, has a simple structure composed of feedforward and sum-pooling (Eq. (4)). Although it is theoretically proven to be a universal approximator of permutation-invariant functions <ref type="bibr" target="#b34">[35]</ref>, static sum-pooling could be limited in capturing interactions of set elements, motivating the use of sophisticated pooling. In particular, the self-attention mechanism of Transformer encoder <ref type="bibr" target="#b30">[31]</ref> was shown to achieve a large performance gain in set modeling via context-aware weighted pooling <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>. To see this, let us write out the Transformer encoder layers.</p><p>A Transformer encoder layer is a function Enc : R n?d ? R n?d consisting of two layers: a selfattention layer Attn : R n?d ? R n?d and an elementwise feedforward layer MLP : R n?d ? R n?d . For a set of n input vectors X ? R n?d , a Transformer layer computes the following:</p><formula xml:id="formula_7">Attn(X) i = X i + H h=1 n j=1 ? h ij X j w V h w O h ,<label>(5)</label></formula><formula xml:id="formula_8">Enc(X) i = Attn(X) i + MLP(Attn(X)) i ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">? h = ?(Xw Q h (Xw K h ) ) is an attention coefficient with an activation ?, H is the number of heads, d H is head size, d F is hidden dimension, and w O h ? R d H ?d , w V h , w K h , w Q h ? R d?d H . 1</formula><p>Now, we show that Transformer layers are generalized first-order linear equivariant functions. By setting ? h ij = 1 and assuming that MLP(Y ) approximates a linear layer Y w F + b F following the universal approximation theorem <ref type="bibr" target="#b15">[16]</ref>, the Transformer layer reduces to the following 2 :</p><formula xml:id="formula_10">Enc(X) i = X i (I n + w F ) + n j=1 X j w V O (I n + w F ) + b F ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_11">w V O = H h=1 w V h w O h . This is equivalent to a DeepSet layer in Eq. (4) with w 1 = I n + w F , w 2 = w V O (I n + w F ), and b = b F .</formula><p>In other words, we can convert DeepSets to Transformers by changing the static pooling to attention and replacing elementwise linear mapping with an MLP. We generalize this approach to higher-order input and output, which leads to the formulation of higher-order Transformers for graphs and hypergraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Higher-order Transformer layers</head><p>In Section 3.1, we showed that Transformer layers are generalized first-order equivariant linear layers L 1?1 . Notably, the generalization procedure was equivalent to changing static pooling to attention and adding feedforward MLP. In this section, we generalize the approach to L k?l with arbitrary orders k and l and formulate higher-order Transformer layer Enc k?l .</p><p>In general, we define higher-order Transformer layer as a function Enc k?l : R n k ?d ? R n l ?d with two layers: a higher-order self-attention layer Attn k?l : R n k ?d ? R n l ?d and a feedforward layer MLP l?l : R n l ?d ? R n l ?d . For an input tensor A ? R n k ?d , a Transformer layer computes:</p><formula xml:id="formula_12">MLP l?l (Attn k?l (A)) = L 2 l?l ReLU(L 1 l?l (Attn k?l (A))) ,<label>(8)</label></formula><formula xml:id="formula_13">Enc k?l (A) = Attn k?l (A) + MLP l?l (Attn k?l (A)),<label>(9)</label></formula><p>where L 1 l?l : R n l ?d?n l ?d F and L 2 l?l : R n l ?d F ?n l ?d are equivariant linear layers with hidden dimension d F . Remaining question is how to define and compute higher-order self-attention Attn k?l (A).</p><p>To design Attn k?l , we remove the bias from Eq. (1) and introduce attention coefficients. It is achieved by changing each B ? ? R n k+l to an attention coefficient tensor ? h,? ? R n k+l with multiple heads:</p><formula xml:id="formula_14">Attn k?l (A) j = H h=1 ? i ? h,? i,j A i w V h,? w O h,? ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_15">w O h,? ? R d H ?d , w V h,? ? R d?d H are learnable parameters,</formula><p>H denotes the number of heads, and d H denotes head size. Then similar to first-order case (Section 3.1), we can show the following: Theorem 1. Enc k?l (Eq. (9)) is a generalization of L k?l (Eq. (1)).</p><p>Proof. Let ? h,? i,j = 1 for all h, ?, and (i, j) ? ?. This renders ? h,? = B ? from the definition of B ? . Additionally, let MLP l?l (Attn k?l (A)) j = ? C ? j b ? for all j ? [n] l . That is, MLP l?l ignores input and reduces to an invariant bias in Eq. (1). Then, Eq. (9) reduces to the following:</p><formula xml:id="formula_16">Enc k?l (A) j = ? i B ? i,j A i H h=1 w V h,? w O h,? + ? C ? j b ? ,<label>(11)</label></formula><p>which is equivalent to Eq.</p><formula xml:id="formula_17">(1) with w ? = H h=1 w V h,? w O h,? .</formula><p>Now, we describe how to compute each attention tensor ? ? ? R n k+l from input A ? R n k ?d (Eq. (10), we drop head index h for brevity). We obtain each attention tensor from higher-order query and key:</p><formula xml:id="formula_18">? ? i,j = ?(Q ? j , K ? i )/Z j (i, j) ? ? 0 otherwise where Q ? = L ? k?l (A), K ? = L ? k?k (A).<label>(12)</label></formula><p>where Z j = i|(i,j)?? ?(Q ? j , K ? i ) is a normalization constant. Note that query and key tensors are computed from the input A using the equivariant linear layers in Eq. (1). This leads to permutation equivariance (or invariance) of Transformer encoder layer Enc k?l in Eq. <ref type="bibr" target="#b8">(9)</ref>.</p><p>Reducing orders of query and key Although Eq. (10) and Eq. (12) provide a simple and generic definition of higher-order self-attention, we observe that there exist a lot of unnecessary computations. Specifically, there exist elements of query Q ? and key K ? that are unused in computation of attention coefficient ? ? i,j , as it depends only on indices satisfying (i, j) ? ?. In fact, it turns out that the effective orders of query and key are much smaller than l and k, as we show below: Proposition 1. From Eq. (12), let u(?) denote the number of unique entries in a multi-index. With u q = u(j), u k = u(i) for some (i, j) ? ?, we can always find suitable linear layers L ? k?uq , L ? k?u k and index space mappings f ? q : [n] l ? [n] uq , f ? k : [n] k ? [n] u k that satisfy the following.</p><formula xml:id="formula_19">? ? i,j = ?(Q ? j ,K ? i )/Z j ?(i, j) ? ?,<label>(13)</label></formula><formula xml:id="formula_20">whereZ j = i|(i,j)?? ?(Q ? j ,K ? i ),Q ? = L ? k?uq (A),K ? = L ? k?u k (A), j = f ? q (j), i = f ? k (i). Proof.</formula><p>We leave the proof in Appendix A.1.1.</p><p>Based on Proposition 1, we can compute query and key in Eq. (12) in a much compact way using linear layers with output orders u q and u k instead of l and k. In our experiments, we observe that this optimization is very useful in reducing the number of parameters and memory footprint to a feasible level without affecting the effective model capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Asymptotically Efficient Higher-Order Transformers</head><p>In Section 3, we formulated higher-order Transformer layers Enc k?l and showed that they generalize linear equivariant layers L k?l . However, this capability comes with a cost; the high asymptotic complexity of the Transformer encoder limits its practical merits. Specifically, we show the following: Property 1. Given input size n, the asymptotic complexity of a linear layer L k?l (Eq. (1)) is O(n k+l ), and complexity of an encoder layer Enc k?l (Eq. (9)) is O(n k+l + n 2k + n 2l ).</p><p>Proof. We leave the proof in Appendix A.1.2.</p><p>Thus, in this section, we further analyze the encoder layer and propose a number of optimization and relaxation to reduce the asymptotic complexity with a minimal impact on capability. Notably, combining all our strategies reduces the complexity to O(m) given a hypergraph with m hyperedges. Even with such efficiency, we show that the reduced version of higher-order Transformer is theoretically more expressive than all message-passing neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear layers with reduced complexity</head><p>A major computation bottleneck within Transformer encoder layer Enc k?l is the higher-order linear layer, since it is used for key, query, and feedforward layer. By exploiting only a subset of basis, we show that we can implement lightweight version of linear layer with reduced asymptotic complexity. Proposition 2. Given a linear layer L k?l in Eq. (1), we can always find a nonempty subset M of equivalence classes such that computation of the following for all j has O(n l ) complexity.</p><formula xml:id="formula_21">L k?l (A) j = ??M i B ? i,j A i w ? + ? C ? j b ? ,<label>(14)</label></formula><p>Proof. We leave the proof in Appendix A.1.3. We term the reduced linear layerL k?l in Eq. <ref type="formula" target="#formula_0">(14)</ref> as a lightweight linear layer. In practice, we choose M among equivalence classes that do not involve summation over input. For instance, for L 1?1 , we use only the basis for elementwise mapping (I n ) and drop sum-pooling (1 n 1 n ) from Eq. (4). This approximation effectively reduces the complexity of linear layers at the cost of losing inter-element dependencies. We employ the lightweight linear layers within the Enc k?l (Eq. (9)) to compute query and key embeddings, while the element dependency within Enc k?l is handled by the higher-order self-attention using all equivalence classes as in Eq. <ref type="bibr" target="#b11">(12)</ref>. This design is coherent to original (first-order) Transformers, where the elements are first linearly projected to query/key with elementwise basis (I n ) and interaction is handled by attention (implicitly 1 n 1 n ). Importantly, this does not hurt Theorem 1 as attention coefficients can still reduce to one and MLP can reduce to bias.</p><p>By using lightweight linear layers in Enc k?l , we can significantly reduce the computational cost. The complexity of L ? k?u k , L ? k?uq , and MLP l?l in Eq. (13) and Eq. (9) reduces to O(n k ), O(n l ), and O(n l ), respectively, and as a result Enc k?l becomes O(n k+l ). As a result, we have higher-order Transformer layers Enc k?l that generalize L k?l while retaining the complexity O(n k+l ). From here we assume that all linear layers for key, query, and MLP are lightweight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sparse Transformer layers</head><p>Even with lightweight linear layers, O(n k+l ) complexity of Enc k?l is still impractical. For example, for graphs, complexity larger than O(n 2 ) is regarded prohibitive while Enc 2?2 is O(n 4 ). Fortunately, leveraging the sparsity inherent in real-world graphs can significantly reduce the complexity. As we show, it reduces the complexity of Enc k?l to O(m 2 ) for processing a hypergraph with m hyperedges.</p><p>Let E the set of hyperedges of input hypergraph G. Each hyperedge is generally represented by a multi-index i ? [n] k , so we denote E = {i 1 , ..., i m } with m hyperedges. Leveraging sparsity of E is straightforward when the order of the hypergraph is fixed (e.g., L k?k ); we can perform computations with only respect to the existing hyperedges i, j ? E. However, in our framework, order can change by layers (e.g., L k?l ), making it difficult to directly transfer the sparsity structure E to different-order output. Inspired by network projection <ref type="bibr" target="#b4">[5]</ref>, we remedy this by constructing E such that for any j ? E , there exists i ? E containing all unique elements of j. For example, for L 3?2 , this corresponds to projection of third-order E to second-order E by obtaining edges from sides of triangles. Despite simplicity, this simple heuristic works well in general and generalizes to any order.</p><p>Then, we integrate the hyperedge sets E, E into computation of linear layer in Eq. (14) as follows:</p><formula xml:id="formula_22">L k?l (A, E) j = ??M i?E B ? i,j A i w ? + ? C ? j b ? j ? E 0 otherwise<label>(15)</label></formula><p>Likewise, integrating E, E into attention computation in Eq. (10) and Eq. (12) we have:</p><formula xml:id="formula_23">Attn k?l (A, E) j = H h=1 ? i?E ? h,? i,j A i w V h,? w O h,? j ? E 0 otherwise (16) where ? h,? i,j = ?(Q h,? j , K h,? i )/Z j (i, j) ? ?, i ? E, j ? E 0 otherwise ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_24">Q h,? =L h,? k?l (A, E), K h,? =L h,? k?k (A, E), Z j = i|(i,j)???i?E ?(Q h,? j , K h,? i ).</formula><p>With the computations, we can show the following: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kernel attention trick</head><p>Section 4.2 shows that, by constraining linear layers within Enc k?l to have sparse input and output, we can reduce O(n k+l ) complexity to quadratic O(m 2 ) to input size. Yet, even O(m 2 ) can be demanding with large or dense input. As the quadratic term comes from self-attention computation, we follow the prior work in kernel attention <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref> and view attention coefficients as pairwise dot-product scores. As we will show, this allows us to further reduce the complexity of Enc k?l to O(n k + n l ) and even O(m) for sparse version, at the cost of relaxing some modeling assumption.</p><p>We begin by approximating attention coefficient in Eq. (12) using pairwise dot-product kernel <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6]</ref>:</p><formula xml:id="formula_25">? ? i,j = ?(Q ? j ) ?(K ? i )/Z j (i, j) ? ? 0 otherwise where Z j = i|(i,j)?? ?(Q ? j ) ?(K ? i )<label>(18)</label></formula><p>where ? :</p><formula xml:id="formula_26">R d H ? R d K + is kernel feature map.</formula><p>The choice of kernel can be flexible, and in our implementation we adopt Performer kernel <ref type="bibr" target="#b5">[6]</ref> that has strong theoretical and empirical guarantee.</p><p>Substituting Eq. (18) in Eq. (10), we have:</p><formula xml:id="formula_27">Attn k?l (A) j = ? Z ?1 j i|(i,j)?? ?(Q ? j ) ?(K ? i )A i w V ? w O ? = ? Z ?1 j ?(Q ? j ) i|(i,j)?? ?(K ? i )A i w V ? w O ? ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_28">Z j = ?(Q ? j ) i|(i,j)?? ?(K ? i ).<label>(20)</label></formula><p>Here, the inner-summations in Eq. <ref type="bibr" target="#b18">(19)</ref> and Eq. <ref type="formula" target="#formula_4">(20)</ref> are over the key index i, which is coupled with the query index j by (i, j) ? ?. This coupling causes the major computational bottleneck, since the inner-summations should be computed for every j-th query. We propose an approximation by decoupling the key and query indices and taking inner-summations over I = j {i|(i, j) ? ?}:</p><formula xml:id="formula_29">Attn k?l (A) j ? ? Z ?1 j ?(Q ? j ) i?I ?(K ? i )A i w V ? w O ? , where Z j ? ?(Q ? j ) i?I ?(K ? i ). (21)</formula><p>The approximation allows a query to attend to some additional keys (depending on ?), but it does not hurt the central requirement of attention that each query can assign different attention weights to the keys. With the approximation, we can compute the summations i?I ?(K ? i )A i and i?I ?(K ? i ) only once and reuse them across all query indices j to reduce the cost. Specifically, we show: Property 3. The encoder Enc k?l with approximation in Eq. (21) has a complexity of O(n k + n l ). Exploiting sparsity further reduces the complexity to O(m), linear to the number of hyperedges m. Proof. We leave the proof in Appendix A.1.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Theoretical analysis and comparison to message-passing</head><p>We showed that exploiting sparsity and adopting kernel attention reduces the computational complexity of the Transformer encoder to linear to input edges. Considering graphs (k = l = 2), this complexity is equivalent to or better than O(n + m) complexity of the message passing operation <ref type="bibr" target="#b2">3</ref> . Still, we show that our (approximate) encoder is theoretically more expressive than message passing.</p><p>Specifically, we show the following: Theorem 2. A composition of two sparse Transformer layers Enc 2?2 with kernel attention can approximate any message passing algorithms <ref type="bibr" target="#b9">(Gilmer et. al. (2017)</ref>  <ref type="bibr" target="#b9">[10]</ref>) to arbitrary precision, while the opposite is not true. Proof. We leave the proof in Appendix A.1.7.</p><p>This leads to the following corollary: Corollary 1. Second-order sparse Transformers with kernel attention are more expressive than any message-passing neural networks within the framework of Gilmer et. al. (2017) <ref type="bibr" target="#b9">[10]</ref>.</p><p>In the proof, we note that local information propagation in message-passing GNNs is carried out in Enc 2?2 by a single ? = {{i 1 }, {i 2 , i 3 , i 4 }}. Other types of ? would carry out different operations, which provides intuition on the powerfulness of Transformers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we demonstrate the capability of higher-order Transformers on a variety of tasks including synthetic data, large-scale graph regression, and set-to-(hyper)graph prediction. Specifically, we use a synthetic node classification dataset from Gu et. al. (2020) <ref type="bibr" target="#b10">[11]</ref>, a molecular graph regression dataset from Hu et. al. (2021) <ref type="bibr" target="#b16">[17]</ref>, two set-to-graph prediction datasets from Serviansky et. al. (2020) <ref type="bibr" target="#b28">[29]</ref>, and three hyperedge prediction datasets used in Zhang et. al. (2020) <ref type="bibr" target="#b35">[36]</ref>. Details including the datasets and hyperparameters can be found in Appendix A.2. We implemented invariant MLP in   <ref type="bibr" target="#b25">[26]</ref> as one of baselines, which we abbreviate as MLP ? . We build our model by gradually adding lightweight linear layers (Sec Runtime and memory analysis To experimentally verify our claims on linear complexity in Sec. 4.3, we conducted a runtime and memory consumption analysis on all second-order models using random graphs; details can be found in Appendix A.2.3. The results are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Consistent with our theoretical claims, sparse second-order Transformer with kernel attention (Ours (S, ?)) is the only variant that linearly scales to input size in terms of both time and memory. Also, it is the only one that successfully scales to graphs with &gt; 50k edges, while still very fast in smaller graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synthetic chains</head><p>To test the ability of higher-order Transformers in modeling long-range interactions in graphs, we used a synthetic dataset where the task is node classification in chain graphs. Binary class information is provided in a terminal node, so a model is required to propagate the information across the chain by handling long-range dependency. We used training and test sets with chains of length 20 and 200 respectively. As baselines, we used 3 message-passing networks, a sparse invariant MLP, and ablated versions of sparse second-order Transformers where ? for global pooling are removed (w/o global). Further details can be found in Appendix A.2.4. The test performances are in <ref type="table" target="#tab_1">Table 1</ref>. We first note that second-order Transformers successfully capture long-range dependency up to 200 nodes apart, while message-passing networks fail. Importantly, when the subset of basis that accounts for global pooling is eliminated, the performance of Transformers drops similar to message-passing networks, showing their importance in modeling long-range interaction. Yet, simply having global basis is not enough, as seen in the failure of MLP ? . This indicates fine-grained interaction modeling via attention is essential even in this simple task.</p><p>Large-scale graph regression To further evaluate higher-order Transformers in large-scale setting, we used the PCQM4M-LSC dataset from Open Graph Benchmark <ref type="bibr" target="#b16">[17]</ref>, which is the largest graphlevel regression dataset composed of 3.8M molecular graphs. As test data is unavailable, we report the Mean Absolute Error (MAE) on validation dataset. In addition to the baselines from the benchmark, we also report the performances of second-order invariant MLP and a vanilla (first-order) Transformer 4 for comparison. Further details can be found in Appendix A.2.5. The results are in <ref type="table" target="#tab_2">Table 2</ref>. Second-order Transformer outperforms the message-passing GNNs (GCN, GIN) by a large margin, including the ones with a virtual node that can model long-range interactions (GCN-VN, GIN-VN). It suggests that higher-order attention is potentially better in handling longrange interactions on graphs than the current practice of augmenting GNNs with a virtual node. Furthermore, second-order Transformer outperforms invariant MLP, indicating that replacing sum-pooling with attention is important for scale-up. Finally, second-order Transformer significantly outperforms vanilla Transformer with Laplacian graph embeddings. This is presumably because node embeddings are insufficient to utilize features associated with edges, while second-order Transformers can naturally use all edge information. We also note that invariant MLP and vanilla Transformer have worse complexity than the second-order Transformer (O(m 2 ) and O(n 2 ), respectively), while the second-order Transformer has O(m) complexity identical to GCN. Set-to-graph prediction An important advantage of our framework distinguished from most existing GNNs is that, by design, it can be applied to any learning scenario with different input and output orders (mixed-order).</p><p>To demonstrate this, we tested higher-order Transformers in set-to-graph prediction tasks where the goal is to predict edge structure of a graph given a set of node features. We used two datasets following the prior work <ref type="bibr" target="#b28">[29]</ref>. The first dataset Jets originates from particle physics experiments, where collision of high-energy particles gives a set of observed particles. The task is to partition the feature set of observed particles according to their origin. By viewing each subset of particles as a fully-connected graph, the problem is cast as a set-to-graph prediction. For the second dataset Delaunay, the task is to predict Delaunay triangulation <ref type="bibr" target="#b6">[7]</ref> given a set of points in 2D space. Two datasets are used for this task, one containing 50 points and the other containing varying number of points ? {20, ..., 80}. For the baselines, we take the scores reported in Serviansky et. al. (2020) <ref type="bibr" target="#b28">[29]</ref>, which includes GNNs and invariant MLPs (S2G, S2G+) with L 1?1 and L 1?2 . Our model is made by substituting the linear layers with Enc 1?1 and Enc 1?2 (mixed-order) respectively. Further details including the datasets, metrics, and baselines can be found in Appendix A.2.6.</p><p>The results are outlined in <ref type="table" target="#tab_3">Table 3</ref>. Mixed-order Transformers, both softmax and kernel attention, have favorable scores over all baselines. Especially, they outperform all baselines by a large margin in Delaunay: note that GNNs fall into trivial solution with high recall but very low precision. We particularly note that the Transformers' performance in Delaunay (20-80) is comparable to Delaunay (50), with 0.3-0.7% drop in F1 score. Compared with S2G that exhibits ? 12% drop, this indicates attention mechanism within Enc 1?2 is helpful in modeling varying number of nodes.  k-uniform hyperedge prediction One major advantage of our framework is that it naturally extends to higher-order data (hypergraphs). To demonstrate this, we consider higher-order extension of set-to-graph prediction task, where the goal is predicting k-uniform hyperedges (e.g., user-locationactivity) from node features. For evaluation, we used three datasets for transductive 3-edge prediction following the prior work <ref type="bibr" target="#b35">[36]</ref>. The first dataset GPS derives from a GPS network <ref type="bibr" target="#b37">[38]</ref>, and contains (user-location-activity) hyperedges. The second dataset MovieLens is a social network dataset of tagging activities <ref type="bibr" target="#b12">[13]</ref>, containing (user-movie-tag) hyperedges. The third dataset Drug comes from a medicine network from FAERS 5 , containing (user-drug-reaction) hyperedges. As baselines, we consider higher-order invariant MLP (S2G+) and the state-of-the-art, self-attention based Hyper-SAGNN <ref type="bibr" target="#b35">[36]</ref>. We implemented higher-order Transformer and S2G+ by substituting Enc 1?2 and L 1?2 in set-to-graph architectures to Enc 1?3 and L 1?3 , respectively. Further details including the datasets, metrics, and baselines can be found in Appendix A.2.2 and Appendix A.2.7.</p><p>The results are in <ref type="table" target="#tab_4">Table 4</ref>. Higher-order Transformer outperforms S2G+ in all datasets, and Hyper-SAGNN in all but one metric in MovieLens. The results suggest that higher-order self-attention is effective in learning higher-order representation beyond second-order graphs. The results are encouraging especially because we did not introduce any form of task-specific heuristics into the model, while some of the baselines such as Hyper-SAGNN depend on many inductive biases (static/dynamic branches, Hadamard power, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we proposed a generalization of Transformers to higher-orders, and applied a number of design strategies that reduce their complexity to a feasible level. Higher-order Transformers are attractive, both in theory and application. In theoretical aspect, it inherits the theoretical completeness and expressive power of invariant MLPs. In application aspect, it is potentially more powerful than message-passing neural networks due to global interaction modeling, and can be extended to a variety of useful mixed-order tasks involving sets, graphs, and hypergraphs.</p><p>At the same time, our work has some limitations that need to be addressed in future work. First, although complexity to input size can be lowered to linear, the number of basis grows rapidly with increasing order due to O((0.792k/ ln(k + 1)) k ) asymptotic formula of k-th Bell number <ref type="bibr" target="#b2">[3]</ref>, still making the model infeasible in higher orders. Improvement approaches such as finding a compact subset of basis that retains universality <ref type="bibr" target="#b28">[29]</ref> and exploiting unorderedness <ref type="bibr" target="#b25">[26]</ref> are promising in this direction. Second, our work builds upon tensor-based representation of graphs, which makes it difficult to be directly extended to hypergraphs containing edges with varying orders (e.g., co-citation networks). This is a common challenge to all tensor-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>, and we believe addressing this would be an important future research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Proofs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Proof of Proposition 1 (Section 3.2)</head><p>We start with the following lemmas: Lemma 1. Let ? an equivalence class of order-(k + l) multi-indices. Then, the set of all i ? [n] k such that (i, j) ? ? for some j ? [n] l is an equivalence class of order-k multi-indices. Likewise, the set of all j such that (i, j) ? ? for some i is an equivalence class of order-l multi-indices.</p><p>Proof. We only prove for i, as proof for j is analogous. For some (i 1 , j 1 ) ? ?, let us denote i 1 's equivalence class as ? k . It is sufficient that we prove i ? ? k ? (i, j) ? ? for some j.</p><p>(?) For all i ? ? k , as i 1 ? i we have i = ?(i 1 ) for some ? ? S n . As ? acts on multi-indices entry-wise, we have ?(i 1 , j 1 ) = (i, ?(j 1 )). As the equivalence pattern is invariant to node permutation by definition, we have ?(i 1 , j 1 ) ? (i, ?(j 1 )) ? (i 1 , j 1 ), and thus (i, ?(j 1 )) ? ?. Therefore, for all i ? ? k , we always have (i, j) ? ? when we set j = ?(j 1 ).</p><p>(?) For all (i, j) ? ?, as (i, j) ? (i 1 , j 1 ) we have (i, j) = ?(i 1 , j 1 ) for some ? ? S n . We have equivalently i = ?(i 1 ) and j = ?(j 1 ) for the ?, which leads to i ? i 1 and therefore i ? ? k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 2.</head><p>Let ? an equivalence class of order-k multi-indices. Then, every i ? ? contains the same number of unique elements, which is equal to |?| i.e., the number of nonempty subsets in ?'s partition.</p><p>Proof. All i ? ? have the same equality pattern, specified by ?'s representative partition. Specifically, for all i ? ?, i a = i b holds iff i a and i b belong to the same subset within ?'s partition. Therefore, each nonempty subset within ?'s partition specifies exactly one value within i, and any i a , i b s.t. i a = i b are contained in distinct subsets within ?'s partition. Thus, each subset in ?'s partition specifies one unique element in i, and we have the number of unique elements in i equal to |?| for all i ? ?. Now, we prove Proposition 1.</p><p>Proof. From Lemma 1, let us denote the set of all i ? [n] k such that (i, j) ? ? as an order-k equivalence class ? k , and denote the set of all j such that (i, j) ? ? as an order-l equivalence class ? q .</p><p>Then, in Eq. (12), to compute ? ? i,j ?(i, j) ? ? it is sufficient that we have K ? i ?i ? ? k and Q ? j ?j ? ? q . Based on the fact, we now analyze and reduce Q ? = L ? k?l (A) (K ? = L ? k?k (A) can be reduced analogously by letting l = k). From Eq. (1) and Eq. (2), we can write the computation of Q ? as follows, with ?, ? equivalence classes of order-(k + l) and order-l multi-indices and k ? [n] k :</p><formula xml:id="formula_30">Q ? j = ? k B ? k,j A k w ? + ? C ? j b ? ,<label>(22)</label></formula><p>where</p><formula xml:id="formula_31">B ? i,j = 1 (k, j) ? ? 0 otherwise ; C ? j = 1 j ? ? 0 otherwise<label>(23)</label></formula><p>A key idea is that, when we want Q ? j only for j ? ? q , only a subset of equivalence classes among ? or ? does effective computation and we can discard the rest. Specifically, we can discard an equivalence class ? if it contains some (k, j) with j / ? ? q . This is because, for such ?, (k, j) / ? ? if j ? ? q , leading to B ? k,j = 0 if j ? ? q . Therefore, such ? does not contribute to Q ? j ?j ? ? q and can be discarded. On the other hand, an equivalence class ? containing some (k, j) with j ? ? q does effective computation and should be kept.</p><p>From that, it turns out that the number of effective ? is ? b(k + u q ), where u q = u(j) = |? q | is the number of unique entries within some j ? ? q (see <ref type="bibr">Lemma 2)</ref>. Recall that for an effective ?, j ? ? q holds for all (k, j) ? ?. Within ?'s representative partition, as each j ? ? q has exactly u q unique values, we always have {j 1 , ..., j l } contained in exactly u q distinct subsets. Thus, the possible number of effective ? is upper-bounded by the number of ways of partitioning a set with k + |? q | elements, which is b(k + u q ). As for the bias, we can repeat the analysis with k = 0 and the number of effective ? is ? b(u q ).</p><formula xml:id="formula_32">? = {{i1, i2}, {j1}} ? q = {{j1}}, ? k = {{i1,i2}} i, j) = (i1, i2, j1) ? i2 i1 j1 i, j) ? Q ? = L 2 1 (A), K ? = L 2 2 (A) Q ? = L 2 1 (A), K ? = L 2 1 (A) ? ? i, j~? ? i1, i2, j1 = ?(Q ? j1, K ? i1, i2) / Zj1 ? ? i1, i2, j1 = ?(Q ? j'1, K ? i'1) / Zj'1~i '1 j'1 i1 j1 i2 (i'1?j'1)</formula><p>Naive Compact Naive:</p><p>Compact:</p><formula xml:id="formula_33">(i1=i2?j1) (k=2, l=1) index map: i'1=i1=i2, j'1=j1</formula><p>index map <ref type="figure">Figure 4</ref>: Exemplar illustration of computing ? ? i,j ?(i, j) ? ? with lower-order query and key, for k = 2, l = 1, i = (i 1 , i 2 ), j = (j 1 ), and ? = {{i 1 , i 2 }, {j 1 }}.</p><p>We now show that a lower-order linear layer L ? k?uq can compute Q ? j ?j ? ? q in Eq. <ref type="bibr" target="#b21">(22)</ref>. Let us denote A the set of all effective ? and L the set of all effective ?. Then we can rewrite Eq. (22) as:</p><formula xml:id="formula_34">Q ? j = ??A k B ? k,j A k w ? + ??L C ? j b ? ,<label>(24)</label></formula><p>where A has ? b(k + u q ) elements and L has ? b(u q ) elements. Assume we have some linear layer L ? k?uq . With j ? [n] uq , and ?, ? equivalence classes of order-(k + u q ) and order-u q multi-indices respectively, we can write:</p><formula xml:id="formula_35">Q ? j = ? k B ? k,j A k w ? + ? C ? j b ? ,<label>(25)</label></formula><formula xml:id="formula_36">where B ? k,j = 1 (k, j ) ? ? 0 otherwise ; C ? j = 1 j ? ? 0 otherwise<label>(26)</label></formula><p>We now identify the condition thatQ ? contains all Q ? j ?j ? ? q . For that, we need to define a mapping between index space ofQ ? and Q ? . To this end, we define a surjection g : [k] ? [u q ] that satisfies j a = j b ? g(a) = g(b). We can always define such g due to the property of equivalence classes that, for all a, b ? [l], j a = j b holds iff j a , j b belong to a same subset within ? q 's partition. By indexing the subsets within ? q 's partition, we define g(a) ?a ? [l] as the index of the subset that a belongs. Then, for every j ? ? q , we can find an order-u q compact form j ? [n] uq containing u q unique elements through g: for c ? [u q ] that c = g(a) = g(b) = ? ? ? , we construct j such that j c = j a = j b = ? ? ? . We define f q ? : [n] q ? [n] uq as a mapping that gives j = f q ? (j) ?j ? ? q . We now reduce Eq. (24) into Eq. <ref type="bibr" target="#b24">(25)</ref>. First, for each ? ? A, we assign a distinct order-(k + u q ) equivalence class ? that satisfies: (k, j) ? ? ? (k, j ) ? ? with j = f q ? (j). This can be done by changing ?'s partition into ?'s, by merging each set of j a = j b = ... into corresponding j c following f q ? . We similarly assign a distinct ? to each ? ? L. Then, we set w ? = w ? for all paired ? and ?, and set w ? = 0 for every ? not paired with any ?. We similarly set b ? = b ? for all paired ? and ?, and b ? = 0 for every ? not paired with any ?. From the definition of basis tensors, B ? k,j = B ? k,j for all paired ? and ?, and C ? j = C ? j for all paired ? and ?. Therefore, we haveQ ? j = Q ? j for all j ? ? q . Conclusively, we can always construct L ? k?uq and f ? q : [n] l ? [n] uq that gives Q ? j ?j ? ? q . As noted in the beginning of the proof, we can perform the same analysis with k = l to show the analogous result for K ? . We now have all entries K ? i ?i ? ? k and Q ? j ?j ? ? q to compute ? ? i,j ?(i, j) ? ? (Eq. (12)) and therefore Proposition 1 holds.</p><p>In <ref type="figure">Figure 4</ref> we provide an example of computing ? ? with lower-order query and key. Proof. We begin by analyzing the complexity of an equivariant linear layer L k?l <ref type="figure" target="#fig_1">(Eq. (1)</ref>). In the inner summation i B ? i,j A i , with u k and u q the number of unique entries in i and j respectively, we have n ?u k effective multiplication and summations for each output index j. Inequality is when j b = i a for some a, b, which corresponds to indexing operation rather than summation. Thus, the number of operations done by the summation is n uq n ?u k . With outer summation over ?, we have ? n uq n ?u k operations. As u q ? l and u k ? k by definition, we have inequality ? n uq n ?u k ? b(k + l)n k+l . Application of w ? gives us dd ? n uq n u k ? b(k + l)dd n k+l number of operations. For the bias, in the inner term C ? j , we need a single addition for each j and thus the number of operations for a ? is n uq ? n l . Summation over ? and application of bias parameters gives us b(l)d n uq ? b(l)d n l operations. Collectively, we have ? b(k + l)dd n k+l + b(l)d n l number of operations. As k, l, d, d are constants that does not depend on n, we obtain O(n k+l ) complexity.</p><formula xml:id="formula_37">(k=1, l=2) j1 j2 i1 ? = {{i1, j1, j2}} ? = {{i1, j1}, {j2}} j1 j2 i1 j1 j2 i1 ? = {{i1, j2}, {j1}} O ? j1j2 = Aj1 O ? j1j2 = ? i1 B ? i1j1j2Ai1 O ? j1j2 = Aj1 O ? j1j2 = Aj2 i1=j1=j2 i1=j1?j2 i1=j2?j1</formula><p>Computation of Enc k?l (A) (Eq. (9)) involves computing Attn k?l (A), MLP l?l (Attn k?l (A)) and adding them. Let us analyze Attn k?l (A) first. To compute ? h,? from input, we need to compute L ? k?uq (A) and L ? k?u k (A), followed by pairwise similarity computation and re-indexing. Assuming that each pairwise similarity computation and indexing has constant complexity, we have O(n k+uq + n k+u k + n uq+u k ). <ref type="bibr" target="#b5">6</ref> As u q ? l and u k ? k, we have O(n k+l + n 2k ). It is worth to note that O(n 2k ) term comes from computation of keys from input. Having computed ? h,? , the inner summation i ? h,? i,j A i , similar to in L k?l , has n uq n ?u k computations. With outer summation over ?, we have ? n uq n ?u k ? b(k + l)n k+l operations. Summation over heads and application of weight matrices gives us ? b(k + l)Hd 2 H dn k+l operations, which is O(n k+l ). For application of MLP l?l (?), we sum the complexity of two linear layers L l?l and element-wise ReLU, which gives us O(n 2l ). Conclusively, the complexity of Enc k?l is O(n 2k + n k+l + n 2l ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Proof of Proposition 2 (Section 4.1)</head><p>Proof. We assume k, l &gt; 0. Among the equivalence classes ? of order-(k + l) multi-indices, let us select a subset M that all ? ? M satisfies the following: for all (i, j) ? ?, i a = j b holds for all a ? [k] and some b ? [l]. In other words, every element in i is identical with at least one element in j, and i becomes a single fixed multi-index when we fix j (we denote the fixed i = fix(j)). This renders B ? i,j = 1 ? i = fix(j) for such ?, and consequently the inner-summation i B ? i,j A i w ? in Eq. (14) reduces to elementwise indexing A fix(j) w ? . As the size of M is upper-bounded by a constant b(k + l), we have O(n l ) complexity when computing Eq. <ref type="bibr" target="#b13">(14)</ref>. With the trivial case ? = {{i 1 , ..., i k , j 1 , ..., j l }} ? M, we can always find nonempty M.</p><p>To provide some intuition, we illustrate all ? ? M for k = 1, l = 2 in <ref type="figure" target="#fig_5">Figure 5</ref>. is a concatenation of X i , X j , E ij , with universal approximation theorem <ref type="bibr" target="#b15">[16]</ref>, we can have the output of the first Enc 2?2 (A) ij = M (X j , X i , E ij )+ 1 ?i, j where 1 is approximation error. This also holds when we leverage sparsity and restrict the index scope to (i, j) ? E ; in this case, we make (i, j) ? E \ E contain zero vectors.</p><p>3. Before feeding the output to the second Enc 2?2 , we concatenate the original input A with the output of the first layer in the channel dimension to make A ? R n?n?(2dv+de+dm) . This gives X i , X j , E ij encoded in the first 2d v + d e channels and M (X j , X i , E ij ) + 1 encoded in the last d m channels of A . This operation can be trivially absorbed within the MLP of the first layer, but we separate for simplicity.</p><p>4. Now, we make the second Enc 2?2 jointly approximate summation of messages over neighbors i?N (j) (?) and update function U (?), so that Enc 2?2 (A ) jj ? H j = U (X j , M j ).</p><p>First, we reduce Attn 2?2 (A ) to summation over neighbors. For this we only need two equivalence classes ? 1 = {{1}, {2, 3, 4}} and ? 2 = {{1, 2, 3, 4}}. Omitting normalization, we can write Eq. (21) as follows. For ? 1 we set u q = 1, u k = 2, i = (i, j), j = (j, j), i = (i, j), j = j, and for ? 2 we set u q = 1, u k = 1, i = (j, j), j = (j, j), i = j, j = j.</p><formula xml:id="formula_38">Attn 2?2 (A ) jj = ?(Q ?1 j ) {i|(i,j)?E } ?(K ?1 ij )A ij w ?1 + ?(Q ?2 j ) ?(K ?2 j )A jj w ?2 .<label>(29)</label></formula><p>Let entries in ?(K ? ), ?(Q ? ) be</p><formula xml:id="formula_39">1 d K ? d K</formula><p>so that their dot product is 1. Eq. (29) reduces to:</p><formula xml:id="formula_40">Attn 2?2 (A ) jj = {i|(i,j)?E } A ij w ?1 + A jj w ?2<label>(30)</label></formula><p>We make w ?1 zero-out the first 2d v + d e channels and w ?2 zero-out the last d e + d m channels. Then, we have Attn 2?2 (A ) jj contain X j in the first d v channels and M j + 2 = i|(i,j)?E (M (X j , X i , E ij )) + 2 in the last d m channels where 2 is approximation error 7 . We then apply MLP l?l on top of it, which can approximate the update function by universal approximation theorem <ref type="bibr" target="#b15">[16]</ref> and we have Attn 2?2 (A ) jj = U (X j , M j ) + 3 where 3 is approximation error.</p><p>Overall, the approximation error i at each step depends on i?1 (i &gt; 1), the MLP that approximates relevant function, and uniform bounds and uniform continuity of the approximated functions <ref type="bibr" target="#b15">[16]</ref>.</p><p>In the opposite, message passing cannot approximate some of the operations done by a single Transformer layer Enc 2?2 . This can be seen from the fact that, given a graph with diameter d(E), we need at least d(E) message passing operations to approximate output of Enc 2?2 . This is because a single Enc 2?2 can impose dependency between any pair of input and output indices i, j, while message passing requires d(E) steps in the worst case. Consequently, the approximation becomes impossible when the graph contains &gt; 1 disconnected components, which leads to d(E) ? +?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experimental details (Section 5)</head><p>In this section, we provide detailed information of the datasets and models used in our experiments in Section 5. We provide the dataset statistics in <ref type="table" target="#tab_5">Table 5</ref>, and model architectures in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Implementation details of higher-order Transformers</head><p>In formulation of higher-order Transformers in the main text, for simplicity we omitted layer normalization (LN) <ref type="bibr">[1]</ref> and used ReLU non-linearity for MLP l?l . In actual implementation, we adopt Pre-Layer Normalization (PreLN) <ref type="bibr" target="#b32">[33]</ref>, and place layer normalization before Attn k?l , before MLP l?l , and before the output linear projection after the last Enc k?l . We also use GeLU non-linearity <ref type="bibr" target="#b14">[15]</ref> in MLP l?l instead of ReLU. This setup worked robustly in all experiments. As additional details, we set the internal dimension of MLP l?l same as the input and output dimension (d F = d), and applied dropout <ref type="bibr" target="#b29">[30]</ref> within Attn k?l and MLP l?l to prevent overfitting. For k-uniform hyperedge prediction in Sec. 5, implementing the higher-order layers Enc 1?k and L 1?k can be challenging due to the large number of equivalence classes, b(1 + k). However, we found that it can be reduced to 1 + k without any approximation. Specifically, we show the following: Property 4. For L 1?k or Enc 1?k , if we only consider k-uniform output hyperedges (output hyperedges without loops; j-th output where j 1 , ..., j k are unique), the layers can be implemented using only 1 + k equivalence classes instead of b(1 + k).</p><p>Proof. As we only care about output hyperedges with unique index elements, only equivalence classes that correspond to partitions of [k + 1] with entries [k] contained in disjoint subsets contribute to output. There are exactly 1 + k such partitions depending on which subset the last entry (k + 1) belongs to, so it is sufficient that we have 1 + k equivalence classes.</p><p>From Property 4, we implement the layers Enc 1?k and L 1?k by only considering the 1 + k equivalence classes that contribute to k-uniform output hyperedges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Runtime and memory analysis</head><p>For runtime and memory analysis, we used Barab?si-Albert random graphs that are made by iteratively adding nodes, where each added node links to 5 random previous nodes. The experiment was done using a single RTX 6000 GPU with 22GB. We repeated the experiment 10 times with different random seeds for graph generation and reported the average; variance was generally low. The architectures of the experimented second-order models are provided in <ref type="table" target="#tab_6">Table 6a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Synthetic chains</head><p>For synthetic chains experiment, we used a small dataset composed of 40 training chains each with 20 nodes, and 20 test chains each with 200 nodes as in <ref type="table" target="#tab_5">Table 5a</ref>. Each chain is randomly assigned with a binary label, which is encoded as one-hot vector at a terminal node. The goal is to classify all nodes in the chain according to the label. As evaluation metrics, we used macro-/micro-F1 that give combined node-wise F1 scores across all test chains. All models, including baselines, have fixed hyperparameters with 2 layers and 16 hidden dimensions. Detailed architectures are provided in <ref type="table" target="#tab_6">Table 6b</ref>. For update function of GIN-0, we used an MLP with of 2 hidden layers followed by batchnorm (Linear(16)-ReLU-Linear(16)-ReLU-BN). For GAT, we used 8 attention heads followed by channelwise sum. For second-order Transformers, we used a simplified architecture with a single attention head. We trained all models with binary cross-entropy loss and Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with learning rate 1e-3 and batch size 16 for 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.5 Large-scale graph regression</head><p>For large-scale graph regression, we used the PCQM4M-LSC quantum chemistry regression dataset from OGB-LSC benchmark <ref type="bibr" target="#b16">[17]</ref>, one of the largest datasets up to date that contains 3.8M molecular graphs. We provide the summary statistics of the dataset in <ref type="table" target="#tab_5">Table 5b</ref>. As the test set is unavailable, we report and compare the Mean Absolute Error (MAE) measured on the validation set. <ref type="table" target="#tab_6">Table 6c</ref> gives the architectures of the models used in our experiment. For second-order models (MLP ? and Ours (S, ?)), we used both node and edge types as input information. For vanilla (firstorder) Transformer that operates on node features only, we used Laplacian graph embeddings <ref type="bibr">[2,</ref><ref type="bibr" target="#b7">8]</ref> in addition to node types so that the model can consider edge structure information. The embeddings are computed by factorizing the graph Laplacian matrix <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_41">? = I ? D ?1/2 AD ?1/2 = U ?U,<label>(31)</label></formula><p>where A is the adjacency matrix, D is the degree matrix, and ?, U are the eigenvalues and eigenvectors respectively. Following prior work <ref type="bibr" target="#b7">[8]</ref>, we used the k smallest eigenvectors of a node.</p><p>We trained all models with L1 loss using AdamW optimizer <ref type="bibr" target="#b23">[24]</ref> with batch size 1024 on 8 RTX 3090 GPUs. For all models, we used dropout rate of 0.1 to prevent overfitting. For the full schedule, we trained our model for 1M steps, and applied linear learning rate warm-up <ref type="bibr" target="#b30">[31]</ref> for 60k steps up to 2e-4 followed by linear decay to 0. For the short schedule (* in <ref type="table" target="#tab_2">Table 2</ref>), we trained the models for 100k steps, and applied learning rate warm-up for 5k steps up to 1e-4 followed by decay to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.6 Set-to-graph prediction</head><p>For set-to-graph prediction experiment, we borrow the datasets, code, and baseline scores from Serviansky et. al. (2020) <ref type="bibr" target="#b28">[29]</ref>. We provide the summary statistics of the datasets in <ref type="table" target="#tab_5">Table 5c</ref>.</p><p>As in main text, Jets is a dataset where the task is to infer partition of a set of observed particles. By viewing each partition as a fully-connected graph, the task becomes graph prediction problem. Each data instance contains 2-14 nodes, each having 10-dimensional features. The entire dataset contains 0.9M instances, divided into 60/20/20% train/val/test sets. Evaluation is done with 3 metrics: F1 score, Rand Index (RI), and Adjusted Rand Index (ARI) which is computed as ARI = (RI ? E[RI])/(1 ? E[RI]). To ensure that the model's prediction gives a correct partitioning, a postprocessing is applied to convert every connected components to cliques. The test set is further separated into 3 types: bossom(B)/charm(C)/light(L), depending on underlying data generation process. This makes typical # of partitions in each set different. Among the baselines, GNN is a message-passing GNN <ref type="bibr" target="#b9">[10]</ref> that operate on k-NN induced graph for k = 5, where edge prediction is done with pairwise dot-product. AVR is an algorithmic baseline typically used in particle physics.</p><p>As in main text, Delaunay datasets involve 2D point sets where the task is performing Delaunay triangulation. Evaluation metrics are typical Accuracy/Precision/Recall/F1 scores based on edgewise binary classification on held-out test set. The baselines are similar to Jets; GNN0/5/10 are message-passing GNNs <ref type="bibr" target="#b9">[10]</ref> that operate on k-NN induced graph for k ? {0, 5, 10}.  <ref type="bibr" target="#b28">[29]</ref>. S2G uses a subset of equivalence classes (?) within L 1?2 , and S2G+ uses full basis <ref type="bibr" target="#b7">8</ref> . Our models, both (D) and (D, ?), are made by substituting Enc 1?1 and Enc 1?2 into S2G+. All models were trained with Adam optimizer to minimize the combination of soft F1 score and binary cross-entropy of edge prediction. For all models, we used dropout rate of 0.1 to prevent overfitting. For Jets, with 400 max epochs, the training is early-stopped based on validation F1 score with 20-epoch tolerance. We used learning rate 1e-4 and batch size 512 for our models, while S2G/S2G+ used learning rate 1e-3 and batch size 2048 <ref type="bibr" target="#b28">[29]</ref>. For Delaunay, with 100 max epochs, we used learning rate 1e-4 for our models, and used batch size 32/16 for Delaunay (50)/(20-80); S2G/S2G+ used learning rate 1e-3 and batch size 64 <ref type="bibr" target="#b28">[29]</ref>. For Ours (D, ?) in Delaunay (20-80), we applied 1-epoch warmup to prevent early training instability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.7 k-uniform hyperedge prediction</head><p>For k-uniform hyperedge prediction experiment, we borrow the datasets, code, and baseline scores from Zhang et. al. (2020) <ref type="bibr" target="#b35">[36]</ref>. As in the main text, we used three datasets for transductive 3edge prediction. The first dataset GPS contains (user-location-activity) hyperedges. The second dataset MovieLens contains (user-movie-tag) hyperedges. The third dataset Drug contains (user-drugreaction) hyperedges. We provide the summary statistics in <ref type="table" target="#tab_5">Table 5a</ref>.</p><p>The experiments were done in a transductive setup, where the hyperedge set is randomly split into the training and test set with 4:1 ratio. We randomly sampled negative edges to be 5 times the amount of the positive edges, so that hyperedge prediction becomes binary classification problem. Thus, the evaluation is done with AUC and AUPR scores. Among the baselines, for Hyper-SAGNN, we reproduced the scores using the open-sourced code <ref type="bibr" target="#b35">[36]</ref> using the provided hyperparameters. For additional baselines including node2vec, we take the scores reported in Zhang et. al. (2020) <ref type="bibr" target="#b35">[36]</ref>. <ref type="table" target="#tab_6">Table 6e</ref> gives the architecture of the models used in our experiment. As Hyper-SAGNN uses autoencoder-based node features, for proper comparison we also adopted and trained them jointly with the full model <ref type="bibr" target="#b35">[36]</ref>. All models (including reproduced Hyper-SAGNN) were trained with Adam optimizer to minimize the combination of binary cross-entropy loss and autoencoder reconstruction loss for 300 epochs with learning rate 1e-3 and batch size 96. For S2G+ (S) and Ours (S, ?), we applied dropout rate of 0.1 to the hidden layers of MLP after L 1?3 or Enc 1?3 to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Potential negative social impacts</head><p>Our framework can be potentially applied to a variety of tasks involving relational data, e.g., molecular structures, social networks, 3D mesh, etc. Advancements in those directions might incur negative sideeffects such as low-cost biochemical weapon, deepening of filter bubbles from enhanced personalized social network services, surveillance with mesh-based face recognition, etc. Such potential negative impacts should be addressed as we conduct domain-specific follow-up works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>35th</head><label></label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2110.14416v2 [cs.LG] 22 Jan 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustrated operations of a message-passing GNN, an equivariant linear layer, and a secondorder Transformer layer. A single output node is highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Property 2 .</head><label>2</label><figDesc>When given E with m elements, the equivariant linear layer in Eq. (15) has O(m) complexity, and self-attention computation in Eq. (16) has O(m 2 ) complexity. Consequently, the computation done by a Enc k?l composed of the layers has O(m 2 ) complexity. Proof. We leave the proof in Appendix A.1.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of all second-order models in terms of forward time, memory consumption, and maximal possible input size. Plots are shown until each model runs into out-of-memory error on a RTX 6000 GPU with 22GB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. 4.1), sparse linear layers (Sec. 4.2), and kernel attention (Sec. 4.3) and denote them by (D), (S), and (?), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Exemplar illustration of all equivalence classes included in lightweight linear layerL 1?2 . A.1.2 Proof of Property 1 (Section 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Chain node classification results.</figDesc><table><row><cell>Method</cell><cell cols="2">Micro-F 1 (%) Macro-F 1 (%)</cell></row><row><cell>GCN</cell><cell>47.78 ? 4.17</cell><cell>33.58 ? 1.86</cell></row><row><cell>GIN-0</cell><cell>53.72 ? 4.17</cell><cell>36.22 ? 1.86</cell></row><row><cell>GAT</cell><cell>47.78 ? 4.17</cell><cell>33.58 ? 1.86</cell></row><row><cell>MLP ? (S)</cell><cell>53.5 ? 4.16</cell><cell>36.04 ? 1.97</cell></row><row><cell>Ours (S) w/o global</cell><cell>53.72 ? 4.17</cell><cell>36.22 ? 1.86</cell></row><row><cell cols="2">Ours (S, ?) w/o global 50.77 ? 5.15</cell><cell>35.22 ? 2.17</cell></row><row><cell>Ours (S)</cell><cell>100 ? 0</cell><cell>100 ? 0</cell></row><row><cell>Ours (S, ?)</cell><cell>100 ? 0</cell><cell>100 ? 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PCQM4M-LSC largescale graph regression results. * indicates results are obtained with a shorter schedule (10% of the full iterations).</figDesc><table><row><cell>Model</cell><cell>Validate MAE</cell></row><row><cell>MLP-FINGERPRINT ([17])</cell><cell>0.2044</cell></row><row><cell>GCN ([17])</cell><cell>0.1684</cell></row><row><cell>GIN ([17])</cell><cell>0.1536</cell></row><row><cell>GCN-VN ([17])</cell><cell>0.1510</cell></row><row><cell>GIN-VN ([17])</cell><cell>0.1396</cell></row><row><cell>Transformer + Laplacian PE*</cell><cell>0.2162</cell></row><row><cell>MLP ? (S)*</cell><cell>0.1464</cell></row><row><cell>Ours (S, ?) ?SMALL *</cell><cell>0.1376</cell></row><row><cell>Ours (S, ?)*</cell><cell>0.1294</cell></row><row><cell>Ours (S, ?)</cell><cell>0.1263</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>Method</cell><cell>F1</cell><cell>RI</cell><cell>ARI</cell><cell></cell><cell cols="2">Method</cell><cell>Acc</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell></cell><cell>AVR</cell><cell cols="3">0.565 0.612 0.318</cell><cell></cell><cell>SIAM</cell><cell></cell><cell cols="2">0.939 0.766 0.653 0.704</cell></row><row><cell></cell><cell>MLP</cell><cell cols="3">0.533 0.643 0.315</cell><cell></cell><cell cols="2">SIAM-3</cell><cell cols="2">0.911 0.608 0.538 0.570</cell></row><row><cell></cell><cell>SIAM</cell><cell cols="3">0.606 0.675 0.411</cell><cell></cell><cell>GNN0</cell><cell></cell><cell cols="2">0.826 0.384 0.966 0.549</cell></row><row><cell>Jets (B)</cell><cell>SIAM-3 GNN S2G</cell><cell cols="3">0.597 0.673 0.396 0.586 0.661 0.381 0.646 0.736 0.491</cell><cell>Delaunay (50)</cell><cell cols="2">GNN5 GNN10 S2G</cell><cell cols="2">0.809 0.363 0.985 0.530 0.759 0.311 0.978 0.471 0.984 0.927 0.926 0.926</cell></row><row><cell></cell><cell>S2G+</cell><cell cols="3">0.655 0.747 0.508</cell><cell></cell><cell>S2G+</cell><cell></cell><cell cols="2">0.983 0.927 0.925 0.926</cell></row><row><cell></cell><cell>Ours (D)</cell><cell cols="3">0.667 0.746 0.520</cell><cell></cell><cell cols="2">Ours (D)</cell><cell cols="2">0.994 0.981 0.967 0.974</cell></row><row><cell></cell><cell cols="4">Ours (D, ?) 0.670 0.751 0.526</cell><cell></cell><cell cols="4">Ours (D, ?) 0.991 0.967 0.952 0.959</cell></row><row><cell></cell><cell>AVR</cell><cell cols="3">0.695 0.650 0.326</cell><cell></cell><cell>SIAM</cell><cell></cell><cell cols="2">0.919 0.667 0.764 0.687</cell></row><row><cell></cell><cell>MLP</cell><cell cols="3">0.686 0.658 0.319</cell><cell></cell><cell cols="2">SIAM-3</cell><cell cols="2">0.895 0.578 0.622 0.587</cell></row><row><cell></cell><cell>SIAM</cell><cell cols="3">0.729 0.695 0.406</cell><cell></cell><cell>GNN0</cell><cell></cell><cell cols="2">0.810 0.387 0.946 0.536</cell></row><row><cell>Jets (C)</cell><cell>SIAM-3 GNN S2G</cell><cell cols="3">0.719 0.710 0.421 0.720 0.689 0.390 0.747 0.727 0.457</cell><cell>Delaunay (20-80)</cell><cell cols="2">GNN5 GNN10 S2G</cell><cell cols="2">0.777 0.352 0.975 0.506 0.746 0.322 0.970 0.474 0.947 0.736 0.934 0.799</cell></row><row><cell></cell><cell>S2G+</cell><cell cols="3">0.751 0.733 0.467</cell><cell></cell><cell>S2G+</cell><cell></cell><cell cols="2">0.947 0.735 0.934 0.798</cell></row><row><cell></cell><cell>Ours (D)</cell><cell cols="3">0.755 0.732 0.469</cell><cell></cell><cell cols="2">Ours (D)</cell><cell cols="2">0.993 0.982 0.960 0.971</cell></row><row><cell></cell><cell cols="4">Ours (D, ?) 0.757 0.735 0.473</cell><cell></cell><cell cols="4">Ours (D, ?) 0.989 0.948 0.956 0.952</cell></row><row><cell></cell><cell>AVR</cell><cell cols="3">0.970 0.965 0.922</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell cols="3">0.960 0.957 0.894</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>SIAM</cell><cell cols="3">0.973 0.970 0.925</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Jets (L)</cell><cell>SIAM-3 GNN S2G</cell><cell cols="3">0.895 0.876 0.729 0.972 0.970 0.929 0.972 0.970 0.931</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>S2G+ Ours (D)</cell><cell cols="3">0.971 0.969 0.929 0.974 0.972 0.935</cell><cell cols="2">Ground Truth</cell><cell cols="2">Ours (D,?)</cell><cell>S2G</cell><cell>FP FN</cell></row><row><cell></cell><cell cols="4">Ours (D, ?) 0.974 0.972 0.935</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Set-to-graph results. Lower-right panel shows Delaunay (20-80) sample from ours and S2G.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>k-uniform hyperedge prediction results. For Hyper-SAGNN, we reproduced the scores using the open-sourced code. For additional baselines including node2vec, we take the scores reported in <ref type="bibr" target="#b35">[36]</ref>.</figDesc><table><row><cell></cell><cell>GPS</cell><cell>MovieLens</cell><cell>Drug</cell></row><row><cell></cell><cell cols="3">AUC AUPR AUC AUPR AUC AUPR</cell></row><row><cell cols="4">node2vec-mean ([36]) 0.563 0.191 0.562 0.197 0.670 0.246</cell></row><row><cell>node2vec-min ([36])</cell><cell cols="3">0.570 0.185 0.539 0.186 0.684 0.258</cell></row><row><cell>DHNE ([36])</cell><cell cols="3">0.910 0.668 0.877 0.668 0.925 0.859</cell></row><row><cell>Hyper-SAGNN-E</cell><cell cols="3">0.947 0.788 0.922 0.792 0.963 0.897</cell></row><row><cell>Hyper-SAGNN-W</cell><cell cols="3">0.907 0.632 0.909 0.683 0.956 0.890</cell></row><row><cell>S2G+ (S)</cell><cell cols="3">0.943 0.726 0.918 0.737 0.963 0.898</cell></row><row><cell>Ours (S, ?)</cell><cell cols="3">0.952 0.804 0.923 0.771 0.964 0.901</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the datasets. (a) Statistics of the synthetic chains dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Statistics of the PCQM4M-LSC dataset.</cell></row><row><cell>Dataset</cell><cell>Chains</cell><cell></cell><cell></cell><cell>Dataset</cell><cell>PCQM4M-LSC</cell></row><row><cell>Size</cell><cell>60</cell><cell></cell><cell></cell><cell>Size</cell><cell>3.8M</cell></row><row><cell># classes</cell><cell>2</cell><cell></cell><cell></cell><cell>Average # node 14.1</cell></row><row><cell cols="3">Average # node 20 (train) / 200 (test)</cell><cell></cell><cell>Average # edge 14.6</cell></row><row><cell></cell><cell cols="4">(c) Dataset statistics for set-to-graph prediction.</cell></row><row><cell cols="2">Dataset</cell><cell>Jets</cell><cell cols="2">Delaunay (50) Delaunay (20-80)</cell></row><row><cell>Size</cell><cell></cell><cell cols="2">0.9M 55k</cell><cell>55k</cell></row><row><cell cols="3">Average # node 7.11</cell><cell>50</cell><cell>50</cell></row><row><cell cols="3">Average # edge 35.9</cell><cell>273.6</cell><cell>273.9</cell></row><row><cell cols="5">(d) Dataset statistics for k-uniform hyperedge prediction. For each dataset,</cell></row><row><cell cols="5">each row under "# nodes" correspond to each row under "Node types".</cell></row><row><cell></cell><cell>Dataset</cell><cell>GPS</cell><cell></cell><cell>MovieLens Drug</cell></row><row><cell></cell><cell></cell><cell>user</cell><cell></cell><cell>user</cell><cell>user</cell></row><row><cell></cell><cell>Node types</cell><cell cols="3">location movie</cell><cell>drug</cell></row><row><cell></cell><cell></cell><cell cols="3">activity tag</cell><cell>reaction</cell></row><row><cell></cell><cell></cell><cell>146</cell><cell></cell><cell>2,113</cell><cell>12</cell></row><row><cell></cell><cell># nodes</cell><cell>70</cell><cell></cell><cell>5,908</cell><cell>1,076</cell></row><row><cell></cell><cell></cell><cell>5</cell><cell></cell><cell>9,079</cell><cell>6,398</cell></row><row><cell></cell><cell># edges</cell><cell cols="2">1,436</cell><cell>47,957</cell><cell>171,756</cell></row><row><cell cols="4">A.2.2 Efficient implementation of 1 ? k layers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Architectures of the models used in our experiments. Enc k?l (d, d H , H) denotes Enc k?l with hidden dimension d, head dimension d H , and number of heads H. L k?l (d) denotes L k?l with output dimension d. MLP(n, d, d out ) denotes an elementwise MLP with n hidden layers, hidden dimension d, output dimension d out , and ReLU non-linearity.</figDesc><table><row><cell></cell><cell cols="3">(a) Architectures for runtime and memory analysis.</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Architecture</cell></row><row><cell cols="2">MLP ? (D/S)</cell><cell></cell><cell>[L 2?2 (32) ? ReLU] ?4 -L 2?0 (32)</cell></row><row><cell cols="4">Ours (D/S, (?)) Enc 2?2,? (32, 8, 4) ?4 -Enc 2?0 (32, 8, 4)-LN-Linear(32)</cell></row><row><cell cols="4">(b) Architectures for chain experiment. Output dimension of a layer is denoted in parenthesis.</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Architecture</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell>GCNConv(16)-ReLU-GCNConv(16)-Linear(2)</cell></row><row><cell>GIN-0</cell><cell></cell><cell></cell><cell>GINConv(16)-ReLU-GINConv(16)-Linear(2)</cell></row><row><cell>GAT</cell><cell></cell><cell></cell><cell>GATConv(16)-ReLU-GATConv(16)-Linear(2)</cell></row><row><cell>MLP ? (S)</cell><cell></cell><cell></cell><cell>L 2?2 (16)-ReLU-L 2?1 (16)-Linear(2)</cell></row><row><cell cols="2">Ours (S) w/o global</cell><cell cols="2">Enc 2?2,ablated (16)-Enc 2?1,ablated (16)-LN-Linear(2)</cell></row><row><cell cols="4">Ours (S, ?) w/o global Enc 2?2,?,ablated (16)-Enc 2?1,?,ablated (16)-LN-Linear(2)</cell></row><row><cell>Ours (S)</cell><cell></cell><cell></cell><cell>Enc 2?2 (16)-Enc 2?1 (16)-LN-Linear(2)</cell></row><row><cell>Ours (S, ?)</cell><cell></cell><cell></cell><cell>Enc 2?2,? (16)-Enc 2?1,? (16)-LN-Linear(2)</cell></row><row><cell></cell><cell cols="3">(c) Architectures for graph regression experiment.</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Architecture</cell></row><row><cell cols="4">Transformer + Laplacian PE Enc 1?1 (256, 16, 16) ?8 -Enc 1?0 (256, 16, 16)-LN-Linear(1)</cell></row><row><cell>MLP ? (S)</cell><cell></cell><cell></cell><cell>[L 2?2 (256) ? ReLU] ?8 -L 2?0 (256)-Linear(1)</cell></row><row><cell cols="2">Ours (S, ?) ?SMALL</cell><cell cols="2">Enc 2?2,? (256, 8, 4) ?8 -Enc 2?0 (256, 16, 8)-LN-Linear(1)</cell></row><row><cell>Ours (S, ?)</cell><cell></cell><cell cols="2">Enc 2?2,? (512, 16, 4) ?8 -Enc 2?0 (512, 16, 16)-LN-Linear(1)</cell></row><row><cell></cell><cell cols="3">(d) Architectures for set-to-graph experiment.</cell></row><row><cell>Method</cell><cell>Dataset</cell><cell></cell><cell>Architecture</cell></row><row><cell></cell><cell>Jets</cell><cell></cell><cell>[L 1?1 (256) ? ReLU] ?5 -L 1?2 (256)-MLP(1, 256, 1)</cell></row><row><cell>S2G/S2G+ [29]</cell><cell cols="2">Delaunay (50)</cell><cell>[L 1?1 (500) ? ReLU] ?7 -L 1?2 (500)-MLP(2, 1000, 1)</cell></row><row><cell></cell><cell cols="2">Delaunay (20-80)</cell><cell>[L 1?1 (500) ? ReLU] ?7 -L 1?2 (500)-MLP(2, 1000, 1)</cell></row><row><cell></cell><cell>Jets</cell><cell></cell><cell>Enc 1?1 (128, 32, 4) ?4 -Enc 1?2 (128, 32, 4)-MLP(1, 256, 1)</cell></row><row><cell>Ours (D)</cell><cell cols="2">Delaunay (50)</cell><cell>Enc 1?1 (256, 64, 4) ?5 -Enc 1?2 (256, 64, 4)-MLP(2, 256, 1)</cell></row><row><cell></cell><cell cols="2">Delaunay (20-80)</cell><cell>Enc 1?1 (256, 64, 4) ?5 -Enc 1?2 (256, 64, 4)-MLP(2, 256, 1)</cell></row><row><cell></cell><cell>Jets</cell><cell></cell><cell>Enc 1?1,? (128, 32, 4) ?4 -Enc 1?2,? (128, 32, 4)-MLP(1, 256, 1)</cell></row><row><cell>Ours (D, ?)</cell><cell cols="2">Delaunay (50)</cell><cell>Enc 1?1,? (256, 64, 4) ?5 -Enc 1?2,? (256, 64, 4)-MLP(2, 256, 1)</cell></row><row><cell></cell><cell cols="3">Delaunay (20-80) Enc 1?1,? (256, 64, 4) ?5 -Enc 1?2,? (256, 64, 4)-MLP(2, 256, 1)</cell></row><row><cell cols="4">(e) Architectures for k-uniform hyperedge prediction experiment.</cell></row><row><cell>Method</cell><cell>Dataset</cell><cell></cell><cell>Architecture</cell></row><row><cell></cell><cell>GPS</cell><cell></cell><cell>[L 1?1 (64) ? ReLU] ?1 -L 1?3 (64)-MLP(4, 64, 1)</cell></row><row><cell>S2G+ (S)</cell><cell>MovieLens</cell><cell></cell><cell>[L 1?1 (64) ? ReLU] ?3 -L 1?3 (64)-MLP(2, 64, 1)</cell></row><row><cell></cell><cell>Drug</cell><cell></cell><cell>[L 1?1 (64) ? ReLU] ?3 -L 1?3 (64)-MLP(2, 64, 1)</cell></row><row><cell></cell><cell>GPS</cell><cell cols="2">Enc 1?1,? (64, 16, 8) ?1 -Enc 1?3,? (64, 16, 8)-MLP(4, 64, 1)</cell></row><row><cell>Ours (S, ?)</cell><cell cols="3">MovieLens Enc 1?1,? (64, 16, 8) ?3 -Enc 1?3,? (64, 16, 8)-MLP(2, 64, 1)</cell></row><row><cell></cell><cell>Drug</cell><cell cols="2">Enc 1?1,? (64, 16, 8)</cell></row></table><note>?3 -Enc 1?3,? (64, 16, 8)-MLP(2, 64, 1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6d</head><label>6d</label><figDesc>provides the architecture of the models used in our experiment, along with relevant baselines S2G/S2G+ from Serviansky et. al. (2020)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we omitted normalization after Attn(?) and MLP(?) for simplicity as in<ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b11">12]</ref>.2  In practice, Transformer employs softmax in attention and deviates from Deepsets due to normalization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In practice, we place node features on the diagonals of the adjacency matrix, leading to O(n + m).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">As vanilla Transformer operates on node features only, we used Laplacian graph embeddings[2,<ref type="bibr" target="#b7">8]</ref> as positional embeddings so that the model can consider edge structure information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.fda.gov/Drugs/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Normalization over keys gives an additive complexity O(n uq +u k ), which can be absorbed to the formula.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that message summation over {i|(i, j) ? E } is equivalent to summation over {i|(i, j) ? E} = N (j) because we set message zero at (i, j) ? E \ E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that the implementation of linear layers in Serviansky et. al. (2020)<ref type="bibr" target="#b28">[29]</ref> is slightly different from ours.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Validity of Proposition 1 when using lightweight linear layers (Section 4.1)</head><p>As stated in the main text, Enc k?l (Eq. (9)) with linear layers for key, query, and MLP changed toL still generalizes L k?l . This can be shown simply by pluggingL into the proof of Proposition 1. We can still assume ? h,? i,j = 1 for all (i, j) ? ? by settingL for key and query to output constants, and can reduce MLP l?l composed ofL to an invariant bias as we subsample ? ? M but keep all ? for the bias. Thus, Eq. (9) can reduce to Eq. (1) and Proposition 1 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.5 Proof of Property 2 (Section 4.2)</head><p>Proof. We begin from sparse equivariant linear layer L k?l (Eq. (15)). In the inner summation i?E B ? i,j A i , the number of multiplication and addition for each j is upper-bounded by m = |E|. As the number of output multi-indices j is bounded by |E | ? m k l , the effective number of operations are ? m 2 k l . With outer summation over ?, we have ? b(k + l) k l m 2 operations, leading to complexity O(m 2 ). For the lightweight linear layersL (Proposition 2) that precludes summation over input, we trivially have O(m) complexity as we do not sum over i. </p><p>where N (j) denotes incoming neighbors of j-th node, i.e., {i|(i, j) ? E}.</p><p>We now show how a composition of two Enc 2?2 can approximate above computation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved bounds on bell numbers and on moments of sums of random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability and Mathematical Statistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random walks on hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Carletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battiston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cencetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bulletin de l&apos;Acad?mie des Sciences de l&apos;URSS, Classe des</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Delaunay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sciences Math?matiques et Naturelles</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="793" to="800" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
	<note>Sur la sph?re vide</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Implicit graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sellke</surname></persName>
		</author>
		<title level="m">Approximating continuous functions by relu nets of minimal width. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep models of interactions across sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<title level="m">Gaussian error linear units (gelus). arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">OGB-LSC: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Set2graph: Learning graphs from sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cranmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hyper-sagnn: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Deep learning on graphs: A survey. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collaborative filtering meets mobile recommendation: A user-centered approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">As a first step, we encode X and E into a single A ? R n?n?(2dv+de) [26]. In the first d v channels, we replicate X on the rows</title>
	</analytic>
	<monogr>
		<title level="m">the next d v channels</title>
		<imprint/>
	</monogr>
	<note>we replicate X on the columns. In the last d e channels, we put E. Additionally, to account for output positions (node features. we augment E with self-loops and make E = E ? {(i, i) ?i ? [n]}</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">To do this, we first reduce Attn 2?2 (A) ij = A ij and apply MLP l?l on top of it. We reduce MLP l?l to entry-wise MLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ij ? M (x J , X I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Then, we make the first Enc 2?2 approximate the message function M (?), so that Enc 2?2 (A)</title>
		<imprint/>
	</monogr>
	<note>As Attn 2?2 (A) ij = A ij</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
