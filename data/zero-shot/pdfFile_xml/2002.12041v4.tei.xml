<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-guided Chained Context Aggregation for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Attention-guided Chained Context Aggregation for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>multi-scale contexts</term>
					<term>series-parallel hybrid flows</term>
					<term>convolutional networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The way features propagate in Fully Convolutional Networks is of momentous importance to capture multi-scale contexts for obtaining precise segmentation masks. This paper proposes a novel series-parallel hybrid paradigm called the Chained Context Aggregation Module (CAM) to diversify feature propagation. CAM gains features of various spatial scales through chain-connected ladder-style information flows and fuses them in a two-stage process, namely pre-fusion and re-fusion. The serial flow continuously increases receptive fields of output neurons and those in parallel encode different region-based contexts. Each information flow is a shallow encoder-decoder with appropriate down-sampling scales to sufficiently capture contextual information. We further adopt an attention model in CAM to guide feature re-fusion. Based on these developments, we construct the Chained Context Aggregation Network (CANet), which employs an asymmetric decoder to recover precise spatial details of prediction maps. We conduct extensive experiments on six challenging datasets, including Pascal VOC 2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. Results evidence that CANet achieves state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation is a vital task in computer vision, aiming to assign corresponding semantic labels to each pixel in images. It has fundamental applications in the fields of automatic driving <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, medical image <ref type="bibr" target="#b2">[3]</ref>, augmented reality, etc. Dominant techniques in modern semantic image segmentation are based on the Fully Convolutional Network (FCN) <ref type="bibr" target="#b3">[4]</ref>, achieving optimal performance in this task. FCN adapts deep image classification models <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> for dense prediction by replacing fully connected layers with convolutions and gains increasing receptive fields and high-level contexts through cascaded convolutional and sub-sampling pooling layers. However, the continuous downsampling process causes the loss of spatial details, resulting in poor object delineation and small spurious regions. <ref type="figure" target="#fig_0">Fig. 1</ref> shows some examples, where segmented objects present only blurry contours such as the pole and sidewalk. In summary, the paradox between semantics and spatial details is a significant predicament of FCN based approaches.</p><p>Combining dilated/atrous convolutions and context modules <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b17">[18]</ref> becomes a popular alternative to reconcile the above contradiction. Dilated convolution can increase the receptive field while maintaining feature maps' resolutions without extra parameters. However, it suffers from the gridding dilemma <ref type="bibr" target="#b18">[19]</ref>, relinquishing part of the neighboring information that is also essential for elaborate semantic segmentation on account of all pixels interacting with surrounding ones to make up objects and form local contexts. Context modules remedy this problem in some way. They join feature maps of various but larger receptive fields to exploit both local and global contexts. Global cues help to understand the entire image scene and, to some extent, reject the ambiguity caused by similar local objects, e.g. cars instead of ships are more likely to show up in a city scene. Many existing methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref> adopt a parallel context module design that encodes contextual information through separate convolutional paths. For example, PSPNet <ref type="bibr" target="#b10">[11]</ref> focuses on global contexts and uses large pooling strides, which seriously injures spatial details. Differently, Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref> employs dilated filters to exploit sub-region contexts but exacerbates the gridding effect <ref type="bibr" target="#b18">[19]</ref>. Both adopt a single convolutional layer of limited learning ability to transmit features. ACFNet <ref type="bibr" target="#b13">[14]</ref> leverages a global image representation to re-weight subregion features, which is computationally inefficient. Those methods explore contextual information solely through parallel information flows that lack feature interaction, resulting in a restricted variety of receptive fields and inconsistency of feature scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2002.12041v4 [cs.CV] 21 May 2021</head><p>Meanwhile, some networks <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref> employ stacked encoder-decoder structures of sufficient learning ability to exploit contextual information and bring fine localization recovery. We can treat them as in-series context modules where the latter features exclusively depend on the previous. Therefore, such in-series structures lack feature diversity and flexibility intrinsically. Besides, they usually lead to deeper networks that are difficult to train. Empirical receptive field size is not proportional to the depth of networks <ref type="bibr" target="#b26">[27]</ref>, on the other hand. To obtain elegant convergence, SDN <ref type="bibr" target="#b25">[26]</ref> introduces inter-unit and intra-unit skip connections and hierarchical supervision, resulting in a sophisticated model construction procedure and training pipeline.</p><p>To address the above obstacles, we propose an innovative paradigm termed the Chained Context Aggregation Module (CAM) with effectiveness and clearness, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. CAM utilizes Flow Guidance Connections to connect multiple information flows in a series-parallel hybrid manner. Each flow is a shallow encoder-decoder with suitable downsampling scales to integrate contextual information. Global Flow (GF) aims to obtain a global receptive field, and Context Flow (CF) captures sub-region based contexts of individual scales. The serial GF and CFs (i.e. several Context Flows) guided by Chained Connections contain multiple encoderdecoder blocks, increasing the receptive fields of output neurons to construct contextual information and recover localization information. Parallel GF and CFs encode contexts of different spatial scales to obtain potentially accurate feature maps for multi-scale objects segmentation.</p><p>Within the series-parallel hybrid architecture, a two-stage feature fusion mechanism is naturally developed to strengthen context interaction, namely pre-fusion and re-fusion. As an extension work, we sharpen the simple yet effective decoder of DeepLabv3+ <ref type="bibr" target="#b20">[21]</ref> with a compact 3 ? 3 convolution to intensify semantic representations of low-level features. Based on these developments, we construct the Chained Context Aggregation Network (CANet) for semantic image segmentation and conduct extensive experiments on six challenging datasets whose results demonstrate the superiority.</p><p>We conclude the critical contributions as follows:</p><p>? We propose the CAM to capture multi-scale contexts via multiple information flows in a series-parallel hybrid manner. Each information flow is a shallow encoderdecoder to integrate contextual information with sufficient learning ability. ? The serial flow increasingly enlarges receptive fields of output neurons while parallel flows encode different region-based contexts. Both are of significant importance to obtain potentially accurate representations and improve model performance considerably. ? Flow Guidance Connections enable sufficient feature interaction of multiple information flows and powerful aggregation of multi-scale contexts through a naturally developed pre-fusion/re-fusion process. They are crucial for model convergence with succinctness at the same time. We further utilize attention models in CAM to boost feature re-fusion and refine the results.</p><p>? We construct a generalized framework termed the CANet and achieve state-of-the-art performance on the benchmarks of Pascal VOC 2012, Pascal Context, Cityscapes, CamVid, SUN-RGBD and GATECH. The remainder of this paper is organized as follows. We briefly review related works in Section II. In Section III, we detail the proposed CANet and represent high-level functions behind the critical series-parallel hybrid CAM. Section IV presents quantitative experimental results that verify the superiority of the proposed framework. Finally, we summarize our work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>With increasing applications of deep learning methods to semantic segmentation in recent years, the task has made breakthroughs on benchmarks. We briefly review related research works in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial Information</head><p>In convolutional networks, each layer handles diverse information. Low-level layers usually have more positional information and high-level ones hold more semantics. Both positions and semantics play a pivotal role in semantic segmentation. FCN <ref type="bibr" target="#b3">[4]</ref> based methods obtain high-level semantics through down-sampling operations at the expense of spatial details, leading to the problems of poor boundary positioning and inconsistent predictions. F. Yu and V. Koltun <ref type="bibr" target="#b9">[10]</ref> develop dilated convolutions to reduce spatial loss during encoding and achieve excellent results. Following this idea, PSPNet <ref type="bibr" target="#b10">[11]</ref>, DeepLabv3+ <ref type="bibr" target="#b20">[21]</ref>, APCNet <ref type="bibr" target="#b13">[14]</ref>, DANet <ref type="bibr" target="#b15">[16]</ref> and CFNet <ref type="bibr" target="#b14">[15]</ref> all apply dilated convolutions in the backbone network to maintain the resolution of feature maps. FRRN <ref type="bibr" target="#b1">[2]</ref> and HR-Net <ref type="bibr" target="#b27">[28]</ref> employ a high-resolution branch to serve the purpose.</p><p>Another idea utilizes the encoder-decoder structure to restore resolution via up-sampling or deconvolution. Typically, they contain an encoder that down-samples the input image to obtain high-level semantics and a decoder that gradually restores the resolution to classify every pixel. Both SegNet <ref type="bibr" target="#b28">[29]</ref> and UNet <ref type="bibr" target="#b2">[3]</ref> utilize a symmetric decoder to obtain finerecovered predictions. GCN <ref type="bibr" target="#b29">[30]</ref> and RefineNet <ref type="bibr" target="#b30">[31]</ref> are further developments with carefully designed decoders. SDN <ref type="bibr" target="#b25">[26]</ref> exploits contextual information by stacking multiple encoderdecoders. However, these methods significantly increase the model's depth, introducing a huge parameter size and aggravating the gradient degradation problem. Deconvolution <ref type="bibr" target="#b31">[32]</ref> and DUpsample <ref type="bibr" target="#b32">[33]</ref> employ up-sampling strategies different from bilinear interpolation as decoders to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale Contexts</head><p>Existing context-based methods aim to capture effective context features that play a vital role in segmenting multiscale objects. At the top of the backbone network, PSP-Net <ref type="bibr" target="#b10">[11]</ref> and DeepLab <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> employ parallel information flows to perceive multi-scale representations by different pooling strides and dilation rates, respectively. APC-Net <ref type="bibr" target="#b13">[14]</ref>, DANet <ref type="bibr" target="#b15">[16]</ref> and CFNet <ref type="bibr" target="#b14">[15]</ref> take advantage of attention models to obtain various contextual information.</p><p>Based on the U-shape structure, RefineNet <ref type="bibr" target="#b30">[31]</ref>, LFNet <ref type="bibr" target="#b33">[34]</ref> and GCN <ref type="bibr" target="#b29">[30]</ref> achieve a productive fusion of hierarchical features. DFANet <ref type="bibr" target="#b34">[35]</ref> implements an in-depth aggregation of hierarchical features by cascading multiple encoders. Besides, Recurrent Neural Networks (RNNs) are also developed to capture long-range dependencies <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In this work, we evaluate semantic image segmentation from a novel spatial perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attention Mechanism</head><p>The core idea of attention mechanism is to assign distinctive attention weights to different parts of the input, just like people focusing on attractive parts of seeing features. Like the non-local block <ref type="bibr" target="#b37">[38]</ref> introducing self-attention <ref type="bibr" target="#b38">[39]</ref> into computer vision, various types of attention mechanisms <ref type="bibr" target="#b39">[40]</ref>- <ref type="bibr" target="#b42">[43]</ref> play an increasingly important role in this field. Methods with attention models for semantic segmentation <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b43">[44]</ref>- <ref type="bibr" target="#b45">[46]</ref> also further improve the performance. EncNet <ref type="bibr" target="#b12">[13]</ref> and ACFNet <ref type="bibr" target="#b13">[14]</ref> leverages a global image representation as guidance to estimate the local affinity coefficients. ACFNet <ref type="bibr" target="#b17">[18]</ref> extracts global contexts via computing class centers. DANet <ref type="bibr" target="#b15">[16]</ref> and CCNet <ref type="bibr" target="#b45">[46]</ref> are both successful instantiations of the non-local network <ref type="bibr" target="#b37">[38]</ref> in semantic segmentation. Those attention-based methods bring various feature fusion mechanisms via adaptively learned attention weights. Differently, this paper mainly focuses on the feature encoding paradigm through diversifying feature propagation and interaction to encode multi-scale contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We propose the Chained Context Aggregation Network to enable flexible capturing and aggregation of multi-scale contextual information and explore its improvement for semantic segmentation. We elaborate on the framework in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we design the Chained Context Aggregation Module (CAM) to diversify feature propagation and encode multi-scale contextual information over feature embedding extracted by a deep convolutional neural network (DCNN). To be specific, following prior works <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, we employ ResNet <ref type="bibr" target="#b4">[5]</ref> with the dilation strategy <ref type="bibr" target="#b8">[9]</ref> as the DCNN backbone to preserve spatial resolution without extra parameters. At the top of the backbone lies the carefully designed CAM, where several shallow encoder-decoders serve as information flows to integrate contextual information at individual scales. CAM utilizes Flow Guidance Connections to develop a series-parallel hybrid structure of information flows, expecting to exploit multi-scale contextual features effectively to improve segmentation performance for objects of different spatial sizes. We further adopt a channel-attention model called the Feature Selection Module (FSM) to promote feature fusion. Finally, CANet mimics the simple yet effective asymmetric decoder proposed by DeepLabv3+ <ref type="bibr" target="#b20">[21]</ref> to upsample the prediction maps to present classification confidence for each class at every pixel. We note that CANet replaces the original 1 ? 1 convolution with 3 ? 3 filters in the decoder to reduce the semantic gap between low-and high-level pixel features. CANet utilizes bilinear interpolation as a naive upsampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Chained Context Aggregation Module</head><p>The CAM is the critical part of CANet to aggregate multiscale contextual information. Based on the backbone network's shared features, CAM further exploits semantic relations at different spatial scales through GF and CF. Both are shallow encoder-decoder structures containing a down-sampling layer to gain different receptive field, a projection layer to integrate sufficient context features, and an up-sampling layer to recover localization information. We unite GF and CFs in a series- parallel hybrid structure to diversify feature propagation and encode multi-scale contexts. The series-parallel hybrid design brings four advantages. Firstly, shallow encoder-decoders of sufficient learning ability serve as information flows to integrate contextual information, yielding fine spatial resolution recovery than a single convolutional layer. Secondly, serial GF and CFs continuously enlarges receptive fields of output neurons and enhances semantic representations. At the same time, it deepens the network, thus increases learning ability. Thirdly, parallel flows encode pixel features at different scales, propitious for multi-scale object segmentation. Finally, such design diversifies feature propagation and facilitates the back-propagation of gradients with simplicity, making the model easy to train.</p><p>Within the series-parallel hybrid architecture, a two-stage feature fusion mechanism is naturally developed. It diversifies feature transmission and alleviates scale inconsistency among different flows. More specifically, CF first concatenates shared features of the backbone and output features of the upper flow and integrate them through a shallow encoder-decoder. We name the process pre-fusion. Residual Connections guide the aggregation of different information flows, followed by an attention model to produce rich contextual information, which we name as re-fusion.</p><p>We can see that during the above process, contexts obtained by a latter information flow do not entirely depend on that of the previous. It is the series-parallel structure that enables robust multi-scale context aggregation. Different combinations of different quantity and down-sampling scales of CFs make up diverse CAM to exploit contexts of various scales. It is another embodiment of CAM's flexibility. <ref type="figure" target="#fig_3">Fig. 4</ref> provides a possible combination where the information flow symbolized by the black dotted arrow makes up a stack of multiple shallow encoder-decoders that can be regarded as a particular case of SDN <ref type="bibr" target="#b25">[26]</ref>. The main components of CAM are described below.</p><p>1) Global Flow: Practices <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref> have witnessed that global pooling features can provide a global receptive field as a reliable cue to distinguish confusing objects. In CAM, it is achieved by applying global average pooling on shared feature maps of the backbone network, which we refer to as Global Flow. <ref type="figure" target="#fig_2">Fig. 3</ref>(a) depicts its details where we employ a single 1 ? 1 convolution to obtain global image representations.</p><p>2) Context Flow: We propose Context Flow to exploit local contexts of different receptive fields, as shown in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. CF is the spotlight where pre-fusion occurs. More specifically, given the two inputs of different spatial scales coming from the shared backbone and upper flow, CF first concats them in channel dimension and down-samples N ? through an average pooling layer to capture sub-region based contexts, which also lessens computation cost and reduces information redundancy. The following two consecutive convolutional layers serve as the projection layer, eliminating the aliasing effect and encode integrated context features, where depth-wise separable convolutions <ref type="bibr" target="#b47">[48]</ref> are adopted to diminish model parameters. Finally, CF up-samples the output to the same size of the input. By setting a suitable down-sampling scale N , we can obtain different receptive fields and accurate position information through the encoder-decoder paradigm.</p><p>3) Flow Guidance Connections: GF and CFs are devised to obtain global and local contexts of various spatial scales, respectively. We propose Flow Guidance Connections, the principal notion of forming the series-parallel hybrid structure, to unite GF and CFs to enhance feature delivery and enrich multiscale contexts. Flow Guidance Connections include Shortcut Connections, Chained Connections and Residual Connections, as separately depicted by the purple, blue and green arrows in <ref type="figure" target="#fig_1">Fig. 2</ref>. Shortcut Connections let CAM reuse the pixel embeddings of the backbone network. They not only decouple lower and upper features but also efficiently promote the acquisition and diversity of multi-scale contexts. Since GF and CFs have distinct receptive fields, Chained Connections intend to guide the pre-fusion that magnifies the flexible feature aggregation and decreases feature inconsistencies between adjacent information flows. Finally, Residual Connections serve as ushers of the re-fusion process and also alleviate gradient vanishing caused by the serial flow increasing the network's depth, which makes the whole framework easy to train. Aggregated feature maps are then fed into the FSM to construct re-fused contexts. 4) Feature Selection Module: Although feature pre-fusion alleviates the scale inconsistency to a certain extent, we still adopt an attention model to guide feature re-fusion. Inspired by SENet <ref type="bibr" target="#b39">[40]</ref>, we adopt a simple channel-attention based FSM to select advantageous features and suppress the useless or harmful at the channel level, balancing effects of different region-based contexts on segmentation results. Assume the integrated local contexts of the i-th CF is X i and G denotes global contexts obtained by GF, then</p><formula xml:id="formula_0">U = F bilinear (G) + i F bilinear (X i )<label>(1)</label></formula><formula xml:id="formula_1">U = ?(F BN (f (U , W 1 ))) (2) U = U ? ?(F BN (f (F GAP (U ), W 2 ))) ? U<label>(3)</label></formula><p>where F bilinear is bilinear interpolation function, F BN and F GAP the normalization layer and global average pooling layer, respectively. f (?, ?) denotes convolution. ? and ? refer to the Sigmoid and ReLU function. W 1 and W 2 are both learnable parameters. ? represents element-wise product and ? element-wise summation. U is the re-fused contexts, which are then up-sampled by the decoder to obtain predictions. <ref type="figure" target="#fig_2">Fig. 3(b)</ref> illustrates the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Function</head><p>We employ standard cross-entropy loss for training, as shown in Eq. 4. Since CAM further deepens the network, here we use an auxiliary loss to serve for a better backbone convergence and a principal loss to supervise the output of the whole network, as Eq. 5 and 6 present.</p><formula xml:id="formula_2">H(y,?) = ? 1 N i y i log? i<label>(4)</label></formula><p>where y denotes the ground-truth and? the prediction of network.</p><formula xml:id="formula_3">y c = F class (c 3 )<label>(5)</label></formula><formula xml:id="formula_4">L = H(y, y d ) + ?H(y, y c )<label>(6)</label></formula><p>where y d is the output of the decoder, c 3 the intermediate output of the backbone as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, and F class a classification layer with 3 ? 3 convolution. ? is adopted to balance the training process. We do not use the auxiliary output when inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>We conduct extensive experiments on Pascal VOC 2012 <ref type="bibr" target="#b48">[49]</ref>, Pascal Context <ref type="bibr" target="#b49">[50]</ref>, Cityscapes <ref type="bibr" target="#b0">[1]</ref>, CamVid <ref type="bibr" target="#b50">[51]</ref>, SUN-RGBD <ref type="bibr" target="#b51">[52]</ref> and GATECH <ref type="bibr" target="#b52">[53]</ref> to evaluate the performance of our proposed CANet. Results are obtained with multi-scale and flipping skills if not specified. We adopt the standard benchmarks, pixel accuracy (PA) and mean intersection over union (mIoU) as the evaluation metrics. We use only mIoU on Pascal VOC 2012, Pascal Context and Cityscapes for common convention. We assume segmentation label space S = {l 0 , l 1 , ..., l k } with a total of k + 1 classes where l 0 is the background or a void class. p ij represents the amount of pixels of class i but inferred to be class j, then</p><formula xml:id="formula_5">PA = k i=0 p ii k i=0 k j=0 p ij (7) mIoU = 1 k + 1 k i=0 p ii k j=0 p ij + k j=0 p ji ? p ii<label>(8)</label></formula><p>We implement experiments by MXNet <ref type="bibr" target="#b53">[54]</ref> and borrow ImageNet <ref type="bibr" target="#b54">[55]</ref> pre-trained backbone from the open-source toolkit GluonCV <ref type="bibr" target="#b55">[56]</ref>. We set the dilation rates of backbone's last two residual modules to 2 and 4, respectively. Thus the resolution of backbone's final output feature map is 1/8 of the input image. Following T. He et al. <ref type="bibr" target="#b56">[57]</ref>, we replace the original 7 ? 7 convolution with three stacked 3 ? 3 convolutional layers in ResNet. The output channels of FSM are 256. Like DeepLabv3+ <ref type="bibr" target="#b20">[21]</ref>, we set output channels of the 3 ? 3 convolution in the decoder to 48 to prevent low-level features from outweighing the importance of the rich contextual information. We adopt ResNet101 <ref type="bibr" target="#b4">[5]</ref> as the backbone for comparisons with prior methods.</p><p>We use the poly learning rate policy lr = baselr ? (1 ? iter total iter ) power and set the power to 0.9. The initial learning rate is 4e-3 for Cityscapes and 1e-3 for the other five datasets. We use the standard mini-batch stochastic gradient descent (SGD) as the optimizer and set momentum to 0.9. To prevent over-fitting, we set the weight decay to 5e-4 for CamVid and GATECH, and 1e-4 for the others. For data augmentation, we first flip input images with a probability of 0.5 and randomly scale them from 0.5 to 2.0 times. Then we crop the images with padding if needed. Finally, a random Gaussian blur is added. Since appropriate crop size influences the model performance, we empirically crop images to 384 ? 384 for CamVid, 768 ? 768 for Cityscapes, and 480 ? 480 for the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results on Pascal VOC 2012</head><p>Pascal VOC 2012 <ref type="bibr" target="#b48">[49]</ref> contains 1464, 1449 and 1456 images for training, validation and testing respectively. All images are pixel-wise labeled with 21 semantic classes, one of which is background. Following prior works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we augment the training set with SBD dataset <ref type="bibr" target="#b57">[58]</ref> for experiments, resulting in 10582 images for training. We first perform sound ablation studies on the validation set to verify the benefits of key ideas in CANet as well as to explore the improvement of different combinations of CFs on the segmentation results. Our baseline is dilated ResNet based FCN <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>. All ablation results are based on single scale inputs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Down-sampling Scales of CFs:</head><p>We believe that different quantities and down-sampling scales of CFs contribute to capturing different scales of contextual information of objects. <ref type="figure" target="#fig_3">Fig. 4</ref> gives a possible example of CFs. We conduct some exploratory experiments on this and <ref type="table" target="#tab_0">Table I</ref> reports the results, where {d 1 , d 2 , . . . , d n } means there are n CFs chainconnected from top down, and the down-sampling scales are d 1 , d 2 , ..., d n respectively. It can be seen that:</p><p>? Compared to the baseline FCN, the segmentation performance is considerably improved no matter what combination of CFs is. And the {2, 4, 8, 16} achieves the best with an 8.71% mIoU improvement. This observation suggests that CAM is contributory to accurate segmentation of multi-scale targets. Besides, different combinations introduce non-negligible performance perturbation. ? CAM gains the best at the quantity of 4 CFs, but the performance degrades noticeably when the number of CFs goes beyond. To this end, we further investigate three representative state-of-the-art methods of PSPNet <ref type="bibr" target="#b10">[11]</ref>, DeepLabv3 <ref type="bibr" target="#b11">[12]</ref> and APCNet <ref type="bibr" target="#b13">[14]</ref> that adopt similar multiple information flow designs. The relationship between the quantity and model performance is shown in <ref type="figure" target="#fig_4">Fig. 5(a)</ref>. Although those methods hold different interpretations of information flows, the results show compatibility. We consider that the increase of flows brings more diverse region-based contexts and improves model performance. However, scale inconsistency increases at the same time, which dilutes semantics during feature fusion and causes performance degradation after exceeding a specific limit. ? Under the same quantity of CFs, more flexible downsampling rates combination yields more diverse local contexts, leading to a slightly better performance.</p><p>In all the following experiments, we adopt the downsampling scales of {2, 4, 8, 16}.</p><p>2) Width of Information Flow: The width of information flows determines the learning ability of CAM, which is an indispensable parameter for encoding multi-scale features. We verify the impact of this hyper-parameter on model performance and report results in <ref type="table" target="#tab_0">Table II</ref>. The results indicate that model performance saturates at 512. Larger width may result in over-fitting. We pick this value in all other experiments.</p><p>3) Validation for the Series-parallel Hybrid Design: Flow Guidance Connections is a fundamental design of CAM that connect multiple information flows with the critical seriesparallel hybrid manner and facilitate model training. To verify the advanced nature of the design, we perform ablation on these connections. We only retain the uppermost Shortcut Connection and the lowermost Residual Connection to form a purely in-series model that is denoted as CANet-S. Besides, we remove all Chained Connections to build CANet-P with a purely parallel structure. We report quantitative comparisons to four similar representative methods in <ref type="table" target="#tab_0">Table III</ref>. Our CANet surpasses all the other context modules with fewer model parameters, which evidences the superiority of the seriesparallel hybrid design. It is the design instead of incremental parameters that obtain performance advancement.</p><p>As illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref>, CANet-S only retains the feature propagation flow represented by the black dotted line, which can be seen as an SDN <ref type="bibr" target="#b25">[26]</ref>    <ref type="table" target="#tab_0">See Table III</ref>. This confirms the contributory to good convergence of the simple yet effective Shortcut Connections and Residual Connections on the other hand. Furthermore, CANet (with pre-fusion and re-fusion) outperforms CANet-P (with only re-fusion) by 0.78 points mIoU, suggesting that the two-stage feature fusion mechanism alleviates feature inconsistency of adjacent information flows, which enhance feature aggregation.</p><p>It is noticed that CANet achieves no significant performance improvement than the DeepLabv3 <ref type="bibr" target="#b11">[12]</ref> method (mIoU 78.68% v.s. 78.45%). We assume the reason lies in small scale changes exhibited in the dataset. To further validate the effectiveness, we conduct comparisons on the Cityscapes dataset <ref type="bibr" target="#b0">[1]</ref> of complicated scenes exhibiting massive scale changes. <ref type="table" target="#tab_0">Table IV</ref> reports the results, where DeepLabv3+ <ref type="bibr" target="#b20">[21]</ref> is based on Xception65 <ref type="bibr" target="#b7">[8]</ref> and others dilated ResNet101 <ref type="bibr" target="#b4">[5]</ref>. Even compared to the DeepLabv3+ model, CANet achieves significant performance improvements and gains the best in 13 out of 19 semantic categories. Qualitative improvements are illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref>, where PSPNet <ref type="bibr" target="#b10">[11]</ref> introduces inconsistent sub-region prediction because of large pooling stride and loss of spatial details, and DeepLabv3 <ref type="bibr" target="#b11">[12]</ref> adopts dilated convolutions but exacerbates the gridding effect <ref type="bibr" target="#b18">[19]</ref>. Differently, CANet employs encoder-decoder information flows with appropriate down-sampling scales, which constructs sufficient context features and brings adequate localization information. Note that we do not use FSM, decoder and auxiliary loss for the moment for fair comparison and to better address the basic series-parallel hybrid design of CAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Ablation for Key Components:</head><p>We show the effectiveness of key components in CANet by adding them to the baseline one by one, and validate the benefits of GF in rejecting local ambiguities. The uppermost CF takes only the shared features as input when there is no GF. The experimental results are listed in <ref type="table">Table V</ref>. We can grasp that: (1) CAM significantly improves the semantic segmentation performance, from mIoU 69.97% to 78.68% due to its powerful modeling ability of multi-scale contexts. (2) As assistant roles, GF obtains a global receptive field and FSM boosts feature refusion at channel domain. Both proffer minor enhancement to model performance. (3) The decoder brings spatial details of the low-level, thus refines the results. Moreover, the utilization of 3?3 convolution enhances semantic representations of both small and large objects of low-level features, thus achieves a double effect than the original 1 ? 1 convolution. <ref type="figure">Fig. 7</ref> presents some visualized interpretations. It can be noticed that the decoder based on 3 ? 3 convolution interjects less turbulence when recovering spatial resolution. (4) For both baseline and CANet, the use of auxiliary loss is beneficial to a better backbone feature extractor. We set the auxiliary loss weight ? = 0.1 to gain better performance, as shown in <ref type="figure" target="#fig_4">Fig. 5(b)</ref>. 5) Improvement Strategies: In this section, we evaluate several improvement strategies that further improve the segmentation performance of CANet in <ref type="table" target="#tab_0">Table VI</ref>, including (1)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on Pascal Context</head><p>Pascal Context <ref type="bibr" target="#b49">[50]</ref> is a scene parsing dataset, containing 4998 images for training and 5105 images for validation. Following prior works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b64">[65]</ref>, we use the most common 59 categories for this benchmark and consider all the other classes as background. Results in <ref type="table" target="#tab_0">Table VIII</ref> demonstrate CANet achieves state-of-the-art performance. Compared with DANet <ref type="bibr" target="#b15">[16]</ref> and ANL <ref type="bibr" target="#b16">[17]</ref> that focus on feature fusion via applying non-local operations on pixel pairs, this paper pays more attention to the feature propagation paradigm and achieves significant performance improvements of 1.7 points and 1.5 points mIoU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on Cityscapes</head><p>Cityscapes <ref type="bibr" target="#b0">[1]</ref> describes complex street scenes with highresolution images that are fine labeled in 19 classes for semantic segmentation. Following the standard settings, we do not use coarse labeled data for evaluation, resulting in 2975 images for training, 500 images for validation and 1525 images for testing. We report quantitative comparisons in <ref type="table" target="#tab_0">Table IX</ref>. Results confirm that CANet can tackle complex scale variations and obtain promising performance for high-resolution street scene parsing. More specifically, the series-parallel hybrid structure of CANet has similarities with DenseASPP [59] that we can obtain the former by removing some dense connections in the latter. The key difference lies in the information flow where CANet adopts a shallow encoder-decoder with suitable down-sampling scales and DenseASPP single dilated 3 ? 3 convolution. Therefore, CANet can integrate more localization information, achieving a significant performance improvement of 2.0 points mIoU. We note that DenseASPP employs a more powerful backbone network of DenseNet <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on CamVid</head><p>CamVid <ref type="bibr" target="#b50">[51]</ref> is a street scene dataset that contains both light and dark conditions. We use the dataset described by SegNet <ref type="bibr" target="#b28">[29]</ref> that contains 367 images for training, 100 images for validation, and 233 images for testing, all labeled with 11 semantic categories. We train CANet with training and validation images and report the performance on the testing images. <ref type="table" target="#tab_8">Table X</ref> reports the results. The proposed CANet is significantly better than existing methods which firmly evidence the superiority. <ref type="figure" target="#fig_0">Fig. 1</ref> gives some visualizations. Because of the capability to capture multi-scale contextual information, CANet is able to ameliorate poor object delineation and small spurious regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on SUN-RGBD</head><p>SUN-RGBD <ref type="bibr" target="#b51">[52]</ref> dataset has a total of 10335 indoor images, of which 5280 images are for training and 5050 images for testing. It provides pixel-wise labeling for 37 semantic labels. There are various objects in one image scene and they differ in shapes, sizes and even spatial poses, which makes SUN-RGBD one of the most challenging datasets. In this paper, we only utilize RGB modality for experiments. Quantitative results reported in <ref type="table" target="#tab_0">Table XI</ref> demonstrate that our CANet achieves optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on GATECH</head><p>We further evaluate CANet on the GATECH dataset <ref type="bibr" target="#b52">[53]</ref> that has somewhat noisy annotations. GATECH is a large outdoor scene video dataset, which contains 8 semantic annotation categories, including 63 videos with a total of 12241 frames for training and 38 videos with a total of 7071 frames for testing. Due to the vast redundancy between video frames, similar to S. J?gou et al. <ref type="bibr" target="#b69">[70]</ref>, we extract images every five frames for training and use all test frames to evaluate the model performance. <ref type="table" target="#tab_0">Table XII</ref> reports the results. Without the use of temporal information, our CANet surpasses existing methods with a large margin, proving that CANet obtains robust context features even with noisy annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This paper proposes the Chained Context Aggregation Module to enrich multi-scale contexts from a novel spatial perspective. With the fundamental series-parallel hybrid design of information flows, CAM effectively encodes diverse regionbased contexts through the naturally developed pre-fusion and re-fusion process, giving a remarkable improvement on performance. The series-parallel hybrid structure not only enhances feature interaction but also is friendly to model training. As an extension work, we adopt 3 ? 3 convolution to refine the decoder, which introduces less turbulence when recovering spatial resolution. Extensive experiments on six challenging datasets indicate the effectiveness and advancement of the proposed CANet.</p><p>However, we still use summation and concatenation for feature fusion apart from the simple attention model FSM. Attention mechanisms show excellent abilities for modeling pixel-wise and region-wise dependencies, taking semantic segmentation a big step forward. We expect to do more in-depth work in multi-scale feature fusion based on the proposed feature encoding paradigm, bringing new vitality to the semantic segmentation community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Some visualized predictions on CamVid test set. First row: input images. Second row: ground truth. Third row: predictions of dilated FCN. Fourth row: predictions of CANet (ours), which obtains more sharper segmentation boundaries such as poles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed CANet. Given an input image, we first adopt a deep convolutional neural network (DCNN) as the backbone to encode a shared feature map, and then the carefully designed Chained Context Aggregation Module (CAM) is applied to enrich multi-scale contexts, followed by an asymmetric decoder to get the final per-pixel prediction. "?N up" means N -time up-sampling operation. Shortcut connections, Chained Connections and Residual Connections are collectively called Flow Guidance Connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Detailed components of CAM. "N ? down" means N -time downsampling of the inputs by average pooling layer, while "?N up" means N -time up-sampling operation. DWConv and PWConv are shorthand for depth-wise convolution and point-wise convolution respectively. We stack two depth-wise separable convolutions to integrate pre-fused features and enlarge receptive fields. ? and ? represents element-wise summation and elementwise product, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>One possible combination of GF and CFs. "N ?" indicates the downsampling scales of CFs. The black dotted arrow points out the direction of multiple in-series encoder-decoder flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>(a) The number of information flow and mIoU performance on Pascal VOC 2012 validation set. Since CFs are proposed to capture sub-region based contexts, we do not take into account global average pooling paths of CANet, DeepLabv3 and PSPNet when counting the number. The 1 ? 1 convolution projection in ASPP of DeepLabv3 is also ignored. Down-sampling scales {2, 4, 8, 16, 32}, dilation rates {6, 12, 18, 24, 36}, pyramid scales {1, 2, 3, 6, 32} and output bin sizes of pooling operations {2, 3, 6, 32, 48} are set for CANet, DeepLabv3, APCNet and PSPNet, respectively. (b) Ablation study of auxiliary loss weight ? as stated in Section III-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visualized comparisons on the Cityscapes val set. First row: input images. Second row: ground truth. Third row: predictions of PSPNet. Fourth row: predictions of DeepLabv3. Fifth row: predictions of the proposed CANet. We zoom in the first-column example at the last row for better illustration. CANet employs shallow encoder-decoder information flows to integrate context features, which provides fine recovery of localization information and eliminates the gridding effect. supervision during training. As a result, CANet-S ends up with terrible performance of 53.39% mIoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I INVESTIGATION</head><label>I</label><figDesc>OF THE QUANTITY AND DOWN-SAMPLING SCALES OF CFS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Backbone</cell><cell></cell><cell>CFs</cell><cell>mIoU(%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>None(baseline)</cell><cell>69.97</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2}</cell><cell>77.80</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 2}</cell><cell>78.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 4}</cell><cell>78.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 2, 2}</cell><cell>78.29</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 4, 4}</cell><cell>78.33</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 4, 8}</cell><cell>78.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 2, 2, 2}</cell><cell>78.51</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 4, 8, 16}</cell><cell>78.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResNet50</cell><cell></cell><cell>{2, 4, 8, 16, 32}</cell><cell>77.88</cell></row><row><cell></cell><cell>78.25 78.50 78.75</cell><cell>CANet APCNet DeepLabv3 PSPNet</cell><cell></cell><cell></cell></row><row><cell>mIoU(%)</cell><cell>77.75 78.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.50</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>77.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3 Quantity of information flow</cell><cell>4</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II INVESTIGATION</head><label>II</label><figDesc>OF THE WIDTH OF INFORMATION FLOWS</figDesc><table><row><cell cols="3">Backbone width</cell><cell cols="2">mIoU(%)</cell></row><row><cell cols="2">ResNet50</cell><cell>256</cell><cell></cell><cell>78.13</cell></row><row><cell cols="2">ResNet50</cell><cell>384</cell><cell></cell><cell>78.36</cell></row><row><cell cols="2">ResNet50</cell><cell>512</cell><cell></cell><cell>78.68</cell></row><row><cell cols="2">ResNet50</cell><cell>768</cell><cell></cell><cell>78.64</cell></row><row><cell cols="2">ResNet50</cell><cell>1024</cell><cell></cell><cell>78.53</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell></row><row><cell cols="5">VALIDATION OF THE CRITICAL SERIES-PARALLEL HYBRID DESIGN BASED</cell></row><row><cell></cell><cell cols="4">ON DILATED RESNET50</cell></row><row><cell>Method</cell><cell cols="2">Design Manner</cell><cell></cell><cell># params mIoU(%)</cell></row><row><cell>PSPNet [11]</cell><cell>in parallel</cell><cell></cell><cell></cell><cell>65.7M</cell><cell>77.64</cell></row><row><cell>DeepLabv3 [12]</cell><cell>in parallel</cell><cell></cell><cell></cell><cell>39.8M</cell><cell>78.45</cell></row><row><cell>APCNet [14]</cell><cell>in parallel</cell><cell></cell><cell></cell><cell>69.9M</cell><cell>78.20</cell></row><row><cell cols="3">DenseASPP [59] densely connected</cell><cell></cell><cell>50.3M</cell><cell>78.03</cell></row><row><cell>CANet-S</cell><cell>in series</cell><cell></cell><cell></cell><cell>28.7M</cell><cell>53.39</cell></row><row><cell>CANet-P</cell><cell>in parallel</cell><cell></cell><cell></cell><cell>31.9M</cell><cell>77.90</cell></row><row><cell>CANet</cell><cell cols="3">series-parallel hybrid</cell><cell>33.0M</cell><cell>78.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>variant. The difference is that we use no complex cross-layer connections and hierarchical</figDesc><table><row><cell>void void</cell><cell>road road</cell><cell>sidewalk sidewalk</cell><cell>building building</cell><cell>wall wall</cell><cell>fence fence</cell><cell>pole pole</cell><cell cols="3">traffic light traffic sign vegetation traffic light traffic sign vegetation</cell></row><row><cell>terrain terrain</cell><cell>sky sky</cell><cell>person person</cell><cell>rider rider</cell><cell>car car</cell><cell>truck truck</cell><cell>bus bus</cell><cell>train train</cell><cell>motorcycle motorcycle</cell><cell>bicycle bicycle</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>IN TERMS OF PER-CLASS SCORES WITH SEMBLABLE STATE-OF-THE-ART BASELINES ON THE CITYSCAPES VAL SET</figDesc><table><row><cell>Method</cell><cell>road</cell><cell cols="2">s.walk build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell cols="2">t-light t-sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell cols="2">person rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell cols="2">train motor</cell><cell>bike</cell><cell>mIoU(%)</cell></row><row><cell>Dilated FCN [4]</cell><cell>97.8</cell><cell>82.7</cell><cell>91.8</cell><cell>52.0</cell><cell>59.1</cell><cell>60.9</cell><cell>70.3</cell><cell>78.2</cell><cell>92.0</cell><cell>60.9</cell><cell>94.1</cell><cell>81.1</cell><cell>59.8</cell><cell>94.3</cell><cell>61.7</cell><cell>78.4</cell><cell>61.7</cell><cell>65.3</cell><cell>77.4</cell><cell>74.7</cell></row><row><cell>PSPNet [11]</cell><cell>98.1</cell><cell>85.0</cell><cell>92.4</cell><cell>56.7</cell><cell>60.4</cell><cell>63.1</cell><cell>72.1</cell><cell>78.9</cell><cell>92.5</cell><cell>64.6</cell><cell>94.5</cell><cell>82.7</cell><cell>63.5</cell><cell>95.3</cell><cell>83.2</cell><cell>85.8</cell><cell>72.4</cell><cell>67.4</cell><cell>78.3</cell><cell>78.3</cell></row><row><cell>DeepLabv3 [12]</cell><cell>98.0</cell><cell>84.4</cell><cell>92.3</cell><cell>52.9</cell><cell>59.5</cell><cell>62.6</cell><cell>70.6</cell><cell>78.1</cell><cell>92.4</cell><cell>65.0</cell><cell>94.5</cell><cell>82.2</cell><cell>63.0</cell><cell>95.3</cell><cell>83.3</cell><cell>87.1</cell><cell>71.5</cell><cell>70.3</cell><cell>77.5</cell><cell>77.9</cell></row><row><cell>DeepLabv3+ [21]</cell><cell>98.2</cell><cell>84.9</cell><cell>92.7</cell><cell>57.3</cell><cell>62.1</cell><cell>65.2</cell><cell>68.6</cell><cell>78.9</cell><cell>92.7</cell><cell>63.5</cell><cell>95.3</cell><cell>82.3</cell><cell>62.8</cell><cell>95.4</cell><cell>85.3</cell><cell>89.1</cell><cell>80.9</cell><cell>64.6</cell><cell>77.3</cell><cell>78.8</cell></row><row><cell>CANet(ours)</cell><cell>98.0</cell><cell>84.2</cell><cell>92.7</cell><cell>51.3</cell><cell>62.7</cell><cell>67.8</cell><cell>73.6</cell><cell>81.4</cell><cell>92.8</cell><cell>65.0</cell><cell>95.2</cell><cell>84.4</cell><cell>67.4</cell><cell>95.7</cell><cell>84.1</cell><cell>91.8</cell><cell>84.0</cell><cell>68.9</cell><cell>79.7</cell><cell>80.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">ABLATION RESULTS OF KEY COMPONENTS AND AUXILIARY LOSS (AL)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CAM</cell><cell></cell><cell>FSM</cell><cell cols="2">Decoder</cell><cell cols="3">AL mIoU(%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(w/o GF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(w/ GF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>78.90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">1 ? 1 Conv</cell><cell></cell><cell></cell><cell>79.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">3 ? 3 Conv</cell><cell></cell><cell></cell><cell>79.48</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">3 ? 3 Conv</cell><cell></cell><cell></cell><cell>79.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI</head><label>VI</label><figDesc>Fig. 7. Some visualized comparisons of different components on Pascal VOC 2012 validation set. First row: input images. Second row: ground truth. Third row: predictions of dilated FCN (baseline). Fourth row: predictions of CANet without FSM. Fifth row: predictions of CANet with 1 ? 1 convolution based decoder. Sixth row: predictions of CANet with 3 ? 3 convolution based decoder.</figDesc><table><row><cell cols="3">VALIDATION OF IMPROVEMENT STRATEGIES</cell></row><row><cell>Backbone</cell><cell>MS-COCO FT MS/Flip</cell><cell>mIoU(%)</cell></row><row><cell>ResNet50</cell><cell></cell><cell>79.69</cell></row><row><cell>ResNet50</cell><cell></cell><cell>82.75</cell></row><row><cell>ResNet50</cell><cell></cell><cell>83.45</cell></row><row><cell>ResNet50</cell><cell></cell><cell>84.32</cell></row><row><cell>ResNet101</cell><cell></cell><cell>84.90</cell></row><row><cell cols="3">MS-COCO: pre-training with MS-COCO [63] dataset, (2) FT:</cell></row><row><cell cols="3">fine-tune on the original dataset, (3) MS/Flip: testing with</cell></row><row><cell cols="3">multi-scale inputs as well as their left-right mirrors, and (4)</cell></row><row><cell cols="2">employing a deeper backbone network.</cell><cell></cell></row></table><note>6) Comparisons with State-of-the-Arts: We conduct exper- iments on the test set to compare with other prior methods based on ImageNet pre-trained ResNet101. CANet is first</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII PER</head><label>VII</label><figDesc>-CLASS SCORES ON PASCAL VOC 2012 TEST SET</figDesc><table><row><cell>Method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell cols="5">horse mbike person plant sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mIoU(%)</cell></row><row><cell>FCN [4]</cell><cell>76.8</cell><cell>34.2</cell><cell cols="2">68.9 49.4</cell><cell>60.3</cell><cell cols="3">75.3 74.7 77.6</cell><cell>21.4</cell><cell>62.5</cell><cell>46.8</cell><cell>71.8</cell><cell>63.9</cell><cell>76.5</cell><cell>73.9</cell><cell>45.2</cell><cell>72.4</cell><cell>37.4</cell><cell>70.9</cell><cell>55.1</cell><cell>62.2</cell></row><row><cell>DeepLabv2 [20]</cell><cell>84.4</cell><cell>54.5</cell><cell cols="2">81.5 63.6</cell><cell>65.9</cell><cell cols="3">85.1 79.1 83.4</cell><cell>30.7</cell><cell>74.1</cell><cell>59.8</cell><cell>79.0</cell><cell>76.1</cell><cell>83.2</cell><cell>80.8</cell><cell>59.7</cell><cell>82.2</cell><cell>50.4</cell><cell>73.1</cell><cell>63.7</cell><cell>71.6</cell></row><row><cell>DeconvNet [32]</cell><cell>89.9</cell><cell>39.9</cell><cell cols="2">79.7 63.9</cell><cell>68.2</cell><cell cols="3">87.4 81.2 86.1</cell><cell>28.5</cell><cell>77.0</cell><cell>62.0</cell><cell>79.0</cell><cell>80.3</cell><cell>83.6</cell><cell>80.2</cell><cell>58.8</cell><cell>83.4</cell><cell>54.3</cell><cell>80.7</cell><cell>65.0</cell><cell>72.5</cell></row><row><cell>DPN [60]</cell><cell>87.7</cell><cell>59.4</cell><cell cols="2">78.4 64.9</cell><cell>70.3</cell><cell cols="3">89.3 83.5 86.1</cell><cell>31.7</cell><cell>79.9</cell><cell>62.6</cell><cell>81.9</cell><cell>80.0</cell><cell>83.5</cell><cell>82.3</cell><cell>60.5</cell><cell>83.2</cell><cell>53.4</cell><cell>77.9</cell><cell>65.0</cell><cell>74.1</cell></row><row><cell>Piecewise [61]</cell><cell>90.6</cell><cell>37.6</cell><cell cols="2">80.0 67.8</cell><cell>74.4</cell><cell cols="3">92.0 85.2 86.2</cell><cell>39.1</cell><cell>81.2</cell><cell>58.9</cell><cell>83.8</cell><cell>83.9</cell><cell>84.3</cell><cell>84.8</cell><cell>62.1</cell><cell>83.2</cell><cell>58.2</cell><cell>80.8</cell><cell>72.3</cell><cell>75.3</cell></row><row><cell>ResNet38 [62]</cell><cell>94.4</cell><cell>72.9</cell><cell cols="2">94.9 68.8</cell><cell>78.4</cell><cell cols="3">90.6 90.0 92.1</cell><cell>40.1</cell><cell>90.4</cell><cell>71.7</cell><cell>89.9</cell><cell>93.7</cell><cell>91.0</cell><cell>89.1</cell><cell>71.3</cell><cell>90.7</cell><cell>61.3</cell><cell>87.7</cell><cell>78.1</cell><cell>82.5</cell></row><row><cell>PSPNet [11]</cell><cell>91.8</cell><cell>71.9</cell><cell cols="2">94.7 71.2</cell><cell>75.8</cell><cell cols="3">95.2 89.9 95.9</cell><cell>39.3</cell><cell>90.7</cell><cell>71.7</cell><cell>90.5</cell><cell>94.5</cell><cell>88.8</cell><cell>89.6</cell><cell>72.8</cell><cell>89.6</cell><cell>64.0</cell><cell>85.1</cell><cell>76.3</cell><cell>82.6</cell></row><row><cell>EncNet [13]</cell><cell>94.1</cell><cell>69.2</cell><cell cols="2">96.3 76.7</cell><cell>86.2</cell><cell>96.3</cell><cell cols="2">90.7 94.2</cell><cell>38.8</cell><cell>90.7</cell><cell>73.3</cell><cell>90.0</cell><cell>92.5</cell><cell>88.8</cell><cell>87.9</cell><cell>68.7</cell><cell>92.6</cell><cell>59.0</cell><cell>86.4</cell><cell>73.4</cell><cell>82.9</cell></row><row><cell>SDN [26]</cell><cell>96.2</cell><cell>73.9</cell><cell cols="2">94.0 74.1</cell><cell>76.1</cell><cell cols="3">96.7 89.9 96.2</cell><cell>44.1</cell><cell>92.6</cell><cell>72.3</cell><cell>91.2</cell><cell>94.1</cell><cell>89.2</cell><cell>89.7</cell><cell>71.2</cell><cell>93.0</cell><cell>59.0</cell><cell>88.4</cell><cell>76.5</cell><cell>83.5</cell></row><row><cell>SeENet [45]</cell><cell>93.7</cell><cell>73.7</cell><cell cols="2">94.4 67.8</cell><cell>82.4</cell><cell cols="2">94.5 90.7</cell><cell>94.1</cell><cell>42.4</cell><cell>92.5</cell><cell>72.1</cell><cell>90.8</cell><cell>92.6</cell><cell>88.3</cell><cell>89.4</cell><cell>76.6</cell><cell>92.9</cell><cell>68.1</cell><cell>88.5</cell><cell>77.2</cell><cell>83.8</cell></row><row><cell>CFNet [15]</cell><cell>95.7</cell><cell>71.9</cell><cell cols="2">95.0 76.3</cell><cell>82.8</cell><cell cols="3">94.8 90.0 95.9</cell><cell>37.1</cell><cell>92.6</cell><cell>73.0</cell><cell>93.4</cell><cell>94.6</cell><cell>89.6</cell><cell>88.4</cell><cell>74.9</cell><cell>95.2</cell><cell>63.2</cell><cell>89.7</cell><cell>78.2</cell><cell>84.2</cell></row><row><cell>APCNet [14]</cell><cell>95.8</cell><cell>75.8</cell><cell cols="2">84.5 76.0</cell><cell>80.6</cell><cell>96.9</cell><cell>90.0</cell><cell>96.0</cell><cell>42.0</cell><cell>93.7</cell><cell>75.4</cell><cell>91.6</cell><cell>95.0</cell><cell>90.5</cell><cell>89.3</cell><cell>75.8</cell><cell>92.8</cell><cell>61.9</cell><cell>88.9</cell><cell>79.6</cell><cell>84.2</cell></row><row><cell>CANet(ours)</cell><cell>96.0</cell><cell>75.4</cell><cell cols="2">95.3 72.0</cell><cell>78.6</cell><cell cols="3">96.3 89.8 95.5</cell><cell>41.5</cell><cell>93.0</cell><cell>72.3</cell><cell>92.3</cell><cell>95.2</cell><cell>90.5</cell><cell>90.3</cell><cell>77.1</cell><cell>92.3</cell><cell>62.4</cell><cell>90.2</cell><cell>79.3</cell><cell>84.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII RESULTS</head><label>VIII</label><figDesc>ON PASCAL CONTEXT DATASET. NOTE THAT WE REPORT MIOU ON 60 CLASSES WITH BACKGROUND pre-trained on the augmented dataset, then fine-tuned with the original trainval images. Results based on multi-scale and flipping testing skills are reported inTable VII. CANet achieves mIoU 84.4% and outperforms all other approaches. To be specific, CANet surpasses the SDN<ref type="bibr" target="#b25">[26]</ref> model of purely in-series encoder-decoder by 0.9 points mIoU with simplicity, which demonstrates the superiority of the series-parallel hybrid design. Additionally, when using MS-COCO<ref type="bibr" target="#b62">[63]</ref> pre-training, we obtain mIoU 87.2%.</figDesc><table><row><cell>Method</cell><cell>Backbone)</cell><cell>mIoU(%)</cell></row><row><cell>FCN-8s [4]</cell><cell>-</cell><cell>37.8</cell></row><row><cell>Piecewise [61]</cell><cell>-</cell><cell>43.4</cell></row><row><cell>DeepLabv2 [20]</cell><cell>ResNet101</cell><cell>45.7</cell></row><row><cell>RefineNet [31]</cell><cell>ResNet152</cell><cell>47.3</cell></row><row><cell>PSPNet [11]</cell><cell>ResNet101</cell><cell>47.8</cell></row><row><cell>EncNet [13]</cell><cell>ResNet101</cell><cell>51.7</cell></row><row><cell>DANet [16]</cell><cell>ResNet101</cell><cell>52.6</cell></row><row><cell>ANL [17]</cell><cell>ResNet101</cell><cell>52.8</cell></row><row><cell>BFP [64]</cell><cell>ResNet101</cell><cell>53.6</cell></row><row><cell>CPNet [65]</cell><cell>ResNet101</cell><cell>53.9</cell></row><row><cell>CANet(ours)</cell><cell>ResNet101</cell><cell>54.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX EXPERIMENTAL</head><label>IX</label><figDesc>RESULTS ON CITYSCAPES TEST SET. ? HERE MEANS EMPLOYING VALIDATION IMAGES FOR TRAINING</figDesc><table><row><cell>Method</cell><cell>Backbone)</cell><cell>mIoU(%)</cell></row><row><cell>DeepLabv2 [20]</cell><cell>ResNet101</cell><cell>70.4</cell></row><row><cell>RefineNet ? [31]</cell><cell>ResNet101</cell><cell>73.6</cell></row><row><cell>ResNet38 [62]</cell><cell>WiderResNet38</cell><cell>78.4</cell></row><row><cell>PSPNet [11]</cell><cell>ResNet101</cell><cell>78.4</cell></row><row><cell>DFN ? [34]</cell><cell>ResNet101</cell><cell>79.3</cell></row><row><cell>PSANet ? [44]</cell><cell>ResNet101</cell><cell>80.1</cell></row><row><cell>DenseASPP ? [59]</cell><cell>DenseNet161</cell><cell>80.6</cell></row><row><cell>SeENet ? [45]</cell><cell>ResNet101</cell><cell>81.2</cell></row><row><cell>ANL ? [17]</cell><cell>ResNet101</cell><cell>81.3</cell></row><row><cell>DANet ? [16]</cell><cell>ResNet101</cell><cell>81.5</cell></row><row><cell>ACFNet ? [18]</cell><cell>ResNet101</cell><cell>81.8</cell></row><row><cell>SPNet ? [66]</cell><cell>ResNet101</cell><cell>82.0</cell></row><row><cell>CANet(ours) ?</cell><cell>ResNet101</cell><cell>82.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE X EXPERIMENTAL</head><label>X</label><figDesc>RESULTS ON CAMVID DATASET (11 CLASSES). ? HERE MEANS EVALUATION WITH 720 ? 960 IMAGES</figDesc><table><row><cell>Method</cell><cell cols="2">PA(%) mIoU(%)</cell></row><row><cell>SegNet [29]</cell><cell>62.5</cell><cell>46.4</cell></row><row><cell>DeconvNet [32]</cell><cell>85.6</cell><cell>48.9</cell></row><row><cell>Bayesian SegNet [67]</cell><cell>86.9</cell><cell>63.1</cell></row><row><cell>Dilation8 [10]</cell><cell>79.0</cell><cell>65.3</cell></row><row><cell>HDCNN-448+TL [68]</cell><cell>90.9</cell><cell>65.6</cell></row><row><cell>Dilation8+FSO [69]</cell><cell>88.3</cell><cell>66.1</cell></row><row><cell>FC-DenseNet103 [70]</cell><cell>91.5</cell><cell>66.9</cell></row><row><cell>DCDN [25]</cell><cell>91.4</cell><cell>68.4</cell></row><row><cell>SeENet [45]</cell><cell>-</cell><cell>68.4</cell></row><row><cell>SDN [26]</cell><cell>91.7</cell><cell>69.6</cell></row><row><cell>BFP [64]</cell><cell>-</cell><cell>74.1</cell></row><row><cell>CANet(ours)</cell><cell>93.9</cell><cell>75.6</cell></row><row><cell>CANet(ours) ?</cell><cell>94.4</cell><cell>78.6</cell></row><row><cell cols="2">TABLE XI</cell><cell></cell></row><row><cell cols="3">QUANTITATIVE RESULTS ON SUN-RGBD DATASET (37 CLASSES) WHICH</cell></row><row><cell cols="3">ONLY USE RGB MODALITY FOR EVALUATION</cell></row><row><cell>Method</cell><cell cols="2">PA(%) mIoU(%)</cell></row><row><cell>FCN [4]</cell><cell>68.2</cell><cell>27.4</cell></row><row><cell>DeconvNet [32]</cell><cell>66.1</cell><cell>22.6</cell></row><row><cell>SegNet [29]</cell><cell>72.6</cell><cell>31.8</cell></row><row><cell>Bayesian SegNet [67]</cell><cell>71.2</cell><cell>30.7</cell></row><row><cell>DeepLabv2 [20]</cell><cell>71.9</cell><cell>32.1</cell></row><row><cell>Piecewise [61]</cell><cell>78.4</cell><cell>42.3</cell></row><row><cell>RefineNet [31]</cell><cell>80.4</cell><cell>45.7</cell></row><row><cell>Ding et al. [71]</cell><cell>81.4</cell><cell>47.1</cell></row><row><cell>CANet(ours)</cell><cell>82.0</cell><cell>48.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE XII RESULTS</head><label>XII</label><figDesc>ON GATECH DATASET</figDesc><table><row><cell>Method</cell><cell cols="2">Temporal Info PA(%)</cell><cell>mIoU(%)</cell></row><row><cell>3D-V2V-scratch [72]</cell><cell>Yes</cell><cell>66.7</cell><cell>-</cell></row><row><cell>3D-V2V-finetune [72]</cell><cell>Yes</cell><cell>76.0</cell><cell>-</cell></row><row><cell>FC-DenseNet103 [70]</cell><cell>No</cell><cell>79.4</cell><cell>-</cell></row><row><cell>HDCNN-448+TL [68]</cell><cell>Yes</cell><cell>82.1</cell><cell>48.2</cell></row><row><cell>DCDN [25]</cell><cell>No</cell><cell>83.5</cell><cell>49.0</cell></row><row><cell>SDN [26]</cell><cell>No</cell><cell>84.6</cell><cell>53.5</cell></row><row><cell>CANet(ours)</cell><cell>No</cell><cell>86.6</cell><cell>56.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4151" to="4160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7151" to="7160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive pyramid context network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7519" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-occurrent features in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Acfnet: Attentional class feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6798" to="6807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">285</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Densely connected deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3085" to="3089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large kernel mattersimprove semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3126" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a discriminative feature network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1857" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9522" to="9531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-level contextual rnns with attention model for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3475" to="3485" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="267" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards bridging semantic gap to improve semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4230" to="4239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Geometric context from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3081" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Gluoncv and gluonnlp: Deep learning in computer vision and natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04433</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Boundary-aware feature propagation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6819" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Hierarchically supervised deconvolutional network for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="437" to="445" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Feature space optimization for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3168" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Context contrasted feature and gated multi-scale aggregation for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2393" to="2402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep end2end voxel2voxel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
