<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Semantic Video Segmentation with Per-frame Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Semantic Video Segmentation with Per-frame Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic video segmentation, temporal consistency</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results when tested on a video sequence. A few methods take the correlations in the video sequence into account, e.g., by propagating the results to the neighbouring frames using optical flow, or extracting frame representations using multi-frame information, which may lead to inaccurate results or unbalanced latency. In contrast, here we explicitly consider the temporal consistency among frames as extra constraints during training and process each frame independently in the inference phase. Thus no computation overhead is introduced for inference. Compact models are employed for real-time execution. To narrow the performance gap between compact models and large models, new temporal knowledge distillation methods are designed. Weighing among accuracy, temporal smoothness and efficiency, our proposed method outperforms previous keyframe based methods and corresponding baselines which are trained with each frame independently on benchmark datasets including Cityscapes and Camvid. Code is available at: https://git.io/vidseg A motion guided temporal loss is employed with the motivation of assigning a consistent label to the same pixel along the time axis. A motion estimation network is introduced to predict the motion (e.g., optical-flow) of each pixel from the current frame to the next frame based on the input frame-pair. Predicted semantic labels are propagated to the next frame to supervise predictions of the next frame. Thus, the temporal consistency is encoded into the segmentation network through this constraint.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation, a fundamental task in computer vision, aims to assign a semantic label to each pixel in an image. In recent years, the development of deep learning has brought significant success to the task of image semantic segmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b4">5]</ref> on benchmark datasets, but often with a high computational cost. This task becomes computationally more expensive when extending to video. For a few real-world applications, e.g., autonomous driving and robotics, it is challenging but crucial to build a fast and accurate video semantic segmentation system.</p><p>Previous works for semantic video segmentation can be categorized into two groups. The first group focuses on improving the performance for video segmentation by performing post-processing among frames <ref type="bibr" target="#b17">[18]</ref>, or employing extra modules to use multi-frames information during inference <ref type="bibr" target="#b7">[8]</ref>. The high computational cost makes it difficult for mobile applications. The second group uses   <ref type="bibr" target="#b12">[13]</ref> propagates and fuses the results from the keyframe (k) to non-key frames (k + 1, . . . ), which may lead to poor results on non-key frames. Baseline: PSPNet18 <ref type="bibr" target="#b36">[37]</ref> trains the model on single frames. Inference on single frames separately can produce temporally inconsistent results. Ours: training the model with the correlations among frames and inferring on single frames separately lead to high quality and smooth results. (b) Comparing our enhanced MobileNetV2 model with previous keyframe based methods: Accel <ref type="bibr" target="#b12">[13]</ref>, DVSN <ref type="bibr" target="#b31">[32]</ref>, DFF <ref type="bibr" target="#b38">[39]</ref> and CC <ref type="bibr" target="#b26">[27]</ref>. The inference speed is evaluated on a single GTX 1080Ti.</p><p>keyframes to avoid processing of each frame, and then propagate <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32]</ref> the outputs or the feature maps to other frames (non-key frames) using optical flows. Keyframe based methods indeed accelerate inference. However, it requires different inference time for keyframes and non-key frames, leading to an unbalanced latency, thus being not friendly for real-time processing. Moreover, accuracy cannot be guaranteed for each frame due to the cumulative warping error, for example, the first row in <ref type="figure" target="#fig_1">Figure 1</ref>(a).</p><p>Efficient semantic segmentation methods on 2D images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23]</ref> have draw much attention recently. Clearly, applying compact networks to each frame of a video sequence independently may alleviate the latency and enable real-time execution. However, directly training the model on each frame independently often produces temporally inconsistent results on the video as shown in the second row of <ref type="figure" target="#fig_1">Figure 1</ref>(a). To address the above problems, we explicitly consider the temporal consistency among frames as extra constraints during the training process and employ compact networks with per-frame inference to ease the problem of latency and achieve real-time inference.</p><p>To narrow the performance gap between compact models and large models, we design a new temporal consistency knowledge distillation strategy to help the training of compact models. Distillation methods are widely used in image recognition tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, and achieve great success. Different from previous distillation methods, which only consider the spatial correlations, we embed the temporal consistency into distillation items. We extract the pair-wise frames dependency by calculating the pair-wise similarities for different locations between two frames, and further encode the multi-frames dependency into a latent embedding by using a recurrent unit, ConvLSTM <ref type="bibr" target="#b27">[28]</ref>. The new distillation methods not only improve temporal consistency but also boost segmentation accuracy. We also include the spatial knowledge distillation methods <ref type="bibr" target="#b18">[19]</ref> of single frames in training to further improve the accuracy.</p><p>We evaluate the proposed methods on semantic video segmentation benchmarks: Cityscapes <ref type="bibr" target="#b5">[6]</ref> and Camvid <ref type="bibr" target="#b2">[3]</ref>. A few compact backbone networks, i.e., PSPNet18 <ref type="bibr" target="#b36">[37]</ref>, MobileNetV2 <ref type="bibr" target="#b25">[26]</ref> and a lightweight HRNet <ref type="bibr" target="#b29">[30]</ref>, are included to verify that the proposed methods can empirically improve the segmentation accuracy and the temporal consistency, without any extra computation and postprocessing during inference. The proposed methods also show superiority in the trade-off of accuracy and the inference speed. For example, with the per-frame inference fashion, our enhanced MobileNetV2 <ref type="bibr" target="#b25">[26]</ref> can achieve higher accuracy with a faster inference speed compared with state-of-the-art keyframe based methods as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b). We summarize our main contributions as follows.</p><p>-We process semantic video segmentation with compact models by per-frame inference, without introducing post-processing and computation overhead, enabling real-time inference without latency. -We explicitly consider the temporal consistency in the training process by using a temporal loss and newly designed temporal consistency knowledge distillation methods. -Empirical experiment results on Cityscapes and Camvid show that with the help of proposed training methods, the compact models outperform previous state-of-the-art semantic video segmentation methods weighing among accuracy, temporal consistency and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Semantic Video Segmentation. Semantic video segmentation requires dense labeling for all pixels in each frame of a video sequence into a few semantic categories. Previous work can be summarized into two streams. The first one focuses on improving the accuracy by exploiting the temporal relations and the unlabelled data in the video sequence. Nilsson and Sminchiesescu <ref type="bibr" target="#b21">[22]</ref> employ a gated recurrent unit to propagate semantic labels to unlabeled frames. Other works like NetWarp <ref type="bibr" target="#b7">[8]</ref>, STFCN <ref type="bibr" target="#b6">[7]</ref>, and SVP <ref type="bibr" target="#b17">[18]</ref> also employ optical-flow or recurrent units to fuse the results of several frames during inferring to improve the segmentation accuracy. Recently, Zhu et al. <ref type="bibr" target="#b39">[40]</ref> propose to use a motion estimation network to propagate labels to unlabeled frames as data augmentation and achieve state-of-the-art performance with the segmentation accuracy. These methods can achieve significant performance but can be difficult to be deployed on mobile devices.</p><p>The second line of works pay attention to reduce the computational cost by re-using the feature maps in the neighbouring frames. ClockNet <ref type="bibr" target="#b26">[27]</ref> proposes to copy the feature map to the next frame directly, thus reducing the computational cost. DFF <ref type="bibr" target="#b38">[39]</ref> employs the optical flow to warp the feature map between the keyframe and non-key frames. Xu et al. <ref type="bibr" target="#b31">[32]</ref> further propose to use an adaptive keyframe selection policy while Zhu et al. <ref type="bibr" target="#b37">[38]</ref> find out that propagating partial region in the feature map can get better performance. Li et al. <ref type="bibr" target="#b16">[17]</ref> propose a lowlatency video segmentation network by optimizing both the keyframe selection and the adaptive feature propagation. Accel <ref type="bibr" target="#b12">[13]</ref> proposes a network fusion policy to use a large model to predict the keyframe and use a compact one in nonkey frames. Keyframe based methods require different inference time and may produce different quantity results between keyframes and other frames. In this work, we solve the real-time video segmentation by per-frame inference with a compact network and propose a temporal loss and the temporal consistency knowledge distillation to ensure both good accuracy and temporal consistency. Temporal Consistency. Applying image processing algorithms to each frame of a video may lead to inconsistent results. The temporal consistency problem has draw much attention in low-level and mid-level applications, such as taskspecific methods including colorization <ref type="bibr" target="#b14">[15]</ref>, style transfer <ref type="bibr" target="#b8">[9]</ref>, and video depth estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref> and task agnostic approaches <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>. Temporal consistency is also essential in semantic video segmentation. Miksik et al. <ref type="bibr" target="#b20">[21]</ref> employ a postprocessing method that learns a similarity function between pixels of consecutive frames to propagate predictions across time. Nilsson and Sminchiesescu <ref type="bibr" target="#b21">[22]</ref> insert the optical flow estimation network into the forward pass and employ a recurrent unit to make use of neighbouring predictions. Our method is more efficient than theirs as we employ per-frame inference. The warped previous predictions work as a constraint only during training. Knowledge Distillation. The effectiveness of knowledge distillation has been verified in classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35]</ref>. The output of the large teacher net, including the final logits and the intermediate feature maps, are treated as soft targets to supervise the compact student net. Previous knowledge distillation methods in semantic segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> design distillation strategies only for improving the segmentation accuracy. To our knowledge, to date no distillation methods consider to improve temporal consistency. In this work, we focus on encoding the motion information into the distillation terms to make the segmentation networks more suitable for the semantic video segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we show how we exploit the temporal information during training. As shown in <ref type="figure" target="#fig_4">Figure 2</ref>   We only show the forward pass of the student net here and apply the same operations on the teacher net to get the dependency cross frames as soft targets. (e) The inference process. All the proposed methods are only applied during training. We can improve the temporal consistency as well as the segmentation accuracy without any extra parameters or post-processing during inference. ure 2(b)) and newly designed temporal consistency knowledge distillation strategies <ref type="figure" target="#fig_4">(Figure 2</ref>(c) and <ref type="figure" target="#fig_4">Figure 2(d)</ref>). The temporal consistency of the single-frame models can be significantly improved by employing temporal loss. However, if compact models are employed for real-time execution, there is still a performance gap between large models and small ones. We design new temporal consistency knowledge distillation to transfer the temporal consistency from large models to small ones. With the help of temporal information, the segmentation accuracy can also be boosted.</p><formula xml:id="formula_0">I ? t I Occlusion t k I ? t t k M ? ? t t k V ? ? t k Q ? t Q t k Q ? (b) FlowNet (e) AT Score</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motion Guided Temporal Consistency</head><p>Training semantic segmentation networks independently on each frame of a video sequence often leads to undesired inconsistency. Conventional methods include previous predictions as an extra input, which introduces extra computational cost during inference. We employ previous predictions as supervised signals to assign consistent labels to each corresponding pixel along the time axis. As shown in <ref type="figure" target="#fig_4">Figure 2</ref>(b), for two input frames I t , I t+k from time t and t + k, we have:</p><formula xml:id="formula_1">tl (I t , I t+k ) = 1 N N i=1 V (i) t?t+k q i t ?q i t+k?t 2 2<label>(1)</label></formula><p>where q i t represents the predicted class probability at the position i of the segmentation map Q t , andq i t+k?t is the warped class probability from frame t + k to frame t, by using a motion estimation network(e.g., FlowNet) f (?). Such an f (?) can predict the amount of motion changes in the x and y directions for each pixel: f (I t+k , I t ) = M t?t+k , where ?i = M t?t+k (i), indicating the pixel on the position i of the frame t moves to the position i + ?i in the frame t + k. Therefore, the segmentation maps between two input frames are aligned by the motion guidance. An occlusion mask V t?t+k is designed to remove the noise caused by the warping error: V t?t+k = exp(? I t ?? t+k ), where? t+k is the warped input frame. We employ a pre-trained optical flow prediction network as the motion estimation net in implementation. We directly consider the temporal consistency during the training process through the motion guided temporal loss by constraining a moving pixel along the time steps to have a consistent semantic label. Similar constraints are proposed in image processing tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>, but rarely discussed in semantic segmentation. We find that the straightforward temporal loss can improve the temporal consistency of single-frame models significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal Consistency Knowledge Distillation</head><p>Inspired by <ref type="bibr" target="#b18">[19]</ref>, we build a distillation mechanism to effectively train the compact student net S by making use of the cumbersome teacher net T. The teacher net T is already well trained with the cross-entropy loss and the temporal loss to achieve a high temporal consistency as well as the segmentation accuracy. Different from previous single frame distillation methods, two new distillation strategies are designed to transfer the temporal consistency from T to S: pairwise-frames dependency (PF) and multi-frame dependency (MF). Pair-wise-Frames Dependency. Following <ref type="bibr" target="#b18">[19]</ref>, we denote an attention (AT) operator to calculate the pair-wise similarity map A X1,X2 of two input tensors X 1 , X 2 , where A X1,X2 ? R N ?N ?1 and X 1 , X 2 ? R N ?C . For the pixel a ij in A, we calculate the cosine similarity between x i 1 and x j 2 from X 1 and X 2 , respectively:</p><formula xml:id="formula_2">a ij = x i 1 x j 2 /( x i 1 2 x j 2 2 )</formula><p>. It is an efficient and easy way to encode the correlations between two input tensors.</p><p>As shown in <ref type="figure" target="#fig_4">Figure 2</ref>(c), we encode the pair-wise dependency between the prediction of every two neighbouring frame pairs by using the AT operator, and get the similarity map A Qt,Q t+k , where Q t is the segmentation map of frame t and a ij of A Qt,Q t+k denotes the similarity between the class probabilities on the location i of the frame t and the location j of the frame t + k. If a pixel on the location i of frame t moves to location j of frame t + k, the similarity a ij may be higher. Therefore, the pair-wise dependency can reflect the motion correlation between two frames.</p><p>We align the pair-wise-frame (PF) dependency between the teacher net T and the student net S,</p><formula xml:id="formula_3">P F (Q t , Q t+k ) = 1 N 2 N i=1 N j=1 (a S ij ? a T ij ) 2 ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">?a S ij ? A S Qt,Q t+k and ?a T ij ? A T Qt,Q t+k .</formula><p>Multi-Frame Dependency. As shown in <ref type="figure" target="#fig_4">Figure 2</ref></p><formula xml:id="formula_5">(d), for a video sequence I = {. . . I t?1 , I t , I t+1 . . . }, the corresponding feature maps F = {. . . F t?1 , F t , F t+1 . . . }</formula><p>are extracted from the output of the last convolutional block before the classification layer. Then, the self-similarity map, A Ft,Ft , for each frame are calculated by using AT operator in order to: 1) capture the structure information among pixels, and 2) align the different feature channels between the teacher net and student net. We employ a ConvLSTM unit to encode the sequence of self-similarity maps into an embedding E ? R 1?De , where D e is the length of the embedding space. For each time step, the ConvLSTM unit takes A Ft,Ft and the hidden state which contains the information of previous t ? 1 frames as input and gives an output embedding E t along with the hidden state of the current time step. We align the final output embedding 4 at the last time step, E T and E S from T and S, respectively. The output embedding encodes the relations of the whole input sequence, named multi-frame dependency (MF). The distillation loss based on multi-frame dependency is termed as:</p><formula xml:id="formula_6">M F (F) = E T ? E S 2 2 .</formula><p>The parameters in the ConvLSTM unit are optimized together with the student net. To extract the multi-frame dependency, both the teacher net and the student net share the weight of the ConvLSTM unit. Note that there exists a model collapse point when the weights and bias in the ConvLSTM are all equal to zero. We clip the weights of ConvLSTM between a certain range and enlarges the E T as a regularization to prevent the model collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>We pre-train the teacher net with the segmentation loss and the temporal loss to attain a segmentation network with a high semantic accuracy and temporal consistency. When optimizing the student net, we fix the weight of the motion estimation net (FlowNet) and the teacher net. These two parts are only used to calculate the temporal loss and the distillation terms, which can be seen as extra regularization terms during the training of the student net. During training, we also employ conventional cross-entropy loss, and the single frame distillation method (SF) proposed in <ref type="bibr" target="#b20">[21]</ref> on every single frame to improve the segmentation accuracy. Details can be found in Section A.1. The whole objective function for a sampled video sequence consists of the conventional cross-entropy loss ce , the single-frame distillation loss SF , temporal loss, and the temporal consistency distillation terms:</p><formula xml:id="formula_7">= T t=1 (t) ce + ?( T t=1 (t) SF + T ?1 i=1 tl (Q t , Q t+1 ) + T ?1 i=1 P F (Q t , Q t+1 ) + M F ), (3)</formula><p>where T is the number of all the frames in one training sequence, and T is the number of labeled frames. Due to the high labeling cost in semantic video segmentation tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>, most of the datasets are only annotated with sparse frames. Our methods can be easily applied to the sparse-labeled dataset, because 1) we can make use of large teacher models to generate soft targets; and 2) we care about the temporal consistency between two frames, which can be selfsupervised through motion. The loss weight for all regularization terms ? is set to 0.1.</p><p>After training the compact network, all the motion-estimation net, teacher net, and the distillation modules can be removed. We only keep the student net as the semantic video segmentation network. Thus, both the segmentation accuracy and the temporal consistency can be improved with no extra computational cost in the per-frame inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation details</head><p>Dataset. We evaluate our proposed method on Camvid <ref type="bibr" target="#b2">[3]</ref> and Cityscapes <ref type="bibr" target="#b5">[6]</ref>, which are standard benchmarks for semantic video segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b21">22]</ref>. More details of the training and evaluation can be found in Section B of Appendix. Network structures. Different from the keyframe based method, which takes several frames as input during inferring, we apply our training methods to a compact segmentation model with per-frame inference. There are three main parts while training the system: -A light-weight segmentation network. We conduct most of the experiments on ResNet18 with the architecture of PSPnet <ref type="bibr" target="#b36">[37]</ref>, namely PSPNet18. We also employ MobileNetV2 <ref type="bibr" target="#b25">[26]</ref> and a light-weight HRNet-w18 <ref type="bibr" target="#b29">[30]</ref> to verify the effectiveness and generalization ability. -A motion estimation network. We use a pre-trained FlowNetV2 <ref type="bibr" target="#b23">[24]</ref> to predict the motion between two frames. Because this module can be removed during inferring, we do not need to consider employing a lightweight flownet for acceleration, like in DFF <ref type="bibr" target="#b38">[39]</ref> and GRFP <ref type="bibr" target="#b21">[22]</ref>. -A teacher network. We adopt widely-used segmentation architecture PSP-Net <ref type="bibr" target="#b36">[37]</ref> with a ResNet101 <ref type="bibr" target="#b9">[10]</ref> as the teacher network, namely PSPNet101, which is used to calculate the soft targets in distillation items. We train the teacher net with the temporal loss to enhance the temporal consistency of the teacher.</p><p>Random sampled policy. In order to reduce the computational cost while training video data, and make use of more unlabeled frames, we randomly sample frames in front of the labelled frame, named 'frame f' and behind of the labelled frame, named 'frame b' to form a training triplet (frame f, labelled frame, frame b), instead of only using the frames right next to the labelled ones. The random sampled policy can take both long term and short term correlations into consideration and achieve better performance. Training on a longer sequence may show better performance with more expensive computation. Evaluation metrics. We evaluate our method on three aspects: accuracy, temporal consistency, and efficiency. The accuracy is evaluated by widely-used mean Intersection over Union (mIoU) and pixel accuracy for semantic segmentation <ref type="bibr" target="#b18">[19]</ref>. We report the model parameters (#Param) and frames per second (fps) to show the efficiency of employed networks. We follow <ref type="bibr" target="#b13">[14]</ref> to measure the temporal stability of a video based on the mean flow warping error between every two neighbouring frames. Different from <ref type="bibr" target="#b13">[14]</ref>, we use the mIoU score instead of the mean square error to evaluate the semantic segmentation results, and more details can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablations</head><p>All the ablation experiments are conducted on the Cityscapes dataset with the PSPNet18.</p><p>Effectiveness of proposed methods. In this section, we verify the effectiveness of the proposed training scheme. Both the accuracy and temporal consistency are shown in <ref type="table" target="#tab_0">Table 1</ref>. We build the baseline scheme a, which is trained on every single labelled frame. Then, we apply three distillation terms: the singleframe dependency (SF), the pair-wise-frame dependency (PF) and multi-frame dependency (MF), separately, to get the scheme b, c and d. The temporal loss is employed in the scheme e. Compared with the baseline scheme, all the schemes can improve accuracy as well as temporal consistency. To compare scheme b with c and d, one can see that the newly designed distillation scheme across frames can improve the temporal consistency to a greater extent. From the scheme e, we can see the temporal loss is most effective for the improvement of temporal consistency.</p><p>To compare scheme f with i, we can see that single frame distillation methods <ref type="bibr" target="#b18">[19]</ref> can improve the segmentation accuracy but may harm the temporal consistency.</p><p>To further improve the performance, we combine the distillation terms with the temporal loss and achieve the mIoU of 73.06% and temporal consistency of 70.56%. We do not increase any parameters or extra computational cost with per-frame inference. Both the distillation terms and the temporal loss can be seen as regularization terms, which can help the training process. Such regularization terms introduce extra knowledge from the pre-trained teacher net and the motion estimation network. Besides, performance improvement also benefits from temporal information and unlabelled data from the video. Impact of the random sample policy. We apply the random sample (RS) policy when training with video sequence in order to make use of more unlabelled images, and capture the long-term dependency. Experiment results are shown in <ref type="table" target="#tab_1">Table 2</ref>. By employing the random sampled policy, both the temporal loss and distillation terms can benefit from more sufficient training data in the video sequences, and obtain an improvement on mIoU from 0.24% to 0.69% as well as the temporal consistency from 0.19% to 0.63%. We employ such a random sampled policy considering the memory cost during training.  Impact of the teacher net. The temporal loss can improve the temporal consistency of both cumbersome models and compact models. We compare the performance of the student net training with different teacher net (i.e., with and without the proposed temporal loss) to verify that the temporal consistency can be transferred with our designed distillation term. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. The temporal consistency of the teacher net (PSPNet101) can be enhanced by training with temporal loss by 1.97%. Meanwhile, the mIoU can also be improved by 0.69%. By using the enhanced teacher net in the distillation framework, the segmentation accuracy is comparable (70.26 vs. 70.32), but the temporal consistency has a significant improvement (69.27 vs. 70.10), indicating that the proposed distillation methods can transfer the temporal consistency from the teacher net.</p><p>Discussions. We focus on improving the accuracy and temporal consistency for real-time models by making use of temporal correlations. Thus, we do not introduce extra parameters during inference. A series of work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23]</ref> focus on designing network structures for fast segmentation on single images and achieve promising results. They do not contradict to our work. We will verify that our methods can generalize to different network structures, e.g. ResNet18, MobileNetV2 and HRNet in the next session. Besides, large models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref> can achieve high segmentation accuracy but have low inference speed. The temporal loss is also effective when applying to large models, e.g., our teacher net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Cityscapes</head><p>Comparison with single-frame based methods. Single-frame methods are trained and inferred on each frame independently. Directly apply such methods to video sequences will produce inconsistent results. We apply our training schemes to several efficient single-frame semantic segmentation networks: PSPNet18 <ref type="bibr" target="#b36">[37]</ref>, MobileNetV2 <ref type="bibr" target="#b25">[26]</ref> and HRNet-w18 <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29]</ref>. Metrics of mIoU, temporal consistency, inference speed, and model parameters are shown in <ref type="table" target="#tab_3">Table 4</ref>. As <ref type="table" target="#tab_3">Table 4</ref> shows, the proposed training scheme works well with a few compact backbone networks (e.g., PSPNet18, HRNet-w18 and MobileNetV2). Both temporal consistency and segmentation accuracy can be improved using the temporal information among frames. We also compare our training methods with the single-frame distillation method <ref type="bibr" target="#b18">[19]</ref>. According to our observation, GAN based distillation methods proposed in <ref type="bibr" target="#b18">[19]</ref>can produce inconsistent results. For example, with the same backbone ResNet18, training with the GAN based distillation methods (SKD-R18) achieves higher mIoU: 72.7 vs. 69.8, and a lower temporal consistency: 67.6 vs. 68.5 compared with the baseline PSPNet18, which is trained with crossentropy loss on each single frame. We replace the GAN based distillation term with our temporal consistency distillation terms and the temporal loss, denoted as "PSPNet18+ALL". Both accuracy and smoothness are improved. Note that we also employ a smaller structure of the PSPNet with half channels than in <ref type="bibr" target="#b18">[19]</ref>. Comparison with video-based methods. Video-based methods are trained and inferred on multi frames, we list current methods including keyframe based methods: CC <ref type="bibr" target="#b26">[27]</ref>, DFF <ref type="bibr" target="#b38">[39]</ref>, DVSN <ref type="bibr" target="#b31">[32]</ref>, Accel <ref type="bibr" target="#b12">[13]</ref> and multi-frame input method: GRFP <ref type="bibr" target="#b21">[22]</ref> in <ref type="table" target="#tab_3">Table 4</ref>. The compact networks with per-frame inference can be more efficient than video-based methods. Besides, with per-frame inference, semantic segmentation networks have no unbalanced latency and can handle every frame independently. <ref type="table" target="#tab_3">Table 4</ref> shows the proposed training schemes can achieve a better trade-off between the accuracy and the inference speed compared  <ref type="bibr" target="#b12">[13]</ref>, training and inferring on multiple frames. The keyframe is selected in every five frames. For better visualization, we zoom the region in the red and orange box. The proposed method can give more consistent labels to the moving train and the trees in the red box. In the orange boxes, we can see our methods have similar quantity results in each frame while the keyframe based methods may generate worse results in the frame (e.g., k + 3) which is far from the keyframe (i.e., k).</p><p>with other state-of-the-art semantic video segmentation methods, especially the MobileNetV2 with the fps of 20.8 and mIoU of 73.9. Although keyframe methods can achieve a high average temporal consistency score, the predictions beyond the keyframe are in low quality. Thus, the temporal consistency will be quiet low between keyframe and non-key frames, as shown in <ref type="figure">Figure 3</ref>. The high average temporal consistency score is mainly from the low-quality predictions on nonkey frames. In contrast, our method can produce stable segmentation results on each frame. Qualitative visualization. Qualitative visualization results are shown in <ref type="figure" target="#fig_6">Figure 4</ref>, in which, we can see, the keyframe-based method Accel-18 will produce unbalanced quality segmentation results between the keyframe (e.g., the orange box of k) and non-key frames (e.g., the orange box of k + 1 and k + 3 ), due to the different forward-networks it chooses. By contrast, ours can produce stable results on the video sequence because we use the same enhanced network on all frames. Compared with the baseline method trained on single frames, we can see our proposed method can produce more smooth results, e.g., the region in red boxes. The improvement of temporal consistency is more clearly shown in the video comparison results. Moreover, we show a case of the temporal consistency between neighbouring frames in a sampled frame sequence in <ref type="figure">Figure 3</ref>. Temporal consistency between two frames is evaluated by the warping pixel accuracy. The higher, the better. The keyframe based method will produce jitters between keyframe and non-key frames, while our training methods can improve the temporal consistency for every frame. The temporal consistency between non-key frames are higher than our methods, but the segmentation performance is lower than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CamVid</head><p>We provide additional experiments on CamVid. We use MobileNetV2 as the backbone in the PSPNet. In <ref type="table" target="#tab_3">Table 4</ref>, the segmentation accuracy, and the temporal consistency are improved compared with the baseline method. We also outperform current state-of-the-art semantic video segmentation methods with a better trade-off between the accuracy and the inference speed. We use the pre-trained weight from cityscapes following VideoGCRF <ref type="bibr" target="#b3">[4]</ref>, and achieve better segmentation results of 78.2 vs. 75.2. VideoGCRF <ref type="bibr" target="#b3">[4]</ref> can achieve 22 fps with 321 ? 321 resolution on a GTX 1080 card. We can achieve 78 fps with the same resolution. The consistent improvements on both datasets verify the value of our training schemes for real-time semantic video segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have developed real-time video segmentation methods that consider not only accuracy but also temporal consistency. To this end, we have proposed to use compact networks with per-frame inference. We explicitly consider the temporal correlation during training by using: the temporal loss and the new temporal consistency knowledge distillation. For inference, the model processes each frame separately, which does not introduce latency and avoids post-processing. The compact networks achieve considerably better temporal consistency and semantic accuracy, without introducing extra computational cost during inference. Our experiments have verified the effectiveness of each component that we have designed. They can improve the performance individually and are complement to each other.</p><p>Acknowledgements Correspondence should be addressed to CS. CS was in part supported by ARC DP 'Deep learning that scales'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of distillation mechanism</head><p>A.1 Single frame distillation Following Liu et.al. <ref type="bibr" target="#b18">[19]</ref>, we employ pixel-wise distillation and pair-wise distillation for each single frame. For the pixel-wise distillation, we use the class probabilities Q produced from the cumbersome model as soft targets for training the compact network.</p><p>The loss function based on the Kullback-Leibler divergence is given as follows,</p><formula xml:id="formula_8">pi = 1 N N i=1 KL(q s i q t i ),<label>(4)</label></formula><p>where q i represent the class probabilities of the ith pixel of the segmentation map and N is the number of the pixels. The pair-wise distillation is built on the self-similarity map A as described in multi-frame dependency. We adopt the squared difference to formulate the pair-wise similarity distillation loss,</p><formula xml:id="formula_9">pa = 1 N 2 N i=1 N i=j (a s ij ? a t ij ) 2 .<label>(5)</label></formula><p>The similarity between two pixels is simply computed from the features f i and f j as a ij = f i f j /( f i 2 f j 2 ). The final loss for sing frame distillation is t SF = pi + pa</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Multi-frame distillation</head><p>We employ a ConvLSTM <ref type="bibr" target="#b27">[28]</ref> unit to capture the correlations among all frames in a video sequence. The input sequence is consists of the self-similarity maps of the feature map for each frame,</p><formula xml:id="formula_10">A = {. . . A Ft?1,Ft?1 , A Ft,Ft , A Ft+1,Ft+1 . . . }.</formula><p>For each time step,the key equations are shown in below:</p><formula xml:id="formula_11">i t =?(W ai * A Ft,Ft + W hi * H t?1 + W ei ? E t?1 + b i ) f t =?(W af * A Ft,Ft + W hf * H t?1 + W ef ? E t?1 + b f ) E t =f t ? E t?1 + i t ? tanh(W ae * A Ft,Ft + W he * H t?1 + b e ) o t =?(W ao * A Ft,Ft + W ho * H t?1 + W eo ? E t + b o ) H t =o t ? tanh(E t )<label>(6)</label></formula><p>where '?' denotes the Hadamard product,' * ' denotes the convolution operator, '?' is the sigmoid activation function and the activation of input gate i t controls whether the new input of this time step will be engaged in the memory cell. f t controls how much to keep from the past cell status E t?1 . o t decides the propagation from E t to the hidden state H t . W and b represent the trainable parameters in the ConvLSTM unit. We employ the memory state of the final time step E T as the distillation item, which contains multi-frame dependency. We align the multi-frame dependency from the teacher net and the student net to enhance the performance of the student net. According to <ref type="bibr" target="#b27">[28]</ref>, the state of ConvLSTM unit can be viewed as the hidden representations of moving objects, therefore the multi-frame dependency distillation can help to transfer the temporal consistency from teacher net to the student net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training and evaluation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Dataset</head><p>Cityscapes <ref type="bibr" target="#b5">[6]</ref> is collected for urban scene understanding and contains 30-frame snippets of the street scene with 17 frames per second. The dataset contains 5, 000 high quality pixel-level finely annotated images at 20 th frame in each snippets, which are divided into 2, 975, 500, 1, 525 images for training, validation and testing. The CamVid dataset <ref type="bibr" target="#b2">[3]</ref> is an automotive dataset. It contains five different videos, which has ground truth labels every 30 frames. Three train videos contain 367 frames, while two test videos contain 233 frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training and inference.</head><p>On Cityscapes, the segmentation networks in this paper are trained by minibatch stochastic gradient descent (SGD) for 200 epochs. We sample 8 training triplets for each mini-batch. The learning rate is initialized as 0.01 and is multiplied by (1 ? iter max?iter ) 0.9 . We randomly cut the images into 769 ? 769 as the training input. Normal data augmentation methods are applied during training, such as random scaling (from 0.5 to 2.1) and random flipping. On Camvid, we use a crop size of 640 ? 640. We use the official implementation of PSPNet in Pytorch[?] and train all the network with 4 cards of Tesla Volta 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Details of the evaluating temporal consistency</head><p>We follow <ref type="bibr" target="#b13">[14]</ref> to measure the temporal stability of a video based on the flow warping error between two frames. Different from <ref type="bibr" target="#b13">[14]</ref>, we use the mIoU score instead of the mean square error to evaluate the semantic segmentation results</p><formula xml:id="formula_12">E warp (Q t?1 , Q t ) = Q t ?Q t?1 Q t ?Q t?1<label>(7)</label></formula><p>where Q t represents for the predict segmentation map of frame t andQ t?1 represents for the warped segmentation map from frame t ? 1 to frame t. We calculate a statistical average warp IoU on each sequence, and using an average mean on the validation set to evaluate the temporal stability:</p><formula xml:id="formula_13">E warp = 1 N N i=1 Q i ?Q i Q i ?Q i<label>(8)</label></formula><p>where Q = {Q 2 , . . . , Q T } andQ = {Q 1 , . . . ,Q T ?1 }. T is the total frames of the sequence and N is the number of the sequence. On Cityscapes <ref type="bibr" target="#b5">[6]</ref>, we random sample 100 video sequence from the validation set, which contains 3000 images to evaluate the temporal stability. On Camvid <ref type="bibr" target="#b2">[3]</ref>, we evaluate the temporal stability of the video sequence 'seq05' from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Description of videos and visualization results</head><p>We include three videos in the supplementary materials, named 'demo seq00.mp4', 'val.mp4', and 'Baseline SKD Accel Ours.mp4' to show the improvement of the temporal consistency. Sampled frames are shown in <ref type="figure">Figure 5</ref>, <ref type="figure" target="#fig_7">Figure 6</ref> and <ref type="figure" target="#fig_8">Figure 7</ref>. From the video, we can see that the proposed method can improve the accuracy and the temporal consistency compared with the baseline models.</p><p>We can also observe that in some situations, both our method and the baseline method will produce inconsistent predictions. From the comparison with Accel <ref type="bibr" target="#b12">[13]</ref> and SKD <ref type="bibr" target="#b18">[19]</ref>, we can see that keyframe based methods suffer from the jitters while GAN based distillation methods will produce inconsistent results. Our method can produce stable and smooth sequence with high accuracy. <ref type="figure" target="#fig_9">Figure 8</ref> shows some segmentation results on CamVid dataset. We can observe that the proposed method outperforms the baseline method in the red region.</p><p>(a) An example in val.mp4 (b) An example in demo seq00.mp4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5:</head><p>We use a white line to divide the whole scene into two parts. 'val.mp4' is the sampled from the validation set we use to evaluate the temporal consistency. In 'val.mp4', our method is shown on the left of the line while the baseline method is on the right. 'demo seq00.mp4' is the prediction results on the provided demo video 'sequence00' in the Cityscapes dataset, and our method is above the line while the baseline is below the line.  . Bottom left: Accel <ref type="bibr" target="#b12">[13]</ref>. Bottom right: Ours. There are jitters between keyframe and normal frame in the results sequence of Accel. More results can be found in the supplementary videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Results on each class</head><p>We compare our method with the baseline methods of PSPNet18 in terms of the accuracy and temporal consistency of each class on Cityscapes. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>. For the moving objects with regular structures, e.g. 'train', 'bus', both segmentation accuracy and temporal consistency are improved significantly. For the 'road', 'sidewalk' and 'terrain', the temporal consistency are also improved although the accuracy only have limited improvements. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Accuracy vs. inference speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>(a) Visualization results on consecutive frames: Keyframe: Accel18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), we introduce two terms: a simple temporal loss(Fig-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>(a) Overall of proposed training scheme: We consider the temporal information by the temporal consistency knowledge distillation (c and d) and the temporal loss (b) during training. (b) Temporal loss (TL) encode the temporal consistency through motion constraints. Both the teacher net and the student net are enhanced by the temporal loss. (c) Pair-wise frame dependency (PF): encode the motion relations between two frames. (d) multi-frame dependency (MF): extract the correlations of the intermediate feature maps among multi-frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>15 Fig. 3 :</head><label>153</label><figDesc>The temporal consistency between neighbouring frames in one sampled sequence on Cityscapes. The keyframe based method Accel shows severe jitters between keyframes and others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative outputs. (a): PSPNet18, training on multi frames and inferring on each frame. (b): PSPNet18, training and inferring on each frame. (c): Accel-18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Consecutive frames in two videos. First row:'demo seq00.mp4'. Our results are on the top right. Second row:'val.mp4'. Our results are on the left. More results can be found in the supplementary videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Consecutive frames in 'Baseline SKD Accel Ours.mp4'. Top left: Baseline.Top right: SKD [19]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Consecutive frames in Camvid dataset. First row: input frames. Second row: MobileNet trained with cross-entropy loss. Third row:MobileNet trained with the temporal loss and distillation items. In the baseline method, the region in the red box keep changing while the proposed method can produce similar results on the still stuff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy and temporal consistency on Cityscapes validation set. SF: singleframe distillation methods, PF: our proposed pair-wise-frame dependency distillation method. MF: our proposed multi-frame dependency distillation method, TL: the temporal loss. The proposed distillation methods and temporal loss can improve both the temporal consistency and accuracy, and they are complementary to each other.</figDesc><table><row><cell cols="4">Scheme index SF PF MF TL mIoU Pixel accuracy Temporal consistency</cell></row><row><cell>a</cell><cell>69.79</cell><cell>77.18</cell><cell>68.50</cell></row><row><cell>b</cell><cell>70.85</cell><cell>78.41</cell><cell>69.20</cell></row><row><cell>c</cell><cell>70.32</cell><cell>77.96</cell><cell>70.10</cell></row><row><cell>d</cell><cell>70.38</cell><cell>77.99</cell><cell>69.78</cell></row><row><cell>e</cell><cell>70.67</cell><cell>78.46</cell><cell>70.46</cell></row><row><cell>f</cell><cell>71.16</cell><cell>78.69</cell><cell>70.21</cell></row><row><cell>g</cell><cell>71.36</cell><cell>78.64</cell><cell>70.13</cell></row><row><cell>h</cell><cell>71.57</cell><cell>78.94</cell><cell>70.61</cell></row><row><cell>i</cell><cell>72.01</cell><cell>79.21</cell><cell>69.99</cell></row><row><cell>j</cell><cell>73.06</cell><cell>80.75</cell><cell>70.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impact of the random sample policy. RS: random sample policy, TC: temporal consistency, TL: temporal loss, Dis: distillation terms, ALL: combine TL with Dis. The proposed random sample policy can improve the accuracy and temporal consistency.</figDesc><table><row><cell>Method</cell><cell>RS mIoU TC</cell></row><row><cell>PSPNet18 + TL</cell><cell>70.04 70.21</cell></row><row><cell>PSPNet18 + TL</cell><cell>70.67 70.46</cell></row><row><cell>PSPNet18 + Dis</cell><cell>71.24 69.48</cell></row><row><cell>PSPNet18 + Dis</cell><cell>72.01 69.99</cell></row><row><cell>PSPNet18 + ALL</cell><cell>72.87 70.05</cell></row><row><cell>PSPNet18 + ALL</cell><cell>73.06 70.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Influence of the teacher net. TL: temporal loss. TC: temporal consistency. We use the pair-wise-frame distillation to show our design can transfer the temporal consistency from the teacher net.</figDesc><table><row><cell>Method</cell><cell>Teacher Model</cell><cell>mIoU</cell><cell>TC</cell></row><row><cell>PSPNet101</cell><cell>None</cell><cell cols="2">78.84 69.71</cell></row><row><cell>PSPNet101 + TL</cell><cell>None</cell><cell cols="2">79.53 71.68</cell></row><row><cell>PSPNet18</cell><cell>None</cell><cell cols="2">69.79 68.50</cell></row><row><cell>PSPNet18</cell><cell>PSPNet101</cell><cell cols="2">70.26 69.27</cell></row><row><cell>PSPNet18</cell><cell cols="3">PSPNet101 + TL 70.32 70.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>We compare our methods with recent efficient image/video semantic segmentation networks on three aspects: accuracy (mIoU,%), smoothness (TC, %) and inference speed (fps, Hz). TL: temporal loss, ALL: all proposed terms, TC: temporal consistency, #Param: parameters of the networks.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>#Params</cell><cell cols="5">Cityscapes mIoU TC fps mIoU TC Camvid</cell><cell>fps</cell></row><row><cell cols="7">Video-based methods: Train and infer on multi frames</cell><cell></cell><cell></cell></row><row><cell>CC [27]</cell><cell>VGG16</cell><cell>-</cell><cell cols="3">67.7 71.2 16.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DFF [39]</cell><cell>ResNet101</cell><cell>-</cell><cell cols="6">68.7 71.4 9.7 66.0 78.0 16.1</cell></row><row><cell>GRFP [22]</cell><cell>ResNet101</cell><cell>-</cell><cell>69.4</cell><cell>-</cell><cell cols="2">3.2 66.1</cell><cell>-</cell><cell>6.4</cell></row><row><cell>DVSN [32]</cell><cell>ResNet101</cell><cell>-</cell><cell>70.3</cell><cell>-</cell><cell>19.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Accel [13]</cell><cell>ResNet101/18</cell><cell>-</cell><cell cols="6">72.1 70.3 3.6 66.7 76.2 7.1</cell></row><row><cell cols="8">Single frame methods: Train and infer on each frame independently</cell><cell></cell></row><row><cell>PSPNet [37]</cell><cell>ResNet101</cell><cell>68.1</cell><cell cols="6">78.8 69.7 1.7 77.6 77.1 4.1</cell></row><row><cell>SKD-MV2 [19]</cell><cell>MobileNetV2</cell><cell>8.3</cell><cell cols="3">74.5 68.2 14.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SKD-R18 [19]</cell><cell>ResNet18</cell><cell>15.2</cell><cell cols="6">72.7 67.6 8.0 72.3 75.4 13.3</cell></row><row><cell>PSPNet18 [37]</cell><cell>ResNet18</cell><cell>13.2</cell><cell cols="3">69.8 68.5 9.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HRNet-w18 [29,30] HRNet</cell><cell>3.9</cell><cell cols="3">75.6 69.1 18.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MobileNetV2 [26]</cell><cell>MobileNetV2</cell><cell>3.2</cell><cell cols="6">70.2 68.4 20.8 74.4 76.8 27.8</cell></row><row><cell cols="8">Ours: Train on multi frames and infer on each frame independently</cell><cell></cell></row><row><cell>Teacher Net</cell><cell>ResNet101</cell><cell>68.1</cell><cell cols="6">79.5 71.7 1.7 79.4 78.6 4.1</cell></row><row><cell>PSPNet18+TL</cell><cell>ResNet18</cell><cell>13.2</cell><cell cols="3">71.1 70.0 9.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSPNet18+ALL</cell><cell>ResNet18</cell><cell>13.2</cell><cell cols="3">73.1 70.6 9.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNet-w18+TL</cell><cell>HRNet</cell><cell>3.9</cell><cell cols="3">76.4 69.6 18.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HRNet-w18+ALL HRNet</cell><cell>3.9</cell><cell cols="3">76.6 70.1 18.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">MobileNetV2+TL MobileNetV2</cell><cell>3.2</cell><cell cols="6">70.7 70.4 20.8 76.3 77.6 27.8</cell></row><row><cell cols="2">MobileNetV2+ALL MobileNetV2</cell><cell>3.2</cell><cell cols="6">73.9 69.9 20.8 78.2 77.9 27.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (mIoU, %) and temporal consistency (TC, %) for each class on Cityscapes. Baseline: PSPNet18 trained on each frame independently. Ours: PSPNet18 trained with temporal loss and distillation items. Name road sidewalk building wall fence pole tra. light tra. sign vegetation terrain</figDesc><table><row><cell cols="3">Baseline 97.0 Class mIoU Ours 97.2</cell><cell>78.7 79.4</cell><cell cols="2">90.1 91.0 49.8 57.4 53.1 41.8 54.7 50.3</cell><cell>63.6 67.0</cell><cell>72.0 73.6</cell><cell>90.8 91.0</cell><cell>60.0 60.0</cell></row><row><cell>TC</cell><cell cols="3">Baseline 97.2 Ours 97.7 81.4 80.2</cell><cell cols="2">91.2 50.0 62.1 42.6 91.6 49.6 62.6 43.9</cell><cell>47.2 48.5</cell><cell>52.6 53.2</cell><cell>91.7 91.9</cell><cell>72.0 73.3</cell></row><row><cell cols="2">Class Name</cell><cell cols="2">sky person</cell><cell>rider</cell><cell>car truck bus</cell><cell cols="3">train motorbike bicycle</cell><cell>mean</cell></row><row><cell>mIoU</cell><cell cols="2">Baseline 92.8 Ours 93.1</cell><cell>75.8 77.1</cell><cell cols="2">52.7 57.1 92.1 65.5 82.2 91.6 61.4 77.1</cell><cell>56.9 73.1</cell><cell>46.9 55.6</cell><cell>71.8 72.8</cell><cell>69.8 73.1</cell></row><row><cell>TC</cell><cell cols="2">Baseline 92.8 Ours 93.0</cell><cell>68.7 69.6</cell><cell cols="2">28.7 30.1 87.0 76.3 82.2 86.4 74.8 78.5</cell><cell>55.5 76.4</cell><cell>55.9 57.5</cell><cell>73.7 74.9</cell><cell>68.5 70.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The details of calculations in ConvLSTM is referred in<ref type="bibr" target="#b27">[28]</ref>, and we also include the key equations in Section A.2 in Appendix.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised depth learning in challenging indoor video: Weak rectification to rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno>arXiv: Comp. Res. Repository abs/2006.02708</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised scale-consistent depth and ego-motion learning from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stfcn: spatio-temporal fully convolutional neural network for semantic segmentation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="493" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4453" to="4462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterizing and improving stability in neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4067" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge adaptation for efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accel: A corrective fusion network for efficient semantic segmentation on video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8866" to="8875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7341" to="7349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5997" to="6005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Surveillance video parsing with single frame supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="413" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient temporal consistency for streaming video scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Automation</title>
		<meeting>IEEE Int. Conf. Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic video segmentation by gated recurrent flow propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6819" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch implementation of flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pottorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/flownet2-pytorch" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets. arXiv: Comp. Res. Repository</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst</title>
		<meeting>Advances in Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv: Comp. Res. Repository abs/1904.04514</idno>
		<title level="m">High-resolution representations for labeling pixels and regions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoders matter for semantic segmentation: Data-dependent decoding enables flexible feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3126" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic video segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6556" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Occlusion-aware video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="777" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learn. Representations</title>
		<meeting>Int. Conf. Learn. Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards high performance video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7210" to="7218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
