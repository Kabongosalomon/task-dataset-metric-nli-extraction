<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Millican</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Las</forename><surname>Casas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelia</forename><surname>Guy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Ring</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saffron</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Maggiore</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jones</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Brock</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Paganini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
						</author>
						<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>All authors from DeepMind, ? Equal contributions, ? Equal senior authorship</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (R ) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25? fewer parameters. After fine-tuning, R performance translates to downstream knowledge-intensive tasks such as question answering. R combines a frozen B retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train R from scratch, yet can also rapidly R fit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions ( 1 , . . . , ) = ( | &lt; ). Neural networks have proven to be powerful language models, first in the form of recurrent architectures <ref type="bibr" target="#b17">(Graves, 2013;</ref><ref type="bibr" target="#b27">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b28">Mikolov et al., 2010)</ref> and more recently in the form of Transformers <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref>, that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters <ref type="bibr" target="#b8">(Brown et al., 2020;</ref><ref type="bibr" target="#b30">Radford et al., 2019)</ref> in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks <ref type="bibr" target="#b8">(Kaplan et al., 2020)</ref>. The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data.</p><p>In this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach. At a high level, our Retrieval Transformer (R ) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers (100 millions parameters) and databases of limited size (up to billions of tokens) <ref type="bibr" target="#b20">(Guu et al., 2020;</ref><ref type="bibr">Khandelwal et al., 2020;</ref><ref type="bibr">Lewis et al., 2020;</ref>. To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by ? 10?. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to degrade, perhaps due to the reduced quality. At evaluation R can be used without retrieval data <ref type="bibr">(R [OFF]</ref>), bringing limited performance degradation compared to baseline transformers.</p><p>contributions are the following.</p><p>? We introduce R , a retrieval-enhanced autoregressive language model ( ?2.2). We use a chunked cross-attention module to incorporate the retrieved text ( ?2.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen B model ( ?2.3) works at scale, removing the need for training and updating a retriever network.</p><p>? We show that our method scales well with model size and database size ( <ref type="figure" target="#fig_0">Fig. 1)</ref>: R provides a constant gain for models ranging from 150M to 7B parameters, and R can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 <ref type="bibr">(Merity et al., 2017)</ref> and the Pile <ref type="bibr">(Gao et al., 2020) ( ?4)</ref>. We show that R can be fine-tuned to achieve competitive performance on downstream tasks such as question answering ( ?4.3). ? We propose an evaluation aware of proximity of test documents with the training set ( ?2.6), addressing the problem of test set leakage <ref type="bibr" target="#b9">(Lee et al., 2021)</ref>. This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of R comes from both explicit neighbour copying and general knowledge extraction ( ?4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We design our retrieval-enhanced architecture to be capable of retrieving from a database with trillions of tokens. For this purpose, we retrieve at the level of contiguous token chunks instead of individual tokens which reduces storage and computation requirements by a large linear factor. Our method first constructs a key-value database, where values store raw chunks of text tokens and keys are frozen B embedddings <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref>. We use a frozen model to avoid having to periodically re-compute embeddings over the entire database during training. Each training sequence is then split into chunks, which are augmented with their -nearest neighbour retrieved from the database. An encoder-decoder architecture integrates retrieval chunks into the model's predictions. We summarize the R architecture in <ref type="figure">Fig. 2</ref>, and detail it in this section. We end the section by introducing CCA(H, E) X Figure 2 | R architecture. Left: simplified version where a sequence of length = 12 is split into = 3 chunks of size = 4. For each chunk, we retrieve = 2 neighbours of = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the C operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk.</p><p>a new methodology to evaluate language models when an evaluation set is partially present in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training dataset</head><p>We use a multi-lingual version of MassiveText <ref type="bibr">(Rae et al., 2021)</ref> for both training and retrieval data. The dataset consists of text documents from multiple sources and multiple languages totalling over 5 trillion tokens (detailed in <ref type="table" target="#tab_1">Table 1</ref>). Sequences are sampled from subsets of the training data, with sampling weights given in the right-most column of <ref type="table" target="#tab_1">Table 1</ref>. We tokenize the dataset using SentencePiece <ref type="bibr">(Kudo and Richardson, 2018)</ref> with a vocabulary of 128,000 tokens. During training (unless otherwise specified), we retrieve from 600B tokens from the training data. The training retrieval database is made of the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub-sample of 4%. The evaluation retrieval database thus contains 1.75T tokens. To limit test set leakage, we compute the 13-gram Jaccard similarity between train and test documents using the MinHash scheme and remove all training documents with high similarity (0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 <ref type="bibr">(Merity et al., 2017)</ref> from our Wikipedia training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Retrieval-enhanced autoregressive token models</head><p>Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens. Formally, we consider sequences of integer tokens in = [1, ], obtained using a text tokenizer 1 . We split each -token-long example = ( 1 , . . . , ) into a sequence of chunks ( 1 , . . . , ) of size = , i.e. 1 ( 1 , . . . , ), . . . , ( ? +1 , . . . , ) ? . We use = 2048 and = 64. We augment each chunk with a set R D ( ) of neighbours from the database D. R D (or R for brevity) is a non-trainable operator specified in ?2.3. Token likelihoods are provided by a model, parameterized by , that takes as input both previous tokens and their retrieved neighbours. This defines the following retrieval-enhanced sequence log-likelihood:</p><formula xml:id="formula_0">( | , D) ?? =1 ?? =1 ( ?1) + |( ) &lt; ( ?1) + , (R D ( )) &lt; .<label>(1)</label></formula><p>We set R ( 1 ) = ?, namely the likelihood of tokens from the first chunk does not depend on any retrieval data. This likelihood definition preserves autoregressivity: the probability of the -th token of the -th chunk, ( ?1) + , only depends on previously seen tokens ( ) 1 &lt; ( ?1) + and on the data retrieved from the previous chunks (R ( )) &lt; . We can therefore directly sample with logprobability , where sampling within the chunk is conditioned on the neighbours (R ( )) &lt; . This makes retrieval-enhanced models directly comparable with the largest language models that are evaluated by sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Nearest neighbour retrieval</head><p>Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two contiguous chunks of tokens which we denote <ref type="bibr">[ , ]</ref> where is the neighbour chunk which is used to compute the key, and is its continuation in the original document. The corresponding key is the B embedding of , averaged over time, that we denote B ( ). For each chunk , we retrieve its approximate -nearest neighbours from our key-value database using the 2 distance on BERT embeddings ( , ) = ||B ( ) ? B ( )|| 2 2 . The model receives the corresponding values R ( ) ( [ 1 , 1 ], . . . , <ref type="bibr">[ , ]</ref>). Both neighbour chunks and their continuations provide meaningful improvements, as illustrated in our ablation study <ref type="bibr">(Appendix D)</ref>. We use a length 64 for both and , thus R ( ) has a shape of ? with = 128. To avoid retrieving the chunk +1 in the retrieval set R ( ), which would break causality during training, we filter out neighbours originating from the same document as the training sequence .</p><p>For a database of elements, we can query the approximate nearest neighbours in O (log ) time. We use the SCaNN library <ref type="bibr" target="#b19">(Guo et al., 2020)</ref> to achieve this. This means that we can query our 2 trillion token database in 10 ms whilst evaluating or sampling from the model; this expense is amortized over a chunk length. Performing retrieval on-the-fly is too slow to keep up with the training calculations-we leverage the frozen aspect of the embedding operator B to precompute all approximate nearest neighbours and save the results as part of the data. In <ref type="figure">Fig. 9</ref> in the Appendix, we show results where we only retrieve neighbours within Wikipedia. We find that neighbours tend to come from 2-3 links away from a given article whereas random articles are more than 5 links apart. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">R model architecture</head><p>Our model relies on an encoder-decoder transformer architecture, integrating the retrieved data through a cross-attention mechanism as introduced in <ref type="bibr" target="#b40">Vaswani et al. (2017)</ref>. First, the retrieved tokens R ( ) are fed into an encoder Transformer, which computes the encoded neighbours set . Denoting the intermediate activations by , our transformer decoder then interleaves R -blocks R ( , ) and standard Transformer blocks L M( ) (the hyperparameter ? [1, ] determines at which layers we use a R -block). These blocks are built from three different residual operators with signature ? ? ? ? ? : a fully-connected layer F , the standard sequence-level self-attention layer A , and a chunked cross-attention layer C (?, ) that incorporates information from the retrieval encoder:</p><formula xml:id="formula_1">R ( , ) F (C (A ( ) , )) , and L ( ) F (A ( ))<label>(2)</label></formula><p>Since F , A and C are all autoregressive operators whose output at position only depends on (? ) , any succession of R and layers, followed by a token classification head defines an autoregressive log-likelihood (1). An overview of the model architecture is given in Algorithm 1 and in <ref type="figure">Fig. 2</ref>. We next describe the retrieval encoder and the chunked cross-attention layer in more detail, and explain how to sample from R .</p><p>Encoding retrieval neighbours. For each chunk , the retrieval neighbours R ( ) are fed into a bi-directional transformer E , yielding the outputs E (R ( ) , ) ? ? ? , where ? [1, ] indexes each neighbour. The retrieval encoder is a non-causal transformer. It is conditioned on , the activations of chunk , through cross-attention layers; this allows the representations of the retrieval encoder to be modulated by the retrieving chunk in a differentiable way. More precisely, the encoding of the th neighbour of the th chunk, R ( ) , depends on the attended activation (? ( ?1) + ) ? [1, ] ? ? ? of chunk at layer min( ). All neighbours for all chunks are encoded in parallel, yielding a full encoded set ( ) ? [1, ], ? [1, ] ? ? ? ? ? . We denote ? ? ? ? as the encoded neighbours for chunk ? [1, ].</p><p>Chunked cross-attention. To perform the C operation, we first split a given intermediate activation ? ? ? into ?1 attending chunks</p><formula xml:id="formula_2">+ (? + ?1 ) ? [1, ] ? ? ? ? [1, ?1]</formula><p>, as depicted on the right of <ref type="figure">Fig. 2</ref>. + holds the intermediary embeddings of the last token in chunk and of the first ? 1 tokens in +1 2 . We compute the cross-attention between + and -the encoded retrieval set obtained from chunk . Attention is computed across time and across neighbours simultaneously, as we merge the neighbour and time dimensions of before applying cross-attention. Since there is a notion of alignment between data chunks and retrieval neighbours, we use relative positional encodings as described in ?B.1.2.</p><p>We concatenate the ?1 outputs of the per-chunk cross-attentions (each of shape ? ) across time, and properly pad the result; we thus form the output activation C ( , ) ? ? ? . Formally, for each chunk and for each token ? [1, ] we set</p><formula xml:id="formula_3">C ( , ) + ?1 C (? + ?1 , ),<label>(3)</label></formula><p>2 The last token of chunk is the first to be able to access the retrieved content while maintaining autoregressivity in (1). Hence, there is a one token overlap between chunk = ( ?1) + ? <ref type="bibr">[1, ]</ref> and the corresponding attending chunk </p><formula xml:id="formula_4">+ ( + ?1 ) ? [1, ] .</formula><formula xml:id="formula_5">? A enc ( ) // Bi-directional attention if ? enc then ? C enc ( , ) ? F enc ( ) return ? E ( ) for ? [1, ] do ? A ( ) // Causal attention if = min( ) then // The neighbour E N C O D E R</formula><p>is conditioned with the decoder activations of the last layer before the first cross-attention</p><formula xml:id="formula_6">= E (R ( ) 1 , ) if ? then ? C ( , ) ? F ( ) ? R ( )</formula><p>where C is the cross-attention residual operator over time-concatenated encoded neighbours. We recall that this operator is defined in its simplest version by three parameter matrices ? ? ? , ? ? ? and ? ? ? . For all ? ? ? and ? ? ? , we define</p><formula xml:id="formula_7">C (?, ) softmax( ?) ,<label>(4)</label></formula><p>where the softmax is performed on the second dimension and all products are matrix products. We use multi-head cross-attention, and add positional encodings to the softmax(see ?B.1.2).</p><p>The first ? 1 tokens cannot attend to any neighbour of a previous chunk; at these positions, we define C as the identity, setting C ( , ) ? for all tokens ? [1, ? 1]. Finally, the last token ? attends to the last retrieval set and we set ? C (? , ) (not shown in <ref type="figure">Fig. 2</ref>). Listing 1 contains a simplified implementation of C . Note that chunked cross-attention is autoregressive: the output of C at position depends on the sequence from tokens from 0 to that is input to C .</p><p>With R models, even though each C cross-attention attends only to the neighbours of the preceding chunk R ( ?1 ), the dependencies over previous neighbours are propagated via the self-attention operations. The activations of the th token in the th chunk therefore potentially depend upon the set of all previous neighbours R ( ) &lt; , without incurring the quadratic cost of cross attending to that set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling.</head><p>When sampling, at the end of a chunk , we use SCaNN to retrieve neighbours R ( ), based on the embedding B ( ). The encoded neighbours = E (R ( )) are then used to condition the generation of the next chunk +1 , which we do incrementally: overall the cost of sampling is thus quadratic in the size of the sampled sequence, as when sampling from regular Transformers; the added cost of retrieval is linear in the number of chunks , and is negligible compared to the token sampling cost in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Baseline Transformer architecture</head><p>We use a transformer <ref type="bibr" target="#b40">(Vaswani et al., 2017</ref>) similar to the one described in <ref type="bibr" target="#b30">(Radford et al., 2019)</ref>, with some minimal changes: we replace LayerNorm with RMSNorm <ref type="bibr" target="#b44">(Zhang and Sennrich, 2019)</ref> and use relative position encodings <ref type="bibr" target="#b12">(Dai et al., 2019)</ref>. As baselines, we train retrieval-free transformers with 132M, 368M, 1.3B and 7.0B parameters (embedding matrices are excluded from parameter counts). The hyperparameters we used are detailed in <ref type="table" target="#tab_3">Table 2</ref>. All retrieval models use the same size encoder for the retrieval data, with = 896 and 2 layers, which roughly adds 19 parameters. The encoder uses relative positional encodings. The retrieval models contain one R -block every 3 blocks, starting from layer 6. For our smallest model, C is applied in layers 6, 9 and 12 of the main pathway and also once for query conditioning in the encoder, which adds an additional 12 parameters. The relative number of extra parameters reduces as we increase the baseline model size. All models are implemented using JAX <ref type="bibr" target="#b6">(Bradbury et al., 2018)</ref> and Haiku <ref type="bibr" target="#b22">(Hennigan et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Quantifying dataset leakage exploitation</head><p>R models may arguably benefit more easily from evaluation dataset leakage, i.e. the fact that we evaluate on data that were also present in the training set. To better understand how retrieval improves language modelling performance, we therefore quantify evaluation likelihood as a function of the overlap between the evaluation and training datasets.</p><p>The following approach can be used with any language model, and depends only on the frozen retriever system presented in ?2.3. We split the evaluation sequences ( ) into chunks of length ? 64, and we see the training data as a set of chunks C. For each evaluation chunk ? C, we retrieve the 10 closest neighbours (of length up to 128) in the training data. We then compute the longest token substring common to both the evaluation chunk and its neighbours. This gives a number ? [0, ]. The value ( ) = , ranging from 0 (chunk never seen) to 1 (chunk entirely seen), gives a reliable indication of how much overlap there is between the evaluation chunk and the training data. For a given model, we then obtain the log-likelihood ( ) of each chunk , and the number of bytes ( ) it encodes. We then consider the filtered bits-per-bytes of the model: which correspond to the bits-per-bytes on the set of chunks that overlap less than % with the training chunks. Note that the full evaluation bit-per-bytes performance is recovered by bpb(1). The function bpb(?) allows us to evaluate the impact of evaluation leakage over predictive performance: for low , bpb( ) gives an indication on how the model performs on chunks that are entirely new; the slope of bpb(?) shows how much the model exploits evaluation leakage.</p><formula xml:id="formula_8">? ? [0, 1], C { ? C, ( ) }, bpb( ) ? C ( ) ? C ( ) ,<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>We first review existing work on using retrieval for language modelling, and compare R to these works (see <ref type="table" target="#tab_4">Table 3</ref>). As we train R models on a large dataset containing a substantial section of the internet, our work raises potential privacy, safety, and fairness issues that we then review. <ref type="bibr" target="#b7">Brants et al. (2007)</ref> show that scaling the training data to trillions of tokens improves the machine translation performance of -gram models. More recently, GPT-2 <ref type="bibr" target="#b30">(Radford et al., 2019)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020), and</ref><ref type="bibr">Jurassic-1 (Lieber et al., 2021)</ref> show that scaling up language models leads to massive improvements on many downstream tasks. At the same time, <ref type="bibr" target="#b9">Carlini et al. (2021)</ref> demonstrate that large-scale language models can perfectly memorise parts of their training data, suggesting that enhancing models with retrieval may lead to further improvements. However, significant leakage between train and test datasets <ref type="bibr" target="#b9">(Lee et al., 2021;</ref><ref type="bibr">Lewis et al., 2021)</ref> makes comparing and evaluating large models trained on large datasets difficult, especially once retrieval capabilities over the training dataset are added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Retrieval for language modelling</head><p>Historically, information retrieval for text relies on inverted index matching such as TF-IDF and BM25 <ref type="bibr" target="#b34">(Robertson and Zaragoza, 2009</ref>). Foundational work use latent topic modelling approaches like LDA <ref type="bibr" target="#b5">(Blei et al., 2003)</ref> to identify relevant neighbours <ref type="bibr" target="#b41">(Wei and Croft, 2006)</ref>. Work in machine translation such as  and <ref type="bibr" target="#b18">Gu et al. (2018)</ref> retrieve translation pairs based on edit distance between source sentences and guide the translation output using the closest retrieved target sentences. The retrieval database may also be structured -for example, <ref type="bibr" target="#b1">Ahn et al. (2016)</ref> use a symbolic knowledge graph to improve an RNN language model. With the success of deep learning, retrieving systems have partly switched to dense learned representations based on a neural network's activations. Continuous cache <ref type="bibr" target="#b16">(Grave et al., 2017)</ref> adds probability mass to tokens for which previous activations resemble the current activation vector, extending the model's context to the local history. <ref type="bibr">NN-LM (Khandelwal et al., 2020)</ref> applies this idea to transformers and extends the retrieval database to English Wikipedia, resulting in   addresses this limitation by adding an extra gating network to post-process the retrieved data; yet most of the network is unaffected by the retrieval during inference.</p><p>The retrieval representations may be trained directly instead of relying on a pre-trained modelretriever systems have been developed for this purpose, primarily on open-domain question answering. For example, D (Karpukhin et al., 2020) trains two B models (for queries and keys respectively) using a contrastive loss to align the representations of a question and of its answers. <ref type="bibr" target="#b13">Lee et al. (2019)</ref> use an inverse cloze task to find semantic representations of passages for retrieval. These works differs from continuous cache and NN-LM in that they embeds passages (or chunks) of text together, as opposed to each token individually. The retriever network is trained in isolation of the downstream task that uses the retrieval data. This potential issue is specifically addressed by R <ref type="bibr" target="#b20">(Guu et al., 2020)</ref>, which trains the retrieval system end-to-end to maximize the final training cross-entropy. This comes with the extra complexity of searching the database during training and periodically updating the embedding table, severely limiting the scale at which it can operate. R AG <ref type="bibr">(Lewis et al., 2020)</ref> and F D <ref type="bibr" target="#b23">(Izacard and Grave, 2021)</ref>  In the open-domain dialogue setting, BlenderBot 2.0 (Komeili et al., 2021) learns to issue textual internet queries, outperforming dense retrieval methods when evaluated on a task measuring how close model responses are to those of humans. This involves collecting a dataset of human dialogues with associated search queries, which limits the scalability of this approach. <ref type="bibr" target="#b21">Hashemi et al. (2020)</ref> introduce the Guided Transformer, a modified Transformer similar to R , for document retrieval and clarifying question selection. Although effective on question answering and other tasks with strong conditioning, none of these methods are designed to model arbitrary text sequences, in contrast with R .</p><p>R shares components with NN-LM and D in that it uses frozen retrieval representations. R models longer sequences than QA examples; this requires to reason at a sub-sequence level, and to retrieve different documents for the different chunks of a sequence. Similar to F D, R processes the retrieved neighbours separately in the encoder, and assemble them in the chunked cross-attention. This differs from e.g. R , that prepends retrieved documents to the prompt. Using chunks allows for repeated retrieval whilst generating a sequence as opposed to retrieving only once based on the prompt alone. Furthermore, retrieval is done during the whole pre-training process in R , and is not simply plugged-in to solve a certain downstream task. Finally, previous methods based on dense query vectors use small models and retrieval datasets with less than 3B tokens (English Wikipedia). <ref type="table" target="#tab_4">Table 3</ref> summarizes the difference of R with existing approaches. , their tendency of amplifying inherent biases in the training data, and their ability to generate toxic language <ref type="bibr" target="#b15">(Gehman et al., 2020)</ref>. In this section we inspect these dangers, focusing on how retrieval augmented language models may exacerbate or mitigate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Privacy, safety and fairness</head><p>Large language models can perfectly memorise parts of their training data <ref type="bibr" target="#b9">(Carlini et al., 2021)</ref>. When coupled with large training datasets gathered from the web or other sources, this has clear privacy and safety implications. Retrieval models such as R that have access to the entire training dataset during inference exacerbate these privacy issues by being able to directly copy training data. However, retrieval systems offer a path towards mitigating these concerns via obliteration of the retrievable data at inference time. In addition, differential privacy training <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> of retrieval models could guarantee that no private information is stored in the model weights, while individualisation on private data could be made by updating the retrieval database at inference time.</p><p>Due to their high training cost, re-training large language model regularly to incorporate new data, languages, and norms is prohibitively expensive. To keep retrieval models up-to-date, it may be sufficient to update the retrieval database, which is orders of magnitude cheaper than re-training a model from scratch. In addition to the benefits of updating models in terms of fairness and bias, simply training large language models has a significant energy cost <ref type="bibr" target="#b36">(Schwartz et al., 2020;</ref><ref type="bibr" target="#b39">Strubell et al., 2019)</ref>. Retrieval mechanisms offer a path to reducing the compute requirements needed to train and update language models that reach a certain performance.</p><p>Large language models are prone to generating toxic outputs, as shown in <ref type="bibr" target="#b15">Gehman et al. (2020)</ref>. <ref type="bibr" target="#b4">Bender et al. (2021)</ref>; <ref type="bibr" target="#b26">Jo and Gebru (2020)</ref> advocate for the importance of better training data curation and documentation. Additionally, if portions of the training data are found to be eliciting biased or toxic outputs after training, retrieval allows for some correction, as the offending retrieval data can be retroactively filtered. However, it is also the case that without careful analysis and intervention, retrieval models may exacerbate biases that are present in the training data. Retrieval models can also add a further source of bias through the selection mechanism for retrieval documents. Further work in this area is required to better understand how retrieval affects the bias and toxicity of the model outputs.</p><p>Finally, samples from large models are difficult to interpret, making mitigating these issues all the more challenging <ref type="bibr" target="#b3">(Belinkov et al., 2020;</ref><ref type="bibr" target="#b25">Jain and Wallace, 2019)</ref>. Retrieval provides more insights in to the outputs of a model, as one can directly visualise or modify the neighbours that are being used. The examples in <ref type="table" target="#tab_7">Table 6</ref>, 7, 20 and 21 illustrate how retrieval makes language models more factual and interpretable by providing more transparent outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We first report results on language modelling benchmarks. Second, we show how to R fit pre-trained Transformer language models into retrieval models with few additional FLOPs. Next, we report R results on question answering. Finally, we report evaluation metrics with leakage filtering, to better understand the source of the gains with retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Language modelling</head><p>Datasets. We evaluate our models on C4 <ref type="bibr" target="#b32">(Raffel et al., 2020</ref><ref type="bibr">), Wikitext103 (Merity et al., 2017</ref>, Curation Corpus (Curation, 2020), Lambada <ref type="bibr" target="#b29">(Paperno et al., 2016)</ref> and the Pile <ref type="bibr" target="#b14">(Gao et al., 2020)</ref>. We also evaluate on a set of manually selected Wikipedia articles that were added or heavily edited in September 2021, months after our pre-training and retrieval dataset was collected (details are given in ?A.2). We construct the dataset with articles from the "future" and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data. For C4, Wikitext103, the Pile, and our Wikipedia dataset we evaluate the language modelling performance on entire documents and measure the bits-per-byte (bpb). We favour bits-per-byte over loss as it is tokenizer agnostic. We evaluate with a sequence length of 2048 tokens but use a stride of 1024 within documents to mitigate boundary effects. On Curation Corpus we concatenate the article, the "TL;DR:" string, and the summary, but only evaluate the bpb on the summary. For Lambada we evaluate the accuracy on the last word, using greedy generation.</p><p>Model scaling. In <ref type="figure" target="#fig_0">Fig. 1(left)</ref> and <ref type="figure" target="#fig_3">Fig. 3</ref> we show the language modelling performance as we scale models from 150 million to 7 billion (non-embedding) parameters. We see that on all datasets, R outperforms the baseline at all model sizes. Furthermore, we observe that improvements do not diminish as we scale the models. The performance is dataset dependent, with the largest gains on Wikitext103 and C4. Wikipedia articles and other web pages are similar to Wikitext103 documents, even if not exact copies ( ?4.4), we thus obtain dramatic improvements on Wikitext103 as our retrieval model is able to directly exploit these overlaps. The smallest gains are for Curation Corpus, where R only slightly outperforms the baseline. This is expected as Curation Corpus summaries are designed to only contain information from the source article and are not included in our retrieval database. On our "future" Wikipedia September 2021 dataset, we also observe consistent gains for all model sizes.</p><p>Data scaling. <ref type="figure" target="#fig_0">Fig. 1 (middle)</ref> shows how scaling the retrieval database at evaluation improves the language modelling performance. We observe dramatic gains as the retrieval data is increased from Wikipedia (4 billion tokens) to all of Massive text (1.7T tokens). <ref type="figure" target="#fig_0">Fig. 1(right)</ref> shows how performance scales as we increase the number of retrieved chunks. Despite being only trained with 2 neighbours, we see consistent improvements for all models when the number of neighbours is increased from 1 to 10. Furthermore, we observe that larger models are able to better utilise more neighbours: the 172M model improves with up to 10 neighbours, whereas the 7B model improves with up to 40 neighbours.</p><p>The Pile. We evaluate our 7B models on the Pile test sets 3 and compare against the 178B parameter Jurrasic-1 (Lieber et al., 2021) model and the 280B parameter Gopher <ref type="bibr">(Rae et al., 2021)</ref> model. We do not compare against GPT-3 as it is outperformed by Jurassic-1 and Gopher on almost all subsets. . We observe that the retrieval model outperforms the baseline on all test sets and outperforms Jurassic-1 on a majority of them, despite being over an order of magnitude smaller.</p><p>7.5B R model, Jurassic-1 and Gopher. Jurassic-1 outperforms the baseline on all datasets except for books, likely due to the inclusion of books in our training data. Gopher and R outperform the baseline on all test sets. Overall, R 7.5B outperforms Jurassic-1 and Gopher on a majority of the test sets. On the dm_mathematics and ubuntu_irc subsets, our R model does not outperform our 7B baseline and underperforms Jurassic-1. We hypothesise that the retrieved neighbours on these datasets are not helpful, due to a combination of what is in our retrieval dataset and the efficacy of the nearest-neighbour search.</p><p>Wikitext103. To validate our approach in a controlled setting, we compare our method with NN-LM (Khandelwal et al., 2020) on the Wikitext103 dataset in <ref type="table" target="#tab_5">Table 4</ref>. We train a baseline transformer on the training set of Wikitext103. This transformer has 24 layers, 1024 hidden units, 16 heads and a key size of 64, as in <ref type="bibr" target="#b2">Baevski and Auli (2019)</ref>. Our baseline does not have adaptive input, and our tokenizer has an open vocabulary, unlike <ref type="bibr" target="#b2">Baevski and Auli (2019)</ref>, which makes our baseline perplexities a bit higher. The full experiment details and hyperparameters are given in ?C.2 and <ref type="table" target="#tab_1">Table 11</ref>.</p><p>We re-implement NN-LM with our tokenizer and baseline transformer to produce embeddings of size 1024 for every token in Wikitext103. NN-LM has probabilities NN-LM = NN + (1 ? ) L with NN ( ) ? exp (? ). We tune = 0.118 and = 0.00785 on the validation set ( <ref type="figure" target="#fig_6">Fig. 7)</ref> and report performance for these hyperparameters on both the validation and test set.</p><p>We fine-tune our baseline transformer into a R model ( <ref type="figure" target="#fig_6">Fig. 7)</ref>, using the Wikitext103 training data and retrieving from Wikipedia with 2 neighbours. We only train the new weights, as explained in ?4.2, and share the embedding weights between the encoder and the main pathway. This is necessary for Wikitext103 which is quite small, as training R from scratch in this setting leads to over-fitting.</p><p>We evaluate the fine-tuned R model with different retrieval sets. We use 10 neighbours at evaluation for both R and NN-LM. When retrieving from Wikipedia, we obtain results comparable to our NN-LM implementation. Furthermore, scaling the retrieval database to MassiveText yields dramatic improvements, though this is partly due to leakage (see ?4.4). For reproducibility, we also include results when retrieving from C4, which are close to previous state-of-the-art and comparable to using 10 % of MassiveText.</p><p>It is worth noting that NN-LM requires 1024 floats for every token in the retrieval dataset, totalling 15 terabytes (Tb) for the 4 billion tokens in Wikipedia. NN-LM and other token-level retrieval approaches therefore don't scale to retrieval databases with trillions of tokens such as MassiveText. In comparison, R only requires 215Gb to index our Wikipedia dataset, and 93Tb for MassiveText. Inspecting the number of retrieval database entries in <ref type="table" target="#tab_5">Table 4</ref> makes it clear why retrieving at the chunk level is necessary when scaling to datasets with trillions of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">R -fitting baseline models</head><p>We extend baseline models into R models by freezing the pre-trained weights and training only chunked cross-attention and neighbour encoder parameters (less than 10% of weights for the 7B model) in <ref type="figure">Fig. 5</ref>. This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences (3% of the pre-training sequences that we used). Additionally, by only training the new weights we ensure that when evaluated without retrieval, the original model performance is exactly maintained. R fitting models quickly surpasses the performance of baseline models and even achieves performance close to that of R models trained from scratch. The experiment hyperparameters are given in ?C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Question answering</head><p>We fine-tune our retrieval models on the Natural Questions (Kwiatkowski et al., 2019) dataset to demonstrate that our retrieval pathway can be used to inject information from arbitrary data sources. We use the version 4 provided by <ref type="bibr" target="#b23">Izacard and Grave (2021)</ref> which is augmented with the retrieved passages from D <ref type="bibr">(Karpukhin et al., 2020)</ref>. We fine-tune all the weights of our 7.5B pre-trained R model for 25,000 steps using the top 20 retrieved passages. We format the data as "question: {question} \n answer: {answer}" and left pad the data such that "answer:" coincides with the end of the first chunk of 64 tokens and thus aligns with the first retrieving chunk. The model has access to the question via the previous tokens in the sequence as well as the top 20 DPR Wikipedia passages and their titles via the chunked cross-attention mechanism.</p><p>Figure 5 | R -fitting a baseline transformer. Any transformer can be fine-tuned into a retrievalenhanced transformer by randomly initializing and training only the chunked cross-attention and retrieval encoder weights. Fine-tuning in this way quickly recovers and surpasses the non-retrieval performance, and almost achieves the same performance as training a retrieval model from scratch (shown by the arrow on the right hand side of each plot). We find good performance R -fitting our models training on only 3% the number of tokens seen during pre-training.</p><p>The exact match scores are shown in <ref type="table" target="#tab_6">Table 5</ref> and the full fine-tuning details are given in ?C.4. Our method is competitive with previous approaches such as R , R AG and D , but underperforms the more recent F D. In contrast with this work, we find that increasing the number of neighbours past 20 does not improve R performance on this task. We hypothesise that the encoder-decoder structure of T5-the base model in F D-and the T5 pre-training objective leads to a model that relies more on the encoder output than R , which is important in the QA setting. To compete with T5-finetuned models, future work should consider ways of forcing R to rely further on the retrieval encoder output when producing tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relating retrieval performance to dataset leakage.</head><p>We report the filtered eval losses as detailed in ?2.6 on C4, Curation Corpus and Wikitext103 in <ref type="figure">Fig. 6</ref>. On C4 and Wikitext103, for which there is leakage into the training set, the slope is negative for both baseline models and R models. R models exploit leakage more strongly than baseline models, as indicated by the more negative slope. This is due to its explicit ability to copy-paste existing training chunks to predict leaked evaluation chunks (see a qualitative example of this model behavior  <ref type="table" target="#tab_1">Table 19</ref>). On Curation Corpus, retrieval provides a constant offset, which is expected as there is by design no leakage between Curation Corpus and the training dataset.</p><p>On the other hand, R outperforms baseline models at all leakage levels, down to = 12.5%. At this level, the loss is computed on chunks with less than 8 contiguous tokens shared with the closest matching chunk in the training dataset-this is a reasonable level of overlap at which we consider that there is no local leakage. Retrieval thus improves predictions on both chunks that are syntactically similar to chunks in the training set, and on chunks that are syntactically different from all training chunks. This points toward a non trivial R capacity of generalizing based on both model parameters and retrieval database. Similar results are found on the Pile dataset (see <ref type="figure" target="#fig_0">Fig. 12</ref>, ?F.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Using R for sampling</head><p>We show examples of samples obtained using the 7.5B R model in <ref type="table" target="#tab_7">Table 6</ref>, <ref type="table" target="#tab_8">Table 7</ref> and Appendix E. For each chunk (the first one being the prompt), we juxtapose sampled chunks with retrieved neighbours R ( ). To give an indication of local overlap, we colour each sampled token in chunk based on the length of the longest common prefix (LCP) found in the retrieved chunks R ( ?1 ). Similarly, we colour the retrieved chunks based on the LCP in the sampled chunk. For the sample in <ref type="table" target="#tab_7">Table 6</ref>, for which we chose the prompt, we observe that the retrieved chunks influence the sample as there are overlaps between the sampled tokens and neighbour tokens. Overall, retrieval reduces hallucinations (in line with the findings of <ref type="bibr" target="#b38">Shuster et al. (2021)</ref>) and makes the model more knowledgeable, when comparing with samples produced with retrieval disabled. In the sample in <ref type="table" target="#tab_8">Table 7</ref>, the model recognises that the prompt is the beginning of the first scene of Hamlet and leverages retrieval data to continue it with only a few mistakes. We provide further examples in Appendix E, including examples from the evaluation sets, as well as the detailed procedure used for colouring the tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Retrieval-Enhanced Transformers (R ), a method for modelling arbitrary text sequences whilst retrieving from databases with trillions of tokens-scaling the data available to models by an order of magnitude compared to what is typically consumed during training. R models gains do not diminish for models with up to at least 7B parameters, and correspond to non-retrieval models with 10? more parameters on certain datasets. On Wikitext103 and the Pile, R outperforms previous models trained on large scale datasets. We also show that R is competitive on retrieval-intensive downstream tasks such as question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R</head><p>models are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models. Conversely, baseline models can be rapidly fine-tuned into R models to obtain nearly the same performance as if trained from scratch. Careful analysis shows that only a modest fraction of the gains obtained by R are due to test set leakage. In general, we caution for such leakage in large-scale language datasets and suggest further work in better understanding the role of test set leakage in the performance of large-scale language models.</p><p>Overall, our work demonstrates at an unprecedented scale that semi-parametric approaches can provide an orthogonal, more efficient approach than raw parameter scaling as we seek to build more powerful language models. [ 1 , 1 ] colored by LCP with +1 [ 2 , 2 ] colored by LCP with +1 colored by LCP with R ( ?1) LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5</p><p>Beavers are interesting animals that Beavers are interesting animals that .Beavers build their lodges in pon naw them into smaller sections and d live near rivers. They build live near rivers. They build ds they have created in wooded areas rag them into the water.Engineers .Like many things in nature, there of the Pond Beavers are interesting is a connection between creatures i animals because they change the hab n the wild.Beaver ponds cause tree itat in which they live. Beavers do s to drown, but the dead trees attra this by blocking up streams to creat ct the great blue heron, which often e ponds. Then they build their homes return year after year. Over time, , called lodges, in these ponds. Bea a beaver pond can attract more than vers' bodies make them well-suited f 50 nests in a colony, called a rooke or underwater building Special muscl ry.An example of this can be found es close off their noses, ears, and in the large pond off Bradford Road throats to keep the water out. Beave at Carter Fields near the Boxford l rs' broad tails act like rudders for ine.Chris Leahy, an expert with th steering. Their two very large, ora e Massachusetts Audubon Society who nge front teeth are used to gnaw dow wrote n trees. They begin building their d am dams to create ponds. Frogs are am their houses called beaver dams in , then they mean that you are very b ar-like tail, and two protruding tee phibians, so they can live in both l the riverbeds. They also live on lan usy. Beavers swim easily in streams, th that are strong enough to gnaw do and and water. They have great camou d.Beavers use their strong teeth an picking up rocks and sticks to buil wn trees. The beaver uses trees, bra flage to hide from predators. The G d strong jaws to cut down trees and d their dams. They gnaw at trees wit nches, and mud to build dams across olden Retriever, or Golden, is a ver branches to build their homes. They h their big front teeth to cut them rivers and streams. These dams creat y big dog. They are very strong, and also use their teeth and jaws to che down. Then they use parts of the tre e a deep pool of water in which the have a thick coat to help them live w up food. Beavers use their big, fl es to build their houses.Beavers ar beaver builds its home. A beaver hom in at tails to swim. They use e clever builders. They know exactly e is called a lodge. A baby beaver o what they need to build their beave r "kit" remains in the family lodge r dams. They use mud from the stream until the age of two.Beaver fur, kn to make their dams stay together. T own as pelt, was once highly popular hey use their tails to pat down the as a trim for hats and coats. How m mud.Beavers put a snug room at the ight the popularity of beaver fur co top of their dams for their babies.</p><p>ntributed to the colonization of New They store their food underwater. Be York?wwwWw avers eat the bark from the cold areas. A kangaroo is an anima their sharp teeth to gnaw at rocks , then they mean that you are very b w at trees with their big front teet l that is very large. It can jump ve and cut trees. They use their flat t usy. Beavers swim easily in streams, h to cut them down. Then they use pa ry high. They can run very fast. A ails to steer when they are swimming picking up rocks and sticks to buil rts of the trees to build their hous meerkat is a small animal that lives .In the winter, beavers have to bui d their dams. They gnaw at trees wit es.Beavers are clever builders. The in Africa. They live in colonies of ld a house called a beaver lodge. Th h their big front teeth to cut them y know exactly what they need to bui up to a hundred of them. They can c ey build a house that is very strong down. Then they use parts of the tre ld their beaver dams. They use mud f limb trees very easily.</p><p>. The walls are made of twigs. The r es to build their houses.Beavers ar rom the stream to make their dams st oof is made e clever builders. They know exactly ay together. They use their tails to what they need to build their beave pat down the mud.Beavers put a snu r dams. They use mud from the stream g room at the top of their dams for to make their dams stay together. T their babies. They store their food hey use their tails to pat down the underwater. Beavers eat the bark fro mud.Beavers put a snug room at the m the trees that they cut down!1. W top of their dams for their babies.</p><p>hat is the main idea of the first pa They store their food underwater. Be ragraph?.2. What is the main idea o avers eat the bark from the f the second paragraph?</p><p>A mouse is a small mammal that lives of branches and other tree parts. T on land. It is a very good climber hey also use their strong jaws to cu and it can run very fast. Penguins t trees. They bring them to their ho are birds that live on Antarctica. T use. They also use their sharp teeth hey have a thick coat to keep them w to chew up the tree parts. They use arm. Rabbits are small animals that their flat tails to swim to the top live in the ground. They of their house. Then they use their teeth and jaws to chew up the tree </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We provide a full description of MassiveText and of our extract of recent Wikipedia articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Full description of MassiveText</head><p>The full break down of MassiveText by source and languages is given in <ref type="table" target="#tab_10">Table 8</ref>. For a full description and analysis of MassiveText, see <ref type="bibr">Rae et al. (2021)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Wikipedia September 2021</head><p>We create an evaluation dataset consisting of 23 Wikipedia articles that were added or heavily edited in September 2021, after we collected our training dataset. In addition, we filter out articles that rely too heavily on templated content, using the method detailed in ?2.6 to identify articles with chunks that have a high overlap with their neighbours. <ref type="figure" target="#fig_0">Fig. 10</ref> show that little overlap remains between our test dataset and the retrieved neighbours from the training dataset. The full list of included articles is given in <ref type="table" target="#tab_11">Table 9</ref>. We first parse articles using mwparserfromhell 5 . We then remove sections with the following titles: "references", "external links", "sources", "further reading", "see also", "citations", and "note". In the remaining sections, we remove Wikilinks and remove the following templates: "reflist", "notelist", "notelist-ua", "notelist-lr", "notelist-ur", and "notelist-lg". We also exclude objects with the "ref " or "table" tag and clean the remaining text with the strip_code function. Finally, we concatenate the title and all the sections and use \n\n to delimitate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on the retrieval architecture</head><p>We give details on the R architecture, and on the fine-tuning procedure we use for R fitting existing language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. R architecture and implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1. Feed-forward architecture</head><p>As mentioned in the main text, the overall encoder-decoder architecture is fully feed-forward. We start with a sequence ? = ( ) 1 , and its pre-computed neighbours (R ( )) 1 and returns logits in ? ? | | . Along with A , F , C and C operators introduced in the main text, we define the decoder embedding layer E : ? ? ? , the S operator that extracts chunked intermediary embeddings S ( ) ( ) 1 ? ? ? ? and the read-out layer R : ? ? ? ? ? | | . We then describe the forward pass in Algorithm 1. In addition to the usual Transformer ones, R architecture hyperparameters involves the layer indices enc and , at which the encoder and the decoder perform cross-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2. Relative positional encoding in the chunked cross-attention layer</head><p>The C operator uses relative positional logits, that are computed from a specific relative distance separating data tokens from retrieval tokens. Indeed, we expect any retrieval neighbour R ( ) and the chunk to be relatively well aligned, and assume that they start at the same position. Therefore, when computing C ( + , ), we set the distance between the data token ? [1, ] of chunk + and the retrieval token ? [1, 2 ] of R ( ) to be ( , ) ? + ? 1.</p><p>When computing the encoder cross-attentions C (R ( ) , ), we set the distance between the retrieval token ? [1, 2 ] and the data token ? [1, ] to be enc ( , ) ? .</p><p>Positional logits are obtained as a linear transform of a cosine vector computed from ( ( , )) , , and are added to content logits, as in a regular self-attention block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.3. Chunked cross-attention implementation</head><p>Our implementation of the C operator, shown in Listing 1, is based on a vectorized application of a cross-attention layer. For simplicity, we omit the multi-head attention logic and use the simplest Q,K,V attention. We omit relative positional logits computation, described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.4. Optional sharing of embedding matrices</head><p>We use disjoint embeddings for the encoder and decoder by default, which allows us to use a different dimensionality for the encoder (typically kept at E = 896) and for the decoder (that we scale up to = 8192). It is possible to share the embeddings, with little difference in training, as we show in the ablation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Baseline to R model fine-tuning</head><p>As shown in <ref type="figure">Fig. 5</ref>, we found that we were able to take a pre-trained baseline transformer and add R through fine-tuning. In all cases, we froze all weights from pre-training and freshly initialised the retrieval encoder and cross-attention weights. In all cases, the cross-attention is added every third layer starting at layer six. The learning rate for the three smaller models was set to 2 ? 10 ?4 and half that for the larger model. We experimented with allowing the entire model to resume training during fine-tuning but consistently found that the best approach was to freeze the pre-trained model. This kept the retrieval-off performance frozen whereas when all weights were tuned the retrieval off performance would degrade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training details and hyperparameters</head><p>We provide the hyperparameters used in the various experiments of ?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Language model pre-training</head><p>In <ref type="table" target="#tab_1">Table 10</ref>, we show the hyperparameters of the different models we train. In all cases, we train for 419,430,400,000 training tokens. The three smaller models are trained with a batch size of 256 and the largest model is trained with a batch size of 1024. The minimum learning rate is set to 0.1 times the maximum learning rate, which is shown in <ref type="table" target="#tab_1">Table 10</ref>. The learning rate is decayed using a cosine cycle length that matches the total number of training tokens. All models are trained using AdamW (Loshchilov and Hutter, 2019) with a weight decay parameter of 0.1. The learning rate linearly increases from 10 ?7 to the maximum learning rate over the first 750 steps of training. All models use ZeRO to shard the optimiser state <ref type="bibr" target="#b33">(Rajbhandari et al., 2020)</ref>. Additional infrastructure details can be found in <ref type="bibr">Rae et al. (2021)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Wikitext103 comparison</head><p>We provide more details on our Wikitext103 results presented in ?4.1 and <ref type="table" target="#tab_5">Table 4</ref>. We train a baseline transformer on the Wikitext103 training set with the hyperparameters presented in <ref type="table" target="#tab_1">Table 11</ref>. The learning rate ramps linearly from 1 ? 10 ?7 to 2.5 ? 10 ?4 in the first 4,000 steps, then decays to 2 ? 10 ?5 at 100,000 steps using a cosine schedule. The baseline checkpoint at step 35,000 has the lowest perplexity on Wikitext103 valid, of 21.58, for overlapping proportion of 75% (sliding window evaluation that only uses probabilities for tokens that have at least 75% of the sequence length of context, when available). We use this checkpoint for all our baseline and NN-LM numbers reported in <ref type="table" target="#tab_5">Table 4</ref>, except that <ref type="table" target="#tab_5">Table 4</ref> reports for an overlapping proportion of 87.5 %, which slightly lowers the perplexity of our baseline to 21.53 on Wikitext103 valid.</p><p>We also use the 35,000 step baseline checkpoint as initialization for a R fit, which otherwise uses the same optimiser and schedule hyperparameters but only trains the new retrieval weights, as explained in ?4.2. Our best R fit checkpoint has a Wikitext103 valid perplexity 18.46, when retrieving from Wikipedia. We use this R checkpoint in <ref type="table" target="#tab_5">Table 4</ref> for all other retrieval sets. The evaluation curves for our baseline and R fit is shown if <ref type="figure" target="#fig_6">Fig. 7 (left)</ref>. In this particular case, because Wikitext103 is quite small, training a R model from scratch led to weaker results than the baseline, at least when retrieving from Wikipedia, as we couldn't find an effective way to mitigate the increased over-fitting due to the additional weights of R .</p><p>We also re-implement NN-LM using the same tokenizer and dataset that we use for our baseline and R fitting experiments. NN-LM has probabilities NN-LM = + (1 ? ) with ( ) ? exp(? ). To tune and , we begin with = 0.0012, which corresponds to the inverse of the standard deviation of the norm of the embeddings that we use as keys and queries for NN-LM. We find the best = 0.118. We then find the best = 0.00785 for that value of . <ref type="figure" target="#fig_6">Fig. 7</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. R fitting baseline models experiments</head><p>In <ref type="table" target="#tab_1">Table 12</ref>, we give the hyperparameters used for R fitting the models on Massive Text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Question answering experiments</head><p>We fine-tune our 7.5B R model for 25,000 steps, using a batch size of 128, a learning rate cosine scheduled from 10 ?6 to 10 ?7 , with a linear ramp of 750 steps. We use dropout in the decoder only, as it performs better than using dropout in both the encoder and the decoder. Each neighbour is formatted as title: {title}, source: {source}. We use the top 20 neighbours from D when training and evaluating. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model ablations</head><p>We validate important design choices by evaluating what happens when we do not include them. We use the 247M parameter model for all experiments and we train on a compressed 157 billion token schedule for all ablation experiments. We describe results relative to the default settings presented in the main text and recalled here. We report C4 evaluation loss at the end of the training process, and also compares how the evaluation loss decrease versus the training time, measured relatively to the baseline training time. Results are reported in <ref type="figure" target="#fig_7">Fig. 8</ref> and <ref type="table" target="#tab_1">Table 13</ref>.</p><p>Using relative encodings in cross-attention. Using relative encodings in cross-attention, as described in ?B.1.2, provides a pure improvement both in the number of steps to reach a given performance and computational efficiency.</p><p>Conditioning the encoder on the previous chunk. Conditioning the encoder on the previous chunk's intermediate embeddings, as described in ?B.1.1, provides a pure improvement both in term of number of steps and computational efficiency.</p><p>Sharing embeddings. Sharing embeddings across the encoder and the decoder does not affect performance. This motivates us using separate embeddings, as it allows to have a narrower encoder than decoder as we scale up the decoder size.</p><p>Attending neighbours and their continuation. R models are trained by attending, for a given chunk, to both the neighbours of the preceding chunk and their continuation in time. We measure how training and evaluating R models on neighbours only and their continuation only affects performance. Overall, attending to neighbours only provides 22% of the performance improvement due to retrieval in R , while attending the future of the neighbours gives 56% of  Training a deeper encoder. All models in the text use a relatively small R encoder. We experimented with a 3? deeper encoder. We found that this resulted in a tiny decrease in loss-0.15% at the cost of a larger training time (+20%). Overall, using a shallow encoder is the best choice in term of training efficiency.</p><p>Training with multiple neighbours. We measure the effect of training on a single retrieved neighbour, as well as training on 4 neighbours (R uses 2 neighbours in training). Training on a single neighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement at the end of training, but induces a large computational overhead. Overall, we find that using 2 neighbours is the best choice in term of training efficiency. Furthermore, evaluation can be done with additional neighbours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency of cross-attention.</head><p>We measure how the frequency of cross-attention in the decoder affects performance. Overall, attending only once at the top or the bottom layer is a bad choice, while attending once on a mid-depth layer is relatively sound. We choose to have cross-attention every 3 layer as this provides a good trade-off between performance and run-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative experiments</head><p>We illustrate the usage of R models by looking at the perplexity of evaluation samples and by producing samples autoregressively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Inspecting neighbours and perplexities on evaluation data</head><p>To build an intuition of what kind of information is leveraged by R models, we suggest to have a closer look at a few evaluation documents and the corresponding retrieved data in <ref type="bibr">Tables 16,</ref><ref type="bibr">17,</ref><ref type="bibr">18 and 19.</ref> In these tables, the 4 rows corresponds to the first 4 chunks of the documents.</p><p>The left-most column shows the chunk from the document being evaluated, where each token is coloured by the negative cross entropy loss difference R</p><p>[O ] ? R , a positive value, coloured in yellow, indicates that R performs better when it has access to neighbours data. The second columns also shows the evaluated chunk but where each token is coloured by the length of the longest common prefix (LCP) with the preceding neighbours, i.e. the largest integer such that the prefix ( ? ?1 , . . . , ) also appears in R ( ?1 ). Conversely, columns three and four show the first two neighbours and their continuation, respectively [ 1 , 1 ] and [ 2 , 2 ] coloured by LCP with subsequent chunk +1 . LCP colouring helps to visually identify where the evaluated document overlaps the retrieved data. Note that the first chunk, 1 , in the second column is not coloured as it does not have any preceding neighbours to compute LCP with. Similarly, we do not show the neighbours of the fourth chunk, as these are not used to condition any of the first four chunks.</p><p>Our qualitative analysis exhibits two major behaviors.</p><p>Firstly, we observe that sometimes, specific facts in can be extracted from the preceding neighbours R ( ?1 ) and that this can correspond to significant reduction in loss from the R model for the corresponding tokens. Some examples of such behavior include the journal name Publishers Weekly in <ref type="table" target="#tab_1">Table 16</ref>, the football team name Tyrone in <ref type="table" target="#tab_1">Table 17</ref> or the event dates 25 August to 6 September 2020 in <ref type="table" target="#tab_1">Table 18</ref>. In these three examples, the evaluated data consists of recent Wikipedia articles written in September 2021, after we built our retrieval dataset (see section ?A.2). Yet, relevant information to predict this new data was available in the pre-existing retrieval data and the R model seems to be able to correctly leverage it.</p><p>On the other hand, we also observe that some of the evaluation data can partially leak in our training and retrieval data, despite the use of deduplication. R can dramatically exploit such leakage. <ref type="table" target="#tab_1">Table 19</ref> illustrates this behavior, where the chunks 2 and 3 largely overlaps R ( 1 ) and R ( 2 ) respectively, up to small formatting differences, which leads to much lower R loss for all the corresponding tokens. <ref type="figure">Fig. 6</ref> shows that it is possible to quantify how much of the R loss reduction is due to each of these two behaviors, by filtering out evaluation chunks that overlaps with the retrieval set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Inspecting samples</head><p>We can follow the same procedure as above on samples generated using R models, in order to better understand where retrieval data had an influence on sampling. We show examples of samples obtained using the 7.5B R model in <ref type="table" target="#tab_7">Table 6</ref>, 7, 20 and 21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Neighbour quantification</head><p>To quantify a notion of distance between the source document and the retrieved chunks, we can ask the distance between source articles when retrieving only from Wikipedia. <ref type="bibr" target="#b10">Consonni et al. (2019)</ref> Figure 9 | Wikipedia link-distance between retrieved articles. For each sequences, chunk combination we compute the link distance between the target and the top-5 neighbours using only Wikipedia. The rank shows the relative neighbour distance, where rank-1 is the first neighbour and rank 5 is the fifth. The different colours represent link distance. Because we do not retrieve from the same document, 1 is the smallest value. We find, on average, the distance between random articles with a path between them is over 5.0 provides a Wikipedia link dataset which, for each article, contains a list of neighbouring articles. Using this, we construct a directed graph and compute the distance from one page to another. In <ref type="figure">Fig. 9</ref> we compute the link-distance between training sequences and the retrieved neighbours. We find that retrieved documents tend to be from articles that are quite close to the article containing the target. Furthermore, we find that on average the distance increases with rank, suggesting that our neighbours are both useful and that the order is reasonable. This provides confidence for our larger-scale experiments where document distance is less well defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Complementary quantitative results</head><p>We report tables corresponding to quantitative figures of the main text, as well as further filtered language model results on the Pile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Main text datasets</head><p>We report the performance of R and baseline models, measured in bits-per-bytes on evaluation set, in <ref type="table" target="#tab_1">Table 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. The Pile</head><p>In <ref type="figure" target="#fig_4">Fig. 4</ref>, we compare R against Jurassic-1 <ref type="figure" target="#fig_0">(Lieber et al., 2021)</ref>. The full bits-per-bytes results are reported in <ref type="table" target="#tab_1">Table 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Filtered results</head><p>Distribution of leaked chunks in our main evaluation sets. We evaluate leakage between the evaluation sets and the training set by measuring the proportion of evaluation chunks with a certain  overlap ( ). We show histograms in <ref type="figure" target="#fig_0">Fig. 10</ref>. We can see that 4 has some slight overlaps between train and evaluation. Similarly, chunks of Wikitext103 appear in the training set despite having removed the actual Wikitext103 evaluation documents from the training set. On the other hand, our Wikipedia September 21 dataset shows almost no leakage (data being original documents that did not exist at training data creation), and neither does Curation Corpus.</p><formula xml:id="formula_11">C4 Eval bpb (900B) - - - - - - - - 0.88 0.83 0.76 0.71 C4 Eval bpb (360B) - - - - - - - - 0.92 0.87 0.80 0.74 C4 Eval bpb (180B) - - - - - - - - 0.94 0.89 0.81 0.75 C4 Eval bpb (90B) - - - - - - - - 0.95 0.89 0.82 0.76 C4 Eval bpb (36B) - - - - - - - - 0.96 0.90 0.83 0.77 C4 Eval bpb (18B) - - - - - - - - 0.96 0.91 0.83 0.77 C4 Eval bpb (9B) - - - - - - - - 0.96 0.91 0.83 0.77 C4 Eval bpb (4B) - - - - - - - - 0.97 0.91 0.84 0.78 C4 Eval bpb (2B) - - - - - - - - 0.97 0.91 0.84 0.78 C4 Eval bpb ( = 1) - - - - - - - - 0.84 0.79 0.73 0.67 C4 Eval bpb ( = 2) - - - - - - - - 0.83 0.78 0.72 0.67 C4 Eval bpb ( = 3) - - - - - - - - 0.82 0.78 0.71 0.66 C4 Eval bpb ( = 4) - - - - - - - - 0.82 0.77 0.71 0.66 C4 Eval bpb ( = 5) - - - - - - - - 0.82 0.77 0.71 0.66 C4 Eval bpb ( = 10) - - - - - - - - 0.82 0.77 0.71 0.66 C4 Eval bpb ( = 20) - - - - - - - - 0.82 0.77 0.71 0.66 C4 Eval bpb ( = 30) - - - - - - - - 0.82 0.77 0.71 0.65 C4 Eval bpb ( = 40) - - - - - - - - 0.83 0.77 0.71 0.65 C4 Eval bpb ( = 50) - - - - - - - - 0.83 0.78 0.71 0.66 C4 Eval bpb ( = 60) - - - - - - - - 0.84 0.78 0.72 0.66 C4 Eval bpb ( = 70) - - - - - - - - 0.84 0.79 0.72 0.66 C4 Eval bpb ( = 80) - - - - - - - - 0.85 0.79 0.73 0.66 C4 Eval bpb ( = 90) - - - - - - - - 0.85 0.79 0.73 0.66 C4 Eval bpb ( = 100) - - - - - - - - 0.</formula><p>Filtered results on the Pile. We report chunk overlap distribution and filtered performance curves on the Pile in <ref type="figure" target="#fig_0">Fig. 12 and Fig. 11</ref>, respectively. The qualitative interpretation of the filtered curves is the same: R models exploit leakage more, but the performance improvement they provide remains significant even on original chunks that haven't been observed in the training set.    <ref type="figure" target="#fig_0">Figure 12</ref> | Distribution of the overlap between evaluation and train chunks for the Pile evaluation sets. working-class Sunderland, who adores unye makes the film she set out to m his mother -"is almost obsessed wi ake in 1990 about African American w th her," as eager Julie tells her ad omen artists, a film that both inven visers. Her idealism is evident from ts an artistic predecessor with whom the start.The advisers are skepti she can identify and also "finds" C cal, and no wonder; Julie's family i heryl herself as the artist that she s posh, with a comfortable country e seeks. As Dunye identifies herself state and .Reception Great Circle received .Reception Great Circle received first edition hardcoverReception The book also debuted at number tw very favorable reviews, with a cumul very favorable reviews, with a cumul The novel debuted at number one on T o on The New York Times Hardcover No ative "Rave" rating at the review ag ative "Rave" rating at the review ag he New York Times fiction best-selle nfiction best-sellers list on July 2 gregator website Book Marks, based o gregator website Book Marks, based o r list. As of the week ending Februa 8, 2019.</p><p>[5] It spent eleven weeks on n 22 book reviews from mainstream li n 22 book reviews from mainstream li ry 20, 2021, the novel has spent 38 the list.</p><p>[6]Reception[edit]At t terary critics. The novel debuted at terary critics. The novel debuted at weeks on the list.At the review ag he review aggregator website Book Ma number fourteen on The New York Tim number fourteen on The New York Tim gregator website Book Marks, which a rks, which assigns individual rating es Hardcover fiction best-seller lis es Hardcover fiction best-seller lis ssigns individual ratings to book re s to book reviews from mainstream li t for the week ending May t for the week ending May views from mainstream literary criti terary critics, the book received a cs, the novel received a cumulative cumulative "Positive" rating based o "Rave" rating based on 38 reviews, w n 29 reviews: 12 "Rave" reviews, 6 " ith only one "mixed" review. Publish</p><p>Positive" reviews, 9 "Mixed" reviews ers Weekly wrote, "Bennett renders h , and 2 "Pan" reviews.  week later, on March 23rd, seven he competition was moved to April an d May to run after the conclusion of the Celtic League competition, with only eight their 11th consecutive final since their 11th consecutive final since 1-16 to 0-15 winners to qualify for which Dublin won by 0-12 to 0-9.D 1989, losing 6 finals in 9 years, wi 1989, losing 6 finals in 9 years, wi their 10th league final in the past ublin are going for an unprecedented th this latest defeat on an identica th this latest defeat on an identica 13 years.They have won seven of t fourth successive Championship win l scoreline to 2020, when Mayo lost l scoreline to 2020, when Mayo lost heir previous league finals under Co over Kerry. Prior to their current r to Dublin.Background were aiming to Dublin.Background were aiming dy since 2002, losing the other two un, which started with the 2011 Allto win their fourth title and first to win their fourth title and first to <ref type="bibr">Waterford (2007 )</ref> and Dublin (201 Ireland final, they had only managed All-Ireland since 1951. Since then, All-Ireland since 1951. Since then, 1 ).Despite the defeat there were two consecutive victories over them they had lost ten finals <ref type="bibr">(1989,</ref><ref type="bibr">1996</ref> they had lost ten finals <ref type="bibr">(1989,</ref><ref type="bibr">1996</ref> some distinct positives from a Galwa on two separate <ref type="bibr">occasions -1909 an , 1997, 2004, 2006, , 1997, 2004, 2006</ref>  <ref type="bibr">2012, 2013, 2016, 2017, 2020). app 2012, 2013, 2016, 2017, 2020</ref>    2012 horror film The Woman in Black, 2012 horror film The Woman in Black, lack, and played beat poet Allen Gin and played beat poet Allen Ginsberg and played beat poet Allen Ginsberg and played beat poet Allen Ginsberg sberg in the 2013 independent film K in the 2013 independent film Kill Y in the 2013 independent film Kill Y in the 2013 independent film Kill Y ill Your Darlings. He has contribute our &lt;unk&gt;.He has contributed to ma our &lt;unk&gt;.He has contributed to ma our Darlings.He has contributed to d to many charities, including Demel ny charities, ny charities, many charities, including Demelza H za House Children's Hospice and The ouse Children's Hospice and The Trev</p><p>Trevor Project. He also made public or Project. He also made public serv service announcements for the latter ice announcements for the latter. In . In 2011, he was awarded the Trevor 2011, he was awarded the Trevor Pro Project's "Hero Award." ject's "Hero Award."Sources disagr ee about Radcliffe's personal wealth ; he was reported to have earned ?1 million for the first Harry Potter including &lt;unk&gt; Hospice Care for Ch including &lt;unk&gt; Hospice Care for Ch ildren, and The Trevor Project for s ildren, and The Trevor Project for s uicide prevention among LGBTQ youth, uicide prevention among LGBTQ youth, which gave him its Hero Award in 20 which gave him its Hero Award in 20 11.= = Early life = =Radcliffe w 11.= = Early life = =Radcliffe w as born in West London, England. He as born in West London, England. He is the only child of Alan George Rad is the only child of Alan George Rad cliffe, a literary agent, and cliffe, a literary agent, and [ 1 , 1 ] colored by LCP with +1 [ 2 , 2 ] colored by LCP with +1 colored by LCP with R ( ?1) LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5</p><p>Article ps, nul individu ne peut exerc ndividu nepeut exercer d'auto rps, nul individu ne peut exer er d'autorit? qui n rit? qui n'en ?mane express?me cer d'autorit? qui n'en ?mane nt.Article 4 -La libert? co express?ment.Article 4 -La nsiste ? pouvoir faire tout ce libert? consiste ? pouvoir fai quine nuit pas ? autrui : ai re tout ce qui ne nuit pas ? a nsi, l'exercice des droits nat utrui : ainsi, l'exercice des urelsde chaque homme n'a de b droits naturels de chaque homm ornes que celles qui assurent e n'a de bornes que celles qui auxautres membres de la soci? assurent aux autres membres d t? la jouissance de e la soci?t? la jouissance de ces m?mes droits. Ces bornes but de toute association est 'en ?mane express?ment.Artic mane express?ment.Article 4 mane express?ment.Article 4 la d?fense des droits de l'hom le 4.-La libert? consiste ? -La libert? consiste ? pouvoi -La libert? consiste ? pouvoi me et du citoyen. Tout citoye pouvoir faire tout ce qui ne r faire tout ce qui ne nuit pa r faire tout ce qui ne nuit pa n a le droit de participer ? l nuit pas ? autrui : ainsi, l'e s ? autrui : ainsi, l'exercice s ? autrui : ainsi, l'exercice a direction des affaires publi xercice des droits naturels de des droits naturels de chaque des droits naturels de chaque ques. Article 5. -L'impuni chaque homme n'a de bornes qu homme n'a de bornes que celle homme n'a de bornes que celle t? n'a jamais ?t? et ne sera j e celles qui assurent aux autr s qui assurent aux autres memb s qui assurent aux autres memb amais une fin en elle-m?me. L' es membres de la soci?t? la jo res de la soci?t? la jouissanc res de la soci?t? la jouissanc imp uissance de ces m?mes e de ces m?mes droits. Ces bor e de ces m?mes droits. Ces bor nes ne peuvent ?tre d?termin?e nes ne peuvent ?tre d?termin?e s que par la loi.Article 5 -s que par la loi.Article 5 -La loi n'a le droit de d?fend La loi n'a le droit de d?fend re que les actions nuisibles ? re que les actions nuisibles ? la soci?t?. Tout ce qui n'est la soci?t?. Tout ce qui n'est pas d?fendu par la loi ne peu pas d?fendu par la loi ne peu t ?tre emp?ch?, et nul ne peut t ?tre emp?ch?, et nul ne peut ?tre contraint ? faire ce qu' ?tre contraint ? faire ce qu' elle n elle n unit?, comme le despotisme, s droits. Ces bornes ne peuvent 'est toujours r?v?l?e ?tre un ?tre d?termin?es que par la l instrument d'oppression. La ty oi.Article 5.-La loi n'a rannie qui s'est ?lue juge su le droit de d?fendre que les a pr?me de la conscience des aut ctions nuisibles ? la soci?t?. res ne peut ?tre jug?e. La jus Tout ce qui n'est pas d?fendu tice se trouve dans la consci par la loi ne peut ?tre emp?c ence de chaque citoyen, h?, et nul ne peut ?tre [ 1 , 1 ] colored by LCP with +1 [ 2 , 2 ] colored by LCP with +1 colored by LCP with R ( ?1) LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 |</head><label>1</label><figDesc>Scaling of R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>build upon D to set the state of the art on question answering benchmarks by training encoder-decoder transformer models. More recently, E 2 (Sachan et al., 2021) extends F D by using an expectation-maximization algorithm to train the retriever end-to-end and achieves state of the art results compared to similarly sized models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b4">Bender et al. (2021);</ref> Weidinger et al. (2021)  highlight several dangers of large language models. Those stem from their ability to memorise training data, their high training cost, the static nature of their training data(Lazaridou et al., 2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 |</head><label>3</label><figDesc>Scaling with respect to model size. (a) LAMBADA top-1 accuracy. (b) Evaluation loss on curation corpus. (c) Perplexity on Wikitext103 valid. (d) Bits-per-byte on selected Wikipedia articles from September 2021.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 |</head><label>4</label><figDesc>Fig. 4shows the relative improvements in bits-per-byte over our 7B transformer baseline for our The Pile: Comparison of our 7B baseline against Jurassic-1, Gopher, and R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Listing 1 |</head><label>1</label><figDesc>Jax implementation of the chunked cross attention, simplified. n = 128 # Sequence length m = 16 # Chunk length r = 32 # Retrieval length k = 4 # Number of neighbours d = 16 # Embedding size l = n // m # Number of chunks # Parameters Q = jnp.zeros((d, d)) K = jnp.zeros((d, d)) V = jnp.zeros((d, d)) def relative_positional_encodings(attending_length, attended_length): # Classical relative positional encodings ... def cross_attention(chunk, neighbour): m, d = chunk.shape r, d = neighbour.shape queries = chunk @ Q keys = neighbour @ K logits = queries @ keys.T values = neighbour @ V return logits, values def multi_neighbour_cross_attention(chunk, neighbours): m, d = chunk.shape k, r, d = neighbours.shape logits, values = jnp.vectorize(cross_attention, signature='(m,d),(r,d)-&gt;(m,r),(r,d)')( chunk, neighbours) assert logits.shape == (k, m, r) assert values.shape == (k, r, d) logits += relative_positional_encodings(m, r)[None, :, :] logits = jnp.moveaxis(logits, 0, -1).reshape((m, r * k)) values = jnp.moveaxis(values, 0, 1).reshape((r * k, d)) return jax.nn.softmax(logits) @ values def multi_chunk_cross_attention(observation, neighbours): attending_chunks = jnp.pad(observation[m-1:], ((0, m -1), (0, 0)), mode='constant').reshape(l, m, d) chunked_output = jnp.vectorize(multi_neighbour_cross_attention, signature='(m,d),(k,r,d)-&gt;(m,d)')( attending_chunks, neighbours) assert chunked_output.shape == (l, m, d) output = jnp.pad(chunked_output.reshape(n, d.zeros((n, d)) # Input neighbours = jnp.zeros((l, k, r, d)) h = multi_chunk_cross_attention(observation, neighbours) assert h.shape == (n, d) # Output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 |</head><label>7</label><figDesc>center and right respectively show the perplexity of NN-LM as a function of and . Wikitext103valid perplexities. Left: Baseline and R fit (initialized from baseline's checkpoint at 35,000 steps) perplexities as a function of training steps. Center and right: NN-LM perplexity as a function of (for = 0.0012) and (for = 0.12) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 |</head><label>8</label><figDesc>Computational efficiency for different variants. We report the training curves plotting C4 evaluation bytes per bits against time, relative to the time taken to train the baseline R model. Overall, our design choices are optimal in term of computational efficiency. the performance. Attending to both neighbours and their continuation is the most efficient choice both in term of final performance and training efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 |Figure 11 |</head><label>1011</label><figDesc>Distribution of the overlap between evaluation and train chunks for C4, Curation Corpus, Wikitext103 and Wikipedia Sept. 2021. Filtered evaluation losses on the Pile, with baseline Transformers and R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>and starred in the series starred in the series for 10 years u starred in the series for 10 years u starred in the series for 10 years u for 10 years until the release of t ntil the release of the eighth and fntil the release of the eighth and f ntil the release of the eighth and f he eighth and final film in 2011.R inal film in 2011.Radcliffe began inal film in 2011.Radcliffe began inal film in 2011.Radcliffe began adcliffe began to branch out to stag to branch out to stage acting in 200 to branch out to stage acting in 200 to branch out to stage acting in 200 e acting in 2007, starring in the Lo 7, starring in the London and New 7, starring in the London and New 7, starring in the London and New Yo ndon and New York productions of Equ rk productions of Equus, and in the us, and in the 2011 Broadway revival 2011 Broadway revival of the musical of the musical How to Succeed in Bu How to Succeed in Business Without siness Without Really Trying. He sta Really Trying. He starred in the 201 rred in the 2012 horror film The Wom 2 horror film The Woman in Black, an an in Black, and played beat poet Al d played beat poet Allen Ginsberg in len Ginsberg in the 2013 independent the 2013 independent film Kill Your film Kill Your Darlings. He has con Darlings.He has contributed to ma tributed to many charities, includin ny charities g Demelza House Children's York productions of Equus, and in t York productions of Equus, and in t York productions of Equus, and in t in the 2011 Broadway revival of the he 2011 Broadway revival of the musi he 2011 Broadway revival of the musi he 2011 Broadway revival of the musi musical How to Succeed in Business cal How to Succeed in Business Witho cal How to Succeed in Business Witho cal How to Succeed in Business Witho Without Really Trying. He starred in ut Really Trying. He starred in the ut Really Trying. He starred in the ut Really Trying. He starred in the the 2012 horror film The Woman in B 2012 horror film The Woman in Black,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 |</head><label>1</label><figDesc>MassiveText. The last column indicates the sampling weight during training. The multilingual subsets include documents in 10 languages. The full breakdown is given in ?A.1.</figDesc><table><row><cell>Source</cell><cell cols="4">Token count (M) Documents (M) Multilingual Sampling frequency</cell></row><row><cell>Web</cell><cell>977,563</cell><cell>1,208</cell><cell>Yes</cell><cell>55%</cell></row><row><cell>Books</cell><cell>3,423,740</cell><cell>20</cell><cell>No</cell><cell>25%</cell></row><row><cell>News</cell><cell>236,918</cell><cell>398</cell><cell>No</cell><cell>10%</cell></row><row><cell>Wikipedia</cell><cell>13,288</cell><cell>23</cell><cell>Yes</cell><cell>5%</cell></row><row><cell>GitHub</cell><cell>374,952</cell><cell>143</cell><cell>No</cell><cell>5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Hyperparam: and enc , indices of layers with cross-attention in the decoder and encoder respectively Hyperparam: and enc , number of decoder layers and number of encoder layers.</figDesc><table><row><cell></cell><cell cols="2">Algorithm 1: Overview of R</cell><cell>model architecture.</cell></row><row><cell cols="3">Input: ? : sequence of tokens. (R ( )) 1 Output: ? ? ? | | : the output logits</cell><cell>: the retrieved neighbours</cell></row><row><cell>def E</cell><cell>(R ( ) 1</cell><cell>, ):</cell></row><row><cell cols="2">( ) ? [1, ] ? S</cell><cell>( )</cell></row><row><cell cols="4">for ? [1, ], ? [1, ] do // Encoder shared across neighbours and chunks</cell></row><row><cell>= E</cell><cell cols="3">enc (R ( ) ) // May be shared with the decoder E M B</cell></row><row><cell cols="2">for ? [1,</cell><cell></cell></row></table><note>enc ] do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 |</head><label>2</label><figDesc>Number of parameters for our baseline and R models, excluding embeddings, along with the corresponding hyperparameters.</figDesc><table><row><cell>Baseline parameters</cell><cell>R</cell><cell></cell><cell>ffw</cell><cell cols="3"># heads Head size # layers</cell></row><row><cell>132M</cell><cell>172M (+30%)</cell><cell>896</cell><cell>3,584</cell><cell>16</cell><cell>64</cell><cell>12</cell></row><row><cell>368M</cell><cell cols="2">425M (+15%) 1,536</cell><cell>6,144</cell><cell>12</cell><cell>128</cell><cell>12</cell></row><row><cell cols="3">1,309M 1,451M (+11%) 2,048</cell><cell>8,192</cell><cell>16</cell><cell>128</cell><cell>24</cell></row><row><cell cols="4">6,982M 7,532M (+8%) 4,096 16,384</cell><cell>32</cell><cell>128</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 | Comparison of R with existing retrieval approaches.</head><label>3</label><figDesc>Wikitext103 evaluation. Continuous cache and NN-LM do not modify the underlying neural-network models, but interpolate at inference between the language model's output and distributions computed from retrieved tokens. These methods can therefore be plugged into any model without additional training, although this limits the model's ability to reason about the retrieved text. S</figDesc><table><row><cell></cell><cell></cell><cell cols="2"># Retrieval tokens Granularity</cell><cell cols="2">Retriever training</cell><cell>Retrieval integration</cell></row><row><cell cols="2">Continuous Cache</cell><cell>O 10 3</cell><cell>Token</cell><cell cols="2">Frozen (L ST M)</cell><cell>Add to probs</cell></row><row><cell cols="2">NN-LM</cell><cell>O 10 9</cell><cell>Token</cell><cell cols="2">Frozen (Transformer)</cell><cell>Add to probs</cell></row><row><cell>S</cell><cell></cell><cell>O 10 9</cell><cell>Token</cell><cell cols="2">Frozen (Transformer)</cell><cell>Gated logits</cell></row><row><cell>D</cell><cell></cell><cell>O 10 9</cell><cell>Prompt</cell><cell cols="2">Contrastive proxy</cell><cell>Extractive QA</cell></row><row><cell>R</cell><cell></cell><cell>O 10 9</cell><cell>Prompt</cell><cell>End-to-End</cell><cell></cell><cell>Prepend to prompt</cell></row><row><cell>R AG</cell><cell></cell><cell>O 10 9</cell><cell>Prompt</cell><cell>Fine-tuned D</cell><cell></cell><cell>Cross-attention</cell></row><row><cell>F D</cell><cell></cell><cell>O 10 9</cell><cell>Prompt</cell><cell>Frozen D</cell><cell></cell><cell>Cross-attention</cell></row><row><cell>E</cell><cell>2</cell><cell>O 10 9</cell><cell>Prompt</cell><cell cols="2">End-to-End (EM)</cell><cell>Cross-attention</cell></row><row><cell>R</cell><cell>(ours)</cell><cell>O 10 12</cell><cell>Chunk</cell><cell>Frozen (B</cell><cell>)</cell><cell>Chunked cross-attention</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 | Perplexities on Wikitext103. When</head><label>4</label><figDesc>using the Wikpedia dataset for retrieval, R performs similarly to our implementation of NN-LM. As we scale the retrieval dataset, R performs much better. The perplexities for retrieving from full MassiveText are quite low, which is partly due to partial overlap with Wikitext103 not caught by our deduplication.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Retrieval Set</cell><cell cols="2">#Database tokens #Database keys</cell><cell>Valid</cell><cell>Test</cell></row><row><cell cols="3">Adaptive Inputs (Baevski and Auli, 2019) -</cell><cell>-</cell><cell cols="3">-17.96 18.65</cell></row><row><cell>S</cell><cell>(Yogatama et al., 2021)</cell><cell>Wikipedia</cell><cell>3B</cell><cell cols="3">3B 17.20 17.60</cell></row><row><cell cols="2">NN-LM (Khandelwal et al., 2020)</cell><cell>Wikipedia</cell><cell>3B</cell><cell cols="3">3B 16.06 16.12</cell></row><row><cell cols="2">Megatron (Shoeybi et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-10.81</cell></row><row><cell cols="2">Baseline transformer (ours)</cell><cell>-</cell><cell>-</cell><cell cols="3">-21.53 22.96</cell></row><row><cell cols="2">NN-LM (ours)</cell><cell>Wikipedia</cell><cell>4B</cell><cell cols="3">4B 18.52 19.54</cell></row><row><cell>R</cell><cell></cell><cell>Wikipedia</cell><cell>4B</cell><cell cols="3">0.06B 18.46 18.97</cell></row><row><cell>R</cell><cell></cell><cell>C4</cell><cell>174B</cell><cell cols="3">2.9B 12.87 10.23</cell></row><row><cell>R</cell><cell></cell><cell>MassiveText (1%)</cell><cell>18B</cell><cell cols="3">0.8B 18.92 20.33</cell></row><row><cell>R</cell><cell></cell><cell>MassiveText (10%)</cell><cell>179B</cell><cell cols="3">4B 13.54 14.95</cell></row><row><cell>R</cell><cell></cell><cell>MassiveText (100%)</cell><cell>1792B</cell><cell>28B</cell><cell>3.21</cell><cell>3.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 |</head><label>5</label><figDesc>Question answering results. Exact match accuracy on Natural Questions.</figDesc><table><row><cell cols="2">Model</cell><cell>Test Accuracy</cell></row><row><cell>R</cell><cell>(Guu et al., 2020)</cell><cell>40.4</cell></row><row><cell>D</cell><cell>(Karpukhin et al., 2020)</cell><cell>41.5</cell></row><row><cell cols="2">R AG (Lewis et al., 2020)</cell><cell>44.5</cell></row><row><cell>E</cell><cell>2 (Sachan et al., 2021)</cell><cell>52.5</cell></row><row><cell cols="2">F D (Izacard and Grave, 2021)</cell><cell>51.4</cell></row><row><cell cols="2">F D + Distill. (Izacard et al., 2020)</cell><cell>54.7</cell></row><row><cell cols="2">Baseline 7B (closed book)</cell><cell>30.4</cell></row><row><cell>R</cell><cell>7.5B (DPR retrieval)</cell><cell>45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 | Sample -Beavers are interesting animals. The</head><label>6</label><figDesc>R [O ] sample quickly diverges to other animals while the R [O ] sample tends to stay focused on the beaver topic due to neighbour conditioning.</figDesc><table><row><cell>Prompt and sample of R</cell><cell>[O ]</cell><cell>Prompt and sample of R</cell><cell>[O ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 |</head><label>7</label><figDesc>The R [O ] sample has correct syntax but is hallucinated, and ends with repetition of one character (FRANCISCO Approach me not). The R [O ] sample is the correct continuation of the original text, and is robust to formatting differences between our prompt and the retrieved data. . Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, 2020. URL https://arxiv. org/abs/2001.08361. V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In Conference on Empirical Methods in Natural Language Processing, Nov. 2020. URL https://aclanthology.org/2020.emnlp-main.550. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.</figDesc><table><row><cell>Prompt and sample of R</cell><cell>[O ]</cell><cell>Prompt and sample of R</cell><cell>[O ]</cell><cell>[ 1 , 1 ] colored by LCP with +1</cell><cell>[ 2 , 2 ] colored by LCP with +1</cell></row><row><cell></cell><cell></cell><cell cols="2">colored by LCP with R ( ?1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>LCP = 0, 1, 2, 3,4, 5</cell><cell></cell><cell>LCP = 0, 1, 2, 3,4, 5</cell><cell>LCP = 0, 1, 2, 3,4, 5</cell></row><row><cell>ACT I SCENE I. Elsinore. A pla</cell><cell></cell><cell>ACT I SCENE I. Elsinore. A pla</cell><cell></cell><cell>ACT I SCENE I. Elsinore. A pla</cell><cell>&gt;&lt;TEXT&gt;ACT I SCENE I. Elsin</cell></row><row><cell cols="2">tform before the castle. FRANC</cell><cell cols="2">tform before the castle. FRANC</cell><cell>tform before the castle. (FRAN</cell><cell>ore. A platform before the cas</cell></row><row><cell>ISCO at his post. Enter to him</cell><cell></cell><cell>ISCO at his post. Enter to him</cell><cell></cell><cell>CISCO at his post. Enter to hi</cell><cell>tle. FRANCISCO at his post. E</cell></row><row><cell cols="2">BERNARDO BERNARDO Who's there</cell><cell cols="2">BERNARDO BERNARDO Who's there</cell><cell>m BERNARDO) BERNARDO Who's the</cell><cell>nter to him BERNARDO BERNARDO</cell></row><row><cell cols="2">? FRANCISCO Nay, answer me: st</cell><cell cols="2">? FRANCISCO Nay, answer me: st</cell><cell>re? FRANCISCO Nay, answer me:</cell><cell>Who's there? FRANCISCO Nay, an</cell></row><row><cell cols="2">and, and unfold yourself. BERN</cell><cell cols="2">and, and unfold yourself. BERN</cell><cell>stand, and unfold yourself. BE</cell><cell>swer me: stand, and unfold you</cell></row><row><cell>ARDO</cell><cell></cell><cell>ARDO</cell><cell></cell><cell>RNARDO Long live the king! FRA</cell><cell>rself. BERNARDO Long live the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>NCISCO Bernardo? BERNARDO He.</cell><cell>king! FRANCISCO Bernardo? BERN</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRANCISCO You come most carefu</cell><cell>ARDO He. FRANCISCO You come mo</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>lly upon your hour. BERNARDO '</cell><cell>st carefully upon your hour. B</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Tis now struck twelve; get the</cell><cell>ERNARDO 'Tis now struck twelve</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>e to bed, Francisco. FRANCISCO</cell><cell>; get thee to bed, Francisco.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>For this relief much thanks:</cell><cell>FRANCISCO For this relief much</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>'tis bitter cold, And I am sic</cell><cell>thanks: 'tis bitter cold, And</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>k at heart. BERNARDO Have you</cell><cell>I am sick at heart.</cell></row><row><cell cols="2">Who calls ? FRANCISCO I am th</cell><cell cols="2">Long live the king! FRANCISCO</cell><cell>Long live the king! FRANCISCO</cell><cell>live the king! FRANCISCO Bern</cell></row><row><cell>e lord here; I, Francisco, tha</cell><cell></cell><cell cols="2">Bernardo? BERNARDO He. FRANCI</cell><cell>Bernardo? BERNARDO He. FRANCI</cell><cell>ardo? BERNARDO He. FRANCISCO Y</cell></row><row><cell>t am sick of grief. [ Aside. B</cell><cell></cell><cell cols="2">SCO You come most carefully up</cell><cell>SCO You come most carefully up</cell><cell>ou come most carefully upon yo</cell></row><row><cell cols="2">ERNARDO The king ! FRANCISCO I</cell><cell cols="2">on your hour. BERNARDO 'Tis no</cell><cell>on your hour. BERNARDO 'Tis no</cell><cell>ur hour. BERNARDO 'Tis now str</cell></row><row><cell cols="2">am sick of that also. BERNARD</cell><cell>w struck twelve; get thee to b</cell><cell></cell><cell>w struck twelve; get thee to b</cell><cell>uck twelve: get thee to bed, F</cell></row><row><cell cols="2">O My lord ? FRANCISCO Do not a</cell><cell cols="2">ed, Francisco. FRANCISCO For t</cell><cell>ed, Francisco. FRANCISCO For t</cell><cell>rancisco. FRANCISCO For this r</cell></row><row><cell>pproach me. BERNARDO</cell><cell></cell><cell>his relief much thanks: 'tis b</cell><cell></cell><cell>his relief much thanks: 'tis b</cell><cell>elief much thanks: 'tis bitter</cell></row><row><cell></cell><cell></cell><cell>itter cold, And I am sick at h</cell><cell></cell><cell>itter cold, And I am sick at h</cell><cell>cold, And I am sick at heart.</cell></row><row><cell></cell><cell></cell><cell>eart. B</cell><cell></cell><cell>eart.&lt;/TEXT&gt;&lt;/DOC&gt;&lt;DOC&gt;&lt;DO</cell><cell>BERNARDO Have you had quiet g</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNO&gt;romeo&lt;/DOCNO&gt;&lt;TEXT&gt;ACT I</cell><cell>uard? FRANCISCO Not a mouse st</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PROLOGUE Two households, bo</cell><cell>irring. BERNARDO Well, good ni</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>th alike in dignity, In fair V</cell><cell>ght. Ifyou do meet Horatio and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>erona, where we lay our scene,</cell><cell>Marcellus, The rivals2 of my</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>From ancient grudge break to</cell><cell>watch, bid them make haste. FR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>new mutiny,</cell><cell>ANCISCO I think I hear them.-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Stand, ho! who is there? EN</cell></row><row><cell>Francisco, I would speak with</cell><cell></cell><cell cols="2">ERNARDO Have you had quiet gua</cell><cell>had quiet guard? FRANCISCO No</cell><cell>ARDO Have you had quiet guard?</cell></row><row><cell cols="2">you. FRANCISCO Approach me not</cell><cell cols="2">rd? FRANCISCO Not a mouse stir</cell><cell>t a mouse stirring. BERNARDO W</cell><cell>FRANCISCO Not a mouse stirrin</cell></row><row><cell cols="2">, but speak. BERNARDO Your han</cell><cell cols="2">ring. BERNARDO Well, good nigh</cell><cell>ell, good night. If you do mee</cell><cell>g. BERNARDO Well, good night.</cell></row><row><cell cols="2">d, your voice FRANCISCO I will</cell><cell>t. If you do meet Horatio and</cell><cell></cell><cell>t Horatio and Marcellus, The r</cell><cell>Ifyou do meet Horatio and Marc</cell></row><row><cell cols="2">not hear thee speak. BERNARDO</cell><cell>Marcellus, The rivals of my wa</cell><cell></cell><cell>ivals of my watch, bid them ma</cell><cell>ellus, The rivals2 of my watch</cell></row><row><cell>Francisco, your hand, I entre</cell><cell></cell><cell cols="2">tch, bid them make haste. FRAN</cell><cell>ke haste. FRANCISCO I think I</cell><cell>, bid them make haste. FRANCIS</cell></row><row><cell cols="2">at thee. FRANCISCO Approach me</cell><cell cols="2">CISCO I think I hear them. Sta</cell><cell>hear them. Stand, ho! Who's th</cell><cell>CO I think I hear them.-Stand</cell></row><row><cell cols="2">not. BERNARDO Francisco FRANC</cell><cell>nd, ho! who is there? Enter</cell><cell></cell><cell>ere? (Enter HORATIO and MARCEL</cell><cell>, ho! who is there? ENTER HORA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LUS) HORATIO Friends to this g</cell><cell>TIO AND MARCELLUS. HORATIO Fri</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>round. MARCELLUS And liegemen</cell><cell>ends to this ground. MARCELLUS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>to the Dane. FRANCISCO Give yo</cell><cell>And liegemen to the Dane.3 FR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>u good night. MARCELLUS O, far</cell><cell>ANCISCO Give you good night. M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ewell, honest soldier: Who hat</cell><cell>ARCELLUS O, farewell, honest s</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>h relieved you? FRANCISCO Bern</cell><cell>oldier: Who hath relieved you?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ardo has my place. Give you go</cell><cell>FRANCISCO Bernardo hath my pl</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>od night. (Exit</cell><cell>ace. Give you good night</cell></row><row><cell cols="2">ISCO Approach me not. BERNARDO</cell><cell cols="2">HORATIO and MARCELLUS HORATIO</cell><cell></cell><cell></cell></row><row><cell cols="2">I have a letter FRANCISCO App</cell><cell cols="2">Friends to this ground. MARCE</cell><cell></cell><cell></cell></row><row><cell cols="2">roach me not. BERNARDO For the</cell><cell cols="2">LLUS And liegemen to the Dane.</cell><cell></cell><cell></cell></row><row><cell cols="2">king. FRANCISCO Approach me n</cell><cell cols="2">FRANCISCO Give you good night</cell><cell></cell><cell></cell></row><row><cell cols="2">ot. BERNARDO There's no treaso</cell><cell cols="2">. MARCELLUS O, farewell, hones</cell><cell></cell><cell></cell></row><row><cell cols="2">n in't. FRANCISCO Approach me</cell><cell>t soldier: Who hath relieved y</cell><cell></cell><cell></cell><cell></cell></row><row><cell>not. BERNARDO I will</cell><cell></cell><cell cols="2">ou? FRANCISCO Bernardo hath my</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>place. Give you good night.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Sample -Hamlet, Act 1, Scene 1.JT. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural Questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 7:452-466, Mar. 2019. URL https://aclanthology. org/Q19-1026.A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska, T. Terzi, M. Gimenez, C. de Mas- son d'Autume, S. Ruder, D. Yogatama, K. Cao, T. Kocisk?, S. Young, and P. Blunsom. Pitfalls of static language modelling. CoRR, 2021. URL https://arxiv.org/abs/2102.01951.K. Lee, M.-W. Chang, and K. Toutanova. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Annual Meeting of the Association for Computational Linguistic, June 2019. URL http://arxiv.org/abs/1906.00300.K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K?ttler, M. Lewis, W.-t. Yih, T. Rockt?schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings. neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question answering datasets. In Conference of the European Chapter of the Association for Computational Linguistics, Apr. 2021. URL https://aclanthology.org/2021.eacl-main.86.O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 2021.I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id= Byj72udxe.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 |</head><label>8</label><figDesc>MassiveText dataset. The final column indicates the sampling weight for each dataset during training. For the retrieval database, the entire dataset is used, with the exception of books for which we use a sub-sample of 4%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 |</head><label>9</label><figDesc>Full set of articles included in our Wikipedia Sept. 2021 evaluation dataset.</figDesc><table><row><cell>Megan Rohrer</cell><cell>Aakashavaani</cell></row><row><cell>Emma Raducanu</cell><cell>Junior Eurovision Song Contest 2021</cell></row><row><cell>Ambra Sabatini</cell><cell>Pavilion Bukit Jalil</cell></row><row><cell>WhyDonate</cell><cell>Blake Desjarlais</cell></row><row><cell>The Juggernaut (company)</cell><cell>2021 All-Ireland Senior Football Championship Final</cell></row><row><cell>Angela Diaz</cell><cell>Drift-barrier hypothesis</cell></row><row><cell>2020 Summer Paralympics</cell><cell>Venomics</cell></row><row><cell>2021 Afghan protests</cell><cell>Great Circle (novel)</cell></row><row><cell>Rexh Xhakli</cell><cell>Hurricane Ida</cell></row><row><cell>Julia Laskin</cell><cell>2021 Montenegrin episcopal enthronement protests</cell></row><row><cell>Cuijk</cell><cell>At War With the Silverfish</cell></row><row><cell>Ghoubet Wind Power Station</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 |</head><label>10</label><figDesc>R model hyperparameters, along with the size of the decoder.</figDesc><table><row><cell>Baseline</cell><cell></cell><cell></cell><cell cols="3"># heads Head size # layers</cell><cell></cell><cell>E</cell><cell>Max LR</cell></row><row><cell>247M</cell><cell>896</cell><cell>3584</cell><cell>16</cell><cell>64</cell><cell>12</cell><cell>[6, 9, 12]</cell><cell>[1]</cell><cell>2?10 ?4</cell></row><row><cell>564M</cell><cell>1536</cell><cell>6144</cell><cell>12</cell><cell>128</cell><cell>12</cell><cell>[6, 9, 12]</cell><cell>[1]</cell><cell>2?10 ?4</cell></row><row><cell cols="2">1,574M 2048</cell><cell>8192</cell><cell>16</cell><cell>128</cell><cell>24</cell><cell>[9, 12, . . . , 24]</cell><cell>[1]</cell><cell>2?10 ?4</cell></row><row><cell cols="3">7,505M 4096 16384</cell><cell>32</cell><cell>128</cell><cell>32</cell><cell>[9, 12, . . . , 32]</cell><cell>[1]</cell><cell>1?10 ?4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 |</head><label>11</label><figDesc>Hyperparameters for the Wikitext103 experiments presented inTable 4. We use the same learning rate schedule for the baseline and the R -fitting. For R -fitting, we reset the schedule i.e. the schedule starts from step 0, not from step 35,000.</figDesc><table><row><cell>Model</cell><cell>Number of layers</cell><cell>18</cell></row><row><cell></cell><cell></cell><cell>1024</cell></row><row><cell></cell><cell>F</cell><cell>4096</cell></row><row><cell></cell><cell>Key size</cell><cell>64</cell></row><row><cell></cell><cell>Value size</cell><cell>64</cell></row><row><cell></cell><cell>Number of heads</cell><cell>16</cell></row><row><cell cols="2">Training data Dataset</cell><cell>Wikitext103train</cell></row><row><cell></cell><cell>Sequence length</cell><cell>3072</cell></row><row><cell></cell><cell>Batch size</cell><cell>128</cell></row><row><cell></cell><cell>Tokenizer vocabulary size</cell><cell>128,000</cell></row><row><cell cols="2">Optimisation optimiser</cell><cell>Adam</cell></row><row><cell></cell><cell>Adam's 1</cell><cell>0.9</cell></row><row><cell></cell><cell>Adam's 2</cell><cell>0.95</cell></row><row><cell></cell><cell>Adam's</cell><cell>1e-8</cell></row><row><cell></cell><cell>Dropout rate</cell><cell>0.25</cell></row><row><cell>Schedule</cell><cell>Learning rate start</cell><cell>1e-7</cell></row><row><cell></cell><cell>Learning rate max</cell><cell>2.5e-4</cell></row><row><cell></cell><cell>Learning rate min</cell><cell>2e-5</cell></row><row><cell></cell><cell>Warmup steps</cell><cell>4,000</cell></row><row><cell></cell><cell>Cosine cycle steps</cell><cell>100,000</cell></row><row><cell>Evaluation</cell><cell>Overlapping proportion</cell><cell>87.5 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 |</head><label>12</label><figDesc>Hyperparameters for the R fitting experiments</figDesc><table><row><cell cols="2">Model Layers with R</cell><cell>-block ( )</cell><cell>Learning rate</cell><cell>Batch size</cell></row><row><cell>172M</cell><cell cols="2">Every 3 rd from 6</cell><cell>2 ? 10 ?4 ? 2 ? 10 ?5</cell><cell>256</cell></row><row><cell>425M</cell><cell cols="2">Every 3 rd from 6</cell><cell>2 ? 10 ?4 ? 2 ? 10 ?5</cell><cell>256</cell></row><row><cell>1.5B</cell><cell cols="2">Every 3 rd from 6</cell><cell>2 ? 10 ?4 ? 2 ? 10 ?5</cell><cell>256</cell></row><row><cell>7.5B</cell><cell cols="2">Every 3 rd from 6</cell><cell>1 ? 10 ?4 ? 1 ? 10 ?5</cell><cell>256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 |</head><label>13</label><figDesc>Performance of R for different variants. Model performance on C4 evaluation set, measured in bytes-per-bits, for a 247M parameter model trained with a 157 billion token schedule.</figDesc><table><row><cell>Ablation group</cell><cell>Ablation</cell><cell>C4 eval bpb</cell></row><row><cell>Model</cell><cell>R</cell><cell>0.822</cell></row><row><cell></cell><cell>No query conditioning</cell><cell>0.829</cell></row><row><cell></cell><cell>No CA positional encodings</cell><cell>0.826</cell></row><row><cell></cell><cell>Shared embeddings</cell><cell>0.823</cell></row><row><cell></cell><cell>6-layer encoder</cell><cell>0.821</cell></row><row><cell>Retrieval values</cell><cell>Neighbours N</cell><cell>0.950</cell></row><row><cell></cell><cell>Continuations F</cell><cell>0.895</cell></row><row><cell></cell><cell>No retrieval</cell><cell>0.987</cell></row><row><cell>Training neighbours</cell><cell>1 training neighbours</cell><cell>0.858</cell></row><row><cell></cell><cell>4 training neighbours</cell><cell>0.847</cell></row><row><cell cols="2">Cross attention position CA top layer (1/12)</cell><cell>0.827</cell></row><row><cell></cell><cell>CA mid layer (6/12)</cell><cell>0.823</cell></row><row><cell></cell><cell>CA top layer (12/12)</cell><cell>0.831</cell></row><row><cell></cell><cell>CA all layers</cell><cell>0.860</cell></row><row><cell></cell><cell>CA every 3 from 1</cell><cell>0.823</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 |</head><label>14</label><figDesc>Full results for the main language modelling datasets. First three sets of rows correspond toFig. 1, last set of rows toFig. 3.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell></cell><cell>R</cell><cell>[Off]</cell><cell></cell><cell></cell><cell>R</cell><cell>[On]</cell></row><row><cell></cell><cell cols="3">172M 425M 1.5B</cell><cell cols="4">7.5B 172M 425M 1.5B</cell><cell cols="4">7.5B 172M 425M 1.5B 7.5B</cell></row><row><cell>C4 Eval bpb</cell><cell>0.98</cell><cell>0.92</cell><cell>0.84</cell><cell>0.78</cell><cell>0.98</cell><cell>0.92</cell><cell>0.84</cell><cell>0.78</cell><cell>0.82</cell><cell cols="2">0.77 0.71 0.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 | Full results on The Pile, measured in bits-per-bytes.</head><label>15</label><figDesc>Jurassic-1 and GPT-3 numbers are taken fromLieber et al. (2021). Gopher numbers are taken fromRae et al. (2021).</figDesc><table><row><cell>Subset</cell><cell cols="5">7B Baseline (Ours) GPT-3 Jurassic-1 Gopher 7.5B R</cell></row><row><cell>arxiv</cell><cell>0.742</cell><cell>0.838</cell><cell>0.680</cell><cell>0.641</cell><cell>0.714</cell></row><row><cell>books3</cell><cell>0.792</cell><cell>0.802</cell><cell>0.835</cell><cell>0.706</cell><cell>0.653</cell></row><row><cell>dm_mathematics</cell><cell>1.177</cell><cell>1.371</cell><cell>1.037</cell><cell>1.135</cell><cell>1.164</cell></row><row><cell>freelaw</cell><cell>0.576</cell><cell>0.612</cell><cell>0.514</cell><cell>0.506</cell><cell>0.499</cell></row><row><cell>github</cell><cell>0.420</cell><cell>0.645</cell><cell>0.358</cell><cell>0.367</cell><cell>0.199</cell></row><row><cell>gutenberg_pg_19</cell><cell>0.803</cell><cell>1.163</cell><cell>0.890</cell><cell>0.652</cell><cell>0.400</cell></row><row><cell>hackernews</cell><cell>0.971</cell><cell>0.975</cell><cell>0.869</cell><cell>0.888</cell><cell>0.860</cell></row><row><cell>nih_exporter</cell><cell>0.650</cell><cell>0.612</cell><cell>0.590</cell><cell>0.590</cell><cell>0.635</cell></row><row><cell>opensubtitles</cell><cell>0.974</cell><cell>0.932</cell><cell>0.879</cell><cell>0.894</cell><cell>0.930</cell></row><row><cell>philpapers</cell><cell>0.760</cell><cell>0.723</cell><cell>0.742</cell><cell>0.682</cell><cell>0.699</cell></row><row><cell>pile_cc</cell><cell>0.771</cell><cell>0.698</cell><cell>0.669</cell><cell>0.688</cell><cell>0.626</cell></row><row><cell>pubmed_abstracts</cell><cell>0.639</cell><cell>0.625</cell><cell>0.587</cell><cell>0.578</cell><cell>0.542</cell></row><row><cell>pubmed_central</cell><cell>0.588</cell><cell>0.690</cell><cell>0.579</cell><cell>0.512</cell><cell>0.419</cell></row><row><cell>stackexchange</cell><cell>0.714</cell><cell>0.773</cell><cell>0.655</cell><cell>0.638</cell><cell>0.624</cell></row><row><cell>ubuntu_irc</cell><cell>1.200</cell><cell>0.946</cell><cell>0.857</cell><cell>1.081</cell><cell>1.178</cell></row><row><cell>uspto_backgrounds</cell><cell>0.603</cell><cell>0.566</cell><cell>0.537</cell><cell>0.545</cell><cell>0.583</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 |</head><label>16</label><figDesc>Great Circle (novel), from Wikipedia September 21. The article is about a recent novel and chunks 3 and 4 are specifically about its reception. The name Publishers Weekly of the journal that reviewed the novel appears both in the neighbours [ 1 ] of chunk 3 and in the subsequent chunk 4 , where the loss for those tokens is significantly reduced by R .</figDesc><table><row><cell>R</cell><cell cols="2">3 , 1 3 ], [ 2 3 , 2 3 colored by loss difference colored by LCP with R ( ?1) [O ] ? R ?0.5, = 0, 0.5 LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 1 , 1 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 2 , 2 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell></row><row><cell cols="2">Great Circle (novel)Great Circle i</cell><cell>Great Circle (novel) Great Circle i</cell><cell>The Dutch House (novel)The Dutch H</cell><cell>The Dutch House (novel)The Dutch H</cell></row><row><cell cols="2">s a 2021 novel by Maggie Shipstead,</cell><cell>s a 2021 novel by Maggie Shipstead,</cell><cell>ouse is a 2019 novel by Ann Patchett</cell><cell>ouse is a 2019 novel by Ann Patchett</cell></row><row><cell cols="2">published on May 4, 2021, by Alfred</cell><cell>published on May 4, 2021, by Alfred</cell><cell>. It was published by Harper on Sept</cell><cell>. It was published by Harper on Sept</cell></row><row><cell cols="2">A. Knopf.The novel has been shortl</cell><cell>A. Knopf. The novel has been shortl</cell><cell>ember 24, 2019. It tells the story o</cell><cell>ember 24, 2019. It tells the story o</cell></row><row><cell cols="2">isted for the 2021 Booker Prize.Sy</cell><cell>isted for the 2021 Booker Prize. Sy</cell><cell>f a brother and sister over the cour</cell><cell>f a brother and sister over the cour</cell></row><row><cell cols="2">nopsis The novel consists of two pa</cell><cell>nopsis The novel consists of two pa</cell><cell>se of five decades.The novel was a</cell><cell>se of five decades.[2]The novel wa</cell></row><row><cell cols="2">rallel narratives about two fictiona</cell><cell>rallel narratives about two fictiona</cell><cell>finalist for the 2020 Pulitzer Priz</cell><cell>s a finalist for the 2020 Pulitzer P</cell></row><row><cell cols="2">l women. One is</cell><cell>l women. One is</cell><cell>e for Fiction.PlotThe Dutch House</cell><cell>rize for Fiction.[3]Plot[edit]Th</cell></row><row><cell></cell><cell></cell><cell></cell><cell>is a mansion located in Elkins Park</cell><cell>e Dutch House is a mansion located i</cell></row><row><cell></cell><cell></cell><cell></cell><cell>, Pennsylvania, a suburb of Philadel</cell><cell>n Elkins Park, Pennsylvania, a subur</cell></row><row><cell></cell><cell></cell><cell></cell><cell>phia. It was built in 1922 by the Va</cell><cell>b of Philadelphia. It was built in 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nHoebeek family, a husband and wife</cell><cell>922 by the VanHoebeek family, a husb</cell></row><row><cell></cell><cell></cell><cell></cell><cell>originally from the Netherlands who</cell><cell>and and wife originally from the Net</cell></row><row><cell></cell><cell></cell><cell></cell><cell>made their fortune in the tobacco in</cell><cell>herlands who made their fortune in t</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dustry. Cyril Conroy, a self-made re</cell><cell>he tobacco industry. Cyril Conroy, a</cell></row><row><cell></cell><cell></cell><cell></cell><cell>al estate mogul</cell><cell>self-</cell></row><row><cell cols="2">about the disappeared 20th-century</cell><cell>about the disappeared 20th-century</cell><cell>on becoming a filmmaker. She has fo</cell><cell>based closely on her own youthful e</cell></row><row><cell cols="2">aviator Marian Graves, while the oth</cell><cell>aviator Marian Graves, while the oth</cell><cell>und a subject for her film project,</cell><cell>xperiences. (She plans the film to b</cell></row><row><cell cols="2">er is about the struggling 21st-cent</cell><cell>er is about the struggling 21st-cent</cell><cell>an obscure African American actress</cell><cell>e the first of two parts, the second</cell></row><row><cell cols="2">ury Hollywood actress Hadley Baxter,</cell><cell>ury Hollywood actress Hadley Baxter,</cell><cell>credited only as "the watermelon wom</cell><cell>dealing with the aftermath of the f</cell></row><row><cell cols="2">who is attempting to make a film ab</cell><cell>who is attempting to make a film ab</cell><cell>an" in old Hollywood films, and the</cell><cell>irst's events.) Byrne plays a young</cell></row><row><cell cols="2">out Marian. Hadley's narrative is to</cell><cell>out Marian. Hadley's narrative is to</cell><cell>subsequent film recounts her search</cell><cell>film student named Julie (Hogg's ava</cell></row><row><cell cols="2">ld in the first-person, while Marian</cell><cell>ld in the first-person, while Marian</cell><cell>for this woman even as it covers, in</cell><cell>tar), who starts her artistic educat</cell></row><row><cell cols="2">'s sections are told in the third-pe</cell><cell>'s sections are told in the third-pe</cell><cell>the manner of the earlier Dunyement</cell><cell>ion with high hopes of making a movi</cell></row><row><cell cols="2">rson</cell><cell>rson</cell><cell>aries, Dunye's friendships and her l</cell><cell>e about a boy named Tony, living in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ove life. InThe Watermelon Woman, D</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 |</head><label>17</label><figDesc>All-Ireland Senior Football Championship Final, from Wikipedia September 21. The name of the team Tyrone appears both in the second neighbours [ 2 1 , 2 1 ] of chunk 1 and in the subsequent chunk 2 , where the loss for those tokens is significantly reduced by R .</figDesc><table><row><cell>R</cell><cell>colored by loss difference [O ] ? R</cell><cell>?0.5, = 0, 0.5</cell><cell>colored by LCP with R ( ?1) LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 1 , 1 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 2 , 2 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell></row><row><cell cols="3">2021 All-Ireland Senior Football Cha</cell><cell>2021 All-Ireland Senior Football Cha</cell><cell>2018 All-Ireland Senior Football Cha</cell><cell>2018 All-Ireland Senior Football Cha</cell></row><row><cell cols="3">mpionship FinalThe 2021 All-Irelan</cell><cell>mpionship Final The 2021 All-Irelan</cell><cell>mpionship FinalThe 2018 All-Irelan</cell><cell>mpionship FinalThe 2018 All-Irelan</cell></row><row><cell cols="3">d Senior Football Championship Final</cell><cell>d Senior Football Championship Final</cell><cell>d Senior Football Championship Final</cell><cell>d Senior Football Championship Final</cell></row><row><cell cols="3">was the 134th final of the All-Irel</cell><cell>was the 134th final of the All-Irel</cell><cell>was the 131st final of the All-Irel</cell><cell>was the 131st final of the All-Irel</cell></row><row><cell cols="3">and Senior Football Championship and</cell><cell>and Senior Football Championship and</cell><cell>and Senior Football Championship and</cell><cell>and Senior Football Championship and</cell></row><row><cell cols="3">the culmination of the 2021 All-Ire</cell><cell>the culmination of the 2021 All-Ire</cell><cell>the culmination of the 2018 All-Ire</cell><cell>the culmination of the 2018 All-Ire</cell></row><row><cell cols="3">land Senior Football Championship. T</cell><cell>land Senior Football Championship. T</cell><cell>land Senior Football Championship in</cell><cell>land Senior Football Championship in</cell></row><row><cell cols="3">he match was played at Croke Park in</cell><cell>he match was played at Croke Park in</cell><cell>Gaelic football. The match was play</cell><cell>Gaelic football. The match was play</cell></row><row><cell cols="3">Dublin on 11 September 2021. It was</cell><cell>Dublin on 11 September 2021. It was</cell><cell>ed at Croke Park in Dublin on 2 Sept</cell><cell>ed at Croke Park in Dublin on 2 Sept</cell></row><row><cell cols="2">originally scheduled</cell><cell></cell><cell>originally scheduled</cell><cell>ember 2018.[3]It was the second ti</cell><cell>ember 2018.It was the second time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>me the teams had met in the final; D</cell><cell>the teams had met in the final; Dubl</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ublin won the first encounter in 199</cell><cell>in won the first encounter in 1995.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>5.The final was shown live in Irel</cell><cell>It was the third consecutive year th</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and on RT? Two as part of The Sunday</cell><cell>at a team qualified under the system</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Game live programme, presented by M</cell><cell>of second chances introduced in 200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ichael Lyster from Croke Park, with</cell><cell>1; Tyrone qualified despite defeat i</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>studio analysis from Joe Brolly,</cell><cell>n its provincial championship.Dubl</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>in won the final by a margin of six</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>points</cell></row><row><cell cols="3">for 28 August but had to be postpon</cell><cell>for 28 August but had to be postpon</cell><cell>game 23-23 after extra time, howeve</cell><cell>with a last-ditch plan of action -</cell></row><row><cell cols="3">ed by two weeks when the -semi-fina</cell><cell>ed by two weeks when the -semi-fina</cell><cell>r Ulster progressed under the compet</cell><cell>play the Munster/Ulster Semi-Final o</cell></row><row><cell cols="3">l was postponed due to a COVID-19 ou</cell><cell>l was postponed due to a COVID-19 ou</cell><cell>ition rules as they scored three tir</cell><cell>n March 16th, with the winners to pl</cell></row><row><cell cols="3">tbreak. Ulster champions Tyrone took</cell><cell>tbreak. Ulster champions Tyrone took</cell><cell>es in the match against Leinster's t</cell><cell>ay Connacht in the following day's F</cell></row><row><cell cols="3">on Connacht champions Mayo, in what</cell><cell>on Connacht champions Mayo, in what</cell><cell>wo. The semi-finals took place in mi</cell><cell>inal.On March 16th then Munster ha</cell></row><row><cell cols="3">was their first ever meeting in a f</cell><cell>was their first ever meeting in a f</cell><cell>d November and saw both the away tea</cell><cell>d an easy win over Ulster (9-07 to 0</cell></row><row><cell cols="3">inal, winning their 4th title after</cell><cell>inal, winning their 4th title after</cell><cell>ms win, as Ulster beat Glasgow and E</cell><cell>-00) but thankfully for the Munster</cell></row><row><cell cols="3">a 2-14 to 0-15 win. Mayo lost</cell><cell>a 2-14 to 0-15 win. Mayo lost</cell><cell>dinburgh beat Connacht. The final wa</cell><cell>players, the pitch cut up so badly d</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>s held on Saturday December 20 at Mu</cell><cell>uring the game, it was decided to po</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rrayfield Stadium and saw Ulster bea</cell><cell>stpone the following day's hurling F</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t Edinburgh 21-27 to win the Celtic</cell><cell>inal (until Easter Sunday) with the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cup.2004-05 seasonThe format of</cell><cell>football Final going ahead on its ow</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>the competition was changed for the</cell><cell>n on St. Patrick's Day.Less than a</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>second edition of the competition. T</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 18 |</head><label>18</label><figDesc>2020 Summer Paralympics, from Wikipedia September 21. The original dates of the event, ] of chunk 1 and in the subsequent chunk 2 , where the loss for those tokens is significantly reduced by R . Interestingly, in this case, the neighbors were written at a time when the event hadn't yet been postponed.</figDesc><table><row><cell cols="3">25 August to 6 September 2020, appears both in the neighbors [ 1 1 , 1 1 ], [ 2 1 , 2 1 colored by loss difference colored by LCP with R ( ?1) [ 1 , 1 ] colored by LCP with +1 R [O ] ? R ?0.5, = 0, 0.5 LCP = 0, 1, 2, 3,4, 5 LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 2 , 2 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell></row><row><cell>2020 Summer ParalympicsThe , brand</cell><cell>2020 Summer Paralympics The , brand</cell><cell>pics Games.* The 2020 Summer Paraly</cell><cell>2020 Summer ParalympicsThe are an</cell></row><row><cell>ed as the Tokyo 2020 Paralympic Game</cell><cell>ed as the Tokyo 2020 Paralympic Game</cell><cell>mpics are an upcoming major internat</cell><cell>upcoming major international multi-</cell></row><row><cell>s, was an international multi-sport</cell><cell>s, was an international multi-sport</cell><cell>ional multi-sport event for athletes</cell><cell>sport event for athletes with disabi</cell></row><row><cell>parasports event held from 24 August</cell><cell>parasports event held from 24 August</cell><cell>with disabilities governed by the I</cell><cell>lities governed by the International</cell></row><row><cell>to 5 September 2021 in Tokyo, Japan</cell><cell>to 5 September 2021 in Tokyo, Japan</cell><cell>nternational Paralympic Committee. S</cell><cell>Paralympic Committee. Scheduled as</cell></row><row><cell>. They were the 16th Summer Paralymp</cell><cell>. They were the 16th Summer Paralymp</cell><cell>cheduled as the 16th Summer Paralymp</cell><cell>the 16th Summer Paralympic Games, th</cell></row><row><cell>ic Games as organized by the Interna</cell><cell>ic Games as organized by the Interna</cell><cell>ic Games, it is planned to be held i</cell><cell>ey are scheduled to be held in Tokyo</cell></row><row><cell>tional Paralympic Committee (IPC).</cell><cell>tional Paralympic Committee (IPC).</cell><cell>n Tokyo, Japan from 25 August to 6 S</cell><cell>, Japan between 24 August and 5 Sept</cell></row><row><cell></cell><cell></cell><cell>eptember 2020.3. 2019 BWF Para-Bad</cell><cell>ember 2021. Originally due to take p</cell></row><row><cell></cell><cell></cell><cell>minton World Championships-The 20</cell><cell>lace between 25 August and 6 Septemb</cell></row><row><cell></cell><cell></cell><cell>19 BWF Para-Badminton World Champion</cell><cell>er 2020. On 24 March 2020, the IOC a</cell></row><row><cell></cell><cell></cell><cell>ships was held from 20 to 25 August</cell><cell>nd the Tokyo Organizing Committee of</cell></row><row><cell></cell><cell></cell><cell>2019 in Basel, Switzerland.-Men's</cell><cell>ficially announced that the 2020 Sum</cell></row><row><cell></cell><cell></cell><cell>event: Gold Medal: Pramod Bhagat in</cell><cell>mer Olympics and 2020 Summer Paralym</cell></row><row><cell></cell><cell></cell><cell>Singles SL3 Event and Pramod Bhagat</cell><cell>pics would be postponed to 2021, due</cell></row><row><cell></cell><cell></cell><cell>and Manoj</cell><cell>to the COVID-19 pandemic, marking t</cell></row><row><cell></cell><cell></cell><cell></cell><cell>he first time that the Paralympics h</cell></row><row><cell></cell><cell></cell><cell></cell><cell>as been postponed. They will still b</cell></row><row><cell></cell><cell></cell><cell></cell><cell>e publicly marketed as</cell></row><row><cell>Originally scheduled to take place f</cell><cell>Originally scheduled to take place f</cell><cell>once submitted.This process was u</cell><cell>Olympiad, have now been postponed a</cell></row><row><cell>rom 25 August to 6 September 2020, i</cell><cell>rom 25 August to 6 September 2020, i</cell><cell>ndertaken following the postponement</cell><cell>nd rescheduled for 23 July to 8 Augu</cell></row><row><cell>n March 2020 both the 2020 Summer Ol</cell><cell>n March 2020 both the 2020 Summer Ol</cell><cell>of the Tokyo 2020 Games due to the</cell><cell>st 2021 in Tokyo, Japan. The Games</cell></row><row><cell>ympics and Paralympics were postpone</cell><cell>ympics and Paralympics were postpone</cell><cell>COVID-19 pandemic, with both the Oly</cell><cell>were postponed in March 2020 as a re</cell></row><row><cell>d by one year due to the COVID-19 pa</cell><cell>d by one year due to the COVID-19 pa</cell><cell>mpics and Paralympics pushed back a</cell><cell>sult of the worldwide Covid-19 pande</cell></row><row><cell>ndemic, with the rescheduled Games s</cell><cell>ndemic, with the rescheduled Games s</cell><cell>year.Now, the Tokyo 2020 Olympics</cell><cell>mic, although they will still keep t</cell></row><row><cell>till referred to as Tokyo 2020 for m</cell><cell>till referred to as Tokyo 2020 for m</cell><cell>are scheduled for July 23 to August</cell><cell>he name Tokyo 2020 for marketing and</cell></row><row><cell>arketing and branding purposes. As</cell><cell>arketing and branding purposes. As</cell><cell>8 while the Paralympics are due to f</cell><cell>branding purposes. This will be th</cell></row><row><cell>with the Olympics, the Games were la</cell><cell>with the Olympics, the Games were la</cell><cell>ollow from August 24 to September 5.</cell><cell>e first time the Olympic Games have</cell></row><row><cell>rgely held behind</cell><cell>rgely held behind</cell><cell>The refund process is separate for</cell><cell>been postponed rather than cancelled</cell></row><row><cell></cell><cell></cell><cell>ticketholders outside of Japan, who</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell>purchased tickets through authorise</cell><cell></cell></row><row><cell></cell><cell></cell><cell>d ticket resellers (ATR).Each ATR</cell><cell></cell></row><row><cell></cell><cell></cell><cell>has its own individual refund proced</cell><cell></cell></row><row><cell></cell><cell></cell><cell>ure.Early figures from the refund</cell><cell></cell></row><row><cell></cell><cell></cell><cell>process for the Tokyo 2020 Olympics</cell><cell></cell></row><row><cell></cell><cell></cell><cell>stated that around 18 per cent</cell><cell></cell></row><row><cell>closed doors with no outside specta</cell><cell>closed doors with no outside specta</cell><cell>has been rescheduled to May 1-4 bec</cell><cell>Olympic Games, when Tokyo became th</cell></row><row><cell>tors due to a state of emergency in</cell><cell>tors due to a state of emergency in</cell><cell>ause of travel restrictions under th</cell><cell>e first city in Asia to host the Oly</cell></row><row><cell>the Greater Tokyo Area and other pre</cell><cell>the Greater Tokyo Area and other pre</cell><cell>e current state of emergency in Toky</cell><cell>mpic and Paralympic Games, but unfor</cell></row><row><cell>fectures. The Games were the second</cell><cell>fectures. The Games were the second</cell><cell>o and other 10 prefectures across Ja</cell><cell>tunately strong winds made it an imp</cell></row><row><cell>Summer Paralympics hosted by Tokyo s</cell><cell>Summer Paralympics hosted by Tokyo s</cell><cell>pan.The Tokyo 2020 organizing comm</cell><cell>ossible task this time around.Memb</cell></row><row><cell>ince 1964, and the third Paralympics</cell><cell>ince 1964, and the third Paralympics</cell><cell>ittee announced that the first of 18</cell><cell>ers of the Tokyo Organising Committe</cell></row><row><cell>held in Japan overall since the 199</cell><cell>held in Japan overall since the 199</cell><cell>test events for the Olympic and Par</cell><cell>e of the Olympic and Paralympic Game</cell></row><row><cell>8 Winter Paralympics in Nagano. Th</cell><cell>8 Winter Paralympics in Nagano. Th</cell><cell>alympic Games will involve wheelchai</cell><cell>s (Tokyo 2020), Tokyo Metropolitan G</cell></row><row><cell>e Games featured</cell><cell>e Games featured</cell><cell>r rugby, which will be held in Yoyog</cell><cell>overnment officials, Tokyo 2020 Torc</cell></row><row><cell></cell><cell></cell><cell>i National Stadium from April 3 to 4</cell><cell>h Relay Official Ambassadors and rep</cell></row><row><cell></cell><cell></cell><cell>.The FINA Diving World Cup will fo</cell><cell>resentatives from Miyagi Prefecture</cell></row><row><cell></cell><cell></cell><cell>llow from April 18 to 23 at the Toky</cell><cell>joined the arrival ceremony.FLAME</cell></row><row><cell></cell><cell></cell><cell>o Aquatics Centre, which will also s</cell><cell>OF RECOVERYThe Olympic flame will</cell></row><row><cell></cell><cell></cell><cell>erve as an Olympic qualifying event.</cell><cell>now be put on display at various loc</cell></row><row><cell></cell><cell></cell><cell>The spread of the COVID-19 pandemi</cell><cell>ations in the Tohoku region, to high</cell></row><row><cell></cell><cell></cell><cell>c has slowed down in Tokyo three wee</cell><cell>light the message of hope in the are</cell></row><row><cell></cell><cell></cell><cell>ks after the Japanese capital entere</cell><cell>as worst affected by the 2011 Great</cell></row><row><cell></cell><cell></cell><cell>d a state of emergency on</cell><cell>East Japan Earthqu</cell></row><row><cell>539 medal events in 22 sports, with</cell><cell>539 medal events in 22 sports, with</cell><cell></cell><cell></cell></row><row><cell>badminton and taekwondo both making</cell><cell>badminton and taekwondo both making</cell><cell></cell><cell></cell></row><row><cell>their Paralympic debut to replace f</cell><cell>their Paralympic debut to replace f</cell><cell></cell><cell></cell></row><row><cell>ootball 7-a-side and sailing. China</cell><cell>ootball 7-a-side and sailing. China</cell><cell></cell><cell></cell></row><row><cell>topped the medal table for the fifth</cell><cell>topped the medal table for the fifth</cell><cell></cell><cell></cell></row><row><cell>consecutive Paralympics, with 96 go</cell><cell>consecutive Paralympics, with 96 go</cell><cell></cell><cell></cell></row><row><cell>lds and 207 total medals. Great Brit</cell><cell>lds and 207 total medals. Great Brit</cell><cell></cell><cell></cell></row><row><cell>ain finished second for the ninth t</cell><cell>ain finished second for the ninth t</cell><cell></cell><cell></cell></row><row><cell>ime,</cell><cell>ime,</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 19 |</head><label>19</label><figDesc>Daniel Radcliffe, from Wikitext103Valid, retrieval data from c4. The chunks 2 and 3 are almost entirely retrieved from neighbours [ 1 , 1 ] and [ 2 , 2 ] respectively, up to formatting differences, which dramatically reduces the loss for these tokens. This example illustrates that when training data leaks into evaluation sets despite deduplication, our R model can directly exploit this leakage. The Tailor of Panama. At of Panama. At age 11, he was cast a age 11, he was cast as Harry Potter age 11, he was cast as Harry Potter age 11, he was cast as Harry Potter s Harry Potter in the first Harry Po in the first Harry Potter film, and in the first Harry Potter film, and in the first Harry Potter film, and tter film,</figDesc><table><row><cell>R</cell><cell>colored by loss difference [O ] ? R</cell><cell>?0.5, = 0, 0.5</cell><cell>colored by LCP with R ( ?1) LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 1 , 1 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell><cell>[ 2 , 2 ] colored by LCP with +1 LCP = 0, 1, 2, 3,4, 5</cell></row><row><cell cols="3">= Daniel Radcliffe =Daniel Jacob R</cell><cell>= Daniel Radcliffe = Daniel Jacob R</cell><cell>Daniel Jacob Radcliffe (born 23 July</cell><cell>Daniel Jacob Radcliffe (born 23 July</cell></row><row><cell cols="3">adcliffe ( born 23 July 1989 ) is an</cell><cell>adcliffe ( born 23 July 1989 ) is an</cell><cell>1989) is an English actor who rose</cell><cell>1989) is an English actor who rose</cell></row><row><cell cols="3">English actor who rose to prominenc</cell><cell>English actor who rose to prominenc</cell><cell>to prominence as the title character</cell><cell>to prominence as the title character</cell></row><row><cell cols="3">e as the title character in the Harr</cell><cell>e as the title character in the Harr</cell><cell>in the Harry Potter film series. He</cell><cell>in the Harry Potter film series. He</cell></row><row><cell cols="3">y Potter film series. He made his ac</cell><cell>y Potter film series. He made his ac</cell><cell>made his acting debut at 10 years o</cell><cell>made his acting debut at 10 years o</cell></row><row><cell cols="3">ting debut at 10 years of age in BBC</cell><cell>ting debut at 10 years of age in BBC</cell><cell>f age in BBC One's 1999 television f</cell><cell>f age in BBC One's 1999 television m</cell></row><row><cell cols="3">One's 1999 television film David Co</cell><cell>One's 1999 television film David Co</cell><cell>ilm David Copperfield, followed by h</cell><cell>ovie David Copperfield, followed by</cell></row><row><cell cols="3">pperfield, followed by his cinematic</cell><cell>pperfield, followed by his cinematic</cell><cell>is cinematic debut in 2001's The Tai</cell><cell>his film debut in 2001's The Tailor</cell></row><row><cell cols="2">debut</cell><cell></cell><cell>debut</cell><cell>lor of Panama. At age 11, he was cas</cell><cell>of Panama. At age 11, he was cast as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t as Harry Potter in the first Harry</cell><cell>Harry Potter in the first Harry Pot</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Potter film, and starred in the ser</cell><cell>ter film, and starred in the series</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ies for 10 years until the release o</cell><cell>for 10 years until the release of th</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>f the eighth and final film in 2011.</cell><cell>e eighth and final film in 2011. Rad</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Radcliffe began to branch out to s</cell><cell>cliffe began to branch out to stage</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>tage acting in 2007, starring in the</cell><cell>acting in 2007, starring in the Lond</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>London and New York productions of</cell><cell>on and New York productions of Equus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Equus, and</cell><cell>, and in the</cell></row><row><cell cols="3">in 2001's The Tailor of Panama. At</cell><cell>in 2001's The Tailor of Panama. At</cell><cell>in 2001's</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 20 |</head><label>20</label><figDesc>Sample -D?claration des droits de l'homme: Article premier. The R[O ] sample has correct syntax and is almost plausible but is hallucinated. The R[O ] sample is correctly copied from neighbour data, and robustly re-formated according to our prompt.</figDesc><table><row><cell>Prompt and sample of R</cell><cell>[O ]</cell><cell>Prompt and sample of R</cell><cell>[O ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 21 |</head><label>21</label><figDesc>Sample -Decimals of . The R [O ] sample quickly diverges two digits after the end of the prompt whereas R [O ] correctly outputs a large number of digits, directly copied from the neighbours data.</figDesc><table><row><cell>Prompt and sample of R</cell><cell>[O ]</cell><cell>Prompt and sample of R</cell><cell>[O ]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the notation [1, ] {1, . . . , } throughout the text.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Improving language models by retrieving from trillions of tokens</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Due to legal and ethical concerns relating to their use, we exclude the Enron Emails and the Youtube Subtitles datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/facebookresearch/FiD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/earwig/mwparserfromhell</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Nikolai Grigorev, Marc'aurelio Ranzato, Cyprien de Masson d'Autume, Po-Sen Huang, Johannes Welbl, Lisa Anne Hendricks, Ethan Perez, Jeff Stanway, Eric Noland, Gregory Wayne, John Jumper, Julian Schrittwieser, Lorrayne Bennett, Devang Agrawal, Dani Yogatama, Susannah Young, Nando de Freitas, Demis Hassabis, and Koray Kavukcuoglu for their help, advice and reviews. Additionally, we would like to thank Zonglin Li, David Simcha, and the ScaNN developers for their help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSAC Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>P?rnamaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxZX20qFQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpretability and analysis in neural NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-tutorials.1</idno>
		<ptr target="https://aclanthology.org/2020.acl-tutorials.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<ptr target="https://jmlr.csail.mit.edu/papers/v3/blei03a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Der Plas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large Language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Extracting training data from large language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wikilinkgraphs: a complete, longitudinal and multilanguage dataset of the wikipedia link networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Consonni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laniado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montresor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI International Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Curation. Curation corpus base</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P19-1285" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800GB dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RealToxicityPrompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.301" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B184E5qee" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Search engine guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1908.10396" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided transformer: Leveraging multiple external sources for representation learning in conversational search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1131" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Haiku: Sonnet for JAX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<ptr target="http://github.com/deepmind/dm-haiku" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.eacl-main.74" />
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A memory efficient baseline for open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15156</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is not Explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1357</idno>
		<ptr target="https://aclanthology.org/N19-1357" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lessons from archives: Strategies for collecting sociocultural data in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">Interspeech</biblScope>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fern?ndez</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="16" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Masson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<editor>Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Scaling language models: Methods, analysis &amp; insights from training Gopher. arXiv submission</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end training of multi-document reader and retriever for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05346</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Megatron-LM: Training multi-billion parameter language models using model parallelism. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.08053" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07567</idno>
		<ptr target="http://arxiv.org/abs/2104.07567" />
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="19" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1148170.1148204" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR International Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legassick</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv submission</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive semiparametric language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Guiding neural machine translation with retrieved translation pieces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
