<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">4DContrast: Contrastive Learning with Dynamic Correspondences for 3D Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-22">22 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
							<email>yujin.chen@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
							<email>niessner@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
							<email>angela.dai@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">4DContrast: Contrastive Learning with Dynamic Correspondences for 3D Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-22">22 Jul 2022</date>
						</imprint>
					</monogr>
					<note>2 Y. Chen et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D scene understanding</term>
					<term>point cloud recognition</term>
					<term>3D se- mantic segmentation</term>
					<term>3D instance segmentation</term>
					<term>3D object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We propose 4DContrast to imbue learned 3D representations with 4D priors. We introduce a data augmentation scheme to composite synthetic 3D objects with realworld 3D scans to create 4D sequence data with inherent correspondence information. We then leverage a combination of 3D-3D, 3D-4D, and 4D-4D constraints within a contrastive learning framework to learn 4D invariance in the 3D representations. The learned features can be transferred to improve performance in various downstream 3D scene understanding tasks.</p><p>Abstract. We present a new approach to instill 4D dynamic object priors into learned 3D representations by unsupervised pre-training. We observe that dynamic movement of an object through an environment provides important cues about its objectness, and thus propose to imbue learned 3D representations with such dynamic understanding, that can then be effectively transferred to improved performance in downstream 3D semantic scene understanding tasks. We propose a new data augmentation scheme leveraging synthetic 3D shapes moving in static 3D environments, and employ contrastive learning under 3D-4D constraints that encode 4D invariances into the learned 3D representations. Experiments demonstrate that our unsupervised representation learning results in improvement in downstream 3D semantic segmentation, object detection, and instance segmentation tasks, and moreover, notably improves performance in data-scarce scenarios. Our results show that our 4D pretraining method improves downstream tasks such as object detection mAP@0.5 by 5.5%/6.5% over training from scratch on ScanNet/SUN RGB-D while involving no additional run-time overhead at test time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. We propose 4DContrast to imbue learned 3D representations with 4D priors. We introduce a data augmentation scheme to composite synthetic 3D objects with realworld 3D scans to create 4D sequence data with inherent correspondence information. We then leverage a combination of 3D-3D, 3D-4D, and 4D-4D constraints within a contrastive learning framework to learn 4D invariance in the 3D representations. The learned features can be transferred to improve performance in various downstream 3D scene understanding tasks.</p><p>Abstract. We present a new approach to instill 4D dynamic object priors into learned 3D representations by unsupervised pre-training. We observe that dynamic movement of an object through an environment provides important cues about its objectness, and thus propose to imbue learned 3D representations with such dynamic understanding, that can then be effectively transferred to improved performance in downstream 3D semantic scene understanding tasks. We propose a new data augmentation scheme leveraging synthetic 3D shapes moving in static 3D environments, and employ contrastive learning under 3D-4D constraints that encode 4D invariances into the learned 3D representations. Experiments demonstrate that our unsupervised representation learning results in improvement in downstream 3D semantic segmentation, object detection, and instance segmentation tasks, and moreover, notably improves performance in data-scarce scenarios. Our results show that our 4D pretraining method improves downstream tasks such as object detection mAP@0.5 by 5.5%/6.5% over training from scratch on ScanNet/SUN 1 Introduction 3D semantic scene understanding has seen remarkable progress in recent years, in large part driven by advances in deep learning as well as the introduction of large-scale, annotated datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. In particular, notable progress has been made to address core 3D scene understanding tasks such as 3D semantic segmentation, object detection, and instance segmentation, which are fundamental to many real-world computer vision applications such as robotics, mixed reality, or autonomous driving. Such approaches have developed various methods to learn on different 3D scene representations, such as sparse or dense volumetric grids <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, point clouds <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, meshes <ref type="bibr" target="#b20">[21]</ref>, or multi-view approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref>. Recently, driven by the success of unsupervised representation learning for transfer learning in 2D, 3D scene understanding methods have been augmented with unsupervised 3D pre-training to further improve performance to downstream 3D scene understanding tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>While such 3D representation learning has focused on feature representations learned from static 3D scenes, we observe that important notions of objectness are given by 4D dynamic observations -for instance, object segmentations can often be naturally intuited by observing objects moving around an environment without any annotations required, which can be more difficult in a static 3D observation. We thus propose to leverage this powerful 4D signal in unsupervised pre-training to imbue 4D object priors into learned 3D representations, that can then be effectively transferred to various downstream 3D scene understanding tasks for improved recognition performance.</p><p>In this work, we introduce 4DContrast to learn about objectness from both static 3D and dynamic 4D information in learned 3D representations. We leverage a combination of static 3D scanned scenes and a database of synthetic 3D shapes, and augment the scenes with moving synthetic shapes to generate 4D sequence data with inherent motion correspondence. We then employ a contrastive learning scheme under both 3D and 4D constraints, correlating local 3D point features with each other as well as with 4D sequence features, thus imbuing learned objectness from dynamic information into the 3D representation learning.</p><p>To demonstrate our approach, we pre-train on ScanNet <ref type="bibr" target="#b8">[9]</ref> along with Model-Net <ref type="bibr" target="#b42">[43]</ref> shapes for unsupervised 3D representation learning. Experiments on 3D semantic segmentation, object detection, and instance segmentation show that 4DContrast learns effective features that can be transferred to achieve improved performance in various downstream 3D scene understanding tasks. 4DContrast can also generalize from pre-training on ScanNet and ModelNet to improved performance on SUN RGB-D <ref type="bibr" target="#b37">[38]</ref>. Additionally, we show that our learned representations remain robust in limited training data scenarios, consistently improving performance under a various amounts of training data available.</p><p>Our main contributions are summarized as follows:</p><p>-We propose the first method to leverage 4D sequence information and constraints for 3D representation learning, showing transferability of the learned features to the downstream 3D scene understanding tasks of 3D semantic segmentation, object detection, and instance segmentation. -Our new unsupervised pre-training based on constructing 4D sequences from synthetic 3D shapes in real-world, static 3D scenes improves performance across a variety of downstream tasks and different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D Semantic Scene Understanding. Driven by rapid developments in deep learning and the introduction of several large-scale, annotated 3D datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>, notable progress has been made in 3D semantic scene understanding, in particular the tasks of 3D semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>, 3D object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>, and 3D instance segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b46">47]</ref>. Many methods have been proposed, largely focusing on learning on various 3D representations, such as sparse or dense volumetric grids <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, point clouds <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, meshes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref>, or multi-view hybrid representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>. In particular, approaches leveraging backbones built with sparse convolutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref> have shown strong effectiveness across a variety of 3D scene understanding tasks and datasets. We propose a new unsupervised pre-training approach to learn 4D priors in learned 3D representations, leveraging sparse convolutional backbones for both 3D and 4D feature extraction. 3D Representation Learning. Inspired by the success of representation learning in 2D, particularly that leveraging instance discrimination with contrastive learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref>, recent works have explored unsupervised learning with 3D pretext tasks that can be leveraged for fine-tuning on downstream 3D scene understanding tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. For instance, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b35">36]</ref> learn feature representations from point-based instance discrimination for object classification and segmentation, and <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref> extend to more complex 3D scenes by generating correspondences from various different views of scene point clouds.</p><p>In particular, given the more scarce data availability of real-world 3D environments, Hou et al. <ref type="bibr" target="#b18">[19]</ref> additionally demonstrate the efficacy of contrastive 3D pretraining for various 3D semantic scene understanding tasks under a variety of limited training data scenarios. In contrast to these methods that employ 3D-only pretext tasks for representation learning, we propose to learn from 4D sequence data to embed 4D priors into learned 3D representations for more effective transfer to downstream 3D tasks.</p><p>Recently, Huang et al. <ref type="bibr" target="#b21">[22]</ref> propose to learn from the inherent sequence data of RGB-D video to incorporate the notion of a temporal sequence. Constraints are established across pairs of frames in the sequence; however, the sequence data itself represents static scenes without any movement within the scene, limiting the temporal signal that can be learned. In contrast, we consider 4D sequence data containing object movement through the scene, which can provide additional semantic signal about objectness through an object's motion. Additionally, Rao et al. <ref type="bibr" target="#b32">[33]</ref> propose to learn from 3D scenes that are synthetically generated by randomly placing synthetic CAD models on a rectangular layout. <ref type="figure">Fig. 2</ref>. Method overview. 4DContrast learns effective 3D feature representations imbued with 4D signal from moving object sequences. During pre-training, we augment static 3D scene data with a moving object from a synthetic shape dataset. We can then establish dynamic correspondences between the spatio-temporal features learned from the 4D sequence with 3D features of individual static frames. We employ contrastive learning under not only 3D geometric correspondence between individual frames, but also with their corresponding 4D counterpart, as well as 4D-4D constraints to anchor the 4D feature learning. This enables 4D-invariant representation learning, which we can apply to various downstream 3D scene understanding tasks.</p><p>They employ object-level contrastive learning on object-level features, resulting in improved 3D object detection performance. We also leverage synthetic CAD models for data augmentation, but we compose them with real-world 3D scan data to generate 4D sequences of objects in motion, and exploit learned 4D features to enhance learned 3D representations, with performance improvement on various downstream 3D scene understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">4D Invariant Representation Learning</head><p>4DContrast presents a new approach to 3D representation learning: our key idea is to employ 4D constraints during pre-training, in order to imbue learned features with 4D invariance from learned objectness from seeing an object in motion. We consider a dataset of 3D scans S = {S i } as well as a dataset of synthetic 3D objects O = {O j }, and construct dynamic sequences with inherent correspondence information by moving a synthetic object O j in a static 3D scan S i . This enables us to establish 4D correspondences along with 3D-4D correspondence as constraints under a contrastive learning framework for unsupervised pre-training. An overview of our approach is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting SimSiam</head><p>We first revisit SimSiam <ref type="bibr" target="#b4">[5]</ref>, which introduced a simple yet powerful approach for contrastive 2D representation learning. Inspired by the effectiveness of SimSiam,</p><formula xml:id="formula_0">3D encoder 3D predictor 4D encoder ?1 4D predictor ?2 0 ? 3D encoder ?1 ?2 0 ? 0 3D ?2 3D ?1 3D 0 3D ?2 3D ?1 3D 4D encoder 0 4D ?2 4D ?1 4D 0 4D ?2 4D ?1 4D ? ? ? ? ? 3D ? 3D4D ? 4D</formula><p>grad grad grad grad grad grad <ref type="figure">Fig. 3</ref>. 4DContrast pre-training. We visualize 3D-3D, 3D-4D, and 4D-4D losses across frame and spatio-temporal correspondence. Note that losses are established across all pairs of frames for L 3D L 4D and across all frames for L 3D4D ; for visualization we only show those associated with frames Ft?2 and Ft?1, and only for Ft?1 for L 3D4D . Each loss only propagates back according to the gradient arrows due to stop-gradient operations for stable training.</p><p>we build an unsupervised contrastive learning scheme for embedding 4D priors into 3D representations. SimSiam considers two augmented variants of an image I, I 1 , and I 2 , which are input to weight-shared encoder network ? 2D (a 2D convolutional backbone followed by a projection MLP). Then a prediction MLP head P 2D transforms the output of one view as p 2D 1 = P 2D (? 2D (I 1 )) to match to the another output z 2D 2 = ? 2D (I 2 ), with minimizing the negative cosine similarity <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_1">D(p 2D 1 , z 2D 2 ) = ? p 2D 1 ||p 2D 1 || 2 ? z 2D 2 ||z 2D 2 || 2 .<label>(1)</label></formula><p>SimSiam also uses a stop-gradient (SG) operation that treats z 2D 2 as a constant during back-propagation, to prevent collapse during the training, thus modifying Eq. 1 as: D(p 2D 1 , SG(z 2D 2 )). A symmetrized loss is defined for the two augmented inputs:</p><formula xml:id="formula_2">L 2D = 1 2 D(p 2D 1 , SG(z 2D 2 )) + 1 2 D(p 2D 2 , SG(z 2D 1 )).<label>(2)</label></formula><p>SimSiam has shown to be very effective at learning invariances under various image augmentations, without requiring negative samples or very large batches. We thus build from this contrastive framework for our 3D-4D constraints, as it allows for our high-dimensional pre-training design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">4D-Invariant Contrastive Learning</head><p>To imbue effective 4D priors into learned 3D features, we consider a static 3D scan S and a synthetic 3D object O as a train sample, and compose them together to form dynamic object movement in the scene {F 0 , ..., F t?1 } for t time steps (as described in Section 3.3). We then establish spatial correspondences between frames (3D-3D), spatio-temporal correspondences (3D-4D), and dynamic correspondences (4D-4D) as constraints. 3D features are extracted with a 3D encoder ? 3D and 4D features with a 4D encoder ? 4D , with respective prediction MLPs P 3D and P 4D . Inter-Frame Spatial Correspondence. For each pair of frames (F i , F j ) in a train sequence F , we consider their spatial correspondence across sequence frames in order to implicitly pose invariance over the dynamic sequence. That is, points that correspond to the same location in the original 3D scene S or original object O should also correspond in feature space. For the set of corresponding point locations A i,j from frames (F i , F j ), we consider each pair of point locations (a i , b j ) ? A, we obtain their 3D backbone features at the respective locations:</p><formula xml:id="formula_3">p 3D i,a = P 3D (? 3D (F i ))(a i ) and z 3D j,b = ? 3D (F j )(b j )</formula><p>. We then compute a symmetrized negative cosine similarity loss between features of corresponding point locations:</p><formula xml:id="formula_4">L 3D Ai,j = (a,b)?Ai,j 1 2 D(p 3D i,a , SG(z 3D j,b )) + 1 2 D(p 3D j,b , SG(z 3D i,a )) .<label>(3)</label></formula><p>In <ref type="figure">Figure 3</ref>, we use green arrows to indicate constraints between frame F t?2 and frame F t?1 . We compute Eq. 3 over each pair of frames in the whole sequence F :</p><formula xml:id="formula_5">L 3D = t?1 i=0 t?1 j=0 i&lt;j L 3D Ai,j .<label>(4)</label></formula><p>By establishing constraints across 3D frames in a 4D sequence, we encode pose invariance of moving objects across varying background into the learned 3D features. Spatio-Temporal Correspondence. In addition to implicitly encoding pose invariance of moving objects, we establish explicit 3D-4D correspondences to learn 4D priors, encouraging 4D-invariance in the learned features. For a train sequence F = {F 0 , ..., F t?1 }, we use the 4D encoder ? 4D and the 4D predictor P 4D to extract 4D features from the whole sequence. Then z 4D i,a indicates the 4D features output by the 4D encoder ? 4D at point location a i in frame i, and p 4D i,a denotes the 4D features output by the 4D predictor P 4D . Then for a frame F i , we consider each 3D point a i ? A i in this set of frame points F i , and establish a constraint between its corresponding 3D feature extracted by 3D network (? 3D and P 3D ) and its corresponding 4D feature extracted by 4D network (? 4D and P 4D ):</p><formula xml:id="formula_6">L 3D4D Ai = a?Ai 1 2 D(SG(p 3D i,a ), z 4D i,a )+ 1 2 D(SG(p 4D i,a ), z 3D i,a ) .<label>(5)</label></formula><p>As shown in <ref type="figure">Figure 3</ref>, we use orange arrows to indicate constraints of frame F t?1 . For the entire input sequence F , we calculate Eq. 5 for every frame, and the 3D-4D contrastive loss L 3D4D is defined as:</p><formula xml:id="formula_7">L 3D4D = t?1 i=0 L 3D4D Ai .<label>(6)</label></formula><p>Additionally, in order to learn spatio-temporally consistent 4D representations, we employ 4D-4D correspondence constraints inherent to the 4D features within the same point cloud sequence. This is formulated analogously to Eq. 3, replacing the 3D features with the 4D features from different time steps that correspond spatially:</p><formula xml:id="formula_8">L 4D Ai,j = (a)?Ai,j 1 2 D(p 4D i,a , SG(z 4D j,a )) + 1 2 D(p 4D j,a , SG(z 4D i,a )) .<label>(7)</label></formula><p>In <ref type="figure">Figure 3</ref>, we use blue arrows to indicate 4D constraints between frame F t?2 and frame F t?1 . We evaluate Eq. 7 over every pair of frames in the entire input sequence F , with the 4D contrastive loss L 4D defined as:</p><formula xml:id="formula_9">L 4D = t?1 i=0 t?1 j=1 i&lt;j L 4D Ai,j .<label>(8)</label></formula><p>Joint Learning. Our overall training loss L consists of three parts including 3D contrastive loss L 3D , 3D-4D contrastive loss L 3D4D , and 4D contrastive loss</p><formula xml:id="formula_10">L 4D : L = w 3D L 3D + w 3D4D L 3D4D + w 4D L 4D ,<label>(9)</label></formula><p>where constant weights w 3D , w 3D4D and w 4D are used to balance the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating 4D Correspondence via Scene-Object Augmentation</head><p>To learn from 4D sequence data to embed 4D priors into learned 3D representations, we leverage existing large-scale real-world 3D scan datasets in combination with synthetic 3D shape datasets. This enables generation of 4D correspondences without requiring any labels -by augmenting static 3D scenes with generated trajectories of a moving synthetic object within the scene, which provides inherent 4D correspondence knowledge across the object motion. Thus for pretraining, we consider pairs of reconstructed scans and an arbitrarily sampled synthetic 3D shape (S, O), and generate a 4D sequence F = {F 0 , ..., F t?1 } by moving the object through the scene. Trajectory Generation. We first generate a trajectory for O in S. We voxelize S at 10cm voxel resolution, and accumulate occupied surface voxels in the height dimension to acquire a 2D map of the scene geometry. Valid object locations are then identified as those in the 2D map with a voxel accumulation ? 1, with the max height of the accumulated voxels near to the ground floor (within 20cm of the average floor height). For the object O, we consider all possible 2D locations, and if O does not exceed the valid region (based on its bounding sphere), then the location is taken as a candidate object position. A random position sampled from these candidate positions is taken as the starting point of the trajectory, we can randomly sample a step distance in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">90]</ref>cm and step direction such that the angular change in trajectory is &lt; 150 ? , and then select the nearest valid candidate position as the second trajectory point. We repeat this process for t time steps in the sequence to obtain 4D scene-object augmentations for pre-training. 4D Sequence Generation. A sequence of point clouds are then generated based on the computed object trajectory for the scan, up to sequence length t, by compositing the object into the scene under its translation and rotation steps per frame. This provides inherent correspondence information between 3D scene locations and 4D object movement through the scene. Scene Augmentation. We augment the 4D sequences by randomly sampling different points across the geometry in each individual frame. We also randomly remove cubic chunks of points in the background 3D scene for additional data variation, with the number of chunks removed randomly sampled from <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> and the size of the chunks randomly sampled in [0.15, 0.45] as a proportion of the scene extent. We discard any sequences that do not have enough correspondences in its frames; that is, ? 30% of the points in the original scan and ? 30% of the points of the synthetic object should be consistently represented in each frame, and each frame must maintain at least 50% of its points through the augmentation process. Additionally, we further augment the static 3D frame interpretations of the sequence (but not the sequence) by applying random rotation, translation, and scaling to each individually considered 3D frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture for Pre-training</head><p>During pre-training, we leverage correspondences induced by our 4D data generation, between encoded 3D frames as well as across the encoded 4D sequence.</p><p>To this end, we employ 3D and 4D feature extractors as meta-architectures for 4D-invariant learning.</p><p>To extract per-point features from a 3D scene, we use a 3D encoder ? 3D and a 3D predictor P 3D . ? 3D is a U-Net architecture based on sparse 3D convolutions with residual block followed by a 1 ? 1 ? 1 sparse convolutional projection layer, and P 3D is two 1 ? 1 ? 1 sparse convolutional layers.</p><p>To extract spatio-temporal features from a 4D sequence, we use a 4D encoder ? 4D and a 4D predictor P 4D . These are structured analogously to the 3D feature extraction, using sparse 4D convolutions instead. For more detailed architecture specifications, we refer to the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>We demonstrate the effectiveness of our 4D-informed pre-training of learned 3D representations for a variety of downstream 3D scene understanding tasks. Pre-training Setup. We use reconstructed 3D scans from ScanNet <ref type="bibr" target="#b8">[9]</ref> and synthetic 3D shapes from ModelNet <ref type="bibr" target="#b42">[43]</ref> to compose our 4D sequence data for pre-training. We use the official ScanNet train split with 1201 train scans, augmented with shapes from ModelNet from eight furniture categories: chair, desk, dresser, nightstand, sofa, table, bathtub, and toilet. For each 3D scan, we generate 20 trajectories of an object moving through the scan, following Section 3.3 with t = 4. For sequence generation we use 2cm resolution for the scene and 1000 randomly sampled points from the synthetic object to compose together.</p><p>The 3D and 4D sparse U-Nets are implemented with MinkowskiEngine [7] using 2cm voxel size for 3D and 5cm voxel size for 4D. For pre-training we consider only geometry information from the scene-object sequence augmentations. We use an SGD optimizer with initial learning rate 0.25 and a batch-size of 12. The learning rate is decreased by a factor of 0.99 every 1000 steps. We train for 50K steps until convergence. Fine-tuning on Downstream Tasks. We use the same pre-trained backbone network in the three 3D scene understanding tasks of semantic segmentation, instance segmentation, and object detection. For semantic segmentation, we directly use the U-Net architecture for dense label prediction, and for object detection and instance segmentation, we use VoteNet <ref type="bibr" target="#b28">[29]</ref> and PointGroup <ref type="bibr" target="#b22">[23]</ref> respectively, both with our pre-trained 3D U-Net backbone. All experiments, including comparisons with state of the art, are trained with geometric information only, unless otherwise noted. Fine-tuning experiments on semantic segmentation are trained with a batch size of 48 for 10K steps, using an initial learning rate of 0.8 with polynomial decay with power 0.9. For instance segmentation, we use the same training setup as PointGroup, and use an initial learning rate of 0.1. For object detection, the network is trained for 500 epochs, and the learning rate is 0.001 and decayed by a factor of 0.5 at epochs 250, 350, and 450. We use a batch size of 6 on ScanNet and 16 on SUN RGB-D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We demonstrate that our learned features under 3D-4D constraints can effectively transfer well to a variety of downstream 3D scene understanding tasks. We consider both in-domain transfer to 3D scene understanding tasks on Scan-Net <ref type="bibr" target="#b8">[9]</ref> (Section 5.1), as well as out-of-domain transfer to SUN RGB-D <ref type="bibr" target="#b37">[38]</ref> (Section 5.2); a summary is shown in <ref type="table" target="#tab_0">Table 1</ref>. We also show data-efficient scene understanding (Section 5.3) and additional analysis (Section 5.4). Note that for all downstream experiments, we do not use the 4D backbone and thus use the same 3D U-Net architecture as PointContrast <ref type="bibr" target="#b44">[45]</ref> and CSC <ref type="bibr" target="#b18">[19]</ref>. All experiments, including our method and all baseline comparisons, are trained on geometric data only without any color information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ScanNet</head><p>We first demonstrate our 4DContrast pre-training in fine-tuning for 3D object detection, semantic segmentation, and instance segmentation on ScanNet <ref type="bibr" target="#b8">[9]</ref>, showing the effectiveness of learning 3D features under 4D constraints. <ref type="table" target="#tab_1">Tables 2,  3</ref>, and 4 evaluate performance on 3D object detection, semantic segmentation, and instance segmentation, respectively. <ref type="table" target="#tab_1">Table 2</ref> shows 3D object detection results, for which our pretraining approach improves over baseline training from scratch (+5.5% mAP@0.5) as well as over the strong 3D-based pre-training methods of RandomRooms <ref type="bibr" target="#b32">[33]</ref>, Point-Contrast <ref type="bibr" target="#b44">[45]</ref> and CSC <ref type="bibr" target="#b18">[19]</ref>.</p><p>In <ref type="table" target="#tab_3">Tables 3 and 4</ref>, we evaluate semantic segmentation in comparison with state-of-the-art 3D pre-training approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref>, as well as a baseline training paradigm from scratch. These pre-training approaches improve notably over training from scratch, and our 4DContrast approach leveraging learned representations under 4D constraints, leads to additional performance improvement over train from scratch (+2.3% mIoU for semantic segmentation and +4.2% mAP@0.5 for instance segmentation). We show qualitative results for semantic segmentation in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SUN RGB-D</head><p>We additionally show that our 4DContrast learning scheme can produce transferable representations across datasets. We leverage our pre-trained weights from ScanNet + ModelNet, and explore downstream 3D object detection on the SUN RGB-D <ref type="bibr" target="#b37">[38]</ref> dataset. SUN RGB-D is a dataset of RGB-D images, containing 10,335 frames captured with a variety of commodity RGB-D sensors. It contains 3D object bounding box annotations for 10 class categories. We follow the official train/test split of 5,285 train frames and 5,050 test frames. <ref type="table" target="#tab_4">Table 5</ref> shows 3D object detection performance on SUN RGB-D, with qualitative results visualized in <ref type="figure">Figure 5</ref>. We use the same pre-training as with Scan-Net, with downstream fine-tuning on SUN RGB-D data. 4DContrast improves over training from scratch (+6.5% mAP@0.5), with our learned representations surpassing the 3D-based pre-training <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48]</ref>. <ref type="table">Table 3</ref>. Semantic segmentation on ScanNet. Our 4D-informed pre-training learns effective features that lead to improved performance boost over training from scratch as well as state-of-the-art 3D-based pre-training of CSC <ref type="bibr" target="#b18">[19]</ref> and PointContrast <ref type="bibr" target="#b44">[45]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Data-efficient 3D Scene Understanding</head><p>We evaluate our approach in the scenario of limited training data, as shown in <ref type="figure" target="#fig_0">Figure 6</ref>. 4DContrast improves over baseline training from scratch as well as over state-of-the-art data-efficient scene understanding CSC <ref type="bibr" target="#b18">[19]</ref> in semantic segmentation and object detection under various different percentages of ScanNet training data. With only 20% of the training data, we can recover 87% of the fine-tuned semantic segmentation performance training with 100% of the train data from scratch. In object detection, our pre-training enables improved performance for all percentage settings, notably in the very limited regime with +3.0/4.5% mAP@0.5 over CSC/training from scratch at 10% data, and +2.5/5.9% mAP@0.5 with 20% data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Studies</head><p>Effect of 3D and 4D Data Augmentation. We consider a baseline variant of our approach that considers only the static 3D scene data without any sceneobject augmentations with 3D-3D constraints during pre-training in <ref type="table" target="#tab_6">Table 6</ref> (Ours (3D data, 3D-3D only)), which provides some improvement over training from scratch but is notably improved with our 4D pre-training formulation. We additionally consider using our 4D scene-object augmentation with only 3D-3D constraints between sequence frames during pre-training (Ours (4D data, 3D-3D only)) in <ref type="table" target="#tab_6">Table 6</ref>, which helps to additionally improve performance with implicitly learned priors from 4D data. Both are further improved by our approach to explicitly learn 4D priors in 3D features. Effect of 4D-invariant Contrastive Priors. In <ref type="table" target="#tab_6">Table 6</ref>, we see that learning 4D-invariant contrastive priors through our 3D-4D and 4D-4D constraints during pretraining improves upon data augmentation variants only. Additionally, <ref type="figure">Fig. 4</ref>. Qualitative results on ScanNet semantic segmentation. Our 4DContrast pretraining to encode 4D priors enables more consistent segmentation results, in comparison to training from scratch as well as 3D-based PointContrast <ref type="bibr" target="#b44">[45]</ref> pre-training.  <ref type="table" target="#tab_7">Table 7</ref> evaluates the 3D variant of our approach with our full 4D-based pretraining across a variety of downstream tasks, showing consistent improvements from learned 4D-based priors. Effect of SimSiam Contrastive Learning. We also consider the effect of our SimSiam contrastive framework as PointContrast <ref type="bibr" target="#b44">[45]</ref> leverages a PointInfoNCE contrastive loss. We note that the 3D variant of our approach (Ours (3D data, 3D-3D only)) reflects a PointContrast <ref type="bibr" target="#b44">[45]</ref> setting using our scene augmentation and SimSiam architecture, which our 4D-based feature learning outperforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>While 4DContrast pre-training demonstrates the effectiveness of leveraging 4D priors for learned 3D representations, various limitations remain. In particular, 4D feature learning with sparse convolutions involves considerable memory  during pre-training, so we use half-resolution for characterizing 4D features relative to 3D features and limited sequence durations. Additionally, we consider a subset of 4D motion when augmenting scenes with moving synthetic objects, and believe exploration of articulated motion or more complex dynamic object interactions would lead to additional insight and robustness of learned feature representations. Memory and Speed. Our 4D-imbued pre-training results in consistent improvements across a variety of tasks and datasets, even with only using the learned 3D backbone for downstream training and inference. Thus, our method maintains the same memory and speed costs for inference as purely 3D-based pre-training approaches. For pre-training, our joint 3D-4D training uses additional parameters (33M for the 4D network in addition to the 38M for the 3D network), but due to jointly learning 4D priors with SimSiam, we do not require as large of a batch size to train as PointContrast <ref type="bibr" target="#b44">[45]</ref> (12 vs their 48), nor as many iterations (up to 30K vs 60K), resulting in slightly less total memory use and pre-training time overall.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented 4DContrast, a new approach for 3D representation learning that incorporates 4D priors into learned features during pre-training. We propose a data augmentation scheme to construct 4D sequences of moving synthetic objects in static 3D scenes, without requiring any semantic labels. This enables learning from 4D sequences, and we and establish contrastive constraints between learned 3D features and 4D features from the inherent correspondences given in the 4D sequence generation. Our experiments demonstrate that our 4D-imbued pre-training results in performance improvement across a variety of 3D downstream tasks and datasets. Additionally, our learned features effectively transfer to limited training data scenarios, significantly outperforming state of the art in the low training data regime. We hope that this will lead to additional insights in 3D representation learning and new possibilities in 3D scene understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Quantitative Analysis</head><p>Alternative 3D Backbone vs 4D Pre-training. Our 4D pre-training can help to learn objectness priors from dynamic object movement, in contrast to multiple 3D backbones. To demonstrate this, we pre-trained with an additional 3D backbone (comparable to our 4D backbone in parameters and UNet structure); this resulted in worse performance than our 4D pre-training which has +0.5 mIoU and +1.4 mAP@0.5 vs. multiple 3D backbones in the tasks of 3D semantic and instance segmentation on ScanNet (as shown in <ref type="table" target="#tab_8">Table 8</ref>).</p><p>Sequence Length Ablation. We study the effect of the sequence length of the generated dynamic data used for pre-training in <ref type="table">Table 9</ref>. We consider sequences of length 3, 4, or 5, and set the batch size (number of sequences) to 16, 12, and 10, respectively, to balance the scene frames in each batch during pre-training. We find a sequence length of 4 results in more effective feature learning for downstream tasks. Comparison of Different Contrastive Frameworks. As analyzed in Section 3.1 of the main paper, SimSiam <ref type="bibr" target="#b4">[5]</ref> enables contrastive learning without requiring negative samples or large batch size. We thus verify how these attributes fit our high-dimensional pre-training design by comparing SimSiam and SimCLR <ref type="bibr" target="#b2">[3]</ref> as our contrastive framework. As shown in <ref type="figure">Figure 7</ref>, Ours (Sim-CLR) removes the 3D and 4D predictors from Ours (SimSiam), and uses a match average pooling to average 4D features in different frames according to spatial correspondences. For each pair of frames (F i , F j ) in a train sequence, we apply a 3D contrastive lossL 3D as L 3D (Eq. 4 in Section 3.2). Similar to L 3D4D (Eq. 6 in Section 3.2), we use a 3D-4D contrastive lossL 3D4D to establish correspondence between 3D features and the averaged 4D features. The Hardest-Contrastive loss is borrowed from FCGF <ref type="bibr" target="#b7">[8]</ref> and PointContrast <ref type="bibr" target="#b44">[45]</ref>. Note that in our implementation, we find that the PointInfoNCE loss <ref type="bibr" target="#b44">[45]</ref> is not as stable as Hardest-Contrastive loss, likely due to different data augmentation methods between Ours (SimCLR) and PointContrast. As shown in <ref type="table" target="#tab_0">Table 10</ref>, 4DContrast coupled with SimSiam framework more effectively leverages the learned representations for improved semantic segmentation performance on ScanNet.</p><p>H3DNet Object Detection with 4DContrast. In <ref type="table" target="#tab_0">Table 11</ref>, we apply our pre-trained weights to H3DNet <ref type="bibr" target="#b48">[49]</ref> (1 descriptor computation tower of its backbone architecture). 4DContrast surpasses training from scratch by 4.3 mAP@0.5 on ScanNet. Mix3D Semantic Segmentation with 4DContrast. While 4DContrast focuses on imbuing 4D priors during pre-training to provide effective features for a variety of downstream tasks, Mix3D <ref type="bibr" target="#b25">[26]</ref> tackles a complementary problem of data augmentation during training. As shown in <ref type="table" target="#tab_0">Table 12</ref>, our pre-training can be used together with Mix3D to further improve semantic segmentation performance on ScanNet (geometry only input). MinkowskiNet 3D Classification with 4DContrast. We evaluate Model-Net classification accuracy in comparison with MinkowskiNet <ref type="bibr" target="#b6">[7]</ref> trained from scratch for various voxel sizes in <ref type="table" target="#tab_0">Table 13</ref>. Our pre-training shows consistent improvements in both settings. S3DIS dataset. We finetune our pre-trained weights for S3DIS <ref type="bibr" target="#b0">[1]</ref> segmentation (geometry-only), and consistently improve over training from scratch (as shown in <ref type="table" target="#tab_0">Table 14</ref>): we achieve +2.4 mIoU in semantic segmentation (61.0 ours vs 58.6 scratch) and +7.4 mAP@0.5 in instance segmentation (53.2 vs 45.8).  <ref type="bibr" target="#b6">[7]</ref>. We use a 34-layer U-Net as the 3D backbone and a 14-layer U-Net as the 4D backbone. For the 3D and 4D projectors, we use a one-layer sparse convolutional layer with kernel size as 1?1?1 and 1?1?1?1, respectively. For the 3D and 4D predictor, we use two sparse convolutional layers. We repeat occupancy into 3-dimension to fit the network input dimension of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Network Architecture Details</head><p>C Visualization of the Generated 4D Data <ref type="figure">Figure 9</ref> shows the generated 4D data by scene-object augmentation (as described in Section 3.3 of the main paper). Repel 3D-4D negative pairs <ref type="figure">Fig. 7</ref>. Network architectures of our method using SimCLR as the contrastive learning framework. Left: we show 3D-3D and 4D-4D losses across frame and spatio-temporal correspondence. We only visualize the inter-frame correspondence for Ft?2 and Ft?1, and only spatio-temporal correspondence for for Ft?2, while those loss are established across all pairs of frames forL 3D and all frames forL 3D4D . Right: we visualize the contrastive losses between 3D feautres of Ft?2 and Ft?1 and the 4D feature after match average pooling. The positive pairs is same with Section 3.2 and the negative losses is only calculated for the hardest negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D Projector</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4D Predictor</head><p>Sparse Conv (kernel_size=3 dimension , stride=1) nf_in: input dimension nf_out: output dimension dimension: 3 or 4, indicated 3D or 4D backbone. N: number of repeated layers Conv/DeConv layers are followed by BN and ReLU 3D Network Architecture 4D Network Architecture Raw Scan Point Cloud Generated Dynamic Point Clouds <ref type="figure">Fig. 9</ref>. Visualization of generated 4D sequence data. Each row corresponds to a sampled scene. From left to right: raw scan mesh vertices as input cloud, generated dynamic point clouds with scene augmentation and object motion (in three frames).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 .</head><label>6</label><figDesc>Data-efficient learning on ScanNet semantic segmentation and object detection. Under limited data scenarios, our 4D-imbued pre-training effectively improves performance over training from scratch as well as the state-of-the-art CSC<ref type="bibr" target="#b18">[19]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 8</head><label>8</label><figDesc>details our network architectures. The backbones are a U-Net architecture with sparse convolutions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Network architectures of 4DContrast for pre-training. For downstream finetuning, only the 3D backbone is kept and fine-tuned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of fine-tuning of 4DContrast for various downstream 3D scene understanding tasks and datasets. Our pre-training approach learns effective, transferable features, resulting in notable improvement over the baseline learning paradigm of training from scratch.</figDesc><table><row><cell>Datasets</cell><cell>Stats (#train / #val)</cell><cell>Task</cell><cell>Gain (from scratch)</cell></row><row><cell></cell><cell></cell><cell>Sem. Seg.</cell><cell>+2.3% mIoU</cell></row><row><cell>ScanNetV2 [9]</cell><cell>1.2K / 312 scenes</cell><cell>Ins. Seg.</cell><cell>+4.2% mAP@0.5</cell></row><row><cell></cell><cell></cell><cell>Obj. Det.</cell><cell>+5.5% mAP@0.5</cell></row><row><cell>SUN RGB-D [38]</cell><cell>5.2K / 5K frames</cell><cell>Obj. Det.</cell><cell>+6.5% mAP@0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>3D object detection on ScanNet. Our 4DContrast pre-training leads to improved performance in comparison with state of the art object detection and 3D pretraining schemes.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>mAP@0.5</cell></row><row><cell>DSS [39]</cell><cell>Geo + RGB</cell><cell>6.8</cell></row><row><cell>F-PointNet [30]</cell><cell>Geo + RGB</cell><cell>10.8</cell></row><row><cell>GSPN [46]</cell><cell>Geo + RGB</cell><cell>17.7</cell></row><row><cell>3D-SIS [18]</cell><cell>Geo + RGB</cell><cell>22.5</cell></row><row><cell>VoteNet [29]</cell><cell>Geo + Height</cell><cell>33.5</cell></row><row><cell>Scratch + VoteNet</cell><cell>Geo only</cell><cell>34.5</cell></row><row><cell>RandomRooms [33] + VoteNet</cell><cell>Geo only</cell><cell>36.2 (+1.7)</cell></row><row><cell>PointContrast [45] + VoteNet</cell><cell>Geo only</cell><cell>38.0 (+3.5)</cell></row><row><cell>CSC [19] + VoteNet</cell><cell>Geo only</cell><cell>39.3 (+4.8)</cell></row><row><cell>Ours + VoteNet</cell><cell>Geo only</cell><cell>40.0 (+5.5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Instance segmentation on ScanNet. Our 4D-imbued pre-training leads to significantly improved results over training from scratch, as well as favorable performance over other 3D-only pretraining schemes.</figDesc><table><row><cell>Method</cell><cell>mAP@0.5</cell><cell>mIoU</cell></row><row><cell>Scratch</cell><cell>53.4</cell><cell>69.0</cell></row><row><cell>PointContrast [45]</cell><cell>55.8 (+2.4)</cell><cell>70.9 (+1.9)</cell></row><row><cell>CSC [19]</cell><cell>56.5 (+3.1)</cell><cell>71.1 (+2.1)</cell></row><row><cell>Ours</cell><cell>57.6 (+4.2)</cell><cell>71.4 (+2.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>3D object detection on SUN RGB-D. Our 4D-based pre-training learns effective 3D representations, improving performance over training from scratch and stateof-the-art 3D pre-training methods.</figDesc><table><row><cell>as a backbone</cell></row></table><note>* indicates that PointNet++ is used</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Additionally ablation variants: compared to a baseline of using 3D-3D constraints on static 3D scene data only, leveraging augmented 4D sequence data improves feature learning even under 3D only constraints. Our final 4DContrast pre-training leveraging constraints with learned 4D features achieves the best performance.</figDesc><table><row><cell>Method/Data Augmentation</cell><cell>Pre-training Loss Term(s)</cell><cell>mIoU</cell><cell>mAcc</cell></row><row><cell>Scratch</cell><cell>-</cell><cell>70.0</cell><cell>78.1</cell></row><row><cell>Ours (3D data, 3D-3D only)</cell><cell>L 3D (static scene)</cell><cell>71.5 (+1.5)</cell><cell>79.7 (+1.6)</cell></row><row><cell>Ours (4D data, 3D-3D only)</cell><cell>L 3D (dynamic scene)</cell><cell>71.7 (+1.7)</cell><cell>80.0 (+1.9)</cell></row><row><cell>Ours</cell><cell>L 3D ,L 3D4D ,L 4D (dynamic scene)</cell><cell>72.3 (+2.3)</cell><cell>80.5 (+2.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Extended ablation of the 3d-only variant of our approach on ScanNet.</figDesc><table><row><cell>Task</cell><cell>Ours (3D data, 3D-3D only)</cell><cell>Ours</cell></row><row><cell>Ins. Seg. (mAP@0.5)</cell><cell>54.1</cell><cell>57.6 (+3.5)</cell></row><row><cell>Obj. Det. (mAP@0.5)</cell><cell>38.7</cell><cell>40.0 (+1.3)</cell></row><row><cell>Sem. Seg. 1% data (mIoU)</cell><cell>27.0</cell><cell>28.2 (+1.2)</cell></row><row><cell>Sem. Seg. 5% data (mIoU)</cell><cell>44.4</cell><cell>45.3 (+0.9)</cell></row><row><cell>Sem. Seg. 10% data (mIoU)</cell><cell>56.3</cell><cell>57.9 (+1.6)</cell></row><row><cell>Sem. Seg. 20% data (mIoU)</cell><cell>60.6</cell><cell>61.9 (+1.3)</cell></row><row><cell>Sem. Seg. 100% data (mIoU)</cell><cell>71.5</cell><cell>72.3 (+0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparisons of alternative 3D backbone and our 4D backbone on ScanNet fine-tuning.</figDesc><table><row><cell>Task</cell><cell>Baseline</cell><cell>Two 3D Backbones</cell><cell>Ours</cell></row><row><cell>Sem.Seg (mIoU)</cell><cell>70.0</cell><cell>71.8 (+1.8)</cell><cell>72.3 (+2.3)</cell></row><row><cell>Ins.Seg (mAP@0.5)</cell><cell>53.4</cell><cell>56.2 (+2.8)</cell><cell>57.6 (+4.2)</cell></row><row><cell cols="4">Table 9. Effect of sequence length of pre-training dynamic data on ScanNet semantic</cell></row><row><cell cols="4">segmentation fine-tuning. A sequence length of 4 helps 4DContrast get higher semantic</cell></row><row><cell>segmentation mIoU.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sequence Length</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>mIoU</cell><cell>71.9</cell><cell>72.3</cell><cell>71.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Comparisons of SimCLR and SimSiam as our contrastive learning framework on ScanNet semantic segmentation fine-tuning.</figDesc><table><row><cell>Contrastive Framework</cell><cell>Baseline</cell><cell>Ours (SimCLR)</cell><cell>Ours (SimSiam)</cell></row><row><cell>mIoU</cell><cell>70.0</cell><cell>71.6 (+1.6)</cell><cell>72.3 (+2.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .Table 12 .</head><label>1112</label><figDesc>3D object detection on ScanNet with H3DNet. Semantic segmentation on ScanNet val with Mix3D.</figDesc><table><row><cell>Method</cell><cell>mAP@0.5</cell></row><row><cell>H3DNet</cell><cell>43.4</cell></row><row><cell>Ours + H3DNet</cell><cell>47.7 (+4.3)</cell></row><row><cell>Method</cell><cell>mIoU</cell></row><row><cell>Mix3D + MinkowskiNet</cell><cell>73.9</cell></row><row><cell>Ours + Mix3D + MinkowskiNet</cell><cell>74.6 (+0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .Table 14 .</head><label>1314</label><figDesc>3D classification on ModelNet with Mix3D. Semantic and instance segmentation on S3DIS.</figDesc><table><row><cell>Method</cell><cell>Voxel Size</cell><cell>Acc</cell></row><row><cell>MinkowskiNet</cell><cell>0.05</cell><cell>86.1</cell></row><row><cell>Ours + MinkowskiNet</cell><cell>0.05</cell><cell>88.5 (+2.4)</cell></row><row><cell>MinkowskiNet</cell><cell>0.02</cell><cell>90.7</cell></row><row><cell>Ours + MinkowskiNet</cell><cell>0.02</cell><cell>91.8 (+1.1)</cell></row><row><cell>Task</cell><cell>scratch</cell><cell>Ours</cell></row><row><cell>Sem.Seg. (mIoU)</cell><cell>58.6</cell><cell>61.0 (+2.4)</cell></row><row><cell>Ins.Seg. (mAP@0.5)</cell><cell>45.8</cell><cell>53.2 (+7.4)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape self-correction for unsupervised point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d-mpa: Multiproposal aggregation for 3d semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Occuseg: Occupancy-aware 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring data-efficient 3d scene understanding with contrastive scene contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional projection network for cross dimension scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Texturenet: Consistent local parametrizations for learning from high-resolution signals on meshes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-temporal self-supervised representation learning for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointgroup: Dual-set point grouping for 3d instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mix3d: Out-ofcontext data augmentation for 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rfd-net: Point scene understanding by semantic instance reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Randomrooms: Unsupervised pre-training from synthetic shapes and randomized layouts for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Language-grounded indoor 3d semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rozenberszki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07761</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Info3d: Representation learning on 3d objects using mutual information maximization and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schult</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<title level="m">Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised point cloud pre-training via occlusion completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised 3d learning for shape analysis via multiresolution instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mlcvnet: Multilevel context votenet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pre-training for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gspn: Generative shape proposal network for 3d instance segmentation in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Point cloud instance segmentation using probabilistic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Sparse (De)Conv (kernel_size=2 dimension , stride=2) Add Sparse Conv</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Conv ResBlock (nf_in, nf_out, dimension, N=2) Sparse Conv (kernel_size=3 dimension , stride=1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Sparse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Sparse DeConv ResBlock</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
	<note>Sparse DeConv ResBlock</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">Sparse DeConv ResBlock</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Sparse Conv ResBlock</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
