<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Spatial Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Emerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Spatial Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatial relations are fundamental to human cognition and are the most basic knowledge for us to understand and communicate about our physical surroundings. In this paper, we ask the critical question: Are current visionand-language models (VLMs) able to correctly understand spatial relations? To answer this question, we propose Visual Spatial Reasoning (VSR), a novel benchmark task with human labelled dataset for investigating VLMs' capabilities in recognising 65 types of spatial relationships (eg., under, in front of, facing etc.) in natural text-image pairs. Specifically, given a caption and an image, the model needs to perform binary classification and decide if the caption accurately describes the spatial relationships of two objects presented in the image. While being seemingly simple and straightforward, the task shows a large gap between human and model performance (human ceiling on the VSR task is above 95% and models only achieve around 70%). With fine-grained categorisation and control on both concepts and relations, our VSR benchmark enables us to perform interesting probing analysis to pinpoint VLMs' failure cases and the reasons behind. We observe that VLMs' by-relation performances have little correlation with number of training examples and the tested models are in general incapable of recognising relations that concern orientations of objects. Also, VLMs have poor zero-shot generalisation towards unseen concepts. The dataset and code are released at github.com/cambridgeltl/ visual-spatial-reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal NLP research has been developing rapidly in the last couple of years with the introduction of a series of pre-trained vision-language models (VLMs) such as ViLBERT <ref type="bibr" target="#b23">(Lu et al., 2019)</ref>, VisualBERT , LXMERT <ref type="bibr" target="#b29">(Tan and Bansal, 2019)</ref>, UNITER <ref type="bibr" target="#b7">(Chen et al., 2019)</ref>, and  ViLT , to name a few. VLMs have pushed state-of-the-art performance up by significant margins on vision-language benchmarks, including tasks such as visual question answering (VQA) <ref type="bibr" target="#b5">(Antol et al., 2015;</ref><ref type="bibr" target="#b13">Johnson et al., 2017;</ref><ref type="bibr" target="#b10">Goyal et al., 2017;</ref><ref type="bibr" target="#b11">Hudson and Manning, 2019;</ref><ref type="bibr" target="#b34">Zellers et al., 2019)</ref>, vision-language reasoning or entailment <ref type="bibr" target="#b27">(Suhr et al., 2017</ref><ref type="bibr" target="#b28">(Suhr et al., , 2019</ref><ref type="bibr" target="#b31">Xie et al., 2019;</ref><ref type="bibr" target="#b19">Liu et al., 2021)</ref>, and referring expression comprehension <ref type="bibr" target="#b33">(Yu et al., 2016;</ref>.</p><p>Existing benchmarks (such as NLVR2 <ref type="bibr" target="#b28">(Suhr et al., 2019)</ref> and VQA <ref type="bibr" target="#b10">(Goyal et al., 2017)</ref>) define generic paradigms for testing VLMs. However, these benchmarks are not ideal for probing VLMs as they typically conflate multiple sources of errors and do not allow controlled analysis on specific lin-guistic or cognitive properties, making it difficult to categorise and fully understand the model failures. Another line of work generates synthetic datasets in a controlled manner to target specific relations and properties when testing VLMs (e.g., CLEVR ), but we will demonstrate later in our paper that synthetic datasets neglect many realistic challenges presented in natural images.</p><p>To address the lack of probing evaluation benchmarks in this field, we present VSR (Visual Spatial Reasoning), a first controlled probing task/benchmark that explicitly tests VLM's spatial reasoning. We choose spatial reasoning as the task focus because it is one of the most fundamental capabilities for both humans and VLMs. Such relations are crucial to how humans organise the mental space and make sense of the physical world, and therefore fundamental for a grounded theory of semantics <ref type="bibr" target="#b16">(Levinson, 2003)</ref>. However, spatial reasoning has been found to be particularly challenging (much more challenging than capturing properties of individual entities) for current multimodal models <ref type="bibr" target="#b0">(Akula et al., 2020)</ref>.</p><p>The VSR dataset contains natural image-text pairs in English. Each example in the dataset consists of an image and a natural language description which states a spatial relation of two objects presented in the image (two examples shown in <ref type="figure" target="#fig_0">Fig. 1 and Fig. 2)</ref>. A VLM needs to classify the image-caption pair as either true or false, indicating whether the caption is correctly describing the spatial relation. The dataset covers 65 spatial relations and has &gt;10k data points, using 6,940 images from MS COCO <ref type="bibr" target="#b18">(Lin et al., 2014)</ref>.</p><p>We test three popular VLMs, VisualBERT, LXMERT, and ViLT on VSR. While human ceiling is above 95%, all three models struggle to reach 70% of accuracy. We conduct comprehensive analysis on the failures of the investigated VLMs and highlight that (1) positional encodings are extremely important for the VSR task, (2) models' by-relation performance barely correlates with the number of training examples, (3) in fact, several spatial relations that concern orientation of objects are especially challenging for current VLMs, (4) VLMs have extremely poor generalisation on unseen concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Creation</head><p>In this section we detail how VSR is constructed. The data collection process can generally be split- Choose a relation to complete the caption: <ref type="figure">Figure 3</ref>: An annotation example of concepts "cat" and "laptop" in the contrastive caption generation phase. The example generates two data points for the VSR benchmark: one "True" instance when the completed caption is paired with image 2 (right) and one "False" instance when paired with image 1 (left).</p><p>ted into two phases (1) contrastive caption generation ( ?2.1) and (2) second-round validation ( ?2.2). We then discuss annotator hiring and payment ( ?2.3), basic statistics of VSR ( ?2.4) and the human ceiling &amp; agreement of VSR ( ?2.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Template-based Caption Generation (Fig. 3)</head><p>In order to highlight spatial relations and avoid annotators frequently choose trivial relations (such as "near to"), we use a contrastive caption generation approach. Specifically, first, a pair of images, each containing two concepts of interests, would be randomly sampled from MS COCO (we use the train and validation set of COCO 2017). Second, an annotator will be given a template containing the two concepts and is required to choose a spatial relation from a pre-defined list <ref type="table" target="#tab_1">(Table 1)</ref> that makes the caption correct for one image but incorrect for the other image. We will detail these steps and explain the rationales in the following.</p><p>Image pair sampling. MS COCO 2017 contains 123,287 images and has labelled the segmentation and classes of 886,284 instances (individual objects). Leveraging the segmentation, we first randomly select two concepts (e.g., "person" and "cow" in <ref type="figure" target="#fig_0">Fig. 1</ref>), then retrieve all images containing the two concepts in COCO 2017 (train and validation sets). Then images that contain multiple instances of any of the concept are filtered out to avoid referencing ambiguity. For the singleinstance images, we also filter out any of the images with instance area size &lt; 30, 000, to prevent extremely small instances. After these filtering steps, we randomly sample a pair in the remaining images. We repeat such process to obtain a large number of individual image pairs for caption generation.</p><p>Fill in the blank: template-based caption generation. Given a pair of images, the annotator needs to come up with a valid caption that makes it correctly describing one image but incorrect for the other. In this way, the annotator could focus on the key difference of the two images (which should be spatial relation of the two objects of interest) and come up with challenging relation that differentiates the two. Similar paradigms are also used in the annotation of previous vision-language reasoning datasets such as NLVR(2) <ref type="bibr" target="#b27">(Suhr et al., 2017</ref><ref type="bibr" target="#b28">(Suhr et al., , 2019</ref> and MaRVL <ref type="bibr" target="#b19">(Liu et al., 2021)</ref>. To regularise annotators from writing modifiers and differentiating the image pair with things beyond accurate spatial relations, we opt for a template-based classification task instead of free-form caption writing. Besides, the template-generated dataset can be easily categorised based on relations and their meta-categories. Specifically, the annotator would be given instance pairs as shown in <ref type="figure" target="#fig_0">Fig. 3. 1</ref> The caption template has the format of "The OBJ1 (is) the OBJ2.", and the annotators are instructed to select a relation from a fixed set to fill in the slot. The copula "is" can be omitted for grammaticality. For example, for "contains", "consists of", and "has as a part", "is" should be discarded in the template when extracting the final caption.</p><p>The fixed set of spatial relations enable us to obtain the full control of the generation process. The full list of used relations are listed in <ref type="table" target="#tab_1">Table 1</ref>. It contains 71 spatial relations and is adapted from the summarised relation table of <ref type="bibr" target="#b9">Fagundes et al. (2021)</ref>. We made minor changes to filter out clearly unusable relations, made relation names grammatical under our template, and reduced repeated relations. 2 In our final dataset, 65 out of the 71 available relations are actually included (the other 6 are either not selected by annotators or are selected but the captions did not pass the validation phase).</p><p>Caption: The pizza is at the edge of the dinning table.</p><p>The caption is:</p><p>True False ? <ref type="figure">Figure 4</ref>: A second-round validation example. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Second-round Human Validation</head><p>Every annotated data point is reviewed by at least two additional human annotators (validators). In validation, given a data point (consists of an image and a caption), the validator gives either a True or False label as shown in <ref type="figure">Fig. 4</ref> (the original label is hidden). <ref type="bibr">3</ref> We exclude data points that have &lt; 2 3 validators agreeing with the original label.</p><p>In the guideline, we communicated to the validators that, for relations such as "left"/"right", "in front of"/"behind", they should tolerate different reference frame: i.e., if the caption is true from either the object's or the viewer's reference, it should be given a True label. As a concrete example, in <ref type="figure" target="#fig_2">Fig. 5</ref>, while the person is at the right side of the bus from the viewer's angle, the person is at the left side if the bus is used as the reference frame. Only when the caption is incorrect under all reference frames (e.g., if the caption is "The person is under the bus."), a False label is assigned. This adds difficulty to the models since they could not naively rely on relative locations of the objects in the images but also need to correctly identify orientations Adjacency Adjacent to, alongside, at the side of, at the right side of, at the left side of, attached to, at the back of, ahead of, against, at the edge of Directional Off, past, toward, down, deep down * , up * , away from, along, around, from * , into, to * , across, across from, through * , down from Orientation Facing, facing away from, parallel to, perpendicular to Projective On top of, beneath, beside, behind, left of, right of, under, in front of, below, above, over, in the middle of Proximity By, close to, near, far from, far away from Topological Connected to, detached from, has as a part, part of, contains, within, at, on, in, with, surrounding, among, consists of, out of, between, inside, outside, touching Unallocated Beyond, next to, opposite to, after * , among, enclosed by of objects to make the best judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Annotator Hiring, Organisation, and Payment</head><p>Annotators are hired from prolific.co. We require them (1) have at least a bachelor's degree, <ref type="bibr">4</ref> (2) are fluent in English or native speaker, and <ref type="formula">(3)</ref> have a &gt;99% historical approval rate on the platform. All annotators are paid with an hourly salary of 12 GBP. Prolific takes an extra 33% of service charge and 20% VAT on the service charge. For caption generation, we release the task with batches of 200 instances and the annotator is required to finish a batch in 80 minutes. An annotator cannot take more than one batch per day. In this way we have a diverse set of annotators and can also prevent annotators from being fatigued. For second round validation, we group 500 data points in one batch and an annotator is asked to label each batch in 90 minutes.</p><p>In total, 24 annotators participated in caption generation and 26 participated in validation. The annotators have diverse demographic background: they were born in 13 different countries; live in 13 different couturiers; and have 14 different nationalities. 57.4% of the annotators identify themselves as females and 42.6% as males.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Basic Statistics of VSR</head><p>After the first phase of "Contrastive template-based Caption Generation" ( ?2.1), we collected 12,809 raw data points. In the phase of the second round validation ( ?2.2) , we collected 30,068 validation labels. On average, every data point received 2.41 validation labels and each data point has at least 2 validation labels. In 73.2% of the data points, all validatiors agree with the original label. 77.7% of the data points have at least 2 3 of the validations agreeing with the original label. We use 2 3 as the threshold and exclude all instances with lower validation agreement (in other words, 77.7% of the raw data points from first round annotation are preserved). After validation, 10,119 data points remained and are used as our final dataset. <ref type="bibr">5</ref> Here we provide basic statistics of the two components in the VSR captions: the concepts and the relations. <ref type="figure" target="#fig_3">Fig. 6</ref> demonstrates the relation distribution. "touching" is most frequently used by annotators. The relations that reflect the most basic relative coordinates of objects are also very frequent, e.g., "behind", "in front of", "on", "under", "at the left/right side of". <ref type="figure" target="#fig_4">Fig. 7</ref> shows the distribution of concepts in the dataset. Note that the set of concepts is bounded by MS COCO and the distribution also largely follow MS COCO. Animals such as "cat", "dog", "person" are the most frequent. Indoor objects such as "dining table" and "bed" are also very dominant. In <ref type="figure" target="#fig_4">Fig. 7</ref>, we separate the concepts that appear at OBJ1 and OBJ2 positions of the sentence and their distributions are generally similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Human Ceiling and Inter-annotator Agreement</head><p>We randomly sample 500 data points from the final test set of the dataset for computing human ceiling and inter-annotator agreement. <ref type="bibr">6</ref> We hide the labels of the 500 examples and two additional annotators are asked to label True/False for them. On average, the two annotators achieve an accuracy of 95.4% on the VSR task. We further compute the Fleiss' kappa among the original annotation and the predictions of the two human. The Fleiss' kappa score is <ref type="bibr">5</ref> We introduce the datasplit later in Experiment section ( ?3.1). <ref type="bibr">6</ref> The final test set is a random subset of the final dataset. See ?3.1.  <ref type="table" target="#tab_9">Table 6</ref> for the full listing. It is clear that the relations follow a long-tailed distribution (however, according to our sample efficiency analysis, it is not the frequency of training examples causing the major challenge). 0.895, indicating near-perfect agreement according to <ref type="bibr" target="#b15">Landis and Koch (1977)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">What has caused the disagreement among annotators?</head><p>While we only preserve data points with high validation agreement for model development, the raw dataset is a valuable resource for understanding Cognitive-and Socio-Linguistic phenomena. 7 Along with the validated VSR dataset, we also release the full raw dataset, with annotator and validators' meta data, as a second version to facilitate Linguistic studies. Specifically, researchers can investigate questions such as where disagreement usually happens and how people from different regions or cultural background perceive spatial relations differently. As an example, we plotted the ratio of excluded instances per relation in <ref type="figure" target="#fig_5">Fig. 8</ref>. The figure suggests that people have the most disagreement on relations with ambiguous nature. Some of these are dependent of the reference frame. E.g. <ref type="bibr">7</ref> We sampled 100 examples where people disagree and found that around 30 of them are caused by annotation error but the rest are genuinely ambiguous and can be interpreted in multiple ways. See Appendix for a couple of examples.</p><p>"at the left side of" and "right of". Some are dependent on subjective judgement. E.g., the notion of closeness: "near" and "close to". By contrast, Part-whole relations, such as "has as a part", "part of", and in/out relations such as "within", "into", "outside" and "inside" have the least disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We first introduce how we split the dataset for experiments in ?3.1, and then baseline models and experimental configurations in ?3.2, finally experimental results and analysis in ?3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Splits</head><p>We create two types of splits from the 10,119 validated data points. The stats of the two split are shown in <ref type="table" target="#tab_3">Table 2</ref>. In the following, we explain how they are created.</p><p>Random split. We split the dataset randomly into train/dev/test with the ratio of 70%/10%/20%.</p><p>Concept zero-shot split. We create another concept zero-shot split where train/dev/test have no overlapping concepts. I.e., if "dog" appears in   train, then it does not appear in dev or test sets. This is done by first randomly grouping concepts into three sets with the ratio of 50%/20%/30% of all concepts. This is a more challenging setup since the model has to learn concepts and the relations in a compositional way instead of remembering co-occurrence statistics of the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines and Experiment Configurations</head><p>Baselines. We test three popular VLMs: Visual-BERT , LXMERT <ref type="bibr" target="#b29">(Tan and Bansal, 2019)</ref>, and ViLT . All three models are stacked Transformers <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> that take image and text pairs as input. The difference mainly lie in how or whether they encode position information of objects. We discuss more later in ?3.3.</p><p>The exact hyperparameters and base pretrained models used are listed in Appendix <ref type="table" target="#tab_10">(Table 7)</ref>. We implement all baselines using PyTorch and the huggingface library.</p><p>Experimental configurations. We save checkpoints every 100 iterations and use the bestperforming checkpoint on dev set for testing. All models are run for three times using three random seeds. All models are trained with AdamW optimiser <ref type="bibr" target="#b22">(Loshchilov and Hutter, 2019</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Results</head><p>In this section, we provide both quantitative and qualitative results of the three baselines performance on VSR. Through analysing the failure cases of the models, we also highlight what are the key abilities needed to solve this task. As shown in <ref type="table" target="#tab_5">Table 3</ref>, the best performing models on random split are LXMERT and ViLT, reaching 70% of accuracy while VisualBERT is just slightly better than chance level. On zero-shot split, all models' performance decline significantly and the best model LXMERT only obtains 63.7% accuracy. These results lag behind the human ceiling by more than 20% and highlight that there is very significant room for improving current VLMs.   encodings of VLMs, serving as a side proof of our observations.</p><p>Random split vs. zero-shot split. It is worth noting that the performance gap between random and zero-shot split is large. As we will show in sample efficiency analysis <ref type="figure" target="#fig_0">(Fig. 15</ref>), the underlying cause is not likely the different number of training examples but concept zero-shot learning is fundamentally a hard task. The gap verifies our hypothesis that learning concept-relation disentangled spatial reasoning is challenging for current models.</p><p>Sensitiveness to random seeds. On both random and zero-shot splits' test sets, LXMERT is the most unstable model, with a standard deviation of 1.4 and 1.7 respectively. In general, the models have larger standard deviation on zero-shot split, probably because that zero-shot dev/test sets are smaller. Due to the fluctuations we observe, we recommend always reporting average performance of multiple runs to make sure the conclusion is reliable.</p><p>Performance by relation. We list the performance by relation of all three models on both random and zero-shot splits in <ref type="figure" target="#fig_7">Fig. 9</ref>. The order from left to right is sorted by frequency of relations in the dataset. Interestingly, there does not seem to be any correlation between performance and frequency of the relation, hinting that specific relations are hard not due to insufficient number of training examples <ref type="figure" target="#fig_0">Figure 12</ref>: Caption: The fire hydrant is facing away from the person. Label: True. All three models predicted correctly. but they are fundamentally challenging for current VLMs. Any relation that requires recognising orientations or facing directions of objects is very hard, e.g., "facing", "facing away from", "parallel to" and "at the back of". As an example, LXMERT failed on the two examples in <ref type="figure" target="#fig_0">Fig. 11</ref> which require understanding the front of a hair drier and a person respectively. In this regard, left-right relations such as "at the left/right side of" and "left/right of" have also suffered since in the annotation scheme we tolerate using either viewer's or object's reference frames. As an example, in <ref type="figure" target="#fig_0">Fig. 10</ref>, all three models predicted False since the potted plant is at the left of the bench if viewer is the reference frame. However, if using the bench as the reference frame, the potted plant is indeed at the right. While generally speaking orientation has been hard, the tested VLMs do well on some seemingly hard cases. E.g., all three models correctly predicted <ref type="figure" target="#fig_0">Fig. 12</ref> -a case that requires compositional zero-shot generalisation capability (to generalise the concept of "face" to a fire hydrant by identifying eyes). <ref type="bibr">8</ref> To get a more high-level understanding of the relations' performance, we group model performance by the meta categories by <ref type="bibr" target="#b9">Fagundes et al. (2021)</ref>: "Adjacency", "Directional", "Orientation", "Projective", "Proximity", "Topological" and "Unallocated" (also shown in <ref type="table" target="#tab_1">Table 1</ref>). The results are shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. Indeed, "Orientation" is the worst performing group on both random and zero-shot splits, and on average all models' performances are at chance level. When comparing random and zero-shot splits, performance has declined to some extent for almost all categories and models (with ViLT suffered the least in "Topological" ). The decrease in "Proximity" is particularly drastic across <ref type="bibr">8</ref> We cannot entirely rule out the possibility that models relied on other spurious cues though.  all models -it declined from close to 75% accuracy in random split to chance level in zero-shot split. "Proximity" contains relations such as "close to", "near" and "far from". We believe it is due to the fact that the notion of proximity is relative and very much dependent on the nature of the concept and its frequent physical context. E.g., for a "person" to be "near" an indoor concept such as "oven" is very different from a person being "near" a frequent outdoor object such as "train" or "truck". Since zero-shot split prevents models seeing test concepts during training, the models have poor grasp of what counts as "close to" or "far from" for these concepts, thus generalising poorly.</p><p>Other Errors. While certain relations are intrinsically hard, we have observed other types of errors that are not bounded to specific relations. Here we give a few examples. Some instances require complex reasoning. In <ref type="figure" target="#fig_0">Fig. 14,</ref> the model needs to recognise that both the cow and the back of the car are in the car's side mirror and also infer the relative position of the back of the car and the cow. As a result, two of the three models predicted wrongly. Some other examples require common sense. E.g., in <ref type="figure" target="#fig_0">Fig. 1, we</ref> can infer the person and the cow's moving direction and can then judge if the person is ahead of the cow. LXMERT failed on this example. In <ref type="figure">Fig. 3 (right)</ref>, the model needs to infer that the main body of the cat is hidden behind the laptop. Interestingly, all three models predicted this example correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sample Efficiency</head><p>In order to understand the correlation between model performance and number of training examples, we conduct sample efficiency analysis on VSR. The results are plotted in <ref type="figure" target="#fig_0">Fig. 15</ref>. For the minimum resource scenario, we randomly sample 100 shots from the training sets of each split. Then we gradually increase the number of training examples to be 10%, 25%, 50% and 75% of the whole training sets. Both LXMERT and ViLT have reasonably good few-shot capability. LXMERT, in particular, reaches 55% accuracy on random split with just 100 shots, and reaches 60% using 10% of random split training set. The zero-shot split is significantly harder and most models appear to have already plateaued at around 75% of training set. For the random split, all models are increasing performance with more data points, though improvement slows down significantly for LXMERT and ViLT after 50% of training data. The fact that LXMERT has the best overall few-shot capability may be suggesting that LXMERT's pre-trained object detector has strong inductive bias for the VSR task as it does not need to learn to recognise concept boundaries and classes from scratch. However, this advantage from LXMERT seems to fade away as the number of training example increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Comparison with synthetic datasets. Syntheic language-vision reasoning datasets, e.g., SHAPES <ref type="bibr" target="#b3">(Andreas et al., 2016)</ref>, CLEVR , NLVR <ref type="bibr" target="#b27">(Suhr et al., 2017)</ref> and SPARTQA <ref type="bibr" target="#b24">(Mirzaee et al., 2021)</ref>, enable full control of dataset generation and could potentially benefit probing of spatial reasoning capability of VLMs. Among them, CLEVR shares the similar goal as us to diagnose VLMs and pinpoint their weaknesses. However, synthetic datasets hugely simply the problem as they have inherently bounded expressivity. In CLEVR, objects can only be spatially related via four relationships: "left", "right", "behind", and "in front" while VSR cover 65 relations. Also, synthetic data do not always accurately reflect the challenges of reasoning in the real world. E.g., objects in synthetic datasets do not have orientations. In real images, orientations matter and human language use depends on that. Also, synthetic images do not take the scene as a context into account. The interpretation of objects relations can depend on such scenes (e.g., the degree of closeness can vary in open space and indoor scenes).</p><p>Also, the vast majority of spatial relationships cannot be determined by rules. Even for the seemingly simple relationships like "left" and "'right', the determination of two objects' spatial relationships can depend on the observer's viewpoint, whether the object has a front, if so, what are their orientations, etc.</p><p>Spatial relations in existing vision-language datasets. Several existing vision-language datasets with natural images also contain spatial relations (e.g., NLVR2, MS COCO, and VQA datasets). <ref type="bibr" target="#b28">Suhr et al. (2019)</ref> summarise that there are 9 prevalent linguistic phenomena/challenges in NLVR2 <ref type="bibr" target="#b28">(Suhr et al., 2019)</ref> such as coreference, existential quantifiers, hard cardinality, spatial relations, etc., and 4 in VQA datasets <ref type="bibr" target="#b5">(Antol et al., 2015;</ref><ref type="bibr" target="#b11">Hudson and Manning, 2019)</ref>. However, the different challenges are entangled in these datasets. Sentences are with complex lexical and syntactic information and can thus conflate different sources of errors, making it hard to identify the exact challenge and preventing categirised analysis. <ref type="bibr" target="#b32">(Yatskar et al., 2016)</ref> extract 6 types of visual spatial relations directly from MS COCO images with annotated bounding boxes. But rule-based automatic extraction can be restrictive as most relations are complex and cannot be identified relying on bounding boxes. Recently, Anonymous (2022) extract captions that contain 28 positional keywords from MS COCO and swap the keywords with their antonyms to construct a challenging probing dataset. However, the COCO captions also have the error-conflation problem. Also, the number of examples and types of relations are restricted by COCO captions.</p><p>There has also been interests in probing models' spatial reasoning capability without visual input. E.g, <ref type="bibr" target="#b8">Collell et al. (2018)</ref>;  probe pretrained text-only models or VLMs' spatial reasoning capabilities with text-only questions. Differently, VSR focuses on the joint understanding of vision and language input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Directions</head><p>We have proposed Visual Spatial Reasoning (VSR), a controlled probing task for testing current visionlanguage models (VLMs)' capabilities of recognising and reasoning about spatial relations in natural image and text pairs. On the VSR benchmark, we tested three popular VLMs and found they lag behind humans by more than 20%. On a more challenging concept zero-shot split, the tested VLMs struggle to reach 60% of accuracy and their performance plateaued even with increased training examples. Among the VLMs, ViLT and LXMERT outperform VisualBERT and we point out that the explicit positonal information in the former two models is crucial in the task. We also performed by-relation analysis and found that the models' performances on certain relation have little correlation with number of training examples and certain rela-tions are inherently more challenging. We identify orientation as the most difficult meta-category of relations for VLMs. Proximity is another challenging relation especially in the zero-shot setup as this relation is highly concept-dependent. We hope the task serves as a handy tool for testing and probing future VLMs.</p><p>In future work, we also plan to adapt the benchmark for investigating whether dual-encoders such as CLIP <ref type="bibr" target="#b25">(Radford et al., 2021)</ref>, ALIGN <ref type="bibr" target="#b12">(Jia et al., 2021)</ref> and LiT <ref type="bibr" target="#b35">(Zhai et al., 2021)</ref> can properly recognise spatial relation. 9 A comparison of dualand cross-encoders' performance on each spatial relation can guide future model design. Very recently, <ref type="bibr">Alayrac et al. (2022)</ref> propose an ultra-large scale VLM. It would be interesting to see if VLM has better spatial reasoning capability when model is scaled up. Another direction is extending the VSR to cover more languages and cultures <ref type="bibr" target="#b19">(Liu et al., 2021;</ref><ref type="bibr" target="#b6">Bugliarello et al., 2022)</ref> and test multilingual VLMs. Along the same line, since we have also collected the metadata of annotators, the VSR corpus can be used as a resource for investigating research questions such as: How is space described among different dialects of English? How is "space" perceived among different populations? We hope that VSR can also serve as a basis for future cross-lingual and cross-cultural Socio-Linguistic research.   Relations included and excluded. Our used set of relations is adapted from <ref type="bibr" target="#b9">Fagundes et al. (2021)</ref>.</p><p>In <ref type="table" target="#tab_7">Table 4</ref>, we copy their original table for reference. We also list the exact modifications we made on top of the original table in <ref type="table" target="#tab_8">Table 5</ref>. The major reasons for excluding relations are: (1) rarely used in describing spatial information of images (e.g., "north of"), (2) repeated with other relations (e.g., "front of" and "in front of"). In order to convert relations into templates, we also inserted some words to make them grammatical. We included several additional relations since we found them very frequent in MS COCO (e.g., "at the edge of" and "touching").</p><p>Full statistics of the relations in VSR. We showed the relation distribution of VSR in the main text <ref type="figure" target="#fig_3">(Fig. 6)</ref>. We list the exact numbers of each relations in <ref type="table" target="#tab_9">Table 6</ref>.</p><p>Genuinely ambiguous annotation instances. As mentioned in the main text, we analysed 100 examples with high disagreement among annotators and found that the majority of the case are truly ambiguous and cannot be assigned a definitive True/False label. Here we show two concrete examples. In <ref type="figure" target="#fig_0">Fig. 18</ref>, we can infer that the keyboard is horizontally lower than the cat. However, the keyboard is not right under the cat's body (but a bit in front of the cat). The ambiguity results from the semantics of the word "below" which can mean both (1) horizontally lower (but not necessarily right under) and also (2) right under. In <ref type="figure" target="#fig_0">Fig. 19</ref>, if seat surface is viewed as the main body of the chair, we can infer that the sandwich is indeed above the chair. However, it is not above all parts of the chair (it is no higher than the backrest of the chair). This case depends on a person's subjective understanding of what counts as "above the chair" and is thus also intrinsically ambiguous.</p><p>While we have excluded such cases in the standard VSR train/val/test split to ensure the dataset has human consensus, we also release the raw dataset without excluding these ambiguous cases for researchers who are interested in studying them further.</p><p>Hyperparameters and pretrained models. The hyperparameters we used for training all three VLMs and the links to the initialisation weights of the three models are listed in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>Model performance on dev sets. For transparency, we also list the models' dev set performances in <ref type="table" target="#tab_11">Table 8</ref>. The gap between dev and tests become much greater on zero-shot split likely due to the size of both dev and test sets have become smaller, making the evaluation less stable.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Caption: The person is ahead of the cow. Label: True.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Caption: The cat is inside the toilet. Label: False.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Caption: The person is at the left side of the bus. Label: True.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Relation distribution of the final dataset (sorted by frequency). Top 40 most frequent relations are included. See Appendix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Concept distribution. Only concepts with &gt; 100 frequencies at subject and object positions are included in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Disagreement ratio of each relation among annotators (sorted by disagreement ratio, the higher the more disagreement). Only relations with &gt; 20 data points are included in thefigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Explicit positional information matters. Both LXMERT and ViLT outperform VisualBERT by large margins (&gt;10%) on both splits. This is expected since LXMERT and ViLT encode explicit positional information while VisualBERT does not. LXMERT has position features as part of the input which encodes the relative coordinates of objects within the image. ViLT slices an image into patches (instead of object regions) and use positional encodings to signal the patches' relative positions. VisualBERT, however, has no explicit position encoding. Very recently, Anonymous (2022) (under review) also highlights the importance of positional (a) random split (b) zero-shot split</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Performance by relation on the random (upper) and zero-shot (lower) split test sets. Relation order sorted by frequency (high to low from left to right). Only relations with more than 15 and 5 occurrences on the random and zero-shot tests respectively are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Caption: The potted plant is at the right side of the bench. Label: True. All models predicted False.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>(Left) Caption: The hair drier is facing away from the person. Label: False. (Right) Caption: The bench is in front of the person. Label: True. LXMERT failed on both examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Performance by meta categories of relations, on the random (left) and zero-shot (right) split test sets. For legend information, seeFig. 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Caption: The cow is at the back of the car. Label: True. LXMERT and VisualBERT predicted False.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>zero-shot split Figure 15: Sample efficiency analysis: model performance under different amount of training data (100shot, 10% of training set till 100% training set). Results on both the random and zero-shot split test sets are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>A screenshot of the annotation interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>A screenshot of the validation interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Caption: The keyboard is below the cat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :</head><label>19</label><figDesc>Caption: The sandwich is above the chair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The available 71 spatial relations. 65 of them appear in our final dataset. Relations with * are not used.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Data statistics of the random and zero-shot splits.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Model performance on VSR. Test set perfor- mances of both random and zero-shot splits are listed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>AdjacencyAdjacent, alongside, side, right side, left side, attached, back side, front of, ahead, against Cardinal direction North of, south side of, east-west, easterly, north-south, south of , detached, has part, part of, contains, within, at, on, in, with, surround, among, consists of, out of, between, inside, outside UnallocatedBeyond, next to, opposite, after, among, enclosed by</figDesc><table><row><cell>Directional</cell><cell>Off, past, toward, down, deep down, up, away from, along, around, from, into, to, across, across from, through, down from</cell></row><row><cell>Orientation</cell><cell>Facing, orientation, parallel, perpendicular</cell></row><row><cell>Projective</cell><cell>On top of, beneath, in back of, beside, behind, left of, right of, under, in front of, below, above, over</cell></row><row><cell>Proximity</cell><cell>By, close to, near, far, far from, far away from</cell></row><row><cell>Topological</cell><cell>Congruent, connected</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>original table from<ref type="bibr" target="#b9">Fagundes et al. (2021)</ref>. alongside, at the side of, at the right side of, at the left side of, attached to, at the back side of, front of, ahead of, against, at the edge of Cardinal direction North of, south side of, east-west, easterly, north-south, south ofDirectional Off, past, toward, down, deep down, up, away from, along, around, from, into, to, across, across from, through, down from Orientation Facing, facing away from, orientation, parallel to, perpendicular to Projective On top of, beneath, in back of, beside, behind, left of, right of, under, in front of, below, above, over, in the middle of Proximity By, close to, near, far, far from, far away from Topological Congruent, connected to, detached from, has as a part, part of, contains, within, at, on, in, with, surrounding, among, consists of, out of, between, inside, outside, touching UnallocatedBeyond, next to, opposite to, after, among, enclosed by</figDesc><table><row><cell>Category</cell><cell>Spatial Relations</cell></row><row><cell>Adjacency</cell><cell>Adjacent to,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Our used table. What we added. What's deleted.</figDesc><table><row><cell>relation</cell><cell>#</cell></row><row><cell>touching</cell><cell>1,229</cell></row><row><cell>behind</cell><cell>657</cell></row><row><cell>in front of</cell><cell>642</cell></row><row><cell>on</cell><cell>568</cell></row><row><cell>under</cell><cell>557</cell></row><row><cell>on top of</cell><cell>486</cell></row><row><cell cols="2">at the right side of 421</cell></row><row><cell>at the left side of</cell><cell>365</cell></row><row><cell>contains</cell><cell>326</cell></row><row><cell>beneath</cell><cell>323</cell></row><row><cell>above</cell><cell>314</cell></row><row><cell>next to</cell><cell>286</cell></row><row><cell>facing</cell><cell>280</cell></row><row><cell>in</cell><cell>257</cell></row><row><cell>below</cell><cell>247</cell></row><row><cell>inside</cell><cell>232</cell></row><row><cell>far away from</cell><cell>216</cell></row><row><cell>at the edge of</cell><cell>198</cell></row><row><cell>left of</cell><cell>183</cell></row><row><cell>beside</cell><cell>170</cell></row><row><cell>facing away from</cell><cell>166</cell></row><row><cell>away from</cell><cell>138</cell></row><row><cell>far from</cell><cell>134</cell></row><row><cell>part of</cell><cell>108</cell></row><row><cell>near</cell><cell>99</cell></row><row><cell>right of</cell><cell>97</cell></row><row><cell>close to</cell><cell>88</cell></row><row><cell>across from</cell><cell>88</cell></row><row><cell>surrounding</cell><cell>88</cell></row><row><cell>at the back of</cell><cell>83</cell></row><row><cell>parallel to</cell><cell>82</cell></row><row><cell>in the middle of</cell><cell>75</cell></row><row><cell>over</cell><cell>74</cell></row><row><cell>adjacent to</cell><cell>67</cell></row><row><cell>off</cell><cell>63</cell></row><row><cell>perpendicular to</cell><cell>62</cell></row><row><cell>attached to</cell><cell>51</cell></row><row><cell>by</cell><cell>49</cell></row><row><cell>at the side of</cell><cell>48</cell></row><row><cell>alongside</cell><cell>48</cell></row><row><cell>against</cell><cell>42</cell></row><row><cell>ahead of</cell><cell>35</cell></row><row><cell>consists of</cell><cell>34</cell></row><row><cell>toward</cell><cell>33</cell></row><row><cell>within</cell><cell>33</cell></row><row><cell>outside</cell><cell>32</cell></row><row><cell>connected to</cell><cell>32</cell></row><row><cell>opposite to</cell><cell>30</cell></row><row><cell>into</cell><cell>29</cell></row><row><cell>has as a part</cell><cell>28</cell></row><row><cell>enclosed by</cell><cell>20</cell></row><row><cell>with</cell><cell>18</cell></row><row><cell>beyond</cell><cell>16</cell></row><row><cell>across</cell><cell>12</cell></row><row><cell>down from</cell><cell>11</cell></row><row><cell>detached from</cell><cell>10</cell></row><row><cell>out of</cell><cell>9</cell></row><row><cell>around</cell><cell>6</cell></row><row><cell>at</cell><cell>5</cell></row><row><cell>past</cell><cell>5</cell></row><row><cell>along</cell><cell>5</cell></row><row><cell>between</cell><cell>5</cell></row><row><cell>down</cell><cell>2</cell></row><row><cell>among</cell><cell>1</cell></row><row><cell>total</cell><cell>10,119</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Number of instances of all relations in the final VSR dataset. Sorted from high to low.</figDesc><table><row><cell>model</cell><cell cols="3">lr batch size epoch</cell><cell>token length</cell></row><row><cell cols="2">VisualBERT 2e-6</cell><cell>32</cell><cell>100</cell><cell>32</cell></row><row><cell cols="2">LXMERT 1e-5</cell><cell>32</cell><cell>100</cell><cell>32</cell></row><row><cell>ViLT</cell><cell>1e-5</cell><cell>12</cell><cell>30</cell><cell>max</cell></row><row><cell>model</cell><cell cols="3">Huggingface URL</cell><cell></cell></row><row><cell cols="5">VisualBERT huggingface.co/uclanlp/visualbert-nlvr2-coco-pre</cell></row><row><cell cols="5">LXMERT huggingface.co/unc-nlp/lxmert-base-uncased</cell></row><row><cell>ViLT</cell><cell cols="4">huggingface.co/dandelin/vilt-b32-mlm</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>A listing of hyperpamters used for all VLMs ("'lr': learning rate) and URLs of the three VLMs' pretrained weights used in this work. VisualBERT 59.2?0.9 57.4?0.9 57.4?2.2 54.0?1.3 LXMERT 73.8?1.2 72.5?1.4 69.2?1.0 63.2?1.7 ViLT 71.9?1.3 71.0?0.7 66.7?1.7 62.4?1.5</figDesc><table><row><cell></cell><cell cols="2">random split</cell><cell cols="2">zero-shot split</cell></row><row><cell>model?</cell><cell>dev</cell><cell>test</cell><cell>dev</cell><cell>test</cell></row><row><cell>human</cell><cell></cell><cell>95.4</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Model performance on VSR. Results of both random and zero-shot splits, both validation and tests are listed. (Dev performance using the best checkpoint on dev set.)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The actual interface for annotation is in AppendixFig. 16.2  The details of each adaptation is listed in Appendix,Table 5.3  The actual validation interface is in AppendixFig. 17.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In the end, the ratio for holding a Bachelor/Master/PhD as highest degree is: 15.9%/70.5%/13.6%.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Some recent evidence from<ref type="bibr" target="#b26">Subramanian et al. (2022)</ref> suggest that spatial reasoning is very challenging for CLIP.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Qian Wang and Rongtian Ye for helping trial the annotation scheme; Zihao Fu for helping set up the annotation server. The project is funded by Cambridge Language Sciences Incubator Fund. FL is supported by by Grace &amp; Thomas C.H.Chan Cambridge Scholarship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Screenshots of the annotation interface. We use the open-sourced label studio (labelstud. io) for managing our annotation tasks. Two screenshots are shown in <ref type="figure">Fig. 16 (caption generation)</ref> and <ref type="figure">Fig. 17 (validation)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Words aren&apos;t enough, their order matters: On the robustness of grounding visual referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.586</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6555" to="6565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">Barr</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<imprint>
			<pubPlace>Malcolm Reynolds</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<editor>Vinyals, Andrew Zisserman, and Karen Simonyan. 2022</editor>
		<imprint/>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.12</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Probing the role of positional information in vision-language models</title>
		<idno>Anonymous. 2022</idno>
		<imprint>
			<publisher>ARR submission</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">IGLUE: A benchmark for transfer learning across modalities, tasks, and languages. ArXiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vuli?</surname></persName>
		</author>
		<idno>abs/2201.11732</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">UNITER: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1909.11740</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acquiring common sense spatial knowledge through implicit spatial templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
			<biblScope unit="page" from="6765" to="6772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A cross-linguistic study of spatial location descriptions in new zealand english and brazilian portuguese natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiane Kutianski Marchi</forename><surname>Fagundes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciene</forename><surname>Delazari</surname></persName>
		</author>
		<idno type="DOI">10.1111/tgis.12815</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. GIS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3159" to="3187" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.670</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6325" to="6334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00686</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.215</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levinson</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511613609</idno>
		<title level="m">Space in Language and Cognition: Explorations in Cognitive Diversity. Language Culture and Cognition</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">VisualBERT: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1908.03557</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<title level="m">Microsoft COCO: Common objects in context. ArXiv preprint</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visually grounded reasoning across languages and cultures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elliott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.818</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10467" to="10485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clevr-ref+: Diagnosing visual reasoning with referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00431</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="4185" to="4194" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Things not written in text: Exploring spatial commonsense from visual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno>abs/2203.08075</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SPARTQA: A textual question answering benchmark for spatial reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roshanak</forename><surname>Mirzaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Hossein Rajaby Faghihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kordjamshidi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.364</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4582" to="4598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ReCLIP: A strong zero-shot baseline for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2204.05991</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A corpus for reasoning about natural language grounded in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1644</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6418" to="6428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Visual entailment: A novel task for fine-grained image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/1901.06706</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stating the obvious: Extracting visual common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1608.00272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00688</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="6720" to="6731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Lit: Zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ArXiv preprint, abs/2111.07991</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
