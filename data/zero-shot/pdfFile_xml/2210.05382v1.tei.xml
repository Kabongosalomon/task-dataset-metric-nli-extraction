<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BREAK THE WALL BETWEEN HOMOPHILY AND HET- EROPHILY FOR GRAPH REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
							<email>lijunzhang@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Guan</surname></persName>
							<email>huiguan@cs.umass.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BREAK THE WALL BETWEEN HOMOPHILY AND HET- EROPHILY FOR GRAPH REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homophily and heterophily are intrinsic properties of graphs that describe whether two linked nodes share similar properties. Although many Graph Neural Network (GNN) models have been proposed, it remains unclear how to design a model so that it can generalize well to the whole spectrum of homophily. This work addresses the challenge by identifying three graph features, including the ego node feature, the aggregated node feature, and the graph structure feature, that are essential for graph representation learning. It further proposes a new GNN model called OGNN (Omnipotent Graph Neural Network) that extracts all three graph features and adaptively fuses them to achieve generalizability across the whole spectrum of homophily. Extensive experiments on both synthetic and real datasets demonstrate the superiority (average rank 1.56) of our OGNN compared with state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have been proven to be a powerful approach to learning graph representations for node classification tasks <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>. Researchers have proposed different GNN model designs based on the underlying assumption of either graph homophily or heterophily. On the one hand, many works <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b8">Hamilton et al., 2017;</ref><ref type="bibr" target="#b30">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b6">Gao et al., 2018;</ref><ref type="bibr" target="#b12">Klicpera et al., 2018;</ref><ref type="bibr" target="#b34">Xu et al., 2018b;</ref><ref type="bibr" target="#b32">Wu et al., 2019)</ref> assume strong graph homophily, which means linked nodes in the graph tend to share similar properties or labels. These GNNs adopt a message passing paradigm that recursively propagates and aggregates node features through the edges in the graph to produce smoothed node representations <ref type="bibr" target="#b7">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018)</ref>. We refer to these GNNs as homophily-based GNNs.</p><p>On the other hand, some recent works empirically demonstrate the poor performance of homophilybased GNNs on heterophilic graphs, which have the opposite property to homophily. Examples of these graphs include online transaction networks where fraudsters are more likely to connect with customers than their colleagues, and protein networks where different amino acid types tend to be connected. GNNs designed for heterophilic graphs leverage various strategies to generalize better on these graphs, such as embedding and mixing graph topology with node features <ref type="bibr" target="#b17">(Lim et al., 2021a)</ref>, ego-neighbor embedding separation, and higher-order neighborhood representation combination <ref type="bibr">(Zhu et al., 2020)</ref>, degree correction and signed messages <ref type="bibr" target="#b35">(Yan et al., 2021)</ref>, and generalized PageRank weights <ref type="bibr" target="#b4">(Chien et al., 2020)</ref>. They perform much better on graphs with a strong heterophily property but achieve only comparable, if not worse, accuracy on homophilic graphs than homophily-based GNNs. We refer to these GNNs as heterophily-based GNNs.</p><p>Neither homophily-based GNNs nor heterophily-based GNNs are ideal, since they invisibly establish a wall between homophily and heterophily for graph representation learning -a GNN can work well on either heterophilic graphs or homophilic graphs, but not both. Real-world graph data, however, could have various levels of homophily. It is a hassle to first classify a graph as homophilic or heterophilic before a suitable GNN model can be identified. What's worse, some graphs cannot be easily classified as homophilic or heterophilic. For example, twitch-gamer <ref type="bibr" target="#b25">(Rozemberczki &amp; Sarkar, 2021</ref>) is a social network graph in which the users follow each other by their game interests, while the nodes (i.e., users) are labeled by gender. Since the following relationships (i.e., the connections) are not dominated by gender (i.e., the label), the homophily of twitch-gamer is 0.55, which falls in the ambiguous intermediate region on the homophily spectrum. One needs to try both homophily-based GNNs and heterophily-based GNNs to identify the suitable models, significantly increasing data analytic costs.</p><p>To address the problems, this paper proposes a model called omnipotent GNN (OGNN) that can generalize well to the whole spectrum of homophily. The design of OGNN is motivated by the observation that each node in a graph can be modeled by different types of features, including the feature of itself, the feature of its neighbors, and its connections with the other nodes. These features are of different importance in determining a node's property depending on the graph's homophily. The basic idea of OGNN is to effectively transform and adaptively fuse these features to derive each node's representation in the graph. Our main contributions are summarized as follows:</p><p>? We identify three aspects of the graph features that are essential for graph representation learning: ego node feature, aggregated node feature, and graph structure feature. We analyze the importance of these graph features and propose a general form for extracting them. Our detailed ablation study demonstrates that these features have different importance for the graphs with different homophily. ? We propose the OGNN model that generalizes to graphs over the homophily spectrum.</p><p>OGNN could effectively learn graph representation by integrating the three graph features with adaptive feature fusion under a bi-level optimization framework. ? We conduct extensive experiments to compare OGNN with state-of-the-art GNNs using synthetic and real graph benchmarks that cover the full homophily spectrum. OGNN outperforms 8 baseline models and achieves an average rank of 1.56 on 9 real datasets. Specifically, OGNN achieves higher node classification accuracy, 4.55% higher than GCN (Kipf &amp; Welling, 2016), 4.39% higher than MIXHOP (Abu-El-Haija et al., 2019), and 3.27% higher than <ref type="bibr">H2GCN (Zhu et al., 2020)</ref>, on average over the real datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATIONS AND PRELIMINARIES</head><p>Notations. Let G = (V, E) denotes a graph with N nodes and M edges, where V and E are the set of nodes and edges respectively, |V| = N and |E| = M . We use A ? {0, 1} N ?N as the adjacency matrix where A</p><formula xml:id="formula_0">[i, j] = 1 if (v i , v j ) ? E otherwise A[i, j] = 0. Each node v i ? V has a raw feature vector x i of size D.</formula><p>The raw feature vectors of all nodes in the graph form a feature matrix X ? R N ?D . The nodes are categorized into C classes. The label of a node v i of class k is represented by a one-hot vector y i of size C, whose k-th dimension is 1 while the other dimensions are 0. All the label vectors form the label matrix Y ? {0, 1} N ?C . This paper focuses on the node classification task <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016)</ref>. The goal of the node classification task is, given a train set V train ? V with label set Y train , learn a mapping F : V ? Y, which maximizes the possibility P (arg max F(v) = arg max y v ) for any vertex v ? V \ V train .</p><p>Homophily and Heterophily. Graph homophily H measures the overall similarity between the nodes connected by an edge in terms of the labels. There are multiple ways for computing graph homophily <ref type="bibr" target="#b24">(Pei et al., 2020;</ref><ref type="bibr">Zhu et al., 2020;</ref><ref type="bibr" target="#b18">Lim et al., 2021b;</ref><ref type="bibr" target="#b1">Apollonio et al., 2022)</ref>. In this paper, we adopt the most widely-used edge homophily (Zhu et al., 2020): Definition 1. Given a graph G = (V, E) with labels Y, the edge homophily is defined as</p><formula xml:id="formula_1">H edge (G, Y) = 1 |E| (u,v)?E 1(y u = y v )</formula><p>, which represents the fraction of the edges that connect two nodes with the same class label.</p><p>The edge homophily ranges from 0 to 1. Graphs with edge homophily close to 1 are called homophilic graphs, while the ones with edge homophily close to 0 are called heterophilic graphs.</p><p>Graph Neural Networks. Most GNNs follow the message passing framework. In the message passing framework, the hidden state of a node v ? V depends on the features of its neighbors and its ego feature. A typical message passing layer is</p><formula xml:id="formula_2">h l+1 v = ? l (h l v , ? l u?N (v) (?(h l u , h l v ))),<label>(1)</label></formula><p>where h l represents the hidden feature of a node at layer l, ? and ? are transformation functions, which could be any differentiable functions like linear transformations or multi-layer perceptron (MLP). ? is a permutation invariant function such as sum, mean, and max, while N (v) is the node set of v's neighborhoods. The message passing layer transforms the features from the neighborhoods with ? as the messages and then aggregates them with ?. After that, ? updates the node feature with the ego feature and the aggregated message. By stacking multiple message passing layers in the model, GNNs could iteratively propagate information to the target node from multi-hop neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH REPRESENTATION LEARNING OVER DIFFERENT HOMOPHILY</head><p>Although many different GNNs have been proposed, most of them are designed under the assumption of either strong homophily or heterophily, making them incapable of generalizing well to a whole spectrum of homophily. To illustrate our statement, we conduct experiments to validate the classification accuracy of several GNN models on a synthetic benchmark syn-cora <ref type="bibr">(Zhu et al., 2020)</ref>. syn-cora provides homophily levels varying from 0.0 to 1.0 with 0.1 as the interval. The nodes and their raw features of syn-cora are from the Cora dataset <ref type="bibr" target="#b26">(Sen et al., 2008;</ref><ref type="bibr" target="#b36">Yang et al., 2016)</ref>, while the edges are randomly generated according to different homophily settings. Details about syn-cora are in Section 5.1. <ref type="figure" target="#fig_0">Figure 1</ref> shows the result of different GNNs and our proposed OGNN on syn-cora with varying homophily levels. Overall, most existing GNNs cannot generalize well over the spectrum of homophily. Specifically, <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref>, and DAGNN  achieve high accuracy on graphs with high homophily, but their performance drops severely as the homophily decreases. When the homophily is lower than 0.5, their performance is even worse than Multi-Layer Perceptron (MLP) which relies solely on the node features without any structural information. Meanwhile, LINKX <ref type="bibr" target="#b17">(Lim et al., 2021a)</ref> outperforms MLP, DAGNN, and GCN on graphs with low homophily, but its performance on the graphs with strong homophily, i.e., 0.7 ? 1.0 is the worst besides MLP.</p><p>Both <ref type="bibr">H2GCN (Zhu et al., 2020)</ref> and GPR-GNN <ref type="bibr" target="#b4">(Chien et al., 2020)</ref> are designed to adapt to both homophilic and heterophilic graphs. However, GPR-GNN cannot compete with MLP on low edge homophily because their model design still follows the message passing paradigm. Although H2GCN is the winning solution from existing GNNs on the syn-cora dataset, it is outperformed by our proposed approach OGNN. Moreover, H2GCN shows worse performance on real-world heterophilic graphs compared to heterophily-based GNNs, e.g., LINKX (see Section 5.2). We suspect that the reason is that H2GCN does not leverage graph structure features as in LINKX and our approach OGNN. In Section 5.2, we empirically show that graph structure is an important feature for learning on real-world heterophilic graphs.</p><p>To summarize, existing GNNs hardly show consistently good performance on graphs with various homophily settings. Therefore, in this paper, we aim at designing a new GNN that can generalize well across graphs with different homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OGNN: A GNN FOR GRAPHS WITH DIFFERENT HOMOPHILY</head><p>In this section, we first analyze three types of features from a graph: the ego-node feature, the aggregated neighborhood feature, and the graph structure feature, which cover multi-faceted information for graph representation learning. Then we present our model OGNN which effectively extracts the three types of features and adaptively fuses them with a bi-level optimization training scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GRAPH INFORMATION ANALYSIS</head><p>Ego-Node Feature. The ego-node feature refers to a node's embedding resulting from transforming the node's raw feature without considering its neighborhoods. Formally, the ego-node feature h ego of a node v with its raw feature x v is:</p><formula xml:id="formula_3">h ego v = f ego (x v ),<label>(2)</label></formula><p>where f ego (?) can be any transformation functions such as linear transformation or MLPs.</p><p>The ego-node feature is the basic and essential information for node classification tasks. This intuition is based on two observations. First, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, applying MLP on only raw node features of heterophilic graphs can significantly outperform many homophily-based GNNs <ref type="bibr" target="#b11">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b30">Veli?kovi? et al., 2017;</ref> that rely on both the raw node features and the aggregated neighborhood features. This empirical observation implies that the ego-node feature alone plays an important role in GNN's generalizability. Second, a recent study <ref type="bibr">(Zhu et al., 2020)</ref> proves that, under some conditions, a GCN layer that separately embeds ego-node features and neighbor-node features is more capable of generalizing to heterophilic graphs than co-embedding them. It is because, in heterophilic settings, the class labels and the raw features of a node and its neighborhoods may be different. The ego-node feature could be a more robust signal for the node's label than the mixture of features from the node itself and its neighborhood.</p><p>We empirically validate this intuition in Section 5.2 and demonstrate that (1) GNNs with ego-node features can achieve 0.25% ? 4.30% higher accuracy than those without ego-node features across graphs with different levels of homophily.</p><p>(2) The importance of ego-node features increases as the homophily of a graph becomes lower. For example, on syn-cora graph with 0.8 homophily, the importance of ego-node features is 0.197 out of 1, while with 0.2 homophily, the importance increases to 0.656.</p><p>Aggregated Neighborhood Feature. The aggregated neighborhood feature complements ego-node features by capturing the information from the neighborhood to the target node. Formally, the aggregated neighborhood feature h agg of a node v with raw feature x v is</p><formula xml:id="formula_4">h agg v = f agg (x v , {x u , u ? N k (v), k = 1, 2, 3, ? ? ? }),<label>(3)</label></formula><p>where N k (v) represents the set of k-hop neighbors and f agg (?) is a function that transforms and aggregates the features from the neighborhood as well as the target node's own feature.</p><p>There are many approaches to materialize the function f agg (?). One approach is to simply stack multiple message passing layers as in many homophily-based GNNs such as GCN and GAT. Each message passing layer first transforms the features and then aggregates them to pass to the next layer, as in Eq. 1. However, this approach allows only a limited size of neighborhoods to be considered since the performance of these GNNs degrades severely when more than three layers are stacked <ref type="bibr" target="#b15">(Li et al., 2018;</ref><ref type="bibr">Zhang et al., 2022</ref>). An alternative approach is a decoupled transformation and aggregation strategy where raw features are transformed first and then go through multiple aggregation layers without being transformed again. Recent works <ref type="bibr" target="#b12">(Klicpera et al., 2018;</ref> demonstrate that the alternative approach could achieve much better generalization performance on homophilic graphs as it can effectively capture the information from a large neighborhood region. We will also adopt this strategy in OGNN.</p><p>Our empirical study shows that (1) GNNs with aggregated neighborhood features can achieve 0.13% ? 13.96% higher accuracy than those without aggregated features across graphs with different homophily, and (2) the importance of the aggregated neighborhood features increases as the homophily of a graph becomes higher. For example, on syn-cora graph with 0.2 homophily, the importance of aggregated features is 0.284 out of 1, while with 0.8 homophily, the importance increases to 0.772.</p><p>Graph Structure Feature. Although the aggregated neighborhood feature involves the local connections around the target node, it loses a fair amount of the graph structural information because of the permutation invariant aggregation process. Therefore, it is necessary to take the graph structure information as an independent information source for graph representation learning. Formally, the graph structure feature is:</p><formula xml:id="formula_5">h strc v = f strc ({A k [v, :], k = 1, 2, 3, ? ? ? }),<label>(4)</label></formula><p>where A k is the k-th power of the adjacency matrix A, A k [v, :] is the v-th row of A k , and f strc (?) could be any transformation functions.</p><p>Graph structure feature has been shown to be very effective in node classification tasks on heterophilic graphs. Both LINK and LINKX <ref type="bibr">(Zheleva &amp; Getoor, 2009;</ref><ref type="bibr" target="#b17">Lim et al., 2021a)</ref> leverage the simplest form of graph structure feature, the adjacency matrix, to learn node embedding and show better performance than many GNNs on heterophilic graphs <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2019;</ref><ref type="bibr" target="#b4">Chien et al., 2020;</ref><ref type="bibr">Zhu et al., 2020)</ref>. However, these works consider only k = 1, i.e., the adjacency matrix with only 1-hop neighbors. In this work, We propose to use the combination of different powers of the adjacency matrix as a more general form of graph structure feature. The intuition behind it is that k = 1 captures local structure information while k &gt; 1 captures regional/global structure information.</p><p>Our empirical study shows that (1) on all the 9 real-world graphs we experiment with, the best performance is achieved when k is larger than one, and (2) graph structure features can improve classification accuracy by 0.78% ? 20.49% across the graphs with different homophily.</p><p>To sum up, the three types of features summarize distinct graph information: the ego-node feature and the aggregated neighborhood feature model the information from a node's feature and its neighborhoods respectively; the graph structure feature models both the local and global structure information regardless of the node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OGNN MODEL DESIGN</head><p>OGNN effectively extracts all three types of graph features and adaptively fuses them together to achieve good generalizability across the whole spectrum of homophily. We now explain its three main components, feature extractors, adaptive feature fusion, and bi-level optimization in details.</p><p>Feature Extractors. The Ego-Node Feature Extractor is a simple linear transformation of the raw node feature matrix X ? R N ?D : H ego = XW ego , where W ego ? R D?d is the transformation matrix and H ego ? R N ?d is the extracted ego-node features. We use linear transformation instead of MLP because linear transformation consistently achieves the best accuracy in our empirical evaluation.</p><p>The Aggregated Neighborhood Feature Extractor adopts the state-of-the-art design in the message passing framework, where feature transformation and propagation are decoupled <ref type="bibr" target="#b37">(Zeng et al., 2021)</ref>. It propagates the ego-node features H ego and combines the features of different propagation steps to enlarge the receptive field:</p><formula xml:id="formula_6">H agg = s1 i=1? i H ego , where? = D ? 1 2 AD ? 1 2</formula><p>is the normalized adjacency matrix, and D is the diagonalized node degrees. s 1 is the maximum propagation step.</p><p>The Graph Structure Feature Extractor computes the combination of different powers of the adjacency matrix after a linear transformation: H strc = s2 j=1 A j W strc , where W strc ? R N ?d is the transformation matrix, and s 2 is the maximum power of the adjacency matrix. The use of a simple linear transformation is for its efficiency, as we do not observe performance gains from an MLP.</p><p>Adaptive Feature Fusion. The three types of features play different roles in modeling graph information and thus their importance in different homophily settings varies. The adaptive weighted feature fusion module assigns a trainable scalar importance score for each feature so that it can automatically learn the features' importance from the input graph:</p><formula xml:id="formula_7">H = ?(? 1 H ego + ? 2 H agg + ? 3 H strc ), ? i = exp p i 3 j=1 exp p j ,<label>(5)</label></formula><p>where H is the fused feature and ? is a non-linear activation function ReLU. P = {p i |i = 1, 2, 3} are trainable parameters, and {? i |i = 1, 2, 3} are the weights for each feature.</p><p>After obtaining the fused feature H, we predict the labels for each node with a linear classifier, Y pred = Softmax(HW pred ), where Y pred ? R N ?C is the predictions and W pred ? R d?C is the predictor's parameters.</p><p>Bi-level Optimization. To train our model parameters and feature fusion weights jointly, we borrow the idea of bi-level optimization <ref type="bibr" target="#b19">(Liu et al., 2018;</ref><ref type="bibr" target="#b5">Dong &amp; Yang, 2019</ref>). Suppose the model parameters is W and the parameters for feature fusion is P. The loss function of the node classification tasks is:</p><formula xml:id="formula_8">L(W, P, G, X, Y) = ? 1 |Y| yi?Y y T i log(? i ),<label>(6)</label></formula><p>where G is the graph, X and Y are raw node features and ground-truth labels respectively, and y i ? Y pred is the prediction of our model for node i. Then the objective of our bi-level optimization is: min</p><formula xml:id="formula_9">P L valid (W * , P, G, X valid , Y valid ), s.t. W * = arg min W L train (W, P, G, X train , Y train ).<label>(7)</label></formula><p>In short, we optimize the model parameters W on the train set, while optimizing the feature fusion parameters P on the validation set alternatively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct experiments on both synthetic datasets and real datasets with various homophily to examine the efficacy of our model in terms of test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EXPERIMENTAL SETTINGS</head><p>Synthetic Dataset. We generate synthetic graphs syn-cora with the approach in H2GCN (Zhu et al., 2020). The syn-cora dataset provides 11 graphs with homophily ranging from 0.0 to 1.0 with 0.1 as the interval. The raw node features and labels for each graph are sampled from the cora dataset <ref type="bibr" target="#b26">(Sen et al., 2008)</ref>. The edges of the graph are generated gradually according to the given homophily. We evaluate the average test accuracy over five trials for all the methods on these graphs with the same train/validation/test data splits (25%, 25%, 50%, for each class).</p><p>Real Dataset. We also evaluate our method and existing GNNs on 9 real-world datasets, whose homophily ranges from 0.222 ? 0.931.  <ref type="bibr" target="#b18">(Lim et al., 2021b;</ref><ref type="bibr">a)</ref>. The data splits of these datasets are explained in Appendix Section A. We report the average and the standard deviation of the test accuracy in the following experiments.  <ref type="bibr" target="#b4">(Chien et al., 2020)</ref>). Our experiments also include MLP because it is a strong baseline for heterophilic graphs. We did grid-based hyper-parameter search for all baselines and our approach (Details in Appendix Section C).</p><p>Hardware Specifications. We run experiments on both synthetic and real world benchmarks with a 12-core CPU, 8 GB Memory, and an NVIDIA GeForce GTX 1080 Ti GPU with 11 GB GPU Memory for all the methods except for H2GCN, because it suffers from the out-of-memory (OOM) problem. For H2GCN, we use a workstation with a 12-core CPU, 32 GB Memory, and an NVIDIA Quadro RTX 8000 GPU with 48 GB GPU Memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">QUANTITATIVE RESULTS</head><p>Results on Synthetic Dataset. <ref type="table" target="#tab_1">Table 1</ref> reports the average test accuracy over five random splits on the graphs in the syn-cora dataset. Overall, OGNN outperforms the existing methods on most homophily settings, with seven settings at top-1 and four at top-2. Especially, in the ambiguous homophily region (about 0.4 ? 0.6), OGNN achieves state-of-the-art results by pushing the best accuracy up to 2.76%. A full version with standard deviation can be found in Appendix Section D.</p><p>The results echo our analysis in Section 3. The performance of homophily-based GNNs such as GCN, GAT and DAGNN are outstanding at high homophily settings. However, they fail to perform well on the heterophilic graphs and are much worse than the baseline MLP, which is graph-agnostic. LINKX is a simple but strong baseline on non-homophilic graphs, but it performs poorly on high homophily settings (? 0.7), merely better than MLP. For H2GCN, MIXHOP, and GPR-GNN, which are designed to generalize well on both homophilic and heterophilic graphs, they show variant performance in our experiments. MIXHOP and GPR-GNN cannot achieve as good accuracy as they claimed on low homophily settings, reflecting their unsatisfying generalization performance. H2GCN shows the most competitive performance among existing GNNs, especially on very low homophily settings (0.0 and 0.1). However, OGNN outperforms it on a larger range of homophily (0.2 ? 0.8), which are also more common cases in real-world datasets. Results on Real Dataset. <ref type="table" target="#tab_2">Table 2</ref> reports the results on the 9 real datasets. Overall, OGNN achieves seven top-1 and one top-2 over 9 datasets, with an average rank of 1.56. Compared with homophily-based GNNs including GCN, GAT, and DAGNN, our method outperforms the best of them (i.e., DAGNN) on the homophilic graphs and is far better than any of them on the heterophilic graphs. OGNN is also competitive compared to the heterophily-based GNNs, including LINKX, H2GCN, MIXHOP, and GPR-GNN. On penn94 and arXiv-year, we improve the accuracy by 0.72 ? 4.31% and 0.5 ? 11.61% respectively. On genius and twitch-gamer, we have small gaps (0.86% and 0.13%) compared with the best method LINKX.  Ablation Study. We present ablation study to show the effectiveness of our design choices: the three graph features, adaptive feature fusion, and bi-level optimization. <ref type="table" target="#tab_3">Table 3</ref> reports the accuracy results from five variants of our model by removing one design element at a time.</p><p>Graph features. The 2nd to the 4th row demonstrate the contribution of the three graph features on the model's accuracy. Overall, models without one of the features suffer from 2.49% ? 5.81% accuracy drop on all the datasets on average, indicating the importance of these features on graph representation learning regardless of their homophily. Specifically, models without the aggregated neighborhood feature have a larger accuracy drop on homophilic graphs (i.e., Cora, CiteSeer, PubMed, Coauthor CS &amp; Physics). It echoes the high performance of message passing-based neural networks (e.g., GCN and DAGNN) on homophilic graphs. On the contrary, models without the graph structure feature suffer more severely on heterophilic graphs (i.e., penn94, arXiv-year, genius, and twitch-gamer). It echoes the high performance of GNNs that rely heavily on graph structures (e.g., LINKX) on heterophilic graphs.</p><p>Adaptive feature fusion. Models without adaptive feature fusion treat the three graph features equally and sum them up without re-weighting to produce the final node feature. It suffers from an accuracy drop of 2.22% on average, indicating the importance of adaptive feature fusion. Another widely-used approach for feature fusion is to concatenate the features. Although concatenation could learn separated parameters for different features, the features could not be balanced well by these parameters, leading to similar unsatisfying results as summing up the features (see <ref type="table" target="#tab_10">Table 9</ref> in Appendix Section D.2).</p><p>Bi-level optimization. Without bi-level optimization, the feature fusion weights are jointly optimized with the model parameters on the training dataset. Its accuracy drops by 2.65% on average. This phenomenon is consistent with the observation in the NAS domain <ref type="bibr" target="#b19">(Liu et al., 2018)</ref>: training model parameters and feature fusion weights jointly on the same training set would cause over-fitting and thus poor generalization performance.</p><p>Feature Fusion Analysis. We measure the importance of the three graph features by computing the proportion for each of them after feature fusion. Formally, the importance of the features is computed by I s = ?i Hs ?1 Hego +?2 Hagg +?3 Hstrc , where s ? {ego, agg, strc} and ? computes the averaged absolute value of a specific feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures 2a</head><p>show the results on syn-cora. We could observe three dominant trends. (1) The importance of the ego-node features increases from close to around 0.05 to 0.8 as the homophily of a graph becomes lower. This indicates that the ego-node feature is a more reliable signal than other features for graphs with low homophily. (2) The importance of the aggregated neighborhood feature increases from 0.2 to 0.8 as the homophily increases, echoing the intuition that a node has similar properties to its neighbors on homophilic graphs. (3) All the features have a non-trivial importance score for graphs with homophily within 0.2 and 0.8 (which is common for real graphs). This indicates the importance of all the features in learning node embeddings, echoing insights from ablation study. One thing worth mentioning is, since the graph links in syn-cora are generated randomly, which means that the graph structure features cannot capture meaningful information from the adjacency matrix as we expected, it's hard to figure out the trend of this feature from <ref type="figure" target="#fig_4">Figure 2a</ref>. <ref type="figure" target="#fig_4">Figure 2b</ref> shows the results on real graphs. Since real graphs have different intrinsic graph properties, including the raw node features and the graph links, we cannot compare the changes in feature importance across graphs like what we did for syn-cora. Instead, we focus on comparing the importance of different features given specific graphs. Our observations are summarized as follows.</p><p>(1) For homophilic graphs (i.e., CiteSeer, PubMed, Cora, Coauthor-CS, and Coauthor-Physics),  This phenomenon echoes what we observe in syn-cora.</p><p>(2) For graphs with low to medium homophily (i.e., arXiv-year, penn94, and twitch-gamer), the graph structure features take the biggest proportion compared to the other two features, indicating the strong impact of this feature. It is consistent with our ablation study where removing the graph structure features causes the most severe accuracy drops. (3) The graph genius almost totally relies on ego-node features. We suspect that the reason is that its node features are sufficient to serve the node classification task. As shown in <ref type="table" target="#tab_2">Table 2</ref>, applying MLP on the raw node feature already forms a strong baseline for this graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>Graph neural networks (GNNs) have achieved remarkable success on node classification tasks <ref type="bibr" target="#b6">(Gao et al., 2018;</ref><ref type="bibr" target="#b12">Klicpera et al., 2018;</ref><ref type="bibr" target="#b34">Xu et al., 2018b;</ref><ref type="bibr" target="#b32">Wu et al., 2019)</ref>. Many GNNs <ref type="bibr" target="#b22">(Niepert et al., 2016;</ref><ref type="bibr" target="#b8">Hamilton et al., 2017;</ref><ref type="bibr" target="#b21">Monti et al., 2017;</ref><ref type="bibr" target="#b30">Veli?kovi? et al., 2017;</ref><ref type="bibr" target="#b6">Gao et al., 2018;</ref><ref type="bibr" target="#b33">Xu et al., 2018a;</ref><ref type="bibr" target="#b31">Wang et al., 2019)</ref> fall into the message passing framework <ref type="bibr" target="#b7">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b2">Battaglia et al., 2018)</ref>, which iteratively transforms and propagate the messages from the spatial neighborhoods through the graph topology to update the embedding of a target node. To name a few, GCN (Kipf &amp; Welling, 2016) designs a layer-wise propagation rule based on a first-order approximation of spectral convolutions on graphs. GraphSAGE <ref type="bibr" target="#b8">(Hamilton et al., 2017)</ref> extends GCN by introducing a recursive node-wise sampling scheme to improve the scalability. Graph attention networks (GAT) <ref type="bibr" target="#b30">(Veli?kovi? et al., 2017)</ref> enhances GCN with the attention mechanism <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. Later works <ref type="bibr" target="#b6">(Gao et al., 2018;</ref><ref type="bibr" target="#b33">Xu et al., 2018a;</ref><ref type="bibr" target="#b31">Wang et al., 2019;</ref><ref type="bibr" target="#b14">Li et al., 2021;</ref><ref type="bibr" target="#b37">Zeng et al., 2021)</ref> try to design more expressive GCN variants by overcoming the over-smoothing problem of GCNs <ref type="bibr" target="#b23">(Oono &amp; Suzuki, 2019;</ref><ref type="bibr">Zhang et al., 2022)</ref>. DeepGCN <ref type="bibr" target="#b13">(Li et al., 2019)</ref> utilizes residual connections, dense connections, and dilated convolutions to build deeper GCNs for point cloud semantic segmentation. APPNP <ref type="bibr" target="#b12">(Klicpera et al., 2018)</ref> leverages personalized PageRank to improve the propagation scheme. DAGNN  proposes to decouple the transformation and the propagation operation to increase receptive fields. However, most of the above-mentioned GNNs fail to achieve good performance on heterophilic graphs <ref type="bibr" target="#b24">(Pei et al., 2020;</ref><ref type="bibr" target="#b17">Lim et al., 2021a)</ref> because they assume strong homophily in graphs.</p><p>Recent works start to pay attention to heterophilic graphs. MixHop <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2019)</ref> proposes a graph convolutional layer that utilizes multiple powers of the adjacency matrix to learn general mixed neighborhood information. <ref type="bibr">H2GCN (Zhu et al., 2020)</ref> identifies three key designs, egoand neighbor-embedding separation, higher-order neighbors, and the combination of intermediate representations to boost the representation learning for heterophilic graphs. Generalized PageRank (GPR) GNN <ref type="bibr" target="#b4">(Chien et al., 2020)</ref> adaptively controls the contribution of different propagation steps. LINKX <ref type="bibr" target="#b17">(Lim et al., 2021a)</ref> separately embeds the adjacency matrix and the node features and then combines them with MLPs. GGCN <ref type="bibr" target="#b35">(Yan et al., 2021)</ref> leverages two strategies, including degree correction for adjusting degree coefficients and signed messages for optionally negating the messages, to overcome the over-smoothing problem. Although many of these methods achieve better performance on heterophilic graphs, they achieve comparable, if not worse, accuracy on homophilic graphs than traditional message passing-based GNNs. In this paper, our goal is to design a GNN that can generalize well to the whole spectrum of homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose a graph neural network OGNN, which integrates three classes of graph features including the ego node feature, the aggregated neighborhood feature, and the graph structure feature. OGNN automatically handles graphs with different homophily via adaptive feature fusion and bi-level optimization. Extensive experiments show that OGNN achieves state-of-the-art accuracy performance compared with strong baselines on both synthetic datasets and real datasets covering the full graph homophily spectrum. Additional ablation studies further illustrate the necessity of the three aspects of the graph features and the proposed adaptive features fusion mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A REAL-WORLD DATASETS DETAILS</head><p>In our experiments, we use the following real-world datasets to evaluate our method and existing GNNs. The homophily of these datasets ranges from 0.222 ? 0.931. <ref type="table" target="#tab_0">Table 4</ref> summarizes the detailed dataset statistics.</p><p>Cora <ref type="bibr" target="#b26">(Sen et al., 2008)</ref>, CiteSeer <ref type="bibr" target="#b26">(Sen et al., 2008)</ref>, PubMed <ref type="bibr" target="#b26">(Sen et al., 2008)</ref>, and Coauthor CS &amp; Physics <ref type="bibr" target="#b27">(Shchur et al., 2018)</ref> have high edge homophily and are usually considered as homophilic graphs. For these graphs, we follow the data split in <ref type="bibr">GCN (Kipf &amp; Welling, 2016)</ref> and DAGNN . For Cora, CiteSeer, and PubMed, we randomly sample 20 nodes from each class as the train set, and sample 500 nodes from the rest as the validation set and 1000 nodes as the test set. For Coauthor CS &amp; Physics, we randomly sample 20 nodes per class as the train set, 30 nodes per class as the validation set, and the rest nodes as the test set.</p><p>penn94 <ref type="bibr" target="#b28">(Traud et al., 2012)</ref>, arXiv-year <ref type="bibr" target="#b9">(Hu et al., 2020)</ref>, genius <ref type="bibr" target="#b16">(Lim &amp; Benson, 2021)</ref>, and twitch-gamer <ref type="bibr" target="#b25">(Rozemberczki &amp; Sarkar, 2021)</ref> are graphs with lower homophily. For these graphs, we follow the data split in LINKX <ref type="bibr" target="#b18">(Lim et al., 2021b;</ref><ref type="bibr">a)</ref>, which uses the 50%/25%/25% nodes as the train/validation/test set respectively.</p><p>For all the datasets, we generate 5 random data splits for computing the average and standard deviation of the models' performance.  <ref type="table" target="#tab_6">Table 5</ref>. Before extracting the ego-node feature, the raw input node features are fed into a Dropout layer. In the aggregated neighborhood feature extraction, we reuse the intermediate results of different hops of neighbors to simplify the computation. For example, when computing? i Hego, we use the dot product of? ? (? i?1 Hego) instead of computing the power of?. Moreover, because? i?1 Hego has the same shape of Hego, computing? ? (? i?1 Hego) is always a sparse-dense matrix multiplication, which is more efficient than computing the power of?. Similarly, we use the same strategy to compute Hstrc. Because we use the original adjacency matrix instead of a normalized one in structure feature extraction (for accuracy performance purposes), the magnitude of A j will grow exponentially with j increasing, which may cause the value out of range problem. Therefore, we utilize Batch Normalization layers to scale down the feature matrix after each adjacency matrix multiplication. In the feature fusion module, we dropout the fused features before activating it with ReLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 TRAINING DETAILS</head><p>To implement our bi-level optimization training scheme, we utilize two optimizers to train the model parameters W and the feature fusion parameters P respectively. The model parameters W are trained with an Adam optimizer (Kingma &amp; Ba, 2014) O1 on the training dataset. The learning rate and weight decay rate of O1 is decided by the hyper-parameter settings, which is described in Section C. On the other hand, the feature fusion parameters P are trained with another Adam optimizer O2 on the validation dataset with a fixed learning rate 0.01. We train P for 10 epochs after training W for every 20 epochs. When training P, we'll set the Dropout </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HYPER-PARAMETER SETTINGS</head><p>In our experiments, we use the hidden channels (d), the propagation steps for aggregated neighborhood feature extraction (s1), the power of the adjacency matrix for graph structure feature extraction (s2), the learning rate ?, the weight decay ?, and the feature normalization (?) as the hyper-parameters. For all the datasets, we perform grid search over the following hyper-parameter options:</p><formula xml:id="formula_10">d ? {64 128} s1 ? {2 5 10 20} s2 ? {1 2 5} ? ? {0.01 0.001} ? ? {0.001 0.0005} ? ? {True False}</formula><p>We also list the best hyper-parameter settings for all the real-world datasets in <ref type="table" target="#tab_7">Table 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 DETAILED SYNTHEIC DATASET RESULTS</head><p>We list the detailed results of the experiments on the synthetic datasets in <ref type="table" target="#tab_8">Table 7</ref> and 8, which includes standard deviation of the accuracy performance compared to <ref type="table" target="#tab_1">Table 1</ref>. Overall, OGNN outperforms the existing methods on most homophily settings with seven settings at top-1 and four at top-2, and pushes the best accuracy boundary for up to 2.76%. <ref type="table" target="#tab_10">Table 9</ref> shows the experimental results of using concatenation as feature fusion approach instead of summing up the features.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 USING CONCATENATION AS FEATURE FUSION</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparisons of our model and other models on synthetic dataset syn-cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Time</head><label></label><figDesc>Complexity. The time complexity of extracting the ego-node features is O(nnz(X) ? d), where nnz(X) is the number of non-zero values in the raw feature matrix X, d is the hidden feature dimension. Extracting the aggregated neighborhood feature takes O(s 1 M d + s 1 N d) to propagate features from s 1 -hop neighbors and sum them up, where M and N are the number of edges and nodes in the graph. Similarly, extracting the graph structure feature takes O(s 2 M d + s 2 N d) with sparse matrix multiplications. The feature fusion step takes O(N d) for reweighting and summing up the features. Finally, the linear classifier takes O(N dC) to do the predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Baselines for Comparison. Our baselines include message-passing based GNNs (GCN (Kipf &amp; Welling, 2016), GAT<ref type="bibr" target="#b30">(Veli?kovi? et al., 2017)</ref>, and DAGNN), GNNs designed for heterophilic graphs (LINKX<ref type="bibr" target="#b17">(Lim et al., 2021a)</ref>,H2GCN (Zhu et al., 2020), MIXHOP (Abu-El-Haija et al., 2019), and GPR-GNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Results on syn-cora dataset.(b) Results on real datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Importance of the features after feature fusion.the aggregated neighborhood features play the most important role to the node classification accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>in Appendix Section A summarizes the datasets detailed statistics. Cora (Sen et al., 2008), CiteSeer (Sen et al., 2008), PubMed (Sen et al., 2008), and Coauthor CS &amp; Physics (Shchur et al., 2018) are widely-used homophilic graphs, while penn94 (Traud et al., 2012), arXiv-year (Hu et al., 2020), genius (Lim &amp; Benson, 2021), and twitch-gamer (Rozemberczki &amp; Sarkar, 2021) are graphs with low to medium homophily</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>69.20 69.20 69.20 69.20 69.20 69.20 69.20 69.20 69.20  69.20 GCN 28.61 30.64 36.03 45.15 51.39 65.04 74.48 82.22 91.21 96.19 99.92 GAT 29.41 30.32 34.83 43.86 51.15 64.80 74.34 81.45 90.46 95.79 100.0 DAGNN 34.32 39.49 45.01 54.48 60.51 72.36 80.00 86.49 93.32 97.48 99.95 LINKX 72.09 70.54 69.76 70.13 71.34 74.53 77.16 80.35 83.30 87.59 89.60 H2GCN 76.43 73.86 71.58 72.17 72.95 78.31 83.27 87.43 92.09 97.00 98.98 MIXHOP 39.44 38.95 41.05 48.93 55.09 64.75 74.45 82.44 91.45 96.25 100.0 GPR-GNN 67.86 61.96 61.21 64.69 67.67 74.56 80.19 86.68 93.54 97.45 100.0 OGNN 72.49 73.67 73.11 74.32 75.71 79.49 84.67 87.32 93.70 94.75 99.95</figDesc><table><row><cell>synh</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell>MLP</cell><cell>69.20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Test accuracy of different methods on the graphs with different homophily in syn-cora dataset. Red and blue represent top-1 and top-2 ranking in terms of accuracy respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average test accuracy ? standard deviation on the real datasets. Red and blue represent top-1 and top-2 ranking in terms of accuracy respectively. OOM means a model runs out of memory on a specific dataset. 70?0.21 73.6?0.40 60.92?0.07 86.68?0.09 50.94?4.20 66.04?2.29 52.56?2.55 83.08?1.00 82.15?5.11 8.56 GCN 46.02?0.26 82.47?0.27 62.18?0.26 87.42?0.37 63.36?2.06 78.12?1.60 77.90?1.18 90.35?0.88 92.39?0.89 5.33 GAT 49.37?0.20 81.45?0.55 62.32?0.23 86.59?1.06 65.90?1.88 76.78?2.38 76.98?1.75 88.86?0.65 92.57?0.60 5.44 DAGNN 37.00?0.20 74.49?0.37 59.80?0.13 80.86?3.82 68.50?0.88 79.94?1.30 83.70?1.12 91.81?0.24 93.79?0.68 4.89 LINKX 56.00?1.34 84.71?0.52 66.06?0.19 90.77?0.27 53.66?3.69 67.66?4.29 62.66?2.12 88.53?1.43 89.37?1.52 5.11 H2GCN 49.09?0.10 81.54?0.56 OOM 90.54?0.16 64.40?1.44 76.30?2.80 79.24?1.75 91.18?0.58 93.56?0.48 4.89 MIXHOP 51.78?0.26 83.63?0.54 65.65?0.30 90.61?0.24 56.98?4.80 76.14?2.37 73.80?4.02 89.79?0.91 93.33?0.75 4.78 GPR-GNN 44.89?0.20 81.12?0.63 62.00?0.25 90.02?0.13 64.72?1.59 79.12?0.87 80.44?1.53 90.74?0.60 93.86?0.36 4.44 OGNN 56.50?0.13 85.43?0.82 65.93?0.17 89.91?0.27 71.50?1.83 81.10?1.29 84.12?1.29 92.83?0.38 93.87?0.43 1.56</figDesc><table><row><cell></cell><cell>arXiv-year</cell><cell>penn94</cell><cell>twitch-gamer</cell><cell>genius</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora</cell><cell>Coauthor CS</cell><cell>Coauthor Physics</cell><cell>Avg. Rank</cell></row><row><cell>Homophily</cell><cell>0.22</cell><cell>0.47</cell><cell>0.55</cell><cell>0.62</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell><cell>0.81</cell><cell>0.93</cell><cell>-</cell></row><row><cell>#Nodes</cell><cell>169,343</cell><cell>41,554</cell><cell>168,114</cell><cell>421,961</cell><cell>3,327</cell><cell>19,717</cell><cell>2,708</cell><cell>18,333</cell><cell>34,493</cell><cell>-</cell></row><row><cell>#Edges</cell><cell>1,166,243</cell><cell>1,362,229</cell><cell>6,797,557</cell><cell>984,979</cell><cell>4,552</cell><cell>44,324</cell><cell>5,278</cell><cell>81,894</cell><cell>247,962</cell><cell>-</cell></row><row><cell>#Classes</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>6</cell><cell>3</cell><cell>7</cell><cell>15</cell><cell>5</cell><cell>-</cell></row><row><cell>MLP</cell><cell>36.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies on the real datasets. 71? 81.83 3.60? 65.68 0.25? 85.61 4.30? 68.44 3.06? 78.92 2.18? 83.12 1.00? 88.84 3.99? 92.56 1.31? 2.49? w/o agg 53.17 3.33? 84.81 0.62? 65.78 0.15? 89.78 0.13? 57.54 13.96? 72.32 8.78? 72.66 11.46? 87.72 5.11? 85.14 8.73? 5.81? w/o strc 36.01 20.49? 75.50 9.93? 61.59 4.34? 86.87 3.04? 69.82 1.68? 81.12 0.02? 83.34 0.78? 92.46 0.37? 93.41 0.46? 4.56? w/o fusion 53.45 3.05? 84.35 1.08? 65.76 0.17? 87.64 2.27? 67.72 3.78? 76.04 5.06? 82.48 1.64? 91.13 1.70? 92.64 1.23? 2.22? w/o bi-level 53.27 3.23? 83.71 1.72? 65.74 0.19? 88.25 1.66? 68.60 2.90? 73.32 7.78? 81.02 3.10? 90.31 2.52? 93.15 0.72? 2.65?</figDesc><table><row><cell></cell><cell>arXiv</cell><cell>penn94</cell><cell>twitch</cell><cell>genius</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora</cell><cell>CS</cell><cell>Physics</cell><cell>? Avg.</cell></row><row><cell cols="2">Homophily 0.22</cell><cell>0.47</cell><cell>0.55</cell><cell>0.62</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell><cell>0.81</cell><cell>0.93</cell><cell>-</cell></row><row><cell>OGNN</cell><cell>56.50</cell><cell>85.43</cell><cell>65.93</cell><cell>89.91</cell><cell>71.50</cell><cell>81.10</cell><cell>84.12</cell><cell>92.83</cell><cell>93.87</cell><cell>-</cell></row><row><cell>w/o ego</cell><cell>53.80 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, and Bin Cui. Model degradation hinders deep graph neural networks. arXiv preprint arXiv:2206.04361, 2022.Elena Zheleva and Lise Getoor. To join or not to join: the illusion of privacy in social networks with mixed public and private user profiles. In Proceedings of the 18th international conference on World wide web, pp.</figDesc><table><row><cell>531-540, 2009.</cell></row><row><cell>Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in</cell></row><row><cell>graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing</cell></row><row><cell>Systems, 33:7793-7804, 2020.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistics for the real-world datasets.The implementation of OGNN basically follows the model we described in Section 4.2, in which we omit some details for simplicity. The complete implementation details of OGNN are listed in</figDesc><table><row><cell></cell><cell cols="5">#Classes #Nodes #Edges #Features degree</cell><cell>edge homophily</cell><cell>Nodes</cell><cell>Edges</cell><cell>Classes</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,278</cell><cell cols="2">1,433 1.949</cell><cell>0.81</cell><cell>papers</cell><cell>citation research field</cell></row><row><cell>CiteSeer</cell><cell>6</cell><cell>3,327</cell><cell>4,552</cell><cell cols="3">3,703 1.368 0.736</cell><cell>papers</cell><cell>citation research field</cell></row><row><cell>PubMed</cell><cell>3</cell><cell cols="2">19,717 44,324</cell><cell>500</cell><cell cols="2">2.248 0.802</cell><cell>papers</cell><cell>citation research field</cell></row><row><cell>Coauthor CS</cell><cell>15</cell><cell cols="2">18,333 81,894</cell><cell cols="3">6,805 4.467 0.808</cell><cell cols="2">authors co-authors research field</cell></row><row><cell>Coauthor Physics</cell><cell>5</cell><cell cols="5">34,493 247,962 8,415 7.189 0.931</cell><cell cols="2">authors co-authors research field</cell></row><row><cell>penn94</cell><cell>2</cell><cell cols="2">41,554 1,362,229</cell><cell>5</cell><cell cols="2">32.782 0.47</cell><cell>peoples</cell><cell>friends</cell><cell>Gender</cell></row><row><cell>arXiv-year</cell><cell>5</cell><cell cols="3">169,343 1,166,243 128</cell><cell cols="2">6.887 0.222</cell><cell>papers</cell><cell>citation</cell><cell>year</cell></row><row><cell>genius</cell><cell>2</cell><cell cols="2">421,961 984,979</cell><cell>12</cell><cell cols="2">2.334 0.618</cell><cell>users</cell><cell>followers</cell><cell>Gender</cell></row><row><cell>twitch-gamer</cell><cell>2</cell><cell cols="2">168,114 6,797,557</cell><cell>7</cell><cell cols="4">40.434 0.545 Twitch users followers</cell><cell>Gender</cell></row><row><cell cols="4">B IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">B.1 MODEL IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>OGNN Model Implementation details Batch Normalization layers to evaluation mode. We early stop the model if the validation accuracy does not increase for 100 epochs or the total number of training epochs reaches 3000.</figDesc><table><row><cell>Module</cell><cell>Implementation Details</cell><cell></cell></row><row><cell>Input Feature</cell><cell>X</cell><cell></cell></row><row><cell>Ego Node Feature</cell><cell>Hego = Linear(Dropout(X))</cell><cell></cell></row><row><cell cols="3">Aggregated Neighborhood Feature Hagg = s 1 i=1? i Hego Graph Structure Feature Hstrc = s 2 j=1 BN j (A)Wstrc, BN j (A) = BN(A ? BN(. . . BN(A)))</cell></row><row><cell></cell><cell>s 2</cell><cell></cell></row><row><cell>Feature Fusion</cell><cell>H = ReLU(Dropout(?1Hego + ?2Hagg + ?3Hstrc)), ?i =</cell><cell>exp p i j=1 exp p j 3</cell></row><row><cell>Prediction Head</cell><cell>Ypred = Softmax(HWpred)</cell><cell></cell></row><row><cell>layers and</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Best hyper-parameter settings for the real-world datasets.</figDesc><table><row><cell></cell><cell cols="9">arXiv penn94 twitch genius CiteSeer PubMed Cora CS Physics</cell></row><row><cell cols="2">Homophily 0.22</cell><cell>0.47</cell><cell cols="2">0.55 0.62</cell><cell>0.74</cell><cell>0.80</cell><cell cols="3">0.81 0.81 0.93</cell></row><row><cell>d</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>64</cell><cell>128</cell></row><row><cell>s1</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>2</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>10</cell><cell>20</cell></row><row><cell>s2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>5</cell><cell>2</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>2</cell></row><row><cell>?</cell><cell>0.01</cell><cell>0.01</cell><cell cols="2">0.01 0.01</cell><cell>0.01</cell><cell>0.01</cell><cell cols="3">0.01 0.01 0.01</cell></row><row><cell>?</cell><cell cols="9">0.0005 0.0005 0.0005 0.001 0.001 0.0005 0.0005 0.001 0.0005</cell></row><row><cell>?</cell><cell cols="4">False False False False</cell><cell>True</cell><cell>True</cell><cell cols="3">True False False</cell></row><row><cell cols="3">D DETAILED RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Test accuracy of different methods on the graphs with different homophily from 0 to 0.5 in syn-cora dataset. Red and blue represent top-1 and top-2 ranking in terms of accuracy respectively.20?1.92 69.20?1.92 69.20?1.92 69.20?1.92 69.20?1.92 69.20?1.92 GCN 28.61?2.38 30.64?1.58 36.03?1.15 45.15?2.18 51.39?1.48 65.04?1.60 GAT 29.41?2.20 30.32?2.52 34.83?1.67 43.86?1.44 51.15?1.25 64.80?1.81 DAGNN 34.32?1.51 39.49?1.18 45.01?2.42 54.48?3.17 60.51?1.40 72.36?1.83 LINKX 72.09?1.65 70.54?1.84 69.76?1.07 70.13?1.30 71.34?1.17 74.53?1.83 H2GCN 76.43?1.27 73.86?3.05 71.58?1.75 72.17?1.44 72.95?0.76 78.31?1.96 MIXHOP 39.44?2.98 38.95?2.21 41.05?2.69 48.93?2.72 55.09?2.30 64.75?1.88 GPR-GNN 67.86?2.66 61.96?2.53 61.21?2.36 64.69?2.88 67.67?3.41 74.56?2.60 OGNN 72.49?1.74 73.67?1.61 73.11?0.94 74.32?2.29 75.71?1.00 79.49?1.58</figDesc><table><row><cell>synh</cell><cell>0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell>MLP</cell><cell>69.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Test accuracy of different methods on the graphs with different homophily from 0.6 to 1 in syn-cora dataset. Red and blue represent top-1 and top-2 ranking in terms of accuracy respectively. 20?1.92 69.20?1.92 69.20?1.92 69.20?1.92 69.20?1.92 GCN 74.48?1.36 82.22?2.20 91.21?1.15 96.19?0.65 99.92?0.18 GAT 74.34?2.03 81.45?1.72 90.46?1.12 95.79?0.71 100.0?0.00 DAGNN 80.00?1.26 86.49?2.13 93.32?1.45 97.48?0.31 99.95?0.07 LINKX 77.16?1.22 80.35?1.47 83.30?1.23 87.59?1.07 89.60?1.86 H2GCN 83.27?1.25 87.43?0.88 92.09?0.46 97.00?0.28 98.98?0.62 MIXHOP 74.45?1.44 82.44?2.69 91.45?1.11 96.25?0.89 100.0?0.00 GPR-GNN 80.19?1.57 86.68?0.69 93.54?1.36 97.45?0.28 100.0?0.00 OGNN 84.67?1.30 87.32?1.25 93.70?1.13 94.75?3.47 99.95?0.07</figDesc><table><row><cell>synh</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell>MLP</cell><cell>69.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Using concatenation as feature fusion. 05? 84.35 1.08? 65.76 0.17? 87.64 2.27? 67.72 3.78? 76.04 5.06? 82.48 1.64? 91.13 1.70? 92.64 1.23? 2.22? w/ concate 56.55 0.05? 82.14 3.29? 65.87 0.06? 89.44 0.47? 66.70 0.47? 74.70 6.40? 81.10 3.02? 89.73 3.10? 92.74 1.13? 2.47?</figDesc><table><row><cell></cell><cell>arXiv</cell><cell>penn94</cell><cell>twitch</cell><cell>genius</cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>Cora</cell><cell>CS</cell><cell>Physics</cell><cell>? Avg.</cell></row><row><cell cols="2">Homophily 0.22</cell><cell>0.47</cell><cell>0.55</cell><cell>0.62</cell><cell>0.74</cell><cell>0.80</cell><cell>0.81</cell><cell>0.81</cell><cell>0.93</cell><cell>-</cell></row><row><cell>OGNN</cell><cell>56.50</cell><cell>85.43</cell><cell>65.93</cell><cell>89.91</cell><cell>71.50</cell><cell>81.10</cell><cell>84.12</cell><cell>92.83</cell><cell>93.87</cell><cell>-</cell></row><row><cell>w/ sum</cell><cell>53.45 3.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A novel method for assessing and measuring homophily in networks through second-order statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Apollonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Franciosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6437" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Expertise and dynamics within crowdsourced musical knowledge curation: A case study of the genius platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="373" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Sijia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser Nam</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="20887" to="20902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01404</idno>
		<title level="m">New benchmarks for learning on non-homophilous graphs</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Twitch gamers: a dataset for evaluating proximity preserving and structural role-based node embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03091</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Social structure of facebook networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Traud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason A</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">391</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4165" to="4180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Two sides of the same coin: Heterophily and oversmoothing in graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decoupling the depth and scope of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
