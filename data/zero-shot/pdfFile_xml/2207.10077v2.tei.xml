<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discover and Mitigate Unknown Biases with Debiasing Alternate Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Hoogs</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Kitware, Inc</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
							<email>chenliang.xu@rochester.eduanthony.hoogs@kitware.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discover and Mitigate Unknown Biases with Debiasing Alternate Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bias Identification</term>
					<term>Bias Mitigation</term>
					<term>Fairness</term>
					<term>Unsupervised Debiasing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep image classifiers have been found to learn biases from datasets. To mitigate the biases, most previous methods require labels of protected attributes (e.g., age, skin tone) as full-supervision, which has two limitations: 1) it is infeasible when the labels are unavailable; 2) they are incapable of mitigating unknown biases-biases that humans do not preconceive. To resolve those problems, we propose Debiasing Alternate Networks (DebiAN), which comprises two networks-a Discoverer and a Classifier. By training in an alternate manner, the discoverer tries to find multiple unknown biases of the classifier without any annotations of biases, and the classifier aims at unlearning the biases identified by the discoverer. While previous works evaluate debiasing results in terms of a single bias, we create Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in a multi-bias setting, which not only reveals the problems in previous methods but also demonstrates the advantage of DebiAN in identifying and mitigating multiple biases simultaneously. We further conduct extensive experiments on real-world datasets, showing that the discoverer in DebiAN can identify unknown biases that may be hard to be found by humans. Regarding debiasing, DebiAN achieves strong bias mitigation performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many studies have verified that AI algorithms learn undesirable biases from the dataset. Some biases provide shortcuts <ref type="bibr">[21]</ref> for the network to learn superficial features instead of the intended decision rule causing robustness issues, e.g., static cues for action recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">45]</ref>. Other biases make AI algorithms discriminate against different protected demographic groups such as genders * <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">[29]</ref><ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr" target="#b51">69,</ref><ref type="bibr">72,</ref><ref type="bibr">77]</ref> and skin tones <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">27]</ref>, leading to serious fairness problems. Therefore, it is imperative to mitigate the biases in AI algorithms. However, most previous bias mitigation methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">65,</ref><ref type="bibr">71,</ref><ref type="bibr">75,</ref><ref type="bibr">77]</ref> are supervised methods-requiring annotations of the biases, which has several limitations: First, bias mitigation cannot be performed when labels are not available due to privacy concerns. Second, they cannot mitigate unknown biases-biases that humans did not preconceive, making the biases impossible to be labeled and mitigated.</p><p>Since supervised debiasing methods present many disadvantages, in this work, we focus on a more challenging task-unsupervised debiasing, which mitigates the unknown biases in a learned classifier without any annotations. Without loss of generality, we focus on mitigating biases in image classifiers. Solving this problem contains two steps <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">40,</ref><ref type="bibr" target="#b35">53,</ref><ref type="bibr" target="#b45">63]</ref>: bias identification and bias mitigation. Due to the absence of bias annotations, the first step is to assign the training samples into different bias groups as the pseudo bias labels, which is challenging since the biases are even unknown. The crux of the problem is to define the unknown bias. Some previous works make strong assumptions about the unknown biases based on empirical observations, such as biases are easier to be learned <ref type="bibr" target="#b35">[53]</ref>, samples from the same bias group are clustered in feature space <ref type="bibr" target="#b45">[63]</ref>, which can be tenuous for different datasets or networks. Other works quantify the unknown biases by inversely using the debiasing objective functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, which can face numerical or convergence problems (more details in Sec. 2). Unlike previous works, we follow an axiomatic principle to define the unknown biases-classifier's predictions that violate a fairness criterion <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">20,</ref><ref type="bibr">24,</ref><ref type="bibr">26,</ref><ref type="bibr" target="#b21">39,</ref><ref type="bibr" target="#b37">55,</ref><ref type="bibr" target="#b48">66]</ref>. Based on this definition, we propose a novel Equal Opportunity Violation (EOV) loss to train a discoverer network to identify the classifier's biases. In specific, it shepherds the discoverer network to predict bias group assignments such that the classifier violates the Equal Opportunity <ref type="bibr">[26,</ref><ref type="bibr" target="#b37">55]</ref> fairness criterion (Figs. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2)</ref>.</p><p>Regarding debiasing as the second step, most previous approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">63]</ref> preprocess the identified biases into pseudo bias labels and resort to other supervised bias mitigation methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">57]</ref> for debiasing. In contrast, we propose a novel Reweighted Cross-Entropy (RCE) loss that leverages soft bias group assignments predicted by the discoverer network to mitigate the biases in the classifier <ref type="figure" target="#fig_0">(Fig. 1</ref>). In this way, the classifier is guided to meet the Equal Opportunity.</p><p>In addition, many previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">63]</ref> treat bias identification and bias mitigation as two isolated steps. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, the biases are identified from an undertrained classifier, which is suboptimal since the classifier may learn different biases at different training stages. Consequently, these two-stage methods fail to mitigate other biases learned by the classifier at later training stages. In contrast, we employ an alternate training scheme to carry out bias identification and bias mitigation simultaneously. We jointly update the discoverer and classifier in an interleaving fashion ( <ref type="figure" target="#fig_0">Figs. 1 and 2</ref>). In this way, the discoverer can repetitively inspect multiple biases that the classifier learns at the entire training stage.</p><p>We integrate our novel losses and training scheme into a unified framework-Debiasing Alternate Networks (DebiAN), which contains two networks-a discoverer D and a classifier C (see <ref type="figure" target="#fig_0">Fig. 1</ref>). We jointly train the two networks in an alternate manner. Supervised by our novel EOV loss, D tries to discover C's multiple unknown biases that violate the Equal Opportunity fairness criterion. Trained with our RCE loss, C aims at mitigating multiple biases identified by the discoverer D to satisfy Equal Opportunity. After the alternate training, the unknown biases in classifier C are mitigated, leading to a fairer and more robust classification model. Besides, when employed with other network explanation methods <ref type="bibr" target="#b41">[59,</ref><ref type="bibr" target="#b42">60,</ref><ref type="bibr">78]</ref>, the discoverer is helpful to interpret the discovered unknown biases, facilitating dataset curators to locate dataset biases <ref type="bibr" target="#b46">[64]</ref>.</p><p>While previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b28">46,</ref><ref type="bibr" target="#b35">53,</ref><ref type="bibr" target="#b39">57]</ref> only evaluate debiasing results in terms of a single bias, we create Multi-Color MNIST dataset with two biases in the dataset, which benchmarks debiasing algorithms in the multi-bias setting. Our new dataset surfaces the problems in previous methods (e.g., LfF <ref type="bibr" target="#b35">[53]</ref>) and demonstrates the advantage of DebiAN in discovering and mitigating multiple biases. We further conduct extensive experiments to verify the efficacy of DebiAN in real-world image datasets. In the face image domain, DebiAN achieves better gender bias mitigation results on CelebA <ref type="bibr" target="#b32">[50]</ref> and bFFHQ <ref type="bibr" target="#b18">[36]</ref> datasets. On the gender classification task, DebiAN achieves better debiasing results on CelebA w.r.t. multiple bias attributes. We further show an interesting unknown bias discovered by DebiAN in gender classification-visible hair area. Lastly, we show that DebiAN applies to other image domains for broader tasks, such as action recognition and scene classification. Our method not only achieves better debiasing results, but also identifies interesting unknown biases in scene classifiers.</p><p>Our contributions are summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a novel objective function, Equal Opportunity Violation (EOV) loss, for identifying unknown biases of a classifier based on Equal Opportunity. <ref type="bibr" target="#b1">(2)</ref> We propose a Reweighted Cross-Entropy (RCE) loss to mitigate the discovered unknown biases by leveraging the soft bias group assignments. (3) We create Multi-Color MNIST dataset to benchmark debiasing algorithms in a multi-bias setting. (4) Our Debiasing Alternate Networks (DebiAN) outperforms previous unsupervised debiasing methods on both synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Bias Identification Most previous works identify known biases based on bias labels. In <ref type="bibr" target="#b11">[12]</ref>, face images are labeled with gender and skin tone to identify the performance gaps across intersectional groups. Balakrishnan et al. <ref type="bibr" target="#b7">[8]</ref> further synthesize intersectional groups of images and analyze the biases with additional labels. Beyond face images, recent works <ref type="bibr" target="#b33">[51,</ref><ref type="bibr" target="#b49">67]</ref> compute the statistics of labels based on the rule mining algorithm <ref type="bibr" target="#b0">[1]</ref> or external tools. <ref type="bibr" target="#b20">[38]</ref> uses clustering on image embeddings to discover unknown biases. <ref type="bibr" target="#b23">[41,</ref><ref type="bibr" target="#b29">47]</ref> discovers unknown biases without labels. However, these works rely on GAN [23,33] to synthesize images, which suffers from image quality issues. In contrast, DebiAN directly classifies real images into different bias attribute groups to discover the unknown biases. Supervised Debiasing Supervised debiasing methods use bias labels for debiasing.</p><p>[32] proposes a supervised reweighing method. <ref type="bibr">Wang et al. [73]</ref> benchmark recent supervised debiasing methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">65,</ref><ref type="bibr">75,</ref><ref type="bibr">77]</ref>. <ref type="bibr" target="#b17">[18]</ref> lets the model be flexibly fair to different attributes during testing. <ref type="bibr" target="#b40">[58]</ref> uses disentanglement for debiasing. Singh et al. <ref type="bibr" target="#b44">[62]</ref> propose a feature splitting approach to mitigate contextual bias. <ref type="bibr">[19,</ref><ref type="bibr">22]</ref> use adversarial training to mitigate biases in face recognition. Known Bias Mitigation with Prior knowledge Without using labels, some works use prior knowledge to mitigate certain known biases. ReBias <ref type="bibr" target="#b6">[7]</ref> uses model capacity as the inductive bias to mitigate texture bias and static bias in image and video classification. HEX <ref type="bibr" target="#b50">[68]</ref> introduces a texture extractor to mitigate the texture bias. Beyond image classification, RUBi <ref type="bibr" target="#b12">[13]</ref> and LearnedMixin <ref type="bibr" target="#b14">[15]</ref> mitigate unimodal bias for visual question answering <ref type="bibr" target="#b4">[5]</ref> with prior knowledge. Unsupervised Debiasing In the field of mitigating unknown biases, Sohoni et al. <ref type="bibr" target="#b45">[63]</ref> apply clustering on samples in each class and use the clustering assignment as the predicted bias labels, which could be inaccurate due to its unsupervised nature. Li et al . <ref type="bibr" target="#b26">[44,</ref><ref type="bibr" target="#b27">45]</ref> fix the parameters of feature extractors and focus on mitigating the representation bias. PI <ref type="bibr" target="#b10">[11]</ref> uses labels of one known bias attribute to interpolate environments, improving robustness against multiple unknown biases. TOFU <ref type="bibr" target="#b9">[10]</ref> infers unstable features from a source task and transfers them to the target task. Concurrent to our work, Bao and Barzilay <ref type="bibr" target="#b8">[9]</ref> propose learning to split for detecting unknown biases, which can then be combined with GroupDRO <ref type="bibr" target="#b39">[57]</ref> for debiasing. LfF <ref type="bibr" target="#b35">[53]</ref> identifies biases by finding easier samples in the training data through training a bias-amplified network supervised by GCE loss <ref type="bibr">[76]</ref>, which up-weights the samples with smaller loss values and down-weights the samples with larger loss values. In other words, GCE loss does not consider the information of the classifier, e.g., the classifier's output. Therefore, LfF's bias-amplified network blindly finds the biases in the data samples instead of the classifier. Unlike LfF, the EOV loss in DebiAN actively identifies biases in the classifier based on the classifier's predictions, leading to better debiasing performance. Following LfF, BiaSwap <ref type="bibr" target="#b18">[36]</ref> uses LfF to discover biases and generate more underrepesented images via style-transfer for training. Other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">40,</ref><ref type="bibr">70]</ref> inversely use the debiasing objective function to maximize an unbounded loss (e.g., gradient norm penalty in IRMv1 <ref type="bibr" target="#b5">[6]</ref>) for bias identification, which may encounter numerical or convergence problems. As a comparison, our EOV loss (Eq. (2)) minimizes negative log-likelihood, which is numerically stable and easier to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Overview The overview of our proposed Debiasing Alternate Networks (DebiAN) is shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. It contains two networks-a discoverer D and a classifier C. As shown in <ref type="figure" target="#fig_3">Fig. 2 (a)</ref>, the discoverer D tries to discover the unknown biases in the classifier C by optimizing our proposed EOV loss (L EOV ) discoverer classifier D &lt; l a t e x i t s h a 1 _ b a s e <ref type="bibr">6 4</ref>    and UA penalty (L UA ) (Sec. 3.1). As shown in <ref type="figure" target="#fig_3">Fig. 2 (b)</ref>, the classifier C's goal is to mitigate the biases identified by D via a novel Reweighted Cross-Entropy loss (L RCE ) (Sec. 3.2). Lastly, we train the two networks in an alternate manner as the full model for discovering and mitigating the unknown biases (Sec. 3.3). Background To better explain our motivation for discovering the unknown biases (without manual annotations of biases), let us first revisit the traditional approach for identifying known biases when labels of biases (e.g., protected attributes) are available, which is illustrated in <ref type="figure">Fig. 3</ref> (a). The following are given for identifying known biases-a well-trained classifier C for predicting a target attribute, n testing images {I i } n i=1 , target attribute labels of each image {y i } n i=1 , and bias attribute labels {b i } n i=1 . We denote the i-th image target attribute as y i ? {1, 2, ...K} and K is the number of classes. We consider the bias attribute that is binary or continuously valued (i.e., b i ? {0, 1} or b i ? [0, 1]), such as biological gender (e.g., female and male) and skin tones (e.g., from dark skin tones to light skin tones in Fitzpatrick skin type scale). We leave bias attributes with multi-class values for future works. Then, the given classifier C is tested for predicting the target attribute? i for each testing image I i . Finally, we check whether the predictions meet a fairness criterion, such as Equal Opportunity [26]:</p><formula xml:id="formula_0">= " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q /</formula><formula xml:id="formula_1">Pr{? = k | b = 0, y = k} = Pr{? = k | b = 1, y = k},<label>(1)</label></formula><p>where the LHS and RHS are true positive rates (TPR) in negative (b = 0) and positive (b = 1) bias attribute groups, respectively. k ? {1...K} is a target attribute class. Equal Opportunity requires the same TPR across two different bias attribute groups. That is, if the TPR is significantly different in two groups of the bias attribute, we conclude that classifier C contains the bias of attribute b because C violates the Equal Opportunity fairness criterion. For example, as shown in <ref type="figure">Fig. 3</ref> (a), although all images are female, a gender classifier may have a larger TPR for the group of long-hair female images than the group of short-hair female images. Thus the gender classifier is biased against different hair lengths.</p><p>predicts the bias attribute groups.   The traditional approach for identifying the known bias attribute (e.g., hair length) by comparing true positive rates (TPR) of the target attribute (e.g., gender) in two groups of bias attributes (e.g., long hair and short hair), where the group assignment of bias attribute is based on the labels of the bias attribute. (b): Our method trains a discoverer (D) to predict the groups of the unknown bias attribute such that the difference of averaged predicted probabilities on the target attribute (e.g., gender) in two groups are maximized (see Eq. 2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unknown Bias Discovery</head><p>As for identifying unknown biases, we do not have the labels to assign images into two groups for comparing TPR since 1) we do not assume images come with bias attribute labels, and 2) the type of bias is even unknown. However, we can compare the difference in TPR for any group assignments based on speculated biases-a significant difference in TPR hints that the Equal Opportunity fairness criterion is most violated (our method mainly focuses on the Equal Opportunity fairness criterion, and we leave other fairness criteria for future work). Motivated by this finding, instead of using labels of bias attribute {b i } n i=1 for group assignment, we train a discoverer D to predict the group assignment for each image, i.e., p(b | I i ) := D(I i ). By optimizing loss functions below, we find the most salient bias of the classifier C that violates the Equal Opportunity fairness criterion, which is illustrated in <ref type="figure">Fig. 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b). Equal Opportunity Violation (EOV) Loss</head><p>To shepherd the discoverer D to find the group assignment where classifier C violates the Equal Opportunity fairness criterion, we propose the Equal Opportunity Violation (EOV) loss, denoted by L EOV , as the objective function to train D. For computing L EOV , we sample a set of n images {I i } n i=1 with the same target attribute labels (i.e., ? i y i = k), e.g., all images in <ref type="figure">Fig. 3</ref> (b) are female. The classifier C has been trained for predicting the target attribute y of the images (i.e., p(? | I i ) := C(I i )). For simplicity, we denote p t as C's prediction on images of the ground-truth class (i.e., p t (I i ) = p(? = y i | I i )). Meanwhile, the same set of images {I i } are fed to the discoverer D for predicting the binary bias attribute group assignment:</p><formula xml:id="formula_2">p(b | I i ) := D(I i )</formula><p>. Finally, we define the EOV loss as:</p><formula xml:id="formula_3">L EOV = ? log P b + (?) ?P b ? (?) ,<label>(2)</label></formula><formula xml:id="formula_4">whereP b + (?) andP b ? (?) are defined by: P b + (?) = n i=1 p(b = 1 | I i )p t (I i ) n i=1 p(b = 1 | I i ) , P b ? (?) = n i=1 p(b = 0 | I i )p t (I i ) n i=1 p(b = 0 | I i )</formula><p>.</p><p>(3)</p><formula xml:id="formula_5">Intuitively,P b + (?) andP b ? (?)</formula><p>are the weighted average predicted probabilities of the target attribute in two bias attribute groups, which can be regarded as a relaxation to Equal Opportunity's true positive rate (Eq. 1) where the predicted probabilities are binarized into predictions with a threshold (e.g., 0.5). Minimizing L EOV leads D to maximize the discrepancy of averaged predicted probabilities of target attributes in two bias attribute groups (i.e., see max <ref type="figure">Fig. 3</ref>), thus finding the bias attribute group assignments where C violates the Equal Opportunity fairness criterion. For example, in <ref type="figure" target="#fig_5">Fig. 3 (b)</ref>, if the gender classifier C is biased against different hair lengths, then by optimizing L EOV , D can assign the female images into two bias attribute groups (i.e., short hair and long hair) with the predicted bias attribute group assignment probability p(b | I i ), such that the difference of averaged predicted probabilities on gender in these two groups is maximized. Unbalanced Assignment (UA) penalty However, we find that optimizing L EOV alone may let the discoverer D find a trivial solution-assigning all images into one bias attribute group. For example, suppose D assigns all images to the positive bias attribute group (i.e., ? i , p(b = 1 | I i ) = 1). In that case,P b ? (?) becomes zero since the negative group contains no images.P b + (?) becomes a large positive number by simply averaging p t (I i ) for all of the n images, which can trivially increase |P b + (?) ?P b ? (?)|, leading to a small L EOV . To prevent this trivial solution, we propose the Unbalanced Assignment (UA) loss denoted by:</p><formula xml:id="formula_6">D |P b + (?) ?P b ? (?)| in</formula><formula xml:id="formula_7">L UA = ? log 1 ? 1 n n i=1 p(b = 1 | I i ) ? p(b = 0 | I i ) .<label>(4)</label></formula><p>Intuitively, minimizing L UA penalizes the unbalanced assignment that leads to large difference between</p><formula xml:id="formula_8">n i=1 p(b = 1 | I i ) and n i=1 p(b = 0 | I i )</formula><p>, which can be regarded as the numbers of images assigned into positive and negative bias attribute groups, respectively. Therefore, L EOV is jointly optimized with L UA to prevent the trivial solution. We acknowledge a limitation of the UA penalty. Although it resolves the trivial solution, it introduces a trade-off since the bias attribute groups are usually spuriously correlated with the target attribute (e.g., more long-hair females than the short-hair females in the dataset). Hence encouraging balanced assignments may make the discoverer harder to find the correct assignment. However, our ablation study shows that the benefits of using L UA outweigh its limitations. The results are shown in Sec. 4.1 and Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unknown Bias Mitigation by Reweighing</head><p>We further mitigate C's unknown biases identified by D. To this end, we propose a novel Reweighted Cross-Entropy loss that adjusts the weight of each image's classification loss. Based on the bias attribute group assignment p(b | I i ) predicted by D, we define the weight W(I i ) of classification loss for each image I i as:</p><formula xml:id="formula_9">W(I i ) =1 P b + (?) ?P b ? (?) p(b = 0 | I i ) + 1 P b + (?) &lt;P b ? (?) p(b = 1 | I i ),<label>(5)</label></formula><p>where 1 is an indicator function. Then, the Reweighted Cross-Entropy loss (L RCE ) is defined by:</p><formula xml:id="formula_10">L RCE = ? 1 n n i=1 (1 + W (I i )) log p t (I i ) .<label>(6)</label></formula><p>For example, when C performs better on images from the positive bias attribute group (i.e.,P b + (?) ?P b ? (?)), we use p(b = 0 | I i ) as the weight, which up-weights the images from the negative bias attribute group, where classifier C is worse-performed. At the same time, it down-weights the images from the positive bias attribute group where C is already better-performed. Adding one to the weight in Eq. (6) lets the loss function degenerate to standard cross-entropy loss when W(I i ) = 0. By minimizing the Reweighted Cross-Entropy loss, C is guided to meet Equal Opportunity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Full Model</head><p>We summarize the proposed losses in Sec. 3.1 and Sec. 3.2 for the full model of Debiasing Alternate Networks (DebiAN), which is shown in <ref type="figure" target="#fig_3">Fig. 2</ref>. When the task is to only discover (i.e., not mitigate) the unknown biases of a given classifier, the classifier's parameters are fixed and we only train the discoverer D by minimizing L EOV (Eq. 2) and L UA (Eq. 4) on the classifier's training data. When the task is to mitigate the unknown biases, we jointly train two networks in an alternate fashion:</p><formula xml:id="formula_11">min D L EOV + L UA ,<label>(7)</label></formula><formula xml:id="formula_12">min C L RCE .<label>(8)</label></formula><p>In Eq. 7, C's parameters are fixed, and D is optimized to identify C's unknown biases where C violates the Equal Opportunity. Through Eq. 8, C is optimized for mitigating the unknown biases discovered by D to satisfy the Equal Opportunity while D's parameters are frozen. After the alternate training, C's unknown biases identified by D are mitigated, leading to a fairer and more robust classifier. The pseudocode of the complete algorithm is in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct extensive experiments to verify the efficacy of DebiAN. First, we evaluate the results on our newly created Multi-Color MNIST dataset (Sec. 4.1) in a multi-bias setting. We further conduct experiments on real-world datasets in multiple image domains-face (Sec. 4.2) and other image domains (e.g., scene, action recognition) (Sec. 4.3). More details (e.g., evaluation metrics) are introduced in each subsection. The code and our newly created Multi-Color MNIST dataset are released at https://github.com/zhihengli-UR/DebiAN. Comparison Methods We mainly compare with three unsupervised debiasing methods: 1) LfF <ref type="bibr" target="#b35">[53]</ref> uses Generalized Cross-entropy (GCE) loss [76] to train a "biased model" for reweighing the classifier; 2) EIIL <ref type="bibr" target="#b16">[17]</ref> identifies the bias groups by optimizing bias group assignment to maximize the IRMv1 <ref type="bibr" target="#b5">[6]</ref> objective function. The identified bias groups will serve as pseudo bias labels for other supervised debiasing methods to mitigate the biases. Following <ref type="bibr" target="#b16">[17]</ref>, IRM <ref type="bibr" target="#b5">[6]</ref> is used as the debiasing algorithm for EIIL. 3) PGI <ref type="bibr" target="#b1">[2]</ref> follows EIIL to identify the biases by training a small multi-layer perceptron for bias label predictions. Concerning debiasing, PGI minimizes the KL-divergence of the classifier's predictions across different bias groups. We use the officially released code of LfF, EIIL, and PGI in our experiment. Besides, we also compare with vanilla models, which do not have any debiasing techniques (i.e., only using standard cross-entropy loss for training).</p><p>On bFFHQ <ref type="bibr" target="#b18">[36]</ref> and BAR <ref type="bibr" target="#b35">[53]</ref> datasets, we also compare with BiaSwap <ref type="bibr" target="#b18">[36]</ref>, which follows LfF to identify unknown biases, and then uses style-transfer to generate more underrepresented images for training. Since its code has not been released, we cannot compare DebiAN with BiaSwap on other datasets. All results shown below are the mean results over three random seeds of runs, and we also report the standard deviation as the error bar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment on Multi-Color MNIST</head><p>Many previous works use synthetic datasets to benchmark bias mitigation performance. For example, Colored MNIST <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b28">46]</ref> adds color bias to the original MNIST <ref type="bibr" target="#b24">[42]</ref> dataset, where each digit class is spuriously correlated with color (see <ref type="figure">Fig. 4</ref> (a)). We compare DebiAN with other methods on the Colored MNIST dataset in Appendix E. However, we believe that the single-bias setting is an oversimplification of the real-world scenario where multiple biases may exist. For instance, Lang et al. <ref type="bibr" target="#b23">[41]</ref> find that gender classifiers are biased with multiple independent bias attributes, including wearing lipsticks, eyebrow thickness, nose width, etc. The benchmarking results on such a single-bias synthetic dataset may not help us to design better debiasing algorithms for real-world usage.</p><p>To this end, we propose Multi-Color MNIST dataset to benchmark debiasing methods under the multi-bias setting. In the training set, each digit class is spuriously correlated with two bias attributes-left color and right color ( <ref type="figure" target="#fig_5">Fig. 4 (b)</ref>). Following the terms used in LfF <ref type="bibr" target="#b35">[53]</ref>, we call samples that can be correctly predicted with the bias attribute as bias-aligned samples. Samples that cannot be correctly predicted with the bias attribute are called bias-conflicting    <ref type="bibr" target="#b35">[53]</ref>, we use the ratio of the bias-aligned samples for each bias attribute to indicate how strong the spurious correlation is in the training set. The two ratios for two bias attributes can be different, which is more common in the real-world scenario. The images in the testing set also contain two background colors, but the testing set has a balanced distribution of bias-aligned and bias-conflicting samples w.r.t. each bias attribute. Evaluation Metrics and Settings Following <ref type="bibr" target="#b35">[53]</ref>, we report the accuracy results in bias-aligned and bias-conflicting samples on the testing set. Since Multi-Color MNIST contains two bias attributes, we report the four accuracy results in the combination of (bias-aligned, bias-conflicting) ? (left color, right color), e.g., middle four rows in Tab. 1 for each method. We also report the unbiased accuracy, which averages the four results above. Here, we choose 0.99 as the ratio of bias-aligned samples w.r.t. left color and 0.95 as the ratio of bias-aligned samples w.r.t. right color. In this way, the left color is a more salient bias than the right color. We report the results of other ratio combinations in Appendix C. <ref type="bibr" target="#b2">3</ref>    The only difference is that LfF reports the results on the validation set of CelebA, whereas we use the validation set to select the epoch with the best validation set accuracy (bias labels in the validation set are not used) to report the results on the testing set. All methods (including LfF) are benchmarked under the same setting. We report results in two evaluation metrics: 1) Average Group Accuracy (Avg. Group Acc.), which calculates the unweighted average of accuracies in four groups between target attribute and bias attribute, i.e., (male, female) ? (blond, not blond); 2) Worst Group Accuracy (Worst Group Acc.) <ref type="bibr" target="#b39">[57]</ref>, which takes the lowest accuracy in the four groups. As shown in Tab. 2, DebiAN achieves better Average and Worst Group accuracy results, which shows that DebiAN can better mitigate gender bias without labels. We also conduct experiments on bFFHQ <ref type="bibr" target="#b18">[36]</ref> where the training data contains the spurious correlation between age and gender. We compare DebiAN with other methods of gender bias mitigation. We strictly follow the setting in <ref type="bibr" target="#b18">[36]</ref>. We report the age accuracy results on the bias-conflicting samples in the testing set in <ref type="table">Tab</ref>   akrishnan et al. <ref type="bibr" target="#b7">[8]</ref> leverages StyleGAN2 [34] to generate high-quality synthesized images and identify the hair length bias of the gender classifier, e.g., longer hair length makes the classifier predict the face as female. Related to their finding, the discoverer D in DebiAN identifies an interesting unknown bias: visible hair area. We use D to predict the bias attribute group assignment on images in CelebA. To better interpret the bias attribute, we further use the identity labels in CelebA to cluster images with the same identity. <ref type="figure">Fig. 6</ref> shows that D assigns images of the same identity into two distinct groups based on the visible hair area, which is verified by D's CAM [78] saliency maps. Strictly speaking, all females in <ref type="figure">Fig. 6</ref> have long hair. However, due to the hairstyle, pose, or occlusion, visible hair areas differ between the two groups. As a result, the gender classifier has lower predicted probabilities on the female images with smaller visible hair areas. More visualizations are shown in Appendix G.1.</p><formula xml:id="formula_13">p(female) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G Q C 8 y v 5 R 6 k M H o c V P N f + 8 N w 6 K V D s = " &gt; A A A C A H i c b V C 7 S g N B F J 3 1 G e M r a m F h M x i E 2 I T d K G g j B G 0 s I 5 g H J E u Y n d x N h s w + m L k r h i W N v 2 J j o Y i t n 2 H n 3 z h J t t D E A 8 M c z r n 3 z t z j x V J o t O 1 v a 2 l 5 Z X V t P b e R 3 9 z a 3 t k t 7 O 0 3 d J Q o D n U e y U i 1 P K Z B i h D q K F B C K 1 b A A k 9 C 0 x v e T P z m A y g t o v A e R z G</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Other Image Domains</head><p>Our method is not limited to synthetic and face image domains. Here we conduct experiments on action recognition and scene classification tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mitigating Place Bias in Action Recognition</head><p>We conduct experiments on Biased Action Recognition (BAR) dataset <ref type="bibr" target="#b35">[53]</ref>, an image dataset with the spurious correlation between action and place in the training set. The testing set only contains bias-conflicting samples. Hence, higher accuracy results on the testing set indicate better debiasing results. The accuracy results in Tab. 5 show that DebiAN achieves better debiasing results than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Cross-dataset Generalization on Scene Classification</head><p>We conduct experiments on the more challenging scene classification task, where datasets are more complex and may contain multiple unknown biases. The biases in this task are underexplored by previous works partly due to the lack of attribute labels. Due to the absence of attribute labels, we use cross-dataset generalization <ref type="bibr" target="#b46">[64]</ref>   <ref type="bibr" target="#b46">[64]</ref> in Places to improve the robustness against distributional shifts between different datasets. Identified Unknown Biases in Scene Classifier DebiAN discovers Places dataset's unknown biases that humans may not preconceive. In <ref type="figure">Fig. 7</ref>, the discoverer separates bedroom and restaurant images based on size of beds and indoor/outdoor. The vanilla classifier performs worse on bedroom images with twin-size beds and outdoor restaurant images (see more in Appendix G.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose Debiasing Alternate Networks to discover and mitigate the unknown biases. DebiAN identifies unknown biases that humans may not preconceive and achieves better unsupervised debiasing results. Our Multi-Color MNIST dataset surfaces previous methods' problems and demonstrates DebiAN's advantages in the multi-bias setting. Admittedly, our work has some limitations, e.g., DebiAN focuses on binary or continuously valued bias attributes, not multi-class ones. We hope our work can facilitate research on bias discovery and mitigation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>In DebiAN, the discoverer and classifier use the same architecture but do not share the parameters. On Multi-Color MNIST dataset (Sec. 4.1), we follow the same setting used in LfF <ref type="bibr" target="#b35">[53]</ref>'s experiment on Colored MNIST. We use Adam <ref type="bibr" target="#b19">[37]</ref> optimizer with 10 ?3 learning rate and 256 batch size. We use an MLP with three hidden layers (obtained from the LfF's official code * ). All models are trained for 100 epochs.</p><p>In the experiments for gender bias mitigation on CelebA <ref type="bibr" target="#b32">[50]</ref> dataset (Sec. 4.2), we follow most of the settings used in LfF. We use ResNet-18 [28] as the network architecture. We use horizontal flip for data augmentation during training. We use Adam optimizer with 10 ?4 learning rate and 256 batch size. All models are trained for 50 epochs. The only difference is that we use CelebA's validation set to choose the epoch where models achieve the best validation set accuracy and report the results on the testing set. Note that validation set accuracy does not use any bias attribute labels because unsupervised debiasing should not rely on any labels of bias attributes. LfF directly reports the results at the 50 epoch on the validation set, which is hard to be replicated as reported by other users in their official code GitHub repository ? .</p><p>In the experiments for gender bias mitigation on bFFHQ dataset <ref type="bibr" target="#b18">[36]</ref>, we use the same setting in <ref type="bibr" target="#b18">[36]</ref>. We use Adam as the optimizer with 256 batch size. All models are trained for 200 epochs. We use ResNet-18 as the backbone. We notice that Lee et al. <ref type="bibr" target="#b25">[43]</ref> use a different setting on bFFHQ dataset with StepLR for the learning rate scheduling, which is more complicated than the one in the original paper <ref type="bibr" target="#b18">[36]</ref>. Thus, we choose the former one as the setting on bFFHQ dataset. In the experiment of mitigating multiple biases in gender classifier on CelebA dataset (Sec. 4.2), we choose 64 as the batch size and ResNet-50 as the backbone of classifiers. All models are trained with 50 epochs. We use CelebA's validation set to choose the epoch that has the best validation set accuracy for each method. We report the results on the testing set. We use Adam as the optimizer with 10 ?4 learning rate.</p><p>On Biased Action Recognition (BAR) <ref type="bibr" target="#b35">[53]</ref> dataset, we use the setting in <ref type="bibr" target="#b35">[53]</ref>. We use Adam as the optimizer with 10 ?4 learning rate. The batch size is 256. We use 224 ? 224 random cropping for data augmentation. All models are trained with 90 epochs. In the scene classification task, we choose ResNet-18 as the backbone of classifiers and 128 as the batch size. We use Adam as the optimizer with 10 ?4 learning rate. All models are trained only on the Places [79] dataset for 50 epochs. We choose the epoch where the model achieves the best accuracy on Places's validation set and report the results on LSUN's [74] validation set.</p><p>The code is based on PyTorch <ref type="bibr" target="#b36">[54]</ref>. We modify LfF's code ? that generates Colored MNIST to create Multi-Color MNIST dataset.</p><p>In implementation, we add ? = 10 ?6 to the denominators ofP b + (?) and P b ? (?) (Eq. (3)) to avoid zero division.</p><p>For the discoverer D, we choose two different implementations for different numbers of classes of the target attribute. When the target attribute is binary (e.g., experiments in Sec. 4.2), i.e. number of classes is two, D predicts one value for each image, which is the predicted bias attribute group. We denote this implementation as "global" since two classes globally share the predicted bias attribute groups. When the target attribute has c &gt; 2 classes, e.g., ten classes in the digit classification, action recognition, and scene classification in Sec. 4.1 and Sec. 4.3, D predicts c values, where each value is the predicted bias attribute group of the corresponding target attribute class. We denote this implementation as "per class" since D predicts bias attribute groups for each target attribute class. We provide an ablation study on this in Appendix C.5.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Batch Size</head><p>In practice, {I i } n i=1 (defined in Sec. 3) is a mini-batch of images sampled from the dataset for optimizing the networks. One may have the concern that the sampled batch may not have enough images from different bias groups for the discoverer to assign. Therefore, we conduct an ablation study on different batch sizes on Multi-Color MNIST dataset with the same setting introduced in Sec 4.1, where the ratio of the left color is 0.99 and right color is 0.95. We report the accuracy results for images that are both bias-conflicting w.r.t. left color and right color bias attributes (see "both bias-conflicting" in Tab. 9). We also report the unbiased accuracy results. The results in Tab. 9 show that DebiAN can achieve better debiasing results under different batch sizes compared with the vanilla model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Ablation Study on Different Ratios</head><p>We conduct an ablation study on different ratios of bias-aligned samples in Multi-Color MNIST's training set. We keep the ratio for the right color bias attribute to 0.95 and use different ratios for left color bias attribute, ranging from 0.995 to 0.95. The results are shown in Tab. 10. DebiAN achieves better unbiased accuracy results and accuracy results on samples that are bias-conflicting w.r.t. both bias attributes. The only exception is the accuracy results of the samples that are bias-conflicting w.r.t. both bias attributes when both ratios are 0.95 (last section in Tab. 10). Both LfF and DebiAN achieve 39.6 accuracy results. However, our method achieves a lower standard deviation (0.2) than LfF (6.9) and achieves much better final unbiased results (81.8 vs. <ref type="bibr">68.5)</ref>. We also notice that LfF's debiasing results have a large standard deviation when the ratios of both bias attributes are 0.95. We provide an explanation in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Alternate Training</head><p>We conduct an ablation study on alternate training on the Multi-Color MNIST dataset with the same setting used in Sec. 4.1. To remove the alternate training from DebiAN, we follow EIIL <ref type="bibr" target="#b16">[17]</ref> and PGI <ref type="bibr" target="#b1">[2]</ref> to train the discoverer to identify the unknown biases in a classifier trained with one epoch. After training discoverer, we fix the parameters of discoverer and only train the classifier to perform debiasing. The results in Tab. 11 show that alternate training can improve the debiasing results, e.g., higher unbiased accuracy and higher accuracy for samples that are bias-conflicting w.r.t. both bias attributes (4th row), which demonstrates the necessity of alternate training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Bias Attribute Groups: Global vs. Per Class</head><p>As mentioned in Appendix B, the predicted bias attribute groups from the discoverer (D) are shared by both classes in the binary classification task. In the multi-class classification setting (e.g., digit classification, scene classification task, etc.), D predicts binary bias group assignments for each class. We justify our implementation choices with the results in Tab. 12.</p><p>For the binary age classification on bFFHQ dataset, there is no significant difference between the two implementation choices (i.e., differences are within error bars). Therefore, we choose "global" discoverer for the binary classification task due to its simplicity.</p><p>However, in the multi-class digit classification on Multi-Color MNIST dataset, we do observe the better result produced by the discoverer that predicts bias group assignment for each class (i.e., improvement is greater than the error bar). We suspect that predicting bias attribute groups per class in the binary classification task is redundant because the binary target attribute is spuriously correlated with the binary bias attribute. For example, if the target attribute age is spuriously correlated with the bias attribute gender, i.e., more young females and old males than old females and young males, then it is not necessary to predict bias attribute group for both genders since both genders share the same bias attribute groups. However, this may not be the case for multi-class settings. For example, in Multi-Color MNIST dataset, each digit class is spuriously correlated with a unique left color, e.g., for bias-aligned samples, digit class 0's left color is red but digit class 1's left color is yellow <ref type="figure">(Fig. 4)</ref>. In other words, the bias attribute values may not be shared globally across different target attribute classes. Therefore, we choose different numbers of outputs for the discoverer under different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Alternative Design for Debiasing: max C L EOV</head><p>One may consider an alternative design for debiasing-train the classifier C to maximize the Equal Opportunity Violation (EOV) loss, or formally max C L EOV . This alternative design, to some degree similar to GAN [23]'s training strategy, may look more "unified" since it lets the discoverer D and classifier C play the minmax game: min</p><formula xml:id="formula_14">C max D |P b + (?) ?P b ? (?)|,<label>(9)</label></formula><p>whereP b + (?) andP b ? (?) are defined in Eq. 3. This alternative design enables C to directly meet the Equal Opportunity [26,55] fairness criterion. More concretely, we implement this alternative design of C's objectives by the following loss function:</p><formula xml:id="formula_15">min C ? log 1 ? P b + (?) ?P b ? (?) + CE(p t (I i ), y i ),<label>(10)</label></formula><p>where the first ? log term implements C's objective in the minmax game (Eq. (9)) and the second term CE is the standard cross-entropy loss. We conduct an ablation study on the design for debiasing (i.e., playing minmax game vs. RCE loss (Eq. 6)) and the results on Multi-Color MNIST dataset are shown in Tab. <ref type="bibr" target="#b12">13</ref>. The results demonstrate that our RCE loss performs much better than the alternative design. We suspect the reason is that C in DebiAN has two goals -1) fooling the discoverer to achieve fairer results; 2) achieving higher accuracy by optimizing the standard cross-entropy loss, which is different from <ref type="bibr">GAN [23]</ref> where the generator only has one goal -fooling the discriminator to achieve better image quality of the synthesized images. Therefore, it is hard to control the balance between the two goals of C in this alternative design. In contrast, our RCE loss can better incorporate the two goals within a single objective function L RCE , leading to better debiasing results.  The three plots are results under three different random seeds. In the first two plots, LfF mainly discovers right color at the early training stage and gradually discovers both biases. In the third seed, it mainly discovers left color bias.</p><p>The results show that LfF are unstable in bias discovery when two biases are equally salient. In contrast, our DebiAN consistently discovers both biases at the early stage under different random seeds and gradually converges 50% bias discovery accuracy as debiasing is performed in the classifier</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Bias Discovery on Multi-Color MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Implementation Details</head><p>To evaluate the bias discovery results, we transform the outputs from LfF's "biased model" and DebiAN's discoverer into bias-aligned / bias-conflicting prediction by the following approaches. For LfF, since the biased model is trained to amplify the biases, the biased model's outputs predict ten colors aligned with digits. Thus, when its predicted color class is the same as the ground-truth digit class, e.g., c-th color for the c-th class, we regard its prediction as bias-aligned. Otherwise, its prediction is bias-conflicting.</p><p>For DebiAN, we use discoverer and classifier to predict each image I i 's predicted bias group assignment p(b | I i ) and predicted probability of the ground-truth class of the target attribute p t (I i ) on the entire testing set, respectively. Then, we compute the weighted average predicted probabilitiesP b + (?) andP b ? (?) (see Eq. 3) in two bias groups. IfP b + (?) ?P b ? (?), we use p(b = 1 | I i ) as the bias-aligned prediction and p(b = 0 | I i ) as the bias-conflicting prediction since the positive bias group has higher weighted average predicted probability, i.e., classifier performs better on the positive bias group. IfP b + (?) &lt;P b ? (?), we use p(b = 1 | I i ) as the bias-conflicting prediction and p(b = 0 | I i ) as the bias-aligned prediction.</p><p>After obtaining bias-aligned and bias-conflicting predictions, we compute the bias-aligned and bias-conflicting accuracy for left color and right color biases as follows. Each testing image has two labels-(1) bias-aligned/biasconflicting w.r.t. left color; (2) bias-aligned/bias-conflicting w.r.t. right color. Thus, the left color (or right color) bias discovery accuracy is computed based on the bias-aligned/bias-conflicting predictions against the left color (or right color) bias-aligned/bias-conflicting labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 More Bias Discovery Results under Different Ratios</head><p>In the main paper, we evaluate LfF and DebiAN's bias discovery results when the ratio of left color is 0.99 in <ref type="figure">Fig. 5</ref>. Here, we show results under more ratios in <ref type="figure">Fig. 8</ref>, where the ratios of left color are 0.98 ( <ref type="figure">Fig. 8 (a)</ref>) and 0.995 ( <ref type="figure" target="#fig_5">Fig. 8 (b)</ref>). The results are consistent with <ref type="figure">Fig. 5</ref>-LfF can only discover the more salient left color and cannot identify the less salient right color bias, whereas DebiAN's discoverer can discover both biases at the early training stage and the bias discovery accuracy gradually converges to 50% when debiasing is performed in the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Bias Discovery Results under Equally Salient Biases</head><p>We further evaluate bias discovery results when left color and right color biases are equally salient, i.e., ratios of both biases are 0.95. We found that LfF shows more unstable results under different random seeds than in previous settings. Therefore, we show bias discovery results under three different random seeds in <ref type="figure" target="#fig_13">Fig. 9</ref>. Under the first two random seeds (left and middle plots in <ref type="figure" target="#fig_13">Fig. 9</ref>), LfF first discovers right color bias and gradually discovers both biases. However, under the third random seed (the right plot in <ref type="figure" target="#fig_13">Fig. 9</ref>), LfF mainly discovers the left color bias. Therefore, our Multi-Color MNIST dataset reveals another weakness of LfF-unstable bias discovery results when two biases are equally salient, which also explains LfF's unstable debiasing results under the equally salient biases (LfF has large error bars, e.g., ?33.7 and ?25.9, of the accuracy results in Tab. 10). In contrast, DebiAN's bias discovery results are stable-consistent results across different random seeds and under different ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Detailed Discussion on Bias Discovery</head><p>Why LfF's bias discovery accuracies do not converge to 50%? In <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 8</ref>, LfF's bias discovery accuracies maintain at about 100% or 50% throughout the training epochs. One may wonder why it does not converge to 50% as DebiAN does. The reason is that DebiAN discovers biases from the classifier (L EOV is based on classifier 's output), whereas LfF identifies biases from the dataset. Concretely, LfF uses the assumption that the bias attribute is easier than the target attribute to define the bias and uses Generalized Crossentropy (GCE) loss [76] to train a biased model. GCE loss is defined by ?p q t log p t , where p t is bias model's predicted probability of the ground-truth class and q is a hyperparameter. Intuitively, it up-weights easy examples (i.e., high p t ) with high weight p q t and down-weights hard examples (i.e., low p t ) with low weight p q t . Therefore, the "biased model" focuses more on the easy examples in the dataset. However, it does not know any biases in the classifier (no classifier's outputs used in GCE). Therefore, whether the classifier is performing debiasing will not affect LfF's bias discovery, making the bias attribute accuracy stays the same throughout the entire training stage. In contrast, DebiAN's discoverer actively identifies biases in the classifier. Therefore, discoverer 's bias discovery results will converge to 50% as debiasing is performing in the classifier, making the discoverer harder to find the biases.</p><p>Bias Discovery: EIIL and PGI Here, we discuss the connection and difference between DebiAN and two previous methods-EIIL and PGI. In contrast to LfF that finds biases from the dataset based on the assumption, all EIIL, PGI, and DebiAN actively find biases from the classifier. However, DebiAN differs from EIIL and PGI in terms of the objective function, network architecture, and training scheme. We mainly introduce EIIL because PGI is a follow-up work for EIIL with a difference in the network architecture.</p><p>In terms of objective function, EIIL (and PGI) inversely uses the debiasing objective function-IRMv1 <ref type="bibr" target="#b5">[6]</ref>. In other words, while minimizing IRMv1 was designed for debiasing in previous works, EIIL maximizes the gradient norm penalty in IRMv1 to identify biases. However, this is suboptimal for two reasons. First, IRMv1 approximates IRM with gradient norm penalty to make it computationally tractable. However, the zero gradient norm may only indicate a local minimum instead of the global minimum. In contrast, DebiAN's EOV loss uses the principled definition to define the bias-violation of equal opportunity fairness criterion. Second, since the gradient norm does not have an upper bound, maximizing it leads to an optimization problem. As a comparison, DebiAN's EOV loss minimizes a bounded negative log-likelihood (Eq. 2), which is easier to be optimized.</p><p>Regarding network architecture, EIIL does not train any networks but directly optimizes a vector q ? R N for N images in the training set to maximize IRMv1's gradient norm, which can be regarded as directly optimize the bias group assignments. Since the vector q is only fitted to the training set, we cannot evaluate EIIL's bias discovery results on the balanced testing set. PGI uses a slightly different approach by training a small MLP that takes the classifier's features as the input and predicts the bias group assignments. PGI's bias discovery objective function is identical with EIIL's-maximizing IRMv1. We use the trained MLP to infer the bias-aligned / bias-conflicting on the testing set on Multi-Color MNIST dataset under the setting used in Sec. 4.1 (ratio w.r.t. left color is 0.99 and ratio w.r.t. right color is 0.95) based on a classifier trained with one epoch (explain in the next paragraph). The bias discovery accuracy results w.r.t. left color and right color are 50.6?1.9 and 49.9?0.3. In contrast, DebiAN's discoverer has the same network architecture as the classifier to predict bias group assignments from raw images, which enables discoverer to learn its own feature of the bias attribute directly from the raw images. As a result, DebiAN achieves about 60% to 75% accuracy at the first epoch (see <ref type="figure">Fig. 5</ref>). Therefore, we empirically show that DebiAN can discover biases more accurately.</p><p>Finally, with respect to the training scheme, by hypothesizing that the classifier learns bias features in the early stage, EIIL and PGI use a fixed classifier trained with one epoch to discover biases. However, this might not be the case in the multi-bias setting where multiple biases may be learned at different training stages. In contrast, DebiAN trains discoverer and classifier in an alternate fashion, enabling discoverer to find biases in the classifier during the entire training stage. Our ablation study on the alternate training (Appendix C.4) further demonstrates its benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results on Colored MNIST (single-bias setting)</head><p>We also compare with other methods on Colored MNIST in a single-bias setting. There are two variants of Colored MNIST datasets in previous works-(1) adding colors to the foreground (i.e., digit) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b28">46]</ref>; <ref type="bibr" target="#b1">(2)</ref> adding colors to the background <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">56]</ref>. Therefore, we conduct experiment on both variants of Colored MNIST dataset. We denote the Colored MNIST with foreground color as "Colored MNIST (foreground)" and the Colored MNIST with background color as "Colored MNIST (background)." Same with the experiment setting on Multi-Color MNIST, we follow the setting used in LfF <ref type="bibr" target="#b35">[53]</ref>, including using MLP as the network architecture, training with 100 epochs, etc. Same with LfF, we report the results under four ratios of bias-aligned samples in the training set-0.995, 0.99, 0.98, and 0.95. In terms of evaluation metrics, we follow LfF to report the accuracy results on bias-conflicting samples and unbiased accuracy. We additionally report the accuracy results on bias-aligned samples.</p><p>We report LfF's reported results on Colored MNIST (foreground). Besides, we also replicate LfF's results via their officially released code. We cannot replicate their reported results. This issue is also reported in the official code's GitHub repository by other people ? .</p><p>The results on Colored MNIST (foreground color) are in Tab. 14. Although LfF achieves better bias-conflicting accuracy results, it also achieves much lower bias-aligned accuracy, revealing that LfF's reweighing method overly focuses on the bias-conflicting samples than the bias-aligned samples. As a result, LfF achieves low unbiased accuracy. Overall, DebiAN achieves comparable or slightly lower unbiased accuracy results.</p><p>The results on Colored MNIST (background color) are in Tab. <ref type="bibr" target="#b14">15</ref>. Similar to the results on Colored MNIST (foreground color), LfF achieves better biasconflicting accuracy but low bias-aligned accuracy and unbiased accuracy. Overall, DebiAN achieves comparable or slightly better unbiased accuracy results.</p><p>We also notice that PGI achieves inconsistent results on Colored MNIST (foreground color) and Colored MNIST (background color). While PGI achieves very good results on Colored MNIST (foreground color), e.g., top-1 unbiased accuracy when ratio = 0.995 and ratio = 0.98 in Tab. 14, it achieves bad results on Colored MNIST (background color), e.g. the lowest unbiased accuracy results in Tab. 15. We suspect the reason is that PGI is overly sensitive to hyperparameters, e.g., the coefficient of the KL-divergence for debiasing. In contrast, our method achieves good results across two Colored MNIST variants without tuning or changing any hyperparameters.</p><p>Finally, we restate that Colored MNIST is a single-bias setting, which may not be the case in real-world scenarios where multiple biases exist. Therefore, we regard that we should focus more on our new Multi-Color dataset to evaluate the debiasing results w.r.t. multiple biases (Tab. 10), where DebiAN achieves better debiasing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More results on Mitigating Multiple Biases in Gender Classification</head><p>In  <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_0">Fig. 10</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 More Examples of Discovered Biases on Scene Images</head><p>We show more examples of discovered biases on the scene images in <ref type="figure" target="#fig_0">Fig. 11</ref>. For bridge images, D predicts two bias groups. When the photos are taken on the bridge, the vanilla classifier performs worse. In comparison, the vanilla classifier performs better when the photos are taken off the bridge with some correlated backgrounds, such as mountains or water. For conference room images, the vanilla classifier performs better when the table is the major object in the scene. However, it performs worse when the conference room images have many tables, and the tables do not occlude the chairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Is DebiAN a hard negative method?</head><p>No. Hard negative methods focus on addressing the imbalanced problem by letting the classifier focus on hard misclassified examples and pay less attention to easy examples. A seminal hard negative method is focal loss <ref type="bibr" target="#b30">[48]</ref>, which reweighs the standard cross-entropy loss (? log(p t ), where p t is the predicted  uses discoverer 's predicted bias group assignments to reweigh the cross-entropy loss, and the discoverer is trained with L EOV to differentiate classifier 's p t on examples from the same target class where the Equal Opportunity is violated. Therefore, DebiAN is not a hard negative method because we do not use easy or hard samples (i.e., low or high p t ) to perform reweighing, but rather use samples' estimated bias group assignments to perform debiasing. We also compare with focal loss on Multi-Color MNIST dataset. We use ? = 0.25 and ? = 2.0 as they perform the best in <ref type="bibr" target="#b30">[48]</ref>. The results are shown in Tab. 17, where focal loss's results are even worse than the vanilla model. The results prove that hard negative methods are not well-suited for debiasing. Since DebiAN is different from hard negative methods by using estimated bias group assignments to mitigate biases, our method achieves much better debiasing results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Is EOV loss simply doing clustering?</head><p>No. EOV loss is used to train discoverer to classify different bias groups values. Therefore, instead of simply clustering classifier 's prediction, EOV loss guide the discoverer to do a classification for the bias group assignment. The results in <ref type="figure">Fig. 5, Fig. 8</ref>, and <ref type="figure" target="#fig_13">Fig. 9</ref> show that discoverer can accurately classify if the samples on the testing set (unseen during training) are bias-aligned or bias-conflicting, demonstrating that EOV loss guides the discoverer to do classification based on different bias groups values and it can generalize to testing set's images.</p><p>H.3 Will classifier achieve 100% accuracy such that discoverer cannot predict bias group assignments?</p><p>No. First, note that discoverer 's EOV loss is based on classifier 's predicted probabilities instead of thresholded hard predictions. Therefore, 100% accuracy does not indicate that discoverer cannot predict the bias group assignments. Second, we show the vanilla model's cross-entropy loss and DebiAN's RCE loss on blond hair classification and gender classification tasks in <ref type="figure" target="#fig_0">Fig. 12</ref>. The results show that the losses do not completely converge to zero, which proves that there always exist samples in the training set that classifier does not achieve 1.0 predicted probabilities. Therefore, it still leaves the room for discoverer to predict bias group assignments based on classifier 's different predictions on different samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4 What if the mini-batch only contains the samples from a single bias group?</head><p>It mainly happens under two conditions-(1) very strong spurious correlation;</p><p>(2) small mini-batch size. For the first case, our ablation study on the ratios on Multi-Color MNIST dataset (Appendix D.2) shows that DebiAN achieves better debiasing results even when the ratio of left color is 0.995 (i.e., very strong spurious correlation). For the second case, our ablation study on different batch sizes (Appendix C.2) shows that our method still achieves strong debiasing results when the batch size is small. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.6 Evaluation on Discovered Unknown Biases</head><p>In <ref type="figure" target="#fig_0">Fig. 6, Fig. 7, Fig. 10, and Fig. 11</ref>, we show some interesting unknown biases that human may not preconceive via saliency map. One may wonder if there exist other approaches to evaluate the results. First, it is hard to directly quantify the findings due to lack of annotations of the discovered bias attributes, e.g., CelebA does not have attribute annotations or segmentation annotations of visible hair area. Using other datasets (e.g., COCO <ref type="bibr" target="#b31">[49]</ref>) with more attribute or segmentation ground-truth may not help since the discovered unknown biases may still be out of the annotations. Second, UDIS <ref type="bibr" target="#b20">[38]</ref>, a recent bias discovery method, also uses saliency maps to interpret the bias. We believe that using saliency maps is an established evaluation protocol in this task. Third, although it is hard to evaluate bias discovery in real-world dataset, our evaluation of bias discovery on Multi-Color MNIST <ref type="figure" target="#fig_13">(Fig. 5, Fig. 8, and Fig. 9</ref>) has shown that DebiAN achieves strong bias discovery results. Finally, we believe that our better debiasing results w.r.t. Hair Length bias attribute on the Transects dataset can also indirectly prove that discoverer identifies visible hair area bias (see Appendix F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7 Why not add two colors to the foreground in Multi-Color MNIST?</head><p>The reason is that foreground digits are not always well aligned to the center of the images. If we assign two colors to the foreground digit based on whether the foreground is on the left or right, we may encounter cases where the digit is mainly on the right and only has a tiny area on the left, e.g. an italic digit "1." Thus, we choose to add colors to the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.8 Difference between Multi-Color MNIST and Biased MNIST</head><p>Shrestha et al. <ref type="bibr" target="#b43">[61]</ref> recently proposed the Biased MNIST dataset, which contains seven biases. However, all seven biases in Biased MNIST share the same biasaligned ratio (i.e., 0.7). In contrast, our Multi-color MNIST contains two biases that are in different bias-aligned ratios, which we believe is more common in real-world scenarios and can better reveal the failure modes of existing debiasing methods. For example, while LfF performs the best in the Biased MNIST benchmark, our Multi-Color MNIST dataset reveals that LfF can only discover the more salient bias-the bias with a larger bias-align ratio.</p><p>H.9 Why evaluate on scene classification task?</p><p>First, we regard that scene classification as a core vision task on par with object classification. Second, while many previous debiasing works create datasets <ref type="bibr" target="#b18">[36,</ref><ref type="bibr" target="#b35">53]</ref> that contain a single bias (e.g., artificially introducing the spurious correlation w.r.t. a single bias), we believe that the classical cross-dataset generalization evaluation approach <ref type="bibr" target="#b46">[64]</ref> does not have the single-bias assumption. The subgroup distribution w.r.t. multiple biases may vary across different datasets, which is closer to the real-world setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.10 Limitations and Future Directions</head><p>We list some limitations that DebiAN has not fully resolved. First, we only assume that the bias attribute is binary or continuously valued from 0 to 1 (i.e., two bias attribute groups). Future works can focus on extending DebiAN to discover and mitigate unknown biases with more than two groups. Second, DebiAN can only discover the biases caused by spurious correlation rather than lack of coverage. For example, suppose a face image dataset only contains long-hair female images and does not contain any short-hair female images. In that case, DebiAN cannot discover the hair length bias attribute because the discoverer does not have samples to categorize female images into two groups in terms of the hair length bias attribute. Finally, in terms of interpreting the discovered biases, DebiAN's approach, using the saliency maps on real-world images, is not as easy as interpreting biases from synthesized counterfactual images <ref type="bibr" target="#b23">[41,</ref><ref type="bibr" target="#b29">47]</ref>. Future works can further explore better interpreting the discovered unknown biases on real-world images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.11 Potential Negative Social Impact</head><p>One potential negative social impact is that DebiAN's discovered biases could be used as a way to choose real-world images as the adversarial images to attack visual models in some safety-critical domains, e.g., self-driving cars. Therefore, we encourage the defender to use DebiAN to mitigate the biases as the defense strategy.</p><p>Since our bias discovery approach relies on the fairness criterion based on equations, e.g., equal true positive rates among two groups, our method cannot identify the biases that a fairness criterion cannot capture, e.g., discrimination against the historically disadvantaged group. To mitigate this issue, we include a model card <ref type="bibr" target="#b34">[52]</ref> in the released code to clarify that our method's intended use case is discovering and mitigating biases that violate the equal opportunity fairness criterion <ref type="bibr">[26]</ref>, and the model's out-of-scope use case is identifying or mitigating other biases that cannot be captured by the equation of a fairness criterion, e.g., discrimination against the historically disadvantaged group [25].   <ref type="table" target="#tab_2">Table 12</ref>: Results of ablation study on discoverer 's outputs. Bolded methods are used to report results in the main paper. For binary classification (i.e., age classification on bFFHQ dataset) task, there is no significant difference whether or not the discoverer predicts bias attribute groups per class or globally. When it comes to the multi-class digit classification on Multi-Color MNIST dataset, predicting bias attribute groups per class has better accuracy results on the images that are bias-conflicting w.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>in . D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; mitigates unknown biases identified by . D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e 7 6 Z k t 0 i S C 9 u I A R b C E d p P G T K d g = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L h 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 t x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p U e s X S 2 7 Z X Y C s E y 8 j J c h Q 7 x e / e o O Y p R F K w w T V u u u 5 i f G n V B n O B M 4 K v V R j Q t m Y D r F r q a Q R a n + 6 O H R G L q w y I G G s b E l D F u r v i S m N t J 5 E g e 2 M q B n p V W 8 u / u d 1 U x P e + l M u k 9 S g Z M t F Y S q I i c n 8 a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z g Q / B W X 1 4 n r U r Z u y p X G t e l 6 l 0 W R x 7 O 4 B w u w Y M b q M I 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J c z j M s = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e 7 6 Z k t 0 i S C 9 u I A R b C E d p P G T K d g = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L h 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 t x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p U e s X S 2 7 Z X Y C s E y 8 j J c h Q 7 x e / e o O Y p R F K w w T V u u u 5 i f G n V B n O B M 4 K v V R j Q t m Y D r F r q a Q R a n + 6 O H R G L q w y I G G s b E l D F u r v i S m N t J 5 E g e 2 M q B n p V W 8 u / u d 1 U x P e + l M u k 9 S g Z M t F Y S q I i c n 8 a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z g Q / B W X 1 4 n r U r Z u y p X G t e l 6 l 0 W R x 7 O 4 B w u w Y M b q M I 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J c z j M s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r U h C M r 8 3 S m j Z y 0 9 F o g V 5 7 5 O J e 3 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 i k q M e C H j y 2 Y G u h D W W z n b R r N 5 u w u x F K 6 S / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b T z v 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l k 4 y x b D J E p G o d k g 1 C i 6 x a b g R 2 E 4 V 0 j g U + B C O b m b + w x M q z R N 5 b 8 Y p B j E d S B 5 x R o 2 V G r e 9 c s V z v T n I K v F z U o E c 9 V 7 5 q 9 t P W B a j N E x Q r T u + l 5 p g Q p X h T O C 0 1 M 0 0 p p S N 6 A A 7 l k o a o w 4 m 8 0 O n 5 M w q f R I l y p Y 0 Z K 7 + n p j Q W O t x H N r O m J q h X v Z m 4 n 9 e J z P R d T D h M s 0 M S r Z Y F G W C m I T M v i Z 9 r p A Z M b a E M s X t r Y Q N q a L M 2 G x K N g R / + e V V 0 r p w / U u 3 2 q h W a m 4 e R x F O 4 B T O w Y c r q M E d 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A J R T j L 0 = &lt; / l a t e x i t &gt; D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z Y 6 C V Y K 1 x y u Q f t i O 1 q X d 4 l t / O S U = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 h E 1 G O h F 4 8 t 2 A 9 o Q 9 l s p + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M B F c G 8 / 7 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k p e N U M W y y W M S q E 1 K N g k t s G m 4 E d h K F N A o F t s N J b e 6 3 n 1 B p H s s H M 0 0 w i O h I 8 i F n 1 F i p U e u X K 5 7 r L U D W i Z + T C u S o 9 8 t f v U H M 0 g i l Y Y J q 3 f W 9 x A Q Z V Y Y z g b N S L 9 W Y U D a h I + x a K m m E O s g W h 8 7 I h V U G Z B g r W 9 K Q h f p 7 I q O R 1 t M o t J 0 R N W O 9 6 s 3 F / 7 x u a o Z 3 Q c Z l k h q U b L l o m A p i Y j L / m g y 4 Q m b E 1 B L K F L e 3 E j a m i j J j s y n Z E P z V l 9 d J 6 8 r 1 b 9 z r x n W l 6 u Z x F O E M z u E S f L i F K t x D H Z r A A O E Z X u H N e X R e n H f n Y 9 l a c P K Z U / g D 5 / M H k s + M v A = = &lt; / l a t e x i t &gt; C Debiasing Alternate Networks (DebiAN). We alternately train two networks-a discover and a classifier. Discoverer actively identifies classifier 's unknown biases. At the same time, the classifier mitigates the biases identified by the discoverer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e 7 6 Z k t 0 i S C 9 u I A R b C E d p P G T K d g = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L h 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 t x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p U e s X S 2 7 Z X Y C s E y 8 j J c h Q 7 x e / e o O Y p R F K w w T V u u u 5 i f G n V B n O B M 4 K v V R j Q t m Y D r F r q a Q R a n + 6 O H R G L q w y I G G s b E l D F u r v i S m N t J 5 E g e 2 M q B n p V W 8 u / u d 1 U x P e + l M u k 9 S g Z M t F Y S q I i c n 8 a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z g Q / B W X 1 4 n r U r Z u y p X G t e l 6 l 0 W R x 7 O 4 B w u w Y M b q M I 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J c z j M s = &lt; / l a t e x i t &gt; freeze weights min D LEOV + LUA &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s X v C / J p + A l 6 c z B A E l t 0 A J y r a m z k = " &gt; A A A C H n i c b V D L S s N A F J 3 4 r P V V d e l m s A i C U B I f 6 L K + w I V g B d M W m h I m 0 6 k O T i Z h 5 k Y s I V / i x l 9 x 4 0 I R w Z X + j Z O 2 C 2 0 9 c O F w z r 3 c e 0 8 Q C 6 7 B t r + t i c m p 6 Z n Z w l x x f m F x a b m 0 s l r X U a I o c 2 k k I t U M i G a C S + Y C B 8 G a s W I k D A R r B H c n u d + 4 Z 0 r z S F 5 D L 2 b t k N x I 3 u W U g J H 8 0 r 4 X c u m f Y i 8 k c E u J S C 8 y 3 w P 2 A O n Z Z T 3 D 2 / 8 Y 7 l H m l 8 p 2 x e 4 D j x N n S M p o i J p f + v Q 6 E U 1 C J o E K o n X L s W N o p 0 Q B p 4 J l R S / R L C b 0 j t y w l q G S h E y 3 0 / 5 7 G d 4 0 S g d 3 I 2 V K A u 6 r v y d S E m r d C w P T m R + r R 7 1 c / M 9 r J d A 9 b K d c x g k w S Q e L u o n A E O E 8 K 9 z h i l E Q P U M I V d z c i u k t U Y S C S b R o Q n B G X x 4 n 9 Z 2 K s 1 v Z u d o r V 4 + H c R T Q O t p A W 8 h B B 6 i K z l E N u Y i i R / S M X t G b 9 W S 9 W O / W x 6 B 1 w h r O r K E / s L 5 + A N s K o v M = &lt; / l a t e x i t &gt; discoverer classifier D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e 7 6 Z k t 0 i S C 9 u I A R b C E d p P G T K d g = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L h 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 t x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p U e s X S 2 7 Z X Y C s E y 8 j J c h Q 7 x e / e o O Y p R F K w w T V u u u 5 i f G n V B n O B M 4 K v V R j Q t m Y D r F r q a Q R a n + 6 O H R G L q w y I G G s b E l D F u r v i S m N t J 5 E g e 2 M q B n p V W 8 u / u d 1 U x P e + l M u k 9 S g Z M t F Y S q I i c n 8 a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z g Q / B W X 1 4 n r U r Z u y p X G t e l 6 l 0 W R x 7 O 4 B w u w Y M b q M I 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J c z j M s = &lt; / l a t e x i t &gt; freeze weights min C LRCE &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V J v F Z H 1 v V X F 5 c l l r H V x y v h R c k F A = " &gt; A A A C B n i c b V B N S 8 N A E N 3 U r 1 q / q h 5 F W C y C p 5 J U Q Y / F I n j w U M W 2 Q h P C Z r t t l 2 4 2 Y X c i l t C T F / + K F w + K e P U 3 e P P f u G l 7 0 N Y H A 4 / 3 Z p i Z F 8 S C a 7 D t b y u 3 s L i 0 v J J f L a y t b 2 x u F b d 3 m j p K F G U N G o l I 3 Q V E M 8 E l a w A H w e 5 i x U g Y C N Y K B r X M b 9 0 z p X k k b 2 E Y M y 8 k P c m 7 n B I w k l / c d 0 M u / R p 2 Q w J 9 S k R 6 N f J d Y A + Q 3 t Q u R n 6 x Z J f t M f A 8 c a a k h K a o + 8 U v t x P R J G Q S q C B a t x 0 7 B i 8 l C j g V b F R w E 8 1 i Q g e k x 9 q G S h I y 7 a X j N 0 b 4 0 C g d 3 I 2 U K Q l 4 r P 6 e S E m o 9 T A M T G d 2 r Z 7 1 M v E / r 5 1 A 9 8 x L u Y w T Y J J O F n U T g S H C W S a 4 w x W j I I a G E K q 4 u R X T P l G E g k m u Y E J w Z l + e J 8 1 K 2 T k u V 6 5 P S t X z a R x 5 t I c O 0 B F y 0 C m q o k t U R w 1 E 0 S N 6 R q / o z X q y X q x 3 6 2 P S m r O m M 7 v o D 6 z P H 0 P 2 m P 8 = &lt; / l a t e x i t &gt; (a) Training the discoverer to find biases in the classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Training the classifier to mitigate biases found by discoverer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of Debiasing Alternate Networks (DebiAN). DebiAN consists of two networks-a discoverer D and a classifier C. D is trained with LEOV and LUA (Sec. 3.1) to find the unknown biases in C. C is optimized with LRCE (Sec. 3.2) to mitigate the biases identified by D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>images with the same target attribute ground-truth (e.g., female images) classifier C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r e 7 6 Z k t 0 i S C 9 u I A R b C E d p P G T K d g = " &gt; A A A B 6 H i c b V D L T g J B E O z F F + I L 9 e h l I j H x R H b R R I 9 E L h 4 h k U c C G z I 7 9 M L I 7 O x m Z t a E E L 7 A i w e N 8 e o n e f N v H G A P C l b S S a W q O 9 1 d Q S K 4 N q 7 7 7 e Q 2 N r e 2 d / K 7 h b 3 9 g 8 O j 4 v F J S 8 e p Y t h k s Y h V J 6 A a B Z f Y N N w I 7 C Q K a R Q I b A f j 2 t x v P 6 H S P J Y P Z p K g H 9 G h 5 C F n 1 F i p U e s X S 2 7 Z X Y C s E y 8 j J c h Q 7 x e / e o O Y p R F K w w T V u u u 5 i f G n V B n O B M 4 K v V R j Q t m Y D r F r q a Q R a n + 6 O H R G L q w y I G G s b E l D F u r v i S m N t J 5 E g e 2 M q B n p V W 8 u / u d 1 U x P e + l M u k 9 S g Z M t F Y S q I i c n 8 a z L g C p k R E 0 s o U 9 z e S t i I K s q M z a Z g Q / B W X 1 4 n r U r Z u y p X G t e l 6 l 0 W R x 7 O 4 B w u w Y M b q M I 9 1 K E J D B C e 4 R X e n E f n x X l 3 P p a t O S e b O Y U / c D 5 / A J c z j M s = &lt; / l a t e x i t &gt; (e.g., gender classifier) t e x i t s h a 1 _ b a s e 6 4 = " L i s 8 R 2 l O 6 t N Y + J + 1 y Z N u Q w e T g D k = " &gt; A A A C A 3 i c b V D L S s N A F L 3 x W e u r 6 k 4 3 g 0 V w V Z I q 6 E Y o u t F d B f u A J o b J d N I O n T y Y m Q g l B N z 4 K 2 5 c K O L W n 3 D n 3 z h p s 9 D W A x c O 5 9 z L v f d 4 M W d S m e a 3 s b C 4 t L y y W l o r r 2 9 s b m 1 X d n b b M k o E o S 0 S 8 U h 0 P S w p Z y F t K a Y 4 7 c a C 4 s D j t O O N r n K / 8 0 C F Z F F 4 p 8 Y x d Q I 8 C J n P C F Z a c i v 7 d o r s A K u h 5 6 c 3 m c u Q n b k p u 7 C y e 2 1 W z Z o 5 A Z o n V k G q U K D p V r 7 s f k S S g I a K c C x l z z J j 5 a R Y K E Y 4 z c p 2 I m m M y Q g P a E / T E A d U O u n k h w w d a a W P / E j o C h W a q L 8 n U h x I O Q 4 8 3 Z m f K 2 e 9 X P z P 6 y X K P 3 d S F s a J o i G Z L v I T j l S E 8 k B Q n w l K F B 9 r g o l g + l Z E h l h g o n R s Z R 2 C N f v y P G n X a 9 Z J r X 5 7 W m 1 c F n G U 4 A A O 4 R g s O I M G X E M T W k D g E Z 7 h F d 6 M J + P F e D c + p q 0 L R j G z B 3 9 g f P 4 A i t u X d A = = &lt; / l a t e x i t &gt; {p t (I i )} n i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c V D 8 S j v A i Q g a r T a A T n L k 7 2 T d J K 4 = " &gt; A A A C C X i c b V C 7 S g N B F J 2 N r x h f U U u b w S D E J u x G Q R s h a K N d B P O A b F x m J 7 P J k N k H M 3 e F s G x r 4 6 / Y W C h i 6 x / Y + T f O J l t o 4 o G B w z n n M v c e N x J c g W l + G 4 W l 5 Z X V t e J 6 a W N z a 3 u n v L v X V m E s K W v R U I S y 6 x L F B A 9 Y C z g I 1 o 0 k I 7 4 r W M c d X 2 V + 5 4 F J x c P g D i Y R 6 / t k G H C P U w J a c s r Y T n D k Q B X b P o G R 6 y U 3 q c O P s Z 0 6 C b + w 0 n s d q Z g 1 c w q 8 S K y c V F C O p l P + s g c h j X 0 W A B V E q Z 5 l R t B P i A R O B U t L d q x Y R O i Y D F l P 0 4 D 4 T P W T 6 S U p P t L K A H u h 1 C 8 A P F V / T y T E V 2 r i u z q Z 7 a v m v U z 8 z + v F 4 J 3 3 E x 5 E M b C A z j 7 y Y o E h x F k t e M A l o y A m m h A q u d 4 V 0 x G R h I I u r 6 R L s O Z P X i T t e s 0 6 q d V v T y u N y 7 y O I j p A h 6 i K L H S G G u g a N V E L U f S I n t E r e j O e j B f j 3 f i Y R Q t G P r O P / s D 4 / A E 0 n J l k &lt; / l a t e x i t &gt; predicted probabilities of the ground-truth class (e.g., ) p(gender = female | Ii) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e U k J y 9 + 0 F N C W + x 8 q X j y h K w F M o p A = " &gt; A A A C G 3 i c b V A 9 S w N B E N 2 L X z F + R S 1 t F o M Q m 3 A X B W 2 E o I 1 2 C k a F J I S 9 v b l k y d 7 e s T s n h i P / w 8 a / Y m O h i J V g 4 b 9 x 8 1 F o 4 o N d 3 r y Z Y W a e n 0 h h 0 H W / n d z c / M L i U n 6 5 s L K 6 t r 5 R 3 N y 6 M X G q O d R 5 L G N 9 5 z M D U i i o o 0 A J d 4 k G F v k S b v 3 e 2 T B / e w / a i F h d Y z + B V s Q 6 S o S C M 7 R S u 1 h N y k 2 E B 8 w 6 o A L Q A 3 p C x 3 E I E Z M w o M 1 I B P Z j 2 P X D 7 G L Q F v v t Y s m t u C P Q W e J N S I l M c N k u f j a D m K c R K O S S G d P w 3 A R b G d M o u B 1 R a K Y G E s Z 7 r A M N S x W L w L S y 0 W 0 D u m e V g I a x t k 8 h H a m / O z I W G d O P f F s 5 X N J M 5 4 b i f 7 l G i u F x K x M q S R E U H w 8 K U 0 k x p k O j a C A 0 c J R 9 S x j X w u 5 K e Z d p x t H a W b A m e N M n z 5 K b a s U 7 q F S v D k u 1 0 4 k d e b J D d k m Z e O S I 1 M g 5 u S R 1 w s k j e S a v 5 M 1 5 c l 6 c d + d j X J p z J j 3 b 5 A + c r x 9 k Z K G f &lt; / l a t e x i t &gt; (a) Traditional approach for identifying the known bias attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( b )</head><label>b</label><figDesc>Our method for discovering the unknown bias attribute.discoverer D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; positive group D &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W o f l C 9 O s Y A D N d O + 1 4 n z c Y B d C A m o = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q Y 9 B P X h M w D w g W c L s p D c Z M z u 7 z M w K I e Q L v H h Q x K u f 5 M 2 / c Z L s Q R M L G o q q b r q 7 g k R w b V z 3 2 8 m t r W 9 s b u W 3 C z u 7 e / s H x c O j p o 5 T x b D B Y h G r d k A 1 C i 6 x Y b g R 2 E 4 U 0 i g Q 2 A p G t z O / 9 Y R K 8 1 g + m H G C f k Q H k o e c U W O l + l 2 v W H L L 7 h x k l X g Z K U G G W q / 4 1 e 3 H L I 1 Q G i a o 1 h 3 P T Y w / o c p w J n B a 6 K Y a E 8 p G d I A d S y W N U P u T + a F T c m a V P g l j Z U s a M l d / T 0 x o p P U 4 C m x n R M 1 Q L 3 s z 8 T + v k 5 r w 2 p 9 w m a Q G J V s s C l N B T E x m X 5 M + V 8 i M G F t C m e L 2 V s K G V F F m b D Y F G 4 K 3 / P I q a V b K 3 k W 5 U r 8 s V W + y O P J w A q d w D h 5 c Q R X u o Q Y N Y I D w D K / w 5 j w 6 L 8 6 7 8 7 F o z T n Z z D H 8 g f P 5 A 5 i 3 j M w = &lt; / l a t e x i t &gt; bias attribute label (e.g., hair length) long hair short hair negative group TPRlong hair 6 = TPRshort hair &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q h Y a k i g f P T i L + l k 6 h d u d e Y 2 Z + F w = " &gt; A A A C J n i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Z e o D 3 Q h F N y 6 r 9 A V t K Z n 0 t g 1 m k j G 5 I 5 a h X + P G X 3 H j o i L i z k 8 x f Q g + D 4 Q c z j m X 5 J 4 g k s K i 7 7 9 5 M 7 N z 8 w u L q a X 0 8 s r q 2 n p m Y 7 N i d W w 4 l L m W 2 t Q C Z k E K B W U U K K E W G W B h I K E a X J + P / O o t G C u 0 K m E / g m b I u k p 0 B G f o p F b m t I F w h 0 m p e D V o T a j U q k t 7 T J g B b S i 4 o b 8 C t q c N T h K t T N b P H f k j 0 N 8 k n x v f f p Z M U W x l h o 2 2 5 n E I C r l k 1 t b z f o T N h B k U X M I g 3 Y g t R I x f s y 7 U H V U s B N t M x m s O 6 K 5 T 2 r S j j T s K 6 V j 9 O p G w 0 N p + G L h k y L B n f 3 o j 8 S + v H m P n p J k I F c U I i k 8 e 6 s S S o q a j z m h b G O A o + 4 4 w b o T 7 K + U 9 Z h h H 1 2 z a l f C 5 K f 2 f V P Z z + Y P c / u V h t n A 2 r S N F t s k O 2 S N 5 c k w K 5 I I U S Z l w c k 8 e y Z A 8 e w / e k / f i v U 6 i M 9 5 0 Z o t 8 g / f + A T W x p 3 Y = &lt; / l a t e x i t &gt; The Hair length attribute is the bias if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>TPRshort hair &lt; l a t e x i t s h a 1 _ b a s e 6 4 =Fig. 3 :</head><label>143</label><figDesc>" h a f y W J i C w T x 6 z 1 8 T d 2 C v D I E l T v E = " &gt; A A A C B X i c d V D L S s N A F J 3 U V 6 2 v q E t d D B b B V U i r o s u i G 5 d V + o I 2 h M l 0 0 g 6 d P J i 5 E U v I x o 2 / 4 s a F I m 7 9 B 3 f + j Z O 2 g s 8 D l 3 s 4 5 1 5 m 7 v F i w R X Y 9 r t R m J t f W F w q L p d W V t f W N 8 z N r Z a K E k l Z k 0 Y i k h 2 P K C Z 4 y J r A Q b B O L B k J P M H a 3 u g 8 9 9 v X T C o e h Q 0 Y x 8 w J y C D k P q c E t O S a u z 1 g N 5 A 2 6 l e Z O 6 V q G E n A Q 8 J l 5 p p l 2 z q 2 c + D f p G J N u l 1 G M 9 R d 8 6 3 X j 2 g S s B C o I E p 1 K 3 Y M T k o k c C p Y V u o l i s W E j s i A d T U N S c C U k 0 6 u y P C + V v r Y j 6 S u E P B E / b q R k k C p c e D p y Y D A U P 3 0 c v E v r 5 u A f + q k P I w T Y C G d P u Q n A k O E 8 0 h w n 0 t G Q Y w 1 I V R y / V d M h 0 Q S C j q 4 k g 7 h 8 1 L 8 P 2 l V r c q h V b 0 8 K t f O Z n E U 0 Q 7 a Q w e o g k 5 Q D V 2 g O m o i i m 7 R P X p E T 8 a d 8 W A 8 G y / T 0 Y I x 2 9 l G 3 2 C 8 f g C q M p l R &lt; / l a t e x i t &gt; TPRlong hair &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S r D + j g u q S 8 4 V I n w b 3 2 k v L k 8 G h P w = " &gt; A A A C B H i c d V D L S g M x F M 3 U V 6 2 v q s t u g k V w V a Z V 0 W X R j c s q f U E 7 D J k 0 b U M z y Z D c E c s w C z f + i h s X i r j 1 I 9 z 5 N 6 Y P w e e B y z 2 c c y / J P U E k u A H X f X c y C 4 t L y y v Z 1 d z a + s b m V n 5 7 p 2 l U r C l r U C W U b g f E M M E l a w A H w d q R Z i Q M B G s F o / O J 3 7 p m 2 n A l 6 z C O m B e S g e R 9 T g l Y y c 8 X u s B u I K n X r l J / R o W S A z w k X K d + v u i W j t 0 J 8 G 9 S L k 2 7 W 0 R z 1 P z 8 W 7 e n a B w y C V Q Q Y z p l N w I v I R o 4 F S z N d W P D I k J H Z M A 6 l k o S M u M l 0 y N S v G + V H u 4 r b U s C n q p f N x I S G j M O A z s Z E h i a n 9 5 E / M v r x N A / 9 R I u o x i Y p L O H + r H A o P A k E d z j m l E Q Y 0 s I 1 d z + F d M h 0 Y S C z S 1 n Q / i 8 F P 9 P m p V S + b B U u T w q V s / m c W R R A e 2 h A 1 R G J 6 i K L l A N N R B F t + g e P a I n 5 8 5 5 c J 6 d l 9 l o x p n v 7 K J v c F 4 / A L c e m M c = &lt; / l a t e x i t &gt;P b + (?) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y l X 9 1 f g S 9 a W f 7 V M l c y w G U x w + C g 0 = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q O D G z W A R K k J J q q D L o h u X F e w D m h g m 0 0 k 7 d D I J M x M h x C z 8 F T c u F H H r b 7 j z b 5 y 2 W W j r g Q u H c + 7 l 3 n v 8 m F G p L O v b K C 0 t r 6 y u l d c r G 5 t b 2 z v m 7 l 5 H R o n A p I 0 j F o m e j y R h l J O 2 o o q R X i w I C n 1 G u v 7 4 e u J 3 H 4 i Q N O J 3 K o 2 J G 6 I h p w H F S G n J M w 8 c H 4 m s l X u Z f 3 + a 1 5 w R U l m a n 3 h m 1 a p b U 8 B F Y h e k C g q 0 P P P L G U Q 4 C Q l X m C E p + 7 Y V K z d D Q l H M S F 5 x E k l i h M d o S P q a c h Q S 6 W b T + 3 N 4 r J U B D C K h i y s 4 V X 9 P Z C i U M g 1 9 3 R k i N Z L z 3 k T 8 z + s n K r h 0 M 8 r j R B G O Z 4 u C h E E V w U k Y c E A F w Y q l m i A s q L 4 V 4 h E S C C s d W U W H Y M + / v E g 6 j b p 9 V m / c n l e b V 0 U c Z X A I j k A N 2 O A C N M E N a I E 2 w O A R P I N X 8 G Y 8 G S / G u / E x a y 0 Z x c w + + A P j 8 w f g L Z Y B &lt; / l a t e x i t &gt;P b (?) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c j M S f + 0 l p J 5 G H M 1 n y S 5 C V M L o U c 8 = " &gt; A A A B / 3 i c b V D L S s N A F J 3 U V 6 2 v q O D G z W A R 6 s K S V E G X R T c u K 9 g H N D F M p p N 2 6 G Q S Z i Z C i F n 4 K 2 5 c K O L W 3 3 D n 3 z h t s 9 D W A x c O 5 9 z L v f f 4 M a N S W d a 3 U V p a X l l d K 6 9 X N j a 3 t n f M 3 b 2 O j B K B S R t H L B I 9 H 0 n C K C d t R R U j v V g Q F P q M d P 3 x 9 c T v P h A h a c T v V B o T N 0 R D T g O K k d K S Z x 4 4 P h J Z K / c y / / 4 0 r z k j p L I 0 P / H M q l W 3 p o C L x C 5 I F R R o e e a X M 4 h w E h K u M E N S 9 m 0 r V m 6 G h K K Y k b z i J J L E C I / R k P Q 1 5 S g k 0 s 2 m 9 + f w W C s D G E R C F 1 d w q v 6 e y F A o Z R r 6 u j N E a i T n v Y n 4 n 9 d P V H D p Z p T H i S I c z x Y F C Y M q g p M w 4 I A K g h V L N U F Y U H 0 r x C M k E F Y 6 s o o O w Z 5 / e Z F 0 G n X 7 r N 6 4 P a 8 2 r 4 o 4 y u A Q H I E a s M E F a I I b 0 A J t g M E j e A a v 4 M 1 4 M l 6 M d + N j 1 l o y i p l 9 8 A f G 5 w / j S Z Y D &lt; / l a t e x i t &gt; max D |P b + (?) P b (?)| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " x O 2 9 J C L k T D a 4 M o L p F Q + 1 k u w I w m M = " &gt; A A A C I X i c b V B N S 8 N A E N 3 U r 1 q / o h 6 9 L B Z B E U t S B T 2 K e v B Y w W q h i W G y 3 b Z L N x / s b s S S 5 q 9 4 8 a 9 4 8 a B I b + K f c d s G 1 O q D g c d 7 M 8 z M 8 2 P O p L K s D 6 M w M z s 3 v 1 B c L C 0 t r 6 y u m e s b N z J K B K F 1 E v F I N H y Q l L O Q 1 h V T n D Z i Q S H w O b 3 1 e + c j / / a e C s m i 8 F r 1 Y + o G 0 A l Z m x F Q W v L M E y e A B + 8 C D x w f R F r L v N S / 2 8 9 2 n S 6 o t J / t 4 Q P 8 w z j 4 N g a e W b Y q 1 h j 4 L 7 F z U k Y 5 a p 4 5 d F o R S Q I a K s J B y q Z t x c p N Q S h G O M 1 K T i J p D K Q H H d r U N I S A S j c d f 5 j h H a 2 0 c D s S u k K F x + r P i R Q C K f u B r z s D U F 0 5 7 Y 3 E / 7 x m o t o n b s r C O F E 0 J J N F 7 Y R j F e F R X L j F B C W K 9 z U B I p i + F Z M u C C B K h 1 r S I d j T L / 8 l N 9 W K f V i p X h 2 V T 8 / y O I p o C 2 2 j X W S j Y 3 S K L l E N 1 R F B j + g Z v a I 3 4 8 l 4 M d 6 N 4 a S 1 Y O Q z m + g X j M 8 v R 3 u j l w = = &lt; / l a t e x i t &gt; backpropagation {bi} n i=1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F P / o I + t N N + L w r P p w j j Y 5 6 w d S G q g = " &gt; A A A B + X i c d V D L S s N A F L 3 x W e s r 6 t L N Y B F c h a Q q u h G K b l x W s A 9 o Y p h M J + 3 Q y Y O Z S a G E / I k b F 4 q 4 9 U / c + T d O 2 g o + D 1 z u 4 Z x 7 m T s n S D m T y r b f j Y X F p e W V 1 c p a d X 1 j c 2 v b 3 N l t y y Q T h L Z I w h P R D b C k n M W 0 p Z j i t J s K i q O A 0 0 4 w u i r 9 z p g K y Z L 4 V k 1 S 6 k V 4 E L O Q E a y 0 5 J u m m 6 P A Z 2 7 h 5 + z C K e 6 0 V L O t U 7 s E + k 0 c a 9 r t G s z R 9 M 0 3 t 5 + Q L K K x I h x L 2 X P s V H k 5 F o o R T o u q m 0 m a Y j L C A 9 r T N M Y R l V 4 + v b x A h 1 r p o z A R u m K F p u r X j R x H U k 6 i Q E 9 G W A 3 l T 6 8 U / / J 6 m Q r P v Z z F a a Z o T G Y P h R l H K k F l D K j P B C W K T z T B R D B 9 K y J D L D B R O q y q D u H z p + h / 0 q 5 b z r F V v z m p N S 7 n c V R g H w 7 g C B w 4 g w Z c Q x N a Q G A M 9 / A I T 0 Z u P B j P x s t s d M G Y 7 + z B N x i v H x w Y k 1 I = &lt; / l a t e x i t &gt; (a):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(b) New dataset for multi-bias setting: Multi-Color MNIST(a) previous dataset for single-bias setting: Colored MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig</head><label></label><figDesc>Fig. 4: Comparison between (a) previous Colored MNIST [7, 35,46] with a single color bias and (b) our new Multi-Color MNIST dataset that contains two bias attributesleft color and right color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>4 A e u H w h e c o Z G 6 h c O 4 1 E F 4 x N S H g E k Y n 9 I r 6 p R t u 1 s o 2 u a a g C 4 S J y N F k q H W L X x 1 e h F P A g i R S 6 Z 1 2 7 F j d F O m U H A z N 9 9 J N M S M D 1 k f 2 o a G L A D t p t M F x v T E K D 3 q R 8 q c E O l U / d 2 R s k D r U e C Z y o D h Q M 9 7 E / E / r 5 2 g f + m m I o w T h J D P H v I T S T G i k z R o T y j g K E e G M K 6 E + S v l A 6 Y Y R 5 N Z 3 o T g z K + 8 S B q V s n N W r t y d F 6 v X W R w 5 c k S O S Y k 4 5 I J U y S 2 p k T r h Z E y e y S t 5 s 5 6 s F + v d + p i V L l l Z z w H 5 A + v z B 5 U u l R U = &lt; / l a t e x i t &gt; p(female) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G Q C 8 y v 5 R 6 k M H o c V P N f + 8 N w 6 K V D s = " &gt; A A A C A H i c b V C 7 S g N B F J 3 1 G e M r a m F h M x i E 2 I T d K G g j B G 0 s I 5 g H J E u Y n d x N h s w + m L k r h i W N v 2 J j o Y i t n 2 H n 3 z h J t t D E A 8 M c z r n 3 z t z j x V J o t O 1 v a 2 l 5 Z X V t P b e R 3 9 z a 3 t k t 7 O 0 3 d J Q o D n U e y U i 1 P K Z B i h D q K F B C K 1 b A A k 9 C 0 x v e T P z m A y g t o v A e R z G 4 A e u H w h e c o Z G 6 h c O 4 1 E F 4 x N S H g E k Y n 9 I r 6 p R t u 1 s o 2 u a a g C 4 S J y N F k q H W L X x 1 e h F P A g i R S 6 Z 1 2 7 F j d F O m U H A z N 9 9 J N M S M D 1 k f 2 o a G L A D t p t M F x v T E K D 3 q R 8 q c E O l U / d 2 R s k D r U e C Z y o D h Q M 9 7 E / E / r 5 2 g f + m m I o w T h J D P H v I T S T G i k z R o T y j g K E e G M K 6 E + S v l A 6 Y Y R 5 N Z 3 o T g z K + 8 S B q V s n N W r t y d F 6 v X W R w 5 c k S O S Y k 4 5 I J U y S 2 p k T r h Z E y e y S t 5 s 5 6 s F + v d + p i V L l l Z z w H 5 A + v z B 5 U u l R U = &lt; / l a t e x i t &gt; small visible hair area large visible hair area p(female) = 0.08 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F R / c 5 z 9 o 7 U / 2 g 9 Q m S A W k g v p G q H 0 = " &gt; A A A C E X i c b V C 7 T g J B F J 3 F F + I L t b S Z S E y w I b t o I o 0 J 0 c Y S E 3 k k Q M j s c B c m z D 4 y c 9 d I N v y C j b 9 i Y 6 E x t n Z 2 / o 3 D Q q H g T S Z z c s 6 9 9 8 w c N 5 J C o 21 / W 5 m V 1 b X 1 j e x m b m t 7 Z 3 c v v 3 / Q 0 G G s O N R 5 K E P V c p k G K Q K o o 0 A J r U g B 8 1 0 J T X d 0 P d W b 9 6 C 0 C I M 7 H E f Q 9 d k g E J 7 g D A 3 V y x e j Y g f h A R M P f C Z h c k o v a U q k q x M F / U l i l + z K p J c v m D s t u g y c O S i Q e d V 6 + a 9 O P + S x D w F y y b R u O 3 a E 3 Y Q p F N w Y 5 T q x h o j x E R t A 2 8 C A + a C 7 S W o 7 o S e G 6 V M v V O Y E S F P 2 9 0 T C f K 3 H v m s 6 f Y Z D va h N y f + 0 d o x e p Z u I I I o R A j 4 z 8 m J J M a T T e G h f K O A o x w Y w r o R 5 K + V D p h h H E 2 L O h O A s f n k Z N M o l 5 6 x U v j 0 v V K / m c W T J E T k m R e K Q C 1 I l N 6 R G 6 o S T R / J M X s m b 9 W S 9 W O / W x 6 w 1 Y 8 1 n D s m f s j 5 / A C E 2 n T E = &lt; / l a t e x i t &gt; p(female) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 2 l 4 b M P k d R n 4 z O 0 A 3 W / Z 9 l i + l h k = " &gt; A A A C E 3 i c b V B N S w M x E M 3 6 b f 2 q e v Q S L I J 6 K L t V 0 I t Q 9 O J R w W q h L S W b z r b B b H Z J Z s W y 7 H / w 4 l / x 4 k E R r 1 6 8 + W / M b n t Q 6 0 D I 4 7 2 Z e c n z Y y k M u u 6 X M z U 9 M z s 3 v 7 B Y W l p e W V 0 r r 2 9 c m y j R H B o 8 k p F u + s y A F A o a K F B C M 9 b A Q l / C j X 9 7 l u s 3 d 6 C N i N Q V D m P o h K y v R C A 4 Q 0 t 1 y / v x b h v h H t M A Q i Y h 2 6 M n t C C K 1 W l f A 6 g s 9 a q u m 3 X L F d f e e d F J 4 I 1 B h Y z r o l v + b P c i n o S g k E t m T M t z Y + y k T K P g 1 q r U T g z E j N + y P r Q s V C w E 0 0 k L 4 4 z u W K Z H g 0 j b o 5 A W 7 M + J l I X G D E P f d o Y M B + a v l p P / a a 0 E g + N O K l S c I C g + M g o S S T G i e U C 0 J z R w l E M L G N f C v p X y A d O M o 4 2 x Z E P w / n 5 5 E l z X q t 5 B t X Z 5 W K m f j u N Y I F t k m + w S j x y R O j k n F 6 R B O H k g T + S F v D q P z r P z 5 r y P W q e c 8 c w m + V X O x z f F S 5 4 U &lt; / l a t e x i t &gt; p(female) = 0.08 &lt; l a t e x i t s h a 1 _ b a s e 6 4= " b Y a s f C / 3 Y r o A M v O z 7 U a 4 z R g e q L E = " &gt; A A A C A H i c b V C 7 T g J B F J 3 F F + I L t b C w m U h M s C G 7 a C K N C d H G E h N 5 J E D I 7 H A X J s w + M n P X S D Y 0 / o q N h c b Y + h l 2 / o 0 D b K H g S S Z z c s 6 9 d + Y e N 5 J C o 2 1 / W 5 m V 1 b X 1 j e x m b m t 7 Z 3 c v v 3 / Q 0 G G s O N R 5 K E P V c p k G K Q K o o 0 A J r U g B 8 1 0 J T X d 0 M / W b D 6 C 0 C I N 7 H E f Q 9 d k g E J 7 g D I 3 U y x 9 F x Q 7 C I y Y e + E z C 5 I x e U b t k V 3 r 5 g r l m o M v E S U m B p K j 1 8 l + d f s h j H w L k k m n d d u w I u w l T K L i Z m + v E G i L G R 2 w A b U M D 5 o P u J r M F J v T U KH 3 q h c q c A O l M / d 2 R M F / r s e + a S p / h U C 9 6 U / E / r x 2 j V + k m I o h i h I D P H / J i S T G k 0 z R o X y j g K M e G M K 6 E + S v l Q 6 Y Y R 5 N Z z o T g L K 6 8 T B r l k n N e K t 9 d F K r X a R x Z c k x O S J E 4 5 J J U y S 2 p k T r h Z E K e y S t 5 s 5 6 s F + v d + p i X Z q y 0 5 5 D 8 g f X 5 A 5 / H l R w = &lt; / l a t e x i t &gt; p(female) = 0.84 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 7 o D 5 s p a K K U E a 1 m + j j B e 3 7 Q C uv Q = " &gt; A A A C A H i c b V C 7 S g N B F J 3 1 G e M r a m F h M x i E 2 I T d G D C N E L S x j G A e k I Q w O 7 m b D J l 9 M H N X D E s a f 8 X G Q h F b P 8 P O v 3 G S b K G J B y 4 c z r l 3 5 t 7 j R l J o t O 1 v a 2 V 1 b X 1 j M 7 O V 3 d 7 Z 3 d v P H R w 2 d B g r D n U e y l C 1 X K Z B i g D q K F B C K 1 L A f F d C 0 x 3 d T P 3 m A y g t w u A e x x F 0 f T Y I h C c 4 Q y P 1 c s d R o Y P w i I k H P p M w O a d X1 C 5 W y r 1 c 3 i 7 a M 9 B l 4 q Q k T 1 L U e r m v T j / k s Q 8 B c s m 0 b j t 2 h N 2 E K R T c v J v t x B o i x k d s A G 1 D A + a D 7 i a z A y b 0 z C h 9 6 o X K V I B 0 p v 6 e S J i v 9 d h 3 T a f P c K g X v a n 4 n 9 e O 0 a t 0 E x F E M U L A 5 x 9 5 s a Q Y 0 m k a t C 8 U c J R j Q x h X w u x K + Z A p x t F k l j U h O I s n L 5 N G q e h c F E t 3 5 X z 1 O o 0 j Q 0 7 I K S k Q h 1 y S K r k l N V I n n E z I M 3 k l b 9 a T 9 W K 9 W x / z 1 h U r n T k i f 2 B 9 / g C l 3 5 U g &lt; / l a t e x i t &gt; Discovered bias of gender classifier: visible hair area based on discoverer 's saliency map. p(female) is vanilla classifier's predicted probability of the face is female. In the two groups predicted by D, the visible hair areas are different, where the classifier has different confidences on gender for the same identity p(bedroom) = 0.89 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U 2 N l u Y 4 X P s r 5 k O u B Y d q 2 d v x G 9 r I = " &gt; A A A C A X i c b V A 9 S w N B E N 3 z M 8 a v U x v B Z j E I s Q l 3 U T A W Q t D G U s F o I D n C 3 m a S L O 7 d H r t z Y j h i 4 1 + x s V D E 1 n 9 h 5 7 9 x E 6 / w 6 8 H A 4 7 0 Z Z u a F i R Q G P e / D m Z q e m Z 2 b L y w U F 5 e W V 1 b d t f V L o 1 L N o c G V V L o Z M g N S x N B A g R K a i Q Y W h R K u w u u T s X 9 1 A 9 o I F V / g M I E g Y v 1 Y 9 A R n a K W O u 5 m U 2 w i 3 m I X Q 1 U p F o 1 1 6 R L 1 K 7 b D j l r y K N w H 9 S / y c l E i O s 4 7 7 3 u 4 q n k Y Q I 5 f M m J b v J R h k T K P g E k b F d m o g Y f y a 9 a F l a c w i M E E 2 + W B E d 6 z S p T 2 l b c V I J + r 3 i Y x F x g y j 0 H Z G D A f m t z c W / / N a K f Z q Q S b i J E W I + d e i X i o p K j q O g 3 a F B o 5 y a A n j W t h b K R 8 w z T j a 0 I o 2 B P / 3 y 3 / J Z b X i 7 1 W q 5 / u l + n E e R 4 F s k W 1 S J j 4 5 I H V y S s 5 I g 3 B y R x 7 I E 3 l 2 7 p 1 H 5 8 V 5 / W q d c v K Z D f I D z t s n m 7 y V r Q = = &lt; / l a t e x i t &gt; p(bedroom) = 0.67 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E T a J a 3 Y L 3 2 k m m 2 1 M x H p e Z Z S i + y o = " &gt; A A A C A X i c b V A 9 S w N B E N 3 z M 8 a v U x v B Z j E I s Q l 3 U Y y N E L S x V D A a S I 6 w t 5 k k i 3 u 3 x + 6 c G I 7 Y + F d s L B S x 9 V / Y + W / c x C v 8 e j D w e G + G m X l h I o V B z / t w p q Z n Z u f m C w v F x a X l l V V 3 b f 3 S q F R z a H A l l W 6 G z I A U M T R Q o I R m o o F F o Y S r 8 P p k 7 F / d g D Z C x R c 4 T C C I W D 8 W P c E Z W q n j b i b l N s I t Z i F 0 t V L R a J c e U a 9 y U O u 4 J a / i T U D / E j 8 n J Z L j r O O + t 7 u K p x H E y C U z p u V 7 C Q Y Z 0 y i 4 h F G x n R p I G L 9 m f W h Z G r M I T J B N P h j R H a t 0 a U 9 p W z H S i f p 9 I m O R M c M o t J 0 R w 4 H 5 7 Y 3 F / 7 x W i r 3 D I B N x k i L E / G t R L 5 U U F R 3 H Q b t C A 0 c 5 t I R x L e y t l A + Y Z h x t a E U b g v / 7 5 b / k s l r x 9 y r V 8 / 1 S / T i P o 0 C 2 y D Y p E 5 / U S J 2 c k j P S I J z c k Q f y R J 6 d e + f R e X F e v 1 q n n H x m g / y A 8 / Y J l a q V q Q = = &lt; / l a t e x i t &gt; (a) Discovered bias in the bedroom class. twin size bed outdoor king / queen size bed (b) Discovered bias in the restaurant class. p(bedroom) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + L + o C T l B y Q / X U / Z d t X F P 2 s c p 3 5 o = " &gt; A A A C A X i c b V D L S g N B E J y N r x h f U S + C l 8 E g x E v Y j Y J e h K A X j x H M A 5 I Q Z i e d Z M j s z j L T K 4 Y l X v w V L x 4 U 8 e p f e P N v n D w O m l j Q d F H V z U y X H 0 l h 0 H W / n d T S 8 s r q W n o 9 s 7 G 5 t b 2 T 3 d 2 r G h V r D h W u p N J 1 n x m Q I o Q K C p R Q j z S w w J d Q 8 w f X Y 7 9 2 D 9 o I F d 7 h M I J W w H q h 6 A r O 0 E r t 7 E G U b y I 8 Y O J D R y s V j E 7 o J f U K r t v O 5 l z b x q C L x J u R H J m h 3 M 5 + N T u K x w G E y C U z p u G 5 E b Y S p l F w C a N M M z Y Q M T 5 g P W h Y G r I A T C u Z X D C i x 1 b p 0 K 7 S t k K k E / X 3 R s I C Y 4 a B b y c D h n 0 z 7 4 3 F / 7 x G j N 2 L V i L C K E Y I + f S h b i w p K j q O g 3 a E B o 5 y a A n j W t i / U t 5 n m n G 0 o W V s C N 7 8 y Y u k W i x 4 p 4 X i 7 V m u d D W L I 0 0 O y R H J E 4 + c k x K 5 I W V S I Z w 8 k m f y S t 6 c J + f F e X c + p q M p Z 7 a z T / 7 A + f w B g 3 e V n Q = = &lt; / l a t e x i t &gt; p(bedr &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + L + o C T l B y Q / X U / Z d t X F P 2 s c p 3 5 o = " &gt; A A A C A X i c b V D L S g N B E J y N r x h f U S + C l 8 E g x E v Y j Y J e h K A X j x H M A 5 I Q Z i e d Z M j s z j L T K 4 Y l X v w V L x 4 U 8 e p f e P N v n D w O m l j Q d F H V z U y X H 0 l h 0 H W / n d T S 8 s r q W n o 9 s 7 G 5 t b 2 T 3 d 2 r G h V r D h W u p N J 1 n x m Q I o Q K C p R Q j z S w w J d Q 8 w f X Y 7 9 2 D 9 o I F d 7 h M I J W w H q h 6 A r O 0 E r t 7 E G U b y I 8 Y O J D R y s V j E 7 o J f U K r t v O 5 l z b x q C L x J u R H J m h 3 M 5 + N T u K x w G E y C U z p u G 5 E b Y S p l F w C a N M M z Y Q M T 5 g P W h Y G r I A T C u Z X D C i x 1 b p 0 K 7 S t k K k E / X 3 R s I C Y 4 a B b y c D h n 0 z 7 4 3 F / 7 x G j N 2 L V i L C K E Y I + f S h b i w p K j q O g 3 a E B o 5 y a A n j W t i / U t 5 n m n G 0 o W V s C N 7 8 y Y u k W i x 4 p 4 X i 7 V m u d D W L I 0 0 O y R H J E 4 + c k x K 5 I W V S I Z w 8 k m f y S t 6 c J + f F e X c + p q M p Z 7 a z T / 7 A + f w B g 3 e V n Q = = &lt; / l a t e x i t &gt; indoor p(restau &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " W C C s k v C q 2 g m S n 4 p f N 6 z p 7 p V 5 + Y 8 = " &gt; A A A C B H i c b V A 9 S w N B E N 3 z M 8 a v U 8 s 0 i 0 G I T b h T Q V M I Q R v L C M Y E k i P s b S a 6 u L d 3 7 M 6 J 4 U h h 4 1 + x s V D E 1 h 9 h 5 7 9 x E 6 / Q 6 I O B x 3 s z z M w L E y k M e t 6 n M z M 7 N 7 + w W F g q L q + s r q 2 7 G 5 u X J k 4 1 h y a P Z a z b I T M g h Y I m C p T Q T j S w K J T Q C m 9 O x 3 7 r F r Q R s b r A Y Q J B x K 6 U G A j O 0 E o 9 t 5 R U u g h 3 m G k w y F L N F I 5 2 6 T H 1 q r V a z y 1 7 V W 8 C + p f 4 O S m T H I 2 e + 9 H t x z y N Q C G X z J i O 7 y U Y Z E y j 4 B J G x W 5 q I G H 8 h l 1 B x 1 L F I j B B N n l i R H e s 0 q e D W N t S S C f q z 4 m M R c Y M o 9 B 2 R g y v z b Q 3 F v / z O i k O j o J M q C R F U P x 7 0 S C V F G M 6 T o T 2 h Q a O c m g J 4 1 r Y W y m / Z p p x t L k V b Q j + 9 M t / y e V e 1 d + v 7 p 0 f l O s n e R w F U i L b p E J 8 c k j q 5 I w 0 S J N w c k 8 e y T N 5 c R 6 c J + f V e f t u n X H y m S 3 y C 8 7 7 F 0 T e l y 0 = &lt; / l a t e x i t &gt; p(restaurant) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T D q p u G d / m k X h z w 2 R o P + m 0 b c h C j E = " &gt; A A A C B H i c b V C 7 S g N B F J 2 N r x h f U c s 0 g 0 G I T d i N g j Z C 0 M Y y g n l A E s L s 5 C Y Z M j u 7 z N w V w 5 L C x l + x s V D E 1 o + w 8 2 + c P A p N P H C 5 h 3 P u Z e Y e P 5 L C o O t + O 6 m V 1 b X 1 j f R m Z m t 7 Z 3 c v u 3 9 Q M 2 G s O V R 5 K E P d 8 J k B K R R U U a C E R q S B B b 6 E u j + 8 n v j 1 e 9 B G h O o O R x G 0 A 9 Z X o i c 4 Q y t 1 s r m o 0 E J 4 w E S D Q R Z r p n B 8 Q i + p V 3 T d T j b v 2 j Y B X S b e n O T J H J V O 9 q v V D X k c g E I u m T F N z 4 2 w n T C N g k s Y Z 1 q x g Y j x I e t D 0 1 L F A j D t Z H r E m B 5 b p U t 7 o b a l k E 7 V 3 x s J C 4 w Z B b 6 d D B g O z K I 3 E f / z m j H 2 L t q J U F G M o P j s o V 4 s K Y Z 0 k g j t C g 0 c 5 c g S x r W w f 6 V 8 w D T j a H P L 2 B C 8 x Z O X S a 1 U 9 E 6 L p d u z f P l q H k e a 5 M g R K R C P n J M y u S E V U i W c P J J n 8 k r e n C f n x X l 3 P m a j K W e + c 0 j + w P n 8 A S s U l x w = &lt; / l a t e x i t &gt; p(restaurant) = 0.68 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T H H n Z x 5 V 7 s W u Q X P 2 2 S C 3 s P 0 E s W 8 = " &gt; A A A C B H i c b V A 9 S w N B E N 3 z M 8 a v U 8 s 0 i 0 G I T b h T 0 T R C 0 M Y y g j G B 5 A h 7 m 4 k u 7 u 0 d u 3 N i O F L Y + F d s L B S x 9 U f Y + W / c x C s 0 + m D g 8 d 4 M M / P C R A q D n v f p z M z O z S 8 s F p a K y y u r a + v u x u a l i V P N o c l j G e t 2 y A x I o a C J A i W 0 E w 0 s C i W 0 w p v T s d + 6 B W 1 E r C 5 w m E A Q s S s l B o I z t F L P L S W V L s I d Z h o M s l Q z h a N d e k y 9 6 m G t 5 5 a 9 q j c B / U v 8 n J R J j k b P / e j 2 Y 5 5 G o J B L Z k z H 9 x I M M q Z R c A m j Y j c 1 k D B + w 6 6 g Y 6 l i E Z g g m z w x o j t W 6 d N B r G 0 p p B P 1 5 0 T G I m O G U W g 7 I 4 b X Z t o b i / 9 5 n R Q H t S A T K k k R F P 9 e N E g l x Z i O E 6 F 9 o Y G j H F r C u B b 2 V s q v m W Y c b W 5 F G 4 I / / f J f c r l X 9 f e r e + c H 5 f p J H k e B l M g 2 q R C f H J E 6 O S M N 0 i S c 3 J N H 8 k x e n A f n y X l 1 3 r 5 b Z 5 x 8 Z o v 8 g v P + B T 7 L l y k = &lt; / l a t e x i t &gt; p(restaurant) = 0.25 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " l O o a 1 G G A F E y E 4 6 N f Q P 8 U m D s I 0 m A = " &gt; A A A C B H i c b V A 9 S w N B E N 3 z M 8 a v q G W a x S D E J t x F R R s h a G M Z w X x A E s L e Z p I s 2 d s 7 d u f E c K S w 8 a / Y W C h i 6 4 + w 8 9 + 4 + S g 0 8 c H A 4 7 0 Z Z u b 5 k R Q G X f f b W V p e W V 1 b T 2 2 k N 7 e 2 d 3 Y z e / t V E 8 a a Q 4 W H M t R 1 n x m Q Q k E F B U q o R x p Y 4 E u o + Y P r s V + 7 B 2 1 E q O 5 w G E E r Y D 0 l u o I z t F I 7 k 4 3 y T Y Q H T D Q Y Z L F m C k f H 9 J K 6 h e J Z O 5 N z C + 4 E d J F 4 M 5 I j M 5 T b m a 9 m J + R x A A q 5 Z M Y 0 P D f C V s I 0 C i 5 h l G 7 G B i L G B 6 w H D U s V C 8 C 0 k s k T I 3 p k l Q 7 t h t q W Q j p R f 0 8 k L D B m G P i 2 M 2 D Y N / P e W P z P a 8 T Y v W g l Q k U x g u L T R d 1 Y U g z p O B H a E R o 4 y q E l j G t h b 6 W 8 z z T j a H N L 2 x C 8 + Z c X S b V Y 8 E 4 K x d v T X O l q F k e K Z M k h y R O P n J M S u S F l U i G c P J J n 8 k r e n C f n x X l 3 P q a t S 8 5 s 5 o D 8 g f P 5 A z Q r l y I = &lt; / l a t e x i t &gt; Discovered biases in Places [79]dataset. We apply CAM on discoverer to generate saliency map. The value p(bedroom) (p(restaurant)) is vanilla classifier's predicted probability of the scene image is bedroom (restaurant)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>7 L 8 update</head><label>78</label><figDesc>19. Dhar, P., Gleason, J., Roy, A., Castillo, C.D., Chellappa, R.: PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition. In: The IEEE International Conference on Computer Vision (ICCV) (2021) 20. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R.: Fairness through awareness. In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (2012) 21. Geirhos, R., Jacobsen, J.H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., Wichmann, F.A.: Shortcut learning in deep neural networks. Nature Machine Intelligence (2020) 22. Gong, S., Liu, X., Jain, A.K.: Jointly De-biasing Face Recognition and Demographic Attribute Estimation. In: The European Conference on Computer Vision (ECCV) (2020) 23. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in Neural Information Processing Systems (2014) 24. Grgic-Hlaca, N., Zafar, M.B., Gummadi, K.P., Weller, A.: The case for process fairness in learning: Feature selection for fair decision making. In: NIPS Symposium on Machine Learning and the Law (2016) 25. Hanna, A., Denton, E., Smart, A., Smith-Loud, J.: Towards a critical race methodology in algorithmic fairness. In: Conference on Fairness, Accountability, and Transparency (2020) 26. Hardt, M., Price, E., Srebro, N.: Equality of Opportunity in Supervised Learning. In: Advances in Neural Information Processing Systems (2016) 27. Hazirbas, C., Bitton, J., Dolhansky, B., Pan, J., Gordo, A., Ferrer, C.C.: Towards Measuring Fairness in AI: The Casual Conversations Dataset. arXiv:2104.02821 [cs] (2021) 28. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 29. Hendricks, L.A., Burns, K., Saenko, K., Darrell, T., Rohrbach, A.: Women also Snowboard: Overcoming Bias in Captioning Models. In: The European Conference on Computer Vision (ECCV) (2018) 30. Jia, S., Meng, T., Zhao, J., Chang, K.W.: Mitigating Gender Bias Amplification in Distribution by Posterior Regularization. In: Annual Meeting of the Association for Computational Linguistics (2020) 31. Joo, J., K?rkk?inen, K.: Gender Slopes: Counterfactual Fairness for Computer Vision Models by Attribute Manipulation. In: International Workshop on Fairness, Accountability, Transparency and Ethics in Multimedia (2020) 32. Kamiran, F., Calders, T.: Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems (2012) 33. Karras, T., Laine, S., Aila, T.: A Style-Based Generator Architecture for Generative Adversarial Networks. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 34. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and Improving the Image Quality of StyleGAN. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 35. Kim, B., Kim, H., Kim, K., Kim, S., Kim, J.: Learning Not to Learn: Training Deep Neural Networks With Biased Data. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019) 70. Wang, T., Yue, Z., Huang, J., Sun, Q., Zhang, H.: Self-Supervised Learning Disentangled Group Representation as Feature. In: Advances in Neural Information Processing Systems (2021) 71. Wang, X., Ang, M.H., Lee, G.H.: Cascaded Refinement Network for Point Cloud Completion. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 72. Wang, Z., Liu, X., Li, H., Sheng, L., Yan, J., Wang, X., Shao, J.: CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval. In: The IEEE International Conference on Computer Vision (ICCV) (2019) 73. Wang, Z., Qinami, K., Karakozis, I.C., Genova, K., Nair, P., Hata, K., Russakovsky, O.: Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 74. Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., Xiao, J.: LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop. arXiv:1506.03365 [cs] (2016) 75. Zhang, B.H., Lemoine, B., Mitchell, M.: Mitigating Unwanted Biases with Adversarial Learning. In: AAAI/ACM Conference on AI, Ethics, and Society (2018) 76. Zhang, Z., Sabuncu, M.: Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels. In: Advances in Neural Information Processing Systems (2018) 77. Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.W.: Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. In: Empirical Methods in Natural Language Processing (2017) 78. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning Deep Features for Discriminative Localization. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 79. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2018) p(? | Ij) := C(Ij), Ij ? B k // C predicts target attribute 6 p(b | Ij) := D(Ij), Ij ? B k // D predicts bias attribute groups k = LEOV + LUA // Compute loss on Bt. D with loss 1 /K K k=1 L k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Algorithm 2 : 2 B 3 for k : 1 . . . K do 4 B 9 B 10 for k : 1 . . . K do 11 B</head><label>22314910111</label><figDesc>Mitigate unknown biases. Input: T : number of iterations, K: number of target attribute classes Output: C: classifier, D: discoverer Data: D: training set 1 for t : 1 . . . T do /* ======== Start: optimize C, freeze D ======== */ := {(Ii, yi)} N i=1 ? D // Sample a batch B with N pairs of images Ii and target attribute labels yi /* for each target attribute class t */ k := {(Ij, yj) | yj = k, (Ij, yj) ? B} M j=1 // Select M pairs from B whose labels are k 5 p(? | Ij) := C(Ij), Ij ? B k // C predicts target attribute 6 p(b | Ij) := D(Ij), Ij ? B k // D predicts bias attribute groups 7 L C k = LRCE // Compute loss on B k 8 update C with loss 1 /K K k=1 L C k /* ======== End: optimize C, freeze D ======== */ /* ======== Start: optimize D, freeze C ======== */ := {(Ii, yi)} N i=1 ? D // Sample a batch B with N pairs of images Ii and target attribute labels yi /* for each target attribute class k */ k := {(Ij, yj) | yj = k, (Ij, yj) ? B} M j=1 // Select M pairs from B whose labels are k 12 p(? | Ij) := C(Ij), Ij ? B k // C predicts target attribute 13 p(b | Ij) := D(Ij), Ij ? B k // D predicts bias attribute groups 14 L D k = LEOV + LUA // Compute loss on B k 15 update D with loss 1 /K K k=1 L D k /* ======== End: optimize D, freeze C ======== */</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 =Fig. 8 :</head><label>148</label><figDesc>" h G E T l y M F o i 1 D 5 a v S y s n 6 l r W m E F o = " &gt; A A A C L H i c b V D L S g M x F M 3 4 r P V V d e k m W M S 6 K T M i W h G h 2 I 3 L C v Y B b S m Z 9 I 4 G M w + S O 2 I Z + k F u / B V B X F j E r d 9 h 2 g 6 o r R c C J + f c c 3 N z 3 E g K j b Y 9 t O b m F x a X l j M r 2 d W 1 9 Y 3 N 3 N Z 2 X Y e x 4 l D j o Q x V 0 2 U a p A i g h g I l N C M F z H c l N N z 7 y k h v P I D S I g x u s B 9 B x 2 e 3 g f A E Z 2 i o b q 7 S R n h E 1 0 s K 7 H B A 2 + d 0 f E d M J H h I x / M H Y y o 5 0 D 9 6 o k b 2 A b 2 g d v G s 1 M 3 l 7 a I 9 L j o L n B T k S V r V b u 6 1 3 Q t 5 7 E O A X D K t W 4 4 d Y S d h C g W X M M i 2 Y w 0 R 4 / f s F l o G B s w H 3 U k m y 9 B 9 w / S o F y p z g n T F 3 4 6 E + V r 3 f d d 0 + g z v 9 L Q 2 I v / T W j F 6 p U 4 i g i h G C P j k I S + W F E M 6 S o 7 2 h A K O s m 8 A 4 0 q Y X S m / Y 4 p x N P l m T Q j O 9 J d n Q f 2 o 6 J w U j 6 + P 8 + X L N I 4 M 2 S V 7 p E A c c k r K 5 I p U S Y 1 w 8 k R e y D s Z W s / W m / V h f U 5 a 5 6 z U s 0 P + l P X 1 D X 6 B p 7 U = &lt; / l a t e x i t &gt; (a) left color's ratio = 0.98 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 3 I t S r C y v f n O q p h R y l D R q q d / y 2 I = " &gt; A A A C L X i c b V D L S g M x F M 3 U d 3 1 V X b o J F l E 3 Z U b q o 4 g g 6 s K l g t V C W 0 o m v d O G Z h 4 k d 8 Q y 9 I f c + C s i u F D E r b 9 h O h 3 w U S 8 E T s 6 5 5 + b m u J E U G m 3 7 1 c p N T E 5 N z 8 z O 5 e c X F p e W C y u r N z q M F Y c q D 2 W o a i 7 T I E U A V R Q o o R Y p Y L 4 r 4 d b t n Q 3 1 2 z t Q W o T B N f Y j a P q s E w h P c I a G a h X O G w j 3 6 H r J t r s z o I 0 j m t 4 R E w k e 0 n T + I K W S L f 2 t J 2 p o H 9 B j a p c q l b 1 W o W i X 7 L T o O H A y U C R Z X b Y K z 4 1 2 y G M f A u S S a V 1 3 7 A i b C V M o u I R B v h F r i B j v s Q 7 U D Q y Y D 7 q Z j L a h m 4 Z p U y 9 U 5 g T Z j j 8 d C f O 1 7 v u u 6 f Q Z d v V f b U j + p 9 V j 9 A 6 b i Q i i G C H g o 4 e 8 W F I M 6 T A 6 2 h Y K O M q + A Y w r Y X a l v M s U 4 2 g C z p s Q n L 9 f H g c 3 u y V n v 1 S + K h d P T r M 4 Z s k 6 2 S D b x C E H 5 I R c k E t S J Z w 8 k C f y S t 6 s R + v F e r c + R q 0 5 K / O s k V 9 l f X 4 B E B G n 9 g = = &lt; / l a t e x i t &gt; (b) left color's ratio = 0.995 More bias discovery results w.r.t. left color and right color biases throughout the training epochs on Multi-Color MNIST dataset under different ratios of biasaligned samples w.r.t. left color. The results are consistent with Fig. 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 :</head><label>9</label><figDesc>Bias discovery results when left color and right color are equally salient (both ratios are 0.95).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>G. 1 Fig. 10 :Fig. 11 :</head><label>11011</label><figDesc>More Examples of Discovered Biases on Face Images While Fig. 5 shows the discovered visible hair area bias of gender classifier on female images, we further show the male image examples in Fig. 10. D focuses on the visible hair area to separate images into "small visible hair area" and "large visible hair area" groups, which is consistent with the female examples shown in Fig. 5. t e x i t s h a 1 _ b a s e 6 4 = " + Z F G 2 2 d 5 D x 7 b h I S m N a K 9 P p C m n m 4 = " &gt; A A A B / n i c b V D L S g M x F M 3 4 r P U 1 K q 7 c B I t Q N 2 W m C r o R i m 5 c V r A P a I e S S T N t a D I z J H f E M h T 8 F T c u F H H r d 7 j z b 8 y 0 s 9 D W A y G H c + 6 9 u T l + L L g G x / m 2 l p Z X V t f W C x v F z a 3 t n V 1 7 b 7 + p o 0 R R 1 q C R i F T b J 5 o J H r I G c B C s H S t G p C 9 Y y x / d Z H 7 r g S n N o / A e x j H z J B m E P O C U g J F 6 9 m F c 7 g J 7 h F Q S w S a n + A q 7 F c f p 2 S X H X B n w I n F z U k I 5 6 j 3 7 q 9 u P a C J Z C F Q Q r T u u E 4 O X E g W c m r n F b q J Z T O i I D F j H 0 J B I p r 1 0 u v 4 E n x i l j 4 N I m R M C n q q / O 1 I i t R 5 L 3 1 R K A k M 9 7 2 X i f 1 4 n g e D S S 3 k Y J 8 B C O n s o S A S G C G d Z 4 D 5 X j I I Y G 0 K o 4 m Z X T I d E E Q o m s a I J w Z 3 / 8 i J p V i v u W a V 6 d 1 6 q X e d x F N A R O k Z l 5 K I L V E O 3 q I 4 a i K I U P a N X 9 G Y 9 W S / W u / U x K 1 2 y 8 p 4 D 9 A f W 5 w 8 F / p Q 2 &lt; / l a t e x i t &gt; p(male) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + Z F G 2 2 d 5 D x 7 b h I S m N a K 9 P p C m n m 4 = " &gt; A A A B / n i c b V D L S g M x F M 3 4 r P U 1 K q 7 c B I t Q N 2 W m C r o R i m 5 c V r A P a I e S S T N t a D I z J H f E M h T 8 F T c u F H H r d 7 j z b 8 y 0 s 9 D W A y G H c + 6 9 u T l + L L g G x / m 2 l p Z X V t f W C x v F z a 3 t n V 1 7 b 7 + p o 0 R R 1 q C R i F T b J 5 o J H r I G c B C s H S t G p C 9 Y y x / d Z H 7 r g S n N o / A e x j H z J B m E P O C U g J F 6 9 m F c 7 g J 7 h F Q S w S a n + A q 7 F c f p 2 S X H X B n w I n F z U k I 5 6 j 3 7 q 9 u P a C J Z C F Q Q r T u u E 4 O X E g W c m r n F b q J Z T O i I D F j H 0 J B I p r 1 0 u v 4 E n x i l j 4 N I m R M C n q q / O 1 I i t R 5 L 3 1 R K A k M 9 7 2 X i f 1 4 n g e D S S 3 k Y J 8 B C O n s o S A S G C G d Z 4 D 5 X j I I Y G 0 K o 4 m Z X T I d E E Q o m s a I J w Z 3 / 8 i J p V i v u W a V 6 d 1 6 q X e d x F N A R O k Z l 5 K I L V E O 3 q I 4 a i K I U P a N X 9 G Y 9 W S / W u / U x K 1 2 y 8 p 4 D 9 A f W 5 w 8 F / p Q 2 &lt; / l a t e x i t &gt; p(male) = 0.94 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u o E S t F W B s Y Q X J H Z N i j t l i R e R 9 L w = " &gt; A A A B / n i c b V D L S g M x F M 3 4 r P U 1 K q 7 c B I t Q N 2 W m F t S F U H T j s o J 9 Q D u U T J p p Q z O Z I b k j l q H g r 7 h x o Y h b v 8 O d f 2 P a z k J b D 1 w 4 n H N v c u / x Y 8 E 1 O M 6 3 t b S 8 s r q 2 n t v I b 2 5 t 7 + z a e / s N H S W K s j q N R K R a P t F M c M n q w E G w V q w Y C X 3 B m v 7 w Z u I 3 H 5 j S P J L 3 M I q Z F 5 K + 5 A G n B I z U t Q / j Y g f Y I 6 Q h E W x 8 i q + w U 7 q s d O 2 C U 3 K m w I v E z U g B Z a h 1 7 a 9 O L 6 J J y C R Q Q b R u u 0 4 M X k o U c G r e z X c S z W J C h 6 T P 2 o Z K E j L t p d P 1 x / j E K D 0 c R M q U B D x V f 0 + k J N R 6 F P q m M y Q w 0 P P e R P z P a y c Q X H g p l 3 E C T N L Z R 0 E i M E R 4 k g X u c c U o i J E h h C p u d s V 0 Q B S h Y B L L m x D c + Z M X S a N c c s 9 K 5 b t K o X q d x Z F D R + g Y F Z G L z l E V 3 a I a q i O K U v S M X t G b 9 W S 9 W O / W x 6 x 1 y c p m D t A f W J 8 / G D S U Q g = = &lt; / l a t e x i t &gt; p(male) = 0.61 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + i h /I W Q y I Y u j h D r K j Y A u 9 T n t X N g = " &gt; A A A B / n i c b V D L S g M x F M 3 U V 6 2 v U X H l J l i E u i k z V d S N U H T j s o J 9 Q D u U T J p p Q 5 O Z I b k j l q H g r 7 h x o Y h b v 8 O d f 2 P a z k J b D 1 w 4 n H N v c u / x Y 8 E 1 O M 6 3 l V t a X l l d y 6 8 X N j a 3 t n f s 3 b 2 G j h J F W Z 1 G I l I t n 2 g m e M j q w E G w V q w Y k b 5 g T X 9 4 M / G b D 0 x p H o X 3 M I q Z J 0 k / 5 A G n B I z U t Q / i U g f Y I 6 S S C D Y + w V f Y K Z + 7 X b v o l J 0 p 8 C J x M 1 J E G W p d + 6 v T i 2 g i W Q h U E K 3 b r h O D l x I Fn J p 3 C 5 1 E s 5 j Q I e m z t q E h k U x 7 6 X T 9 M T 4 2 S g 8 H k T I V A p 6 q v y d S I r U e S d 9 0 S g I D P e 9 N x P + 8 d g L B p Z f y M E 6 A h X T 2 U Z A I D B G e Z I F 7 X D E K Y m Q I o Y q b X T E d E E U o m M Q K J g R 3 / u R F 0 q i U 3 d N y 5 e 6 s W L 3 O 4 s i j Q 3 S E S s h F F 6 i K b l E N 1 R F F K X p G r + j N e r J e r H f r Y 9 a a s 7 K Z f f Q H 1 u c P D x m U P A = = &lt; / l a t e x i t &gt; Discovered bias of gender classifier: visible hair area. D's CAM saliency map is paired with each image. The vanilla gender classifier train on CelebA dataset performs worse for males with larger visible hair area (a) Discovered bias in the bridge class. on the bridge many small tables with chairs off the bridge &amp; co-occur with mountain / water (b) Discovered bias in the conference room class.table as the main objectp(bridge) = 0.76 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y u i a i / V z i x x Z O r 9 v 5 R P 4 c C f J L u s = " &gt; A A A C A H i c b V D J S g N B E O 1 x j X G L e v D g p T E I 8 R J m o h g v Q t C L x w h m g W Q I P T 2 V p E n P Q n e N G I Z c / B U v H h T x 6 m d 4 8 2 / s L A d N f F D w e K + K q n p e L I V G 2 / 6 2 l p Z X V t f W M x v Z z a 3 t n d 3 c 3 n 5 d R 4 n i U O O R j F T T Y x q k C K G G A i U 0 Y w U s 8 C Q 0 v MH N 2 G 8 8 g N I i C u 9 x G I M b s F 4 o u o I z N F I n d x g X 2 g i P m H p K + D 0 Y n d I r a h f L F 5 1 c 3 i 7 a E 9 B F 4 s x I n s x Q 7 e S + 2 n 7 E k w B C 5 J J p 3 X L s G N 2 U K R R c w i j b T j T E j A 9 Y D 1 q G h i w A 7 a a T B 0 b 0 x C g + 7 U b K V I h 0 o v 6 e S F m g 9 T D w T G f A s K / n v b H 4 n 9 d K s H v p p i K M E 4 S Q T x d 1 E 0 k x o u M 0 q C 8 U c J R D Q x h X w t x K e Z 8 p x t F k l j U h O P M v L 5 J 6 q e i c F U t 3 5 / n K 9 S y O D D k i x 6 R A H F I m F X J L q q R G O B m R Z / J K 3 q w n 6 8 V 6 t z 6 m r U v W b O a A / I H 1 + Q O s H p U k &lt; / l a t e x i t &gt; p(bridge) = 0.87 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L B 8 + U 1 + 1 v 8 T + 4 i t 0 T q W I f c p A K a I = " &gt; A A A C A H i c b V D J S g N B E O 1 x j X G L e v D g p T E I 8 R J m o p B c h K A X j x H M A s k Q e j o 1 S Z O e h e 4 a M Q y 5 + C t e P C j i 1 c / w 5 t / Y W Q 6 a + K D g 8 V 4 V V f W 8 W A q N t v 1 t r a y u r W 9 s Z r a y 2 z u 7 e / u 5 g 8 O G j h L F o c 4 j G a m W x z R I E U I d B U p o x Q p Y 4 E l o e s O b i d 9 8 A K V F F N 7 j K A Y 3 Y P 1 Q + I I z N F I 3 d x w X O g i P m H p K 9 P o w P q d X 1 C 5 W y t 1 c 3 i 7 a U 9 B l 4 s x J n s x R 6 + a + O r 2 I J w G E y C X T u u 3 Y M b o p U y i 4 h H G 2 k 2 i I G R + y P r Q N D V k A 2 k 2 n D 4 z p m V F 6 1 I + U q R D p V P 0 9 k b J A 6 1 H g m c 6 A 4 U A v e h P x P 6 + d o F 9 x U x H G C U L I Z 4 v 8 R F K M 6 C Q N 2 h M K O M q R I Y w r Y W 6 l f M A U 4 2 g y y 5 o Q n M W X l 0 m j V H Q u i q W 7 y 3 z 1 e h 5 H h p y Q U 1 I g D i m T K r k l N V I n n I z J M 3 k l b 9 a T 9 W K 9 W x + z 1 h V r P n N E / s D 6 / A G v J 5 U m &lt; / l a t e x i t &gt; p(bridge) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U D V J M m b v R 0 O t U o K w r I 0 O U a 8 J M P E = " &gt; A A A C A H i c b V C 7 S g N B F J 3 1 G e M r a m F h M x i E 2 I T d K G g j B G 0 s I 5 g H J E u Y n d x N h s w + m L k r h m U b f 8 X G Q h F b P 8 P O v 3 H y K D T x w O U e z r m X m X u 8 W A q N t v 1 t L S 2 v r K 6 t 5 z b y m 1 v b O 7 u F v f 2 G j h L F o c 4 j G a m W x z R I E U I d B U p o x Q p Y 4 E l o e s O b s d 9 8 A K V F F N 7 j K A Y 3 Y P 1 Q + I I z N F K 3 c B i X O g i P m H p K 9 P q Q n d I r 6 p R t u 1 s o 2 q a N Q R e J M y N F M k O t W / j q 9 C K e B B A i l 0 z r t m P H 6 K Z M o e A S s n w n 0 R A z P m R 9 a B s a s g C 0 m 0 4 O y O i J U X r U j 5 S p E O l E / b 2 R s k D r U e C Z y Y D h Q M 9 7 Y / E / r 5 2 g f + m m I o w T h J B P H / I T S T G i 4 z R o T y j g K E e G M K 6 E + S v l A 6 Y Y R 5 N Z 3 o T g z J + 8 S B q V s n N W r t y d F 6 v X s z h y 5 I g c k x J x y A W p k l t S I 3 X C S U a e y S t 5 s 5 6 s F + v d + p i O L l m z n Q P y B 9 b n D 5 n q l R g = &lt; / l a t e x i t &gt; p(bridge) = 0.99 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 4 p 3 p W 5 1 A + r b 8 l h 2 Y 2 4 D E U K z 2 c w = " &gt; A A A C A H i c b V D J S g N B E O 1 x j X G L e v D g p T E I 8 R J m o q A 5 C E E v H i O Y B Z I h 9 P R U k i Y 9 C 9 0 1 Y h h y 8 V e 8 e F D E q 5 / h z b + x s x w 0 8 U H B 4 7 0 q q u p 5 s R Q a b f v b W l p e W V 1 b z 2 x k N 7 e 2 d 3 Z z e / t 1 H S W K Q 4 1 H M l J N j 2 m Q I o Q a C p T Q j B W w w J P Q 8 A Y 3 Y 7 / x A E q L K L z H Y Q x u w H q h 6 A r O 0 E i d 3 G F c a C M 8 Y u o p 4 f d g d E q v q F 0 s l z u 5 v F 2 0 J 6 C L x J m R P J m h 2 s l 9 t f 2 I J w G E y C X T u u X Y M b o p U y i 4 h F G 2 n W i I G R + w H r Q M D V k A 2 k 0 n D 4 z o i V F 8 2 o 2 U q R D p R P 0 9 k b J A 6 2 H g m c 6 A Y V / P e 2 P x P 6 + V Y P f S T U U Y J w g h n y 7 q J p J i R M d p U F 8 o 4 C i H h j C u h L m V 8 j 5 T j K P J L G t C c O Z f X i T 1 U t E 5 K 5 b u z v O V 6 1 k c G X J E j k m B O O S C V M g t q Z I a 4 W R E n s k r e b O e r B f r 3 f q Y t i 5 Z s 5 k D 8 g f W 5 w + z t J U p &lt; / l a t e x i t &gt; p(conference room) = 0.78 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 T O E r O d D j H W C Q r 2 g F J x S M / z f v u s = " &gt; A A A C C X i c b V C 7 S g N B F J 3 1 b X x F L W 0 G g x C b s B s F b Q T R x j K C 0 U A S w u z k b j J k d m a Z u S u G J a 2 N v 2 J j o Y i t f 2 D n 3 z h J t v B 1 Y O B w z r 3 c O S d M p L D o + 5 / e z O z c / M L i 0 n J h Z X V t f a O 4 u X V t d W o 4 1 L m W 2 j R C Z k E K B X U U K K G R G G B x K O E m H J y P / Z t b M F Z o d Y X D B N o x 6 y k R C c 7 Q S Z 0 i T c o t h D v M u F Y R G F A c q N E 6 H u 3 T E + p X j o 4 7 x Z J f 8 S e g f 0 m Q k x L J U e s U P 1 p d z d M Y F H L J r G 0 G f o L t j B k U X M K o 0 E o t J I w P W A + a j i o W g 2 1 n k y Q j u u e U L o 2 0 c U 8 h n a j f N z I W W z u M Q z c Z M + z b 3 9 5 Y / M 9 r p h g d t z O h k h R d x u m h K J U U N R 3 X Q r v C A E c 5 d I R x I 9 x f K e 8 z w z i 6 8 g q u h O B 3 5 L / k u l o J D i r V y 8 P S 6 V l e x x L Z I b u k T A J y R E 7 J B a m R O u H k n j y S Z / L i P X h P 3 q v 3 N h 2 d 8 f K d b f I D 3 v s X u U G Z C A = = &lt; / l a t e x i t &gt; p(conference room) = 0.32 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A o I c e G p + 0 w l A g z W O 1 h 7 8 2 H R J 9 5 U = " &gt; A A A C C X i c b V C 7 S g N B F J 2 N r x h f U U u b w S B o E 3 Y T Q R s h a G M Z w a i Q h D A 7 u a t D Z m e W m b t i W N L a + C s 2 F o r Y + g d 2 / o 2 T Z A t f B w Y O 5 9 z L n X P C R A q L v v / p F W Z m 5 + Y X i o u l p e W V 1 b X y + s a F 1 a n h 0 O J a a n M V M g t S K G i h Q A l X i Q E W h x I u w 8 H J 2 L + 8 B W O F V u c 4 T K A b s 2 s l I s E Z O q l X p s l u B + E O M 6 5 V B A Y U B 2 q 0 j k d 7 9 I j 6 1 X q t V 6 7 4 V X 8 C + p c E O a m Q H M 1 e + a P T 1 z y N Q S G X z N p 2 4 C f Y z Z h B w S W M S p 3 U Q s L 4 g F 1 D 2 1 H F Y r D d b J J k R H e c 0 q e R N u 4 p p B P 1 + 0 b G Y m u H c e g m Y 4 Y 3 9 r c 3 F v / z 2 i l G h 9 1 M q C R F l 3 F 6 K E o l R U 3 H t d C + M M B R D h 1 h 3 A j 3 V 8 p v m G E c X X k l V 0 L w O / J f c l G r B v V q 7 W y / 0 j j O 6 y i S L b J N d k l A D k i D n J I m a R F O 7 s k j e S Y v 3 o P 3 5 L 1 6 b 9 P R g p f v b J I f 8 N 6 / A K o V m P 4 = &lt; / l a t e x i t &gt; p(conference room) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b P 6 N s O D 0 I Q Y k X H Z J 5 4 f L u + u Y V I = " &gt; A A A C C X i c b V D L S g M x F M 3 4 r P V V d e k m W A T d l B k V d C M U 3 b i s Y F u h L S W T 3 m m D m W R I 7 o h l 6 N a N v + L G h S J u / Q N 3 / o 3 p Y 6 G t B 0 I O 5 9 x L c k 6 Y S G H R 9 7 + 9 u f m F x a X l 3 E p + d W 1 9 Y 7 O w t V 2 z O j U c q l x L b W 5 D Z k E K B V U U K O E 2 M c D i U E I 9 v L s c + v V 7 M F Z o d Y P 9 B F o x 6 y o R C c 7 Q S e 0 C T Q 6 a C A + Y c a 0 i M K A 4 U K N 1 P D i k 5 z Q o + X 6 7 U P T d N Q S d J c G E F M k E l X b h q 9 n R P I 1 B I Z f M 2 k b g J 9 j K m E H B J Q z y z d R C w v g d 6 0 L D U c V i s K 1 s l G R A 9 5 3 S o Z E 2 7 i i k I / X 3 R s Z i a / t x 6 C Z j h j 0 7 7 Q 3 F / 7 x G i t F Z K x M q S d F l H D 8 U p Z K i p s N a a E c Y 4 C j 7 j j B u h P s r 5 T 1 m G E d X X t 6 V E E x H n i W 1 o 1 J w X D q 6 P i m W L y Z 1 5 M g u 2 S M H J C C n p E y u S I V U C S e P 5 J m 8 k j f v y X v x 3 r 2 P 8 e i c N 9 n Z I X / g f f 4 A p A W Y + g = = &lt; / l a t e x i t &gt; p(conference room) = 1.00 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 b P 6 N s O D 0 I Q Y k X H Z J 5 4 f L u + u Y V I = " &gt; A A A C C X i c b V D L S g M x F M 3 4 r P V V d e k m W A T d l B k V d C M U 3 b i s Y F u h L S W T 3 m m D m W R I 7 o h l 6 N a N v + L G h S J u / Q N 3 / o 3 p Y 6 G t B 0 I O 5 9 x L c k 6 Y S G H R 9 7 + 9 u f m F x a X l 3 E p + d W 1 9 Y 7 O w t V 2 z O j U c q l x L b W 5 D Z k E K B V U U K O E 2 M c D i U E I 9 v L s c + v V 7 M F Z o d Y P 9 B F o x 6 y o R C c 7 Q S e 0 C T Q 6 a C A + Y c a 0 i M K A 4 U K N 1 P D i k 5 z Q o + X 6 7 U P T d N Q S d J c G E F M k E l X b h q 9 n R P I 1 B I Z f M 2 k b g J 9 j K m E H B J Q z y z d R C w v g d 6 0 L D U c V i s K 1 s l G R A 9 5 3 S o Z E 2 7 i i k I / X 3 R s Z i a / t x 6 C Z j h j 0 7 7 Q 3 F / 7 x G i t F Z K x M q S d F l H D 8 U p Z K i p s N a a E c Y 4 C j 7 j j B u h P s r 5 T 1 m G E d X X t 6 V E E x H n i W 1 o 1 J w X D q 6 P i m W L y Z 1 5 M g u 2 S M H J C C n p E y u S I V U C S e P 5 J m 8 k j f v y X v x 3 r 2 P 8 e i c N 9 n Z I X / g f f 4 A p A W Y + g = = &lt; / l a t e x i t &gt; Discovered biases of vanilla scene classifiers. D's CAM is paired with the image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 :</head><label>12</label><figDesc>Vanilla's cross-entropy loss and DebiAN's RCE loss in (a) blond hair classification and (b) gender classification probability of the ground-truth class) to ?? t (1 ? p t ) ? log(p t ), where ? t and ? are hyperparameters. Intuitively, it uses classifier's predicted probability to reweigh the cross-entropy loss, where hard examples (i.e., low p t ) are up-weighted with high ? t (1 ? p t ) ? weights and easy examples (i.e., high p t ) are down-weighted with low ? t (1 ? p t ) ? weights. Different from focal loss, our RCE loss (Eq. 6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Evaluating bias discovery w.r.t. left color, right color biases throughout the training epochs on Multi-Color MNIST. LfF only finds the more salient left color bias (ra-tio=0.99), whereas DebiAN's discoverer finds both biases at the early training stage. Then accuracies gradually converge to 50% as debiasing is performed in the classifier, making the discoverer harder to find biases samples. For example, if most digit "0" images are in red left color in the training set, we call them bias-aligned samples w.r.t. left color attribute, and we regard digit "0" images in a different left color (e.g., yellow) as bias-conflicting samples. Since the dataset contains two bias attributes, there exist images that are bias-aligned w.r.t. to left color and bias-conflicting w.r.t. right color simultaneously, or vice versa. Following</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bias aligned / conflicting accuracy</cell><cell>0 20 40 60 80</cell><cell>0</cell><cell>20</cell><cell>40</cell><cell>epoch</cell><cell>60</cell><cell>80 DebiAN (Ours) Method LfF Bias Attribute left color (ratio=0.99) 100 right color (ratio=0.95)</cell></row><row><cell>Fig. 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>. 4: Comparison between</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) previous Colored MNIST [7,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>35,46] with a single color</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bias and (b) our new Multi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Color MNIST dataset that</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>contains two bias attributes-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>left color and right color</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. We strictly use the same set of hyperparameters used in (single) Colored MNIST in LfF. More details are in Appendix B.Debiasing Results on Multi-Color MNIST The debiasing results are shown in Tab. 1. Except for LfF, all other methods achieve higher accuracy results on left color bias-aligned samples (1st and 2nd rows) than right color bias-aligned samples (1st and 3rd rows), indicating that most methods are more biased w.r.t. the more salient bias, i.e., left color (ratio=0.99) in the</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Debiasing results on Multi-Color MNIST dataset. The accuracy results in the four combinations of two bias attributes, (i.e., left color and right color) and (bias-aligned and bias-conflicting) are reported. Unbiased accuracy averages the results over all four combinations. We bold top-2 results and underline lowest results multi-bias setting. Unlike all other methods, LfF gives abnormal results-high accuracy results (e.g., 99.6, 98.6) for the right color bias-aligned samples and low accuracy results (e.g., 4.7, 5.1) for the right color bias-conflicting samples. Consequently, LfF achieves the worst unbiased accuracy (52.0). The results indicate that LfF only mitigates the more salient left color bias, rendering the classifier to learn the less salient right color bias (ratio=0.95). Compared with all other methods, DebiAN achieves better unbiased accuracy results (72.0). More importantly, DebiAN achieves much better debiasing result (16.0) in bias-conflicting samples w.r.t. both left color and right color attributes, where neither color can provide the shortcut for the correct digit class prediction, demonstrating better debiasing results of DebiAN for mitigating multiple biases simultaneously in the multi-bias setting, which is closer to the real-world scenarios. Bias Discovery: LfF vs. DebiAN We further evaluate the bias discovery results throughout the entire training epochs, which helps us better understand LfF's abnormal results and DebiAN's advantages. We use LfF's "biased model" and DebiAN's discoverer to predict if a given image is bias-aligned or biasconflicting w.r.t. a bias attribute (i.e., binary classification, more details in Appendix D.1). We show the accuracy results of bias discovery w.r.t. each bias attribute at the end of each epoch inFig. 5, which shows that LfF only discovers the more salient left color bias attribute (100% accuracy), but completely ignores the less salient right color bias (50% accuracy) throughout the entire training stage. It reveals the problem of LfF's definition of the unknown biasan attribute in the dataset that is easier, which only holds in the single-bias setting but does not generalize to the multi-bias setting. In contrast, DebiAN uses the principled definition to define the bias-classifier's predictions that violate equal opportunity, enabling discoverer to find both biases accurately at the beginning (it achieves about 60% to 70% accuracy because debiasing is simultaneously performed before the end of the first epoch). At the same time, DebiAN's alternate training scheme lets the classifier mitigate both biases, making the discoverer harder to predict the biases, e.g., accuracies of both bias attributes gradually converge to 50%. More discussions are in Appendix D.4. Ablation Study on UA penalty We conduct an ablation study to show the effectiveness of Unbalanced Assignment (UA) penalty (Sec. 3.1). Tab. 1 shows</figDesc><table><row><cell cols="3">left color ratio = 0.99 ratio = 0.95 right color vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI w/o LUA (Ours) DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell cols="4">bias-aligned 100.0?0.0 99.6?0.5 100.0?0.0 98.6?2.3</cell><cell>100.0?0.0</cell><cell>100.0?0.0</cell></row><row><cell cols="5">bias-aligned bias-conflicting 97.1?0.5 4.7?0.5 97.2?1.5 82.6?19.6</cell><cell>97.2?0.5</cell><cell>95.6?0.8</cell></row><row><cell cols="5">bias-conflicting bias-aligned 27.5?3.6 98.6?0.4 70.8?4.9 26.6?5.5</cell><cell>71.6?0.7</cell><cell>76.5?0.7</cell></row><row><cell cols="5">bias-conflicting bias-conflicting 5.2?0.4 5.1?0.4 10.9?0.8 9.5?3.2</cell><cell>13.8?1.1</cell><cell>16.0?1.8</cell></row><row><cell cols="2">unbiased accuracy</cell><cell cols="3">57.4?0.7 52.0?0.1 69.7?1.0 54.3?4.0</cell><cell>70.6?0.3</cell><cell>72.0?0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of mitigating the gender bias of Blond Hair classifier on CelebA [50] Avg Group Acc. 79.8?0.3 80.9?1.4 82.0?1.1 81.6?0.3 84.0?1.4 Worst Group Acc. 37.9?1.1 43.3?3.0 46.1?4.9 40.9?6.4 52.9?4.7</figDesc><table><row><cell>vanilla LfF</cell><cell>EIIL</cell><cell>PGI DebiAN (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results on biasconflicting samples on bFFHQ<ref type="bibr" target="#b18">[36]</ref> vanilla LfF PGI EIIL BiaSwap DebiAN 51.03 55.61 55.2?5.3 59.2?1.9 58.87 62.8?0.6 that L UA improves the debiasing results (see w/o L UA ). Besides, we also conduct ablation studies on different batch sizes, which are included in Appendix C.2. 4.2 Experiments on Face Image Dataset Gender Bias Mitigation In the face image domain, we conduct experiments to evaluate gender bias mitigation results on CelebA [50] dataset, which contains 200K celebrity faces annotated with 40 binary attributes. The dataset has spurious correlations between gender and Blond Hair, leading to gender biases when performing hair color classification. We follow most of the settings used in LfF, such as using ResNet-18 [28] as the backbone, using Adam [37] optimizer, etc.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. 3. The results of vanilla, LfF, and BiaSwap are from<ref type="bibr" target="#b18">[36]</ref> and<ref type="bibr" target="#b18">[36]</ref> does not provide the error bars. DebiAN achieves the best unsupervised results for mitigating gender bias.Mitigating Multiple Biases in Gender ClassifierThe results on Multi-Color MNIST dataset suggest that DebiAN better mitigates multiple biases in the classifier. In the face image domain, a recent study<ref type="bibr" target="#b23">[41]</ref> shows that gender classifier is biased by multiple attributes, such as Heavy Makeup and Wearing Lipstick. Hence, we train gender classifiers on CelebA dataset and evaluate Average Group Accuracy and Worst Group Accuracy w.r.t. these two bias attributes. As shown in Tab. 4, DebiAN achieves better debiasing results w.r.t. both bias attributes, proving that the discoverer can find multiple biases in the classifier C during the alternate training, enabling classifier to mitigate multiple biases simultaneously. Identified Unknown Bias in Gender Classifier Gender classifier can have more biases beyond Wearing Lipstick and Heavy Makeup. For example, Bal-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of mitigating multiple biases (i.e., Wearing Lipstick and Heavy Makeup) in gender classifier on CelebA dataset Group Acc. 86.6?0.4 87.0?0.9 86.9?3.1 86.3?1.0 88.5?1.1 Worst Group Acc. 53.9?1.2 55.3?3.6 56.0?11.7 52.4?3.2 61.7?4.2 Heavy Makeup Avg. Group Acc. 85.1?0.0 85.5?0.6 85.4?3.4 84.0?1.2 87.8?1.3 Worst Group Acc. 45.4?0.0 46.9?2.6 46.9?13.1 40.9?4.5 56.0?5.2</figDesc><table><row><cell>bias attribute</cell><cell>metric</cell><cell>vanilla LfF</cell><cell>PGI</cell><cell>EIIL DebiAN (Ours)</cell></row><row><cell>Wearing Lipstick Avg. ID: 1901</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ID: 1983</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results 85?5.92 62.98?2.76 65.19?1.32 65.44?1.17 52.44 69.88?2.92</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>on Biased Action</cell></row><row><cell cols="4">Recognition (BAR) [53] dataset</cell></row><row><cell>vanilla</cell><cell>LfF</cell><cell>PGI</cell><cell>EIIL BiaSwap DebiAN</cell></row><row><cell>51.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Scene classification accuracy results on the unseen LSUN [74] dataset</figDesc><table><row><cell>vanilla LfF</cell><cell>PGI</cell><cell cols="2">EIIL DebiAN (Ours)</cell></row><row><cell cols="3">79.3?0.3 71.1?1.0 74.1?1.9 79.4?0.2</cell><cell>80.0?0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on Unbalanced Assignment (UA) penalty (i.e., LUA) in mitigating gender bias of Blond Hair classifier on CelebA [50] dataset w/o LUA DebiAN Avg group Acc. 79.6?1.7 84.0?1.4 Worst group Acc. 38.5?4.7 52.9?4.7</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on Unbalanced Assignment (UA) penalty (i.e., LUA) in mitigating multiple biases of gender classifier on CelebA [50] dataset Group Acc. 87.7?0.4 88.5?1.1 Worst Group Acc. 58.1?1.2 61.7?4.2 Heavy Makeup Avg. Group Acc. 85.6?1.2 87.8?1.3 Worst Group Acc. 46.9?5.2 56.0?5.2</figDesc><table><row><cell>bias attribute</cell><cell>metric</cell><cell>w/o LUA DebiAN</cell></row><row><cell>Wearing Lipstick</cell><cell>Avg.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Ablation study on different batch sizes on Multi-Color MNIST dataset. We report the accuracy results for images that are both bias-conflicting w.r.t. left color and right color bias attributes. We also report the unbiased accuracy results.Here we show more ablation study results on the Unbalanced Assignment (UA) penalty on CelebA dataset. The results are shown in Tabs. 7 and 8, which further proves that L UA can improve fairness results by avoiding the trivial solutionassigning all images into a single bias group (see Unbalanced Assignment (UA) penalty in Sec. 3.1).</figDesc><table><row><cell>DebiAN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Tab. 4, we show better DebiAN's better debiasing results w.r.t. Wearing Lipstick and Heavy Makeup. To demonstrate that DebiAN's better debiasing results w.r.t. more bias attributes, we evaluate on Transects [8] dataset (mentioned on L567 in the main paper). Transects dataset contains high-quality face images synthesized by StyleGAN2 [34], which are also balanced w.r.t. multiple biases such as Hair Length and Skin Color. The dataset does not contain training split because it is designed as a testing set to identify biases in gender classifier. We use gender classifiers trained on CelebA dataset (same setting used in Sec. 4.2) to evaluate their debiasing performance w.r.t. Hair Length and Skin Color bias attributes on Transects dataset. The results are shown in Tab. 16, which demonstrates DebiAN's better capability in mitigating multiple biases simultaneously. Furthermore, the better results w.r.t. Hair Length can also reflect that DebiAN's discoverer identifies visible hair area bias attribute (</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>H.5 RCE loss compared with previous reweighing-based methodsLfF and focal loss are two previous reweighing-based methods for unsupervised debiasing. At a high level, LfF, focal loss, and DebiAN's RCE loss all target at up-weighting worse performed samples and down-weighting better-performed samples. The difference is how to compute the weight. LfF uses the ratio of cross-entropy loss between the biased model and the classifier to compute weights. Focal loss, as a hard negative method (see Appendix H.1), directly uses classifier's predicted probabilities to compute the weights. Different from previous methods, RCE loss uses discoverer 's predicted bias group assignment to compute the weights. Compared with LfF and focal, DebiAN achieves better debiasing results (Tab. 1-6 and Tab. 17).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Ablation study on the ratios on Multi-Color MNIST dataset. Top-1 accuracy results are bolded, and the lowest accuracy results are underlined 100.0?0.0 96.3?0.5 100.0?0.0 100.0?0.0 100.0?0.0 bias-aligned bias-conflicting 98.7?0.6 7.6?1.0 98.4?0.2 92.2?11.1 98.1?0.4 bias-conflicting bias-aligned 6.5?1.0 96.5?1.6 46.8?0.5 27.7?14.2 55.4?2.1 bias-conflicting bias-conflicting 2.0?0.4 5.9?1.3 7.4?0.1 6.3?2.4 9.2?0.8 unbiased 51.8?0.2 51.6?0.6 63.2?0.1 56.5?4.3 65.7?0.7 100.0?0.0 99.6?0.5 100.0?0.0 98.6?2.3 100.0?0.0 bias-aligned bias-conflicting 97.1?0.5 4.7?0.5 97.2?1.5 82.6?19.6 95.6?0.8 bias-conflicting bias-aligned 27.5?3.6 98.6?0.4 70.8?4.9 26.6?5.5 76.5?0.7 bias-conflicting bias-conflicting 5.2?0.4 5.1?0.4 10.9?0.8 9.5?3.2 16.0?1.8 unbiased 57.4?0.7 52.0?0.1 69.7?1.0 54.3?4.0 72.0?0.8 100.0?0.0 99.0?1.7 100.0?0.0 89.0?19.0 100.0?0.0 bias-aligned bias-conflicting 96.6?1.2 9.7?0.7 96.0?0.3 78.6?32.4 97.1?0.8 bias-conflicting bias-aligned 64.4?2.3 98.3?0.9 84.0?0.6 69.5?27.7 85.1?3.4 bias-conflicting bias-conflicting 12.4?1.1 11.5?1.1 16.0?1.7 16.4?1.1 19.4?1.3 unbiased 68.3?1.4 54.6?0.5 74.0?0.5 63.42?19.3 75.4?0.9 100.0?0.0 93.4?5.8 100.0?0.0 100.0?0.0 100.0?0.0 bias-aligned bias-conflicting 91.1?2.3 71.1?33.7 92.7?0.5 76.5?17.8 94.7?0.9 bias-conflicting bias-aligned 87.0?3.7 69.8?25.9 90.0?1.1 74.4?17.7 92.7?1.3 bias-conflicting bias-conflicting 26.0?1.3 39.6?6.9 34.7?3.3 15.8?4.7 39.6?0.2 unbiased 76.0?1.6 68.5?3.2 79.3?0.7 66.7?10.0 81.8?0.6</figDesc><table><row><cell cols="2">left color ratio = 0.995 ratio = 0.95 right color</cell><cell>vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row><row><cell cols="2">bias-aligned bias-aligned left color right color ratio = 0.99 ratio = 0.95</cell><cell>vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row><row><cell cols="2">bias-aligned bias-aligned left color right color ratio = 0.98 ratio = 0.95</cell><cell>vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row><row><cell cols="2">bias-aligned bias-aligned left color right color ratio = 0.95 ratio = 0.95</cell><cell>vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell>bias-aligned</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Ablation study on alternate training on Multi-Color MNIST dataset</figDesc><table><row><cell>left color ratio = 0.99</cell><cell cols="3">right color w/o alternate training DebiAN ratio = 0.95</cell></row><row><cell>bias-aligned</cell><cell>bias-aligned</cell><cell>100.0?0.0</cell><cell>100.0?0.0</cell></row><row><cell cols="2">bias-aligned bias-conflicting</cell><cell>97.3?0.1</cell><cell>95.6?0.8</cell></row><row><cell cols="2">bias-conflicting bias-aligned</cell><cell>74.0?0.3</cell><cell>76.5?0.7</cell></row><row><cell cols="2">bias-conflicting bias-conflicting</cell><cell>11.5?0.7</cell><cell>16.0?1.8</cell></row><row><cell cols="2">unbiased accuracy</cell><cell>70.7?0.1</cell><cell>72.0?0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Results of an alternative design for debiasing in DebiAN -D and C play the minmax game (see DebiAN (minmax)) on Multi-Color MNIST dataset. Our RCE loss significantly outperforms the alternative design</figDesc><table><row><cell>left color ratio = 0.99</cell><cell cols="3">right color DebiAN (minmax) DebiAN (RCE) ratio = 0.95</cell></row><row><cell>bias-aligned</cell><cell>bias-aligned</cell><cell>97.3?4.6</cell><cell>100.0?0.0</cell></row><row><cell cols="2">bias-aligned bias-conflicting</cell><cell>95.7?0.8</cell><cell>95.6?0.8</cell></row><row><cell cols="2">bias-conflicting bias-aligned</cell><cell>64.5?2.0</cell><cell>76.5?0.7</cell></row><row><cell cols="2">bias-conflicting bias-conflicting</cell><cell>7.7?1.9</cell><cell>16.0?1.8</cell></row><row><cell cols="2">unbiased accuracy</cell><cell>66.3?0.9</cell><cell>72.0?0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Results on Colored MNIST (foreground color) under different ratios of bias-aligned samples in the training set. We bold top-1 results (except LfF's results reported in the original paper since they cannot be replicated by their officially released code) and underline the lowest results based on the mean value. Although LfF achieves better bias-conflicting accuracy, it achieves lower bias-aligned accuracy, resulting in low unbiased accuracy. Overall, DebiAN achieves comparable or slightly lower unbiased accuracy results compared with other methods</figDesc><table><row><cell>ratio</cell><cell>vanilla</cell><cell>LfF (reported in the paper) (replicate via official code) LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Results on Colored MNIST (background color) under different ratios of bias-aligned samples in the training set. We bold top-1 results and underline the lowest results based on the mean value. Similar to the results in Colored MNIST (foreground color) (Tab. 14), although LfF achieves better bias-conflicting accuracy, it achieves lower bias-aligned accuracy, resulting in low unbiased accuracy. Overall, DebiAN achieves comparable or slightly higher unbiased accuracy results compared with other methods 97?0.05 42.98?1.67 100.00?0.00 100.00?0.00 100.00?0.00 bias-conflicting 1.98?0.35 61.37?1.69 14.16?4.89 9.95?3.28 20.94?3.92 unbiased accuracy 50.98?0.18 52.17?1.62 57.08?2.44 54.98?1.64 60.47?1.96 0.99 bias-aligned 100.00?0.00 52.24?1.84 99.97?0.05 99.26?1.10 100.00?0.00 bias-conflicting 5.43?0.32 67.94?0.88 35.71?1.78 12.89?1.62 42.57?0.36 unbiased accuracy 52.75?0.16 60.09?1.26 67.84?0.90 56.07?0.81 71.29?0.18 0.98 bias-aligned 99.97?0.05 71.90?4.86 99.90?0.09 97.83?3.54 99.90?0.00 bias-conflicting 21.84?5.39 80.28?0.98 51.25?2.86 18.06?5.47 53.41?1.21 unbiased accuracy 60.91?2.71 76.09?2.91 75.58?1.46 59.75?4.02 76.66?0.60 0.95 bias-aligned 99.87?0.15 69.71?7.07 99.90?0.16 96.78?4.90 99.90?0.01 bias-conflicting 55.14?2.06 81.67?4.54 69.17?3.15 33.90?15.55 70.70?0.76 unbiased accuracy 77.51?1.09 75.69?5.50 84.53?1.54 65.34?9.80 85.30?0.37</figDesc><table><row><cell>ratio</cell><cell>vanilla</cell><cell>LfF</cell><cell>EIIL</cell><cell>PGI</cell><cell>DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell>99.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.995</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 16 :</head><label>16</label><figDesc>Average group accuracy results of gender classification on Transects [8] dataset. All models are trained on CelebA dataset and evaluated on Transects w.r.t. two bias attributes-Hair Length and Skin Color. DebiAN achieves better results, which demonstrates that DebiAN better mitigate multiple biases simultaneously in the real-world multi-bias setting. Besides, it also reflects that DebiAN discovers visible hair area bias attribute to achieve better debiasing results w.r.t. Hair Length bias attribute bias attribute vanilla LfF EIIL PGI DebiAN (Ours) Hair Length 55.1?5.8 54.7?2.9 54.0?0.4 56.2?1.3 60.5?1.7 Skin Color 53.5?5.3 53.3?2.9 53.1?0.08 57.4?0.3 60.1?1.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 17 :</head><label>17</label><figDesc>Comparing with focal loss [48], a hard negative method, on Multi-Color 100.0?0.0 100.0?0.0 100.0?0.0 bias-aligned bias-conflicting 97.1?0.5 95.7?0.6 95.6?0.8 bias-conflicting bias-aligned 27.5?3.6 3.3?2.0 76.5?0.7 bias-conflicting bias-conflicting 5.2?0.4 2.4?0.3 16.0?1.8 bias-aligned bias-conflicting 96.6?1.2 85.1?2.1 97.1?0.8 bias-conflicting bias-aligned 64.4?2.3 15.9?4.4 85.1?3.4 bias-conflicting bias-conflicting 12.4?1.1 6.0?0.1 19.4?1.3 bias-aligned bias-conflicting 91.1?2.3 63.7?3.8 94.7?0.9 bias-conflicting bias-aligned 87.0?3.7 54.4?4.1 92.7?1.3 bias-conflicting bias-conflicting 26.0?1.3 11.3?0.1 39.6?0.2 unbiased 76.0?1.6 57.3?1.2 81.8?0.6</figDesc><table><row><cell>MNIST dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">left color skew = 0.995 skew = 0.95 right color</cell><cell>vanilla</cell><cell cols="2">focal DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell cols="3">bias-aligned 100.0?0.0 100.0?0.0</cell><cell>100.0?0.0</cell></row><row><cell cols="4">bias-aligned bias-conflicting 98.7?0.6 97.9?0.9</cell><cell>98.1?0.4</cell></row><row><cell cols="2">bias-conflicting bias-aligned</cell><cell>6.5?1.0</cell><cell>0.4?0.2</cell><cell>55.4?2.1</cell></row><row><cell cols="3">bias-conflicting bias-conflicting 2.0?0.4</cell><cell>1.2?0.3</cell><cell>9.2?0.8</cell></row><row><cell cols="2">unbiased</cell><cell cols="2">51.8?0.2 49.2?0.2</cell><cell>65.7?0.7</cell></row><row><cell>left color skew = 0.99</cell><cell>right color skew = 0.95</cell><cell>vanilla</cell><cell cols="2">focal DebiAN (Ours)</cell></row><row><cell cols="2">bias-aligned bias-aligned unbiased</cell><cell cols="2">57.4?0.7 50.3?0.4</cell><cell>72.0?0.8</cell></row><row><cell>left color skew = 0.98</cell><cell>right color skew = 0.95</cell><cell>vanilla</cell><cell cols="2">focal DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell cols="3">bias-aligned 100.0?0.0 100.0?0.0</cell><cell>100.0?0.0</cell></row><row><cell cols="2">unbiased</cell><cell cols="2">68.3?1.4 51.7?0.8</cell><cell>75.4?0.9</cell></row><row><cell>left color skew = 0.95</cell><cell>right color skew = 0.95</cell><cell>vanilla</cell><cell cols="2">focal DebiAN (Ours)</cell></row><row><cell>bias-aligned</cell><cell cols="3">bias-aligned 100.0?0.0 100.0?0.0</cell><cell>100.0?0.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">for k : 1 . . . K do 4 B k := {(Ij, yj) | yj = k, (Ij, yj) ? B} M j=1 // Select M pairs from B whose labels are k 5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* https://github.com/alinlab/LfF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? https://github.com/alinlab/LfF/issues/2</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? https://github.com/alinlab/LfF/blob/master/make_dataset.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? https://github.com/alinlab/LfF/issues/1</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Pseudocode of DebiAN</head><p>We present the pseudocode of DebiAN for two tasks -1) discover the unknown biases (Alg. 1); 2) mitigate the unknown biases (Alg. 2). To ensure that the sampled images have the same target attribute labels, we select images with the same target attribute label in a batch to compute the loss (line 3-7 in Alg. 1, 2, and line 10-14 in Alg. 2). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast Algorithms for Mining Association Rules in Large Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Very Large Data Bases</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Systematic generalisation with group invariant predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of Gender Inequality In Face Recognition Accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Albiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vangara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision Workshops</title>
		<imprint>
			<publisher>WACVW</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nellaaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision Workshop (ECCVW)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Invariant Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning De-biased Representations with Biased Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards causal benchmarking of bias in face analysis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to Split for Automatic Bias Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Stable Classifiers by Transferring Unstable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predict then Interpolate: A Simple Algorithm to Learn Stable Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RUBi: Reducing Unimodal Biases for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why Can&apos;t I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C E</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Don&apos;t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithmic Decision Making and the Cost of Fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Environment Inference for Invariant Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flexibly Fair Representation Learning by Disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BiaSwap: Removing dataset bias with bias-tailored swapping augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference, BMVC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Counterfactual Fairness</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fairness without Demographics through Adversarially Reweighted Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lahoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Prost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining in Style: Training a GAN to explain a classifier in StyleSpace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Debiased Representation via Disentangled Feature Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-Driven Text-To-Image Synthesis via Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RESOUND: Towards Action Recognition without Representation Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">REPAIR: Removing Representation Bias by Dataset Resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discover the Unknown Biased Attribute of an Image Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Learning Face Attributes in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explicit Bias Discovery in Visual Question Answering Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model Cards for Model Reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning from Failure: Training Debiased Classifier from Biased Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On Fairness and Calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero-Soriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shabanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Honari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fairness by Learning Orthogonal Disentangled Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gradcam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An Investigation of Critical Issues in Bias Mitigation Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Don&apos;t Judge an Object by Its Context: Learning to Overcome Contextual Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simultaneous Deep Transfer Across Domains and Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fairness Definitions Explained</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Workshop on Software Fairness (FairWare)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">REVISE: A Tool for Measuring and Mitigating Bias in Image Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning Robust Representations by Projecting Superficial Statistics Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
