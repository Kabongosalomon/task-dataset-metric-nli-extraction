<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Graph Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
							<email>ehsanr@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
							<email>armanihm@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Duffield</surname></persName>
							<email>duffieldng@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<email>mingyuan.zhou@mccombs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
							<email>xqian@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">McCombs School of Business</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Graph Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Node embedding maps each node in a graph to a vector in a low-dimensional latent space, in which classical feature vector-based machine learning formulations can be adopted <ref type="bibr" target="#b4">[5]</ref>. Most of the existing node embedding techniques assume that the graph is static and that learning tasks are performed on fixed sets of nodes and edges <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1]</ref>. However, many real-world problems are modeled by dynamic graphs, where graphs are constantly evolving over time. Such graphs have been typically observed in social networks, citation networks, and financial transaction networks. A naive solution to node embedding for dynamic graphs is simply applying static methods to each snapshot of dynamic graphs. Among many potential problems of such a naive solution, it is clear that it ignores the temporal dependencies between snapshots.</p><p>Several node embedding methods have been proposed to capture the temporal graph evolution for both networks without attributes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref> and attributed networks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>. However, all of the existing dynamic graph embedding approaches represent each node by a deterministic vector in a low-dimensional space <ref type="bibr" target="#b1">[2]</ref>. Such deterministic representations lack the capability of modeling uncertainty of node embedding, which is a natural consideration when having multiple information sources, i.e. node attributes and graph structure.</p><p>In this paper, we propose a novel node embedding method for dynamic graphs that maps each node to a random vector in the latent space. More specifically, we first introduce a dynamic graph autoencoder model, namely graph recurrent neural network (GRNN), by extending the use of graph convolutional neural networks (GCRN) <ref type="bibr" target="#b20">[21]</ref> to dynamic graphs. Then, we argue that GRNN lacks the expressive power for fully capturing the complex dependencies between topological evolution and time-varying node attributes because the output probability in standard RNNs is limited to either a simple unimodal distribution or a mixture of unimodal distributions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>. Next, to increase the expressive power of GRNN in addition to modeling the uncertainty of node latent representations, we propose variational graph recurrent neural network (VGRNN) by adopting high-level latent random variables in GRNN. Our proposed VGRNN is capable of learning interpretable latent representation as well as better modeling of very sparse dynamic graphs.</p><p>To further boost the expressive power and interpretability of our new VGRNN method, we integrate semi-implicit variational inference <ref type="bibr" target="#b24">[25]</ref> with VGRNN. We show that semi-implicit variational graph recurrent neural network (SI-VGRNN) is capable of inferring more flexible and complex posteriors. Our experiments demonstrate the superior performance of VGRNN and SI-VGRNN in dynamic link prediction tasks in several real-world dynamic graph datasets compared to baseline and state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Graph convolutional recurrent networks (GCRN). GCRN was introduced by Seo et al. <ref type="bibr" target="#b20">[21]</ref> to model time series data defined over nodes of a static graph. Series of frames in videos and spatio-temporal measurements on a network of sensors are two examples of such datasets. GCRN combines graph convolutional networks (GCN) <ref type="bibr" target="#b3">[4]</ref> with recurrent neural networks (RNN) to capture spatial and temporal patterns in data. More precisely, given a graph G with N nodes, whose topology is determined by the adjacency matrix A ? R N ?N , and a sequence of node attributes X = {X <ref type="bibr" target="#b0">(1)</ref> , X <ref type="bibr" target="#b1">(2)</ref> , . . . , X (T ) }, GCRN reads M -dimensional node attributes X (t) ? R N ?M and updates its hidden state h t ? R p at each time step t:</p><formula xml:id="formula_0">h t = f A, X (t) , h t?1 .</formula><p>(1)</p><p>Here f is a non-probabilistic deep neural network. It can be any recursive network including gated activation functions such as long short-term memory (LSTM) or gated recurrent units (GRU), where the deep layers inside them are replaced by graph convolutional layers. GCRN models node attribute sequences by parameterizing a factorization of the joint probability distribution as a product of conditional probabilities such that</p><formula xml:id="formula_1">p X (1) , X (2) , . . . , X (T ) | A = T t=1 p X (t) | X (&lt;t) , A ; p X (t) | X (&lt;t) , A = g(A, h t?1 ).</formula><p>Due to the deterministic nature of the transition function f , the choice of the mapping function g here effectively defines the only source of variability in the joint probability distributions p(X <ref type="bibr" target="#b0">(1)</ref> , X (2) , . . . , X (T ) | A) that can be expressed by the standard GCRN. This can be problematic for sequences that are highly variable. More specifically, when the variability of X is high, the model tries to map this variability in hidden states h, leading to potentially high variations in h and thereafter overfitting of training data. Therefore, GCRN is not fully capable of modeling sequences with high variations. This fundamental problem of autoregressive models has been addressed for non-graph-structured datasets by introducing stochastic hidden states to the model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In this paper, we integrate GCN and RNN into a graph RNN (GRNN) framework, which is a dynamic graph autoencoder model. While GCRN aims to model dynamic node attributes defined over a static graph, GRNN can get different adjacency matrices at different time snapshots and reconstruct the graph at time t by adopting an inner-product decoder on the hidden state h t . More specifically, h t can be viewed as node embedding of the dynamic graph at time t. To further improve the expressive power of GRNN, we introduce stochastic latent variables by combining GRNN with variational graph autoencoder (VGAE) <ref type="bibr" target="#b13">[14]</ref>. This way, not only we can capture time dependencies between graphs without making smoothness assumption, but also each node is represented with a distribution in the latent space. Moreover, the prior construction devised in VGRNN allows it to predict links in the future time steps.</p><p>Semi-implicit variational inference (SIVI). SIVI has been shown effective to learn posterior distributions with skewness, kurtosis, multimodality, and other characteristics, which were not captured by the existing variational inference methods <ref type="bibr" target="#b24">[25]</ref>. To characterize the latent posterior q(z|x), SIVI introduces a mixing distribution on the parameters of the original posterior distribution to expand the variational family with a hierarchical construction: z ? q(z|?) with ? ? q ? (?). ? denotes the distribution parameter to be inferred. While the original posterior q(z|?) is required to have an analytic form, its mixing distribution is not subject to such a constraint, and so the marginal posterior distribution is often implicit and more expressive that has no analytic density function. It is also common that the marginal of the hierarchy is implicit, even if both the posterior and its mixing distribution are explicit. We will integrate SIVI in our new model to infer more flexible and interpretable node embedding for dynamic graphs.</p><p>3 Variational graph recurrent neural network (VGRNN)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We consider a dynamic graph G = {G <ref type="bibr" target="#b0">(1)</ref> , G <ref type="bibr" target="#b1">(2)</ref> , . . . ,</p><formula xml:id="formula_2">G (T ) } where G (t) = (V (t) , E (t) )</formula><p>is the graph at time step t with V (t) and E (t) being the corresponding node and edge sets, respectively. In this paper, we aim to develop a model that is universally compatible with potential changes in both node and edge sets. In particular, the cardinality of both V (t) and E (t) can change across time. There are no constraints on the relationships between (V (t) , E (t) ) and (V (t+1) , E (t+1) ), namely new nodes can join the dynamic graph and create edges to the existing nodes or previous nodes can disappear from the graph. On the other hand, new edges can form between snapshots while existing edges can disappear. Let N t denotes the number of nodes , i.e., the cardinality of V (t) , at time step t. Therefore, VGRNN can take as input a variable-length adjacency matrix sequence A = {A <ref type="bibr" target="#b0">(1)</ref> , A (2) , . . . , A (T ) }. In addition, when considering node attributes, different attributes can be observed at different snapshots with a variable-length node attribute sequence X = {X <ref type="bibr" target="#b0">(1)</ref> , X (2) , . . . , X (T ) }. Note that A (t) and X (t) are N t ? N t and N t ? M matrices, respectively, where M is the dimension of the node attributes that is constant across time. Inspired by variational recurrent neural networks (VRNN) <ref type="bibr" target="#b2">[3]</ref>, we construct VGRNN by integrating GRNN and VGAE so that complex dependencies between topological and node attribute dynamics are modeled sufficiently and simultaneously. Moreover, each node at each time is represented with a distribution, hence uncertainty of latent representations of nodes are also modelled in VGRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">VGRNN model</head><p>Generation. The VGRNN model adopts a VGAE to model each graph snapshot. The VGAEs across time are conditioned on the state variable h t?1 , modeled by a GRNN. Such an architecture design will help each VGAE to take into account the temporal structure of the dynamic graph. More critically, unlike a standard VGAE, our VGAE in VGRNN takes a new prior on the latent random variables by allowing distribution parameters to be modelled by either explicit or implicit complex functions of information of the previous time step. More specifically, instead of imposing a standard multivariate Gaussian distribution with deterministic parameters, VGAE in our VGRNN learns the prior distribution parameters based on the hidden states in previous time steps. Hence, our VGRNN allows more flexible latent representations with greater expressive power that captures dependencies between and within topological and node attribute evolution processes. In particular, we can write the construction of the prior distribution adopted in our experiments as follows,</p><formula xml:id="formula_3">p Z (t) = Nt i=1 p Z (t) i ; Z (t) i ? N ? (t) i,prior , diag((? (t) i,prior ) 2 ) , ? (t) prior , ? (t) prior = ? prior (h t?1 ), (2) where ? (t)</formula><p>prior ? R Nt?l and ? prior , respectively. Moreover, the generating distribution will be conditioned on Z (t) as:</p><formula xml:id="formula_4">A (t) | Z (t) ? Bernoulli ? (t) , ? (t) = ? dec Z (t) ,<label>(3)</label></formula><p>where ? (t) denotes the parameter of the generating distribution; ? prior and ? dec can be any highly flexible functions such as neural networks. On the other hand, the backbone GRNN enables flexible modeling of complex dependency involving both graph topological dynamics and node attribute dynamics. The GRNN updates its hidden states using the recurrence equation:</p><formula xml:id="formula_5">X (t) A (t) Z (t) h t h t?1 X (t) A (t) Z (t) h t h t?1 X (t) A (t) Z (t) h t h t?1 X (t) A (t) Z (t) h t h t?1 (a) Prior (b) Generation (c) Recurrence (d) Inference</formula><formula xml:id="formula_6">h t =f A (t) , ? x X (t) , ? z Z (t) , h t?1 ,<label>(4)</label></formula><p>where f is originally the transition function from equation <ref type="formula">(1)</ref>. Unlike the GRNN defined in <ref type="bibr" target="#b20">[21]</ref>, graph topology can change in different time steps as it does in real-world dynamic graphs, and the adjacency matrix A (t) is time dependent in VGRNN. To further enhance the expressive power, ? x and ? z are deep neural networks which operate on each node independently and extract features from X (t) and Z (t) , respectively. These feature extractors are crucial for learning complex graph dynamics. Based on (4), h t is a function of A ?(t) , X ?(t) , and Z ?(t) . Therefore, the prior and generating distributions in equations <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref> define the distributions p(Z (t) | A (&lt;t) , X (&lt;t) , Z (&lt;t) ) and p(A (t) | Z (t) ), respectively. The generative model can be factorized as</p><formula xml:id="formula_7">p A (?T ) , Z (?T ) | X (&lt;T ) = T t=1 p Z (t) | A (&lt;t) , X (&lt;t) , Z (&lt;t) p A (t) | Z (t) ,<label>(5)</label></formula><p>where the prior of the first snapshot is considered to be a standard multivariate Gaussian distribution, i.e. p(Z</p><formula xml:id="formula_8">(0) i | ?) ? N (0, I) for i ? {1, .</formula><p>. . , N 0 } and h 0 = 0. Also, if a previously unobserved node is added to the graph at snapshot t, we consider the hidden state of that node at snapshot t ? 1 is zero and hence the prior for that node at time t is N (0, I). If node deletion occurs, we assume that the identity of nodes can be maintained thus removing a node, which is equivalent to removing all the edges connected to it, will not affect the prior construction for the next step. More specifically, the sizes of A and X can change in time while their latent space maintains across time.</p><p>Inference. With the VGRNN framework, the node embedding for dynamic graphs can be derived by inferring the posterior distribution of Z (t) which is also a function of h t?1 . More specifically,</p><formula xml:id="formula_9">q Z (t) | A (t) , X (t) , h t?1 = Nt i=1 q Z (t) i | A (t) , X (t) , h t?1 = Nt i=1 N ? (t) i,enc , diag((? (t) i,enc ) 2 ) , ? (t) enc = GNN ? A (t) , CONCAT ? x X (t) , h t?1 , ? (t) enc = GNN ? A (t) , CONCAT ? x X (t) , h t?1 ,<label>(6)</label></formula><p>where ? enc , respectively. GNN ? and GNN ? are the encoder functions and can be any of the various types of graph neural networks, such as GCN <ref type="bibr" target="#b14">[15]</ref>, GCN with Chebyshev filters <ref type="bibr" target="#b3">[4]</ref> and GraphSAGE <ref type="bibr" target="#b12">[13]</ref>.</p><p>Learning. The objective function of VGRNN is derived from the variational lower bound at each snapshot. More precisely, using equation <ref type="formula" target="#formula_7">(5)</ref> , the evidence lower bound of VGRNN can be written as follows,</p><formula xml:id="formula_10">L = T t=1 E Z (t) ?q(Z (t) | A (?t) ,X (?t) ,Z (&lt;t) ) log p A (t) | Z (t) ? KL q Z (t) | A (?t) , X (?t) , Z (&lt;t) || p Z (t) | A (&lt;t) , X (&lt;t) , Z (&lt;t) .<label>(7)</label></formula><p>We learn the parameters of the generative and inference models jointly by optimizing the variational lower bound with respect to the variational parameters. The graphical representation of VGRNN is illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, operations (a)-(d) correspond to equations (2) -(4), and (3.2), respectively. We note that if we don't use hidden state variables h t?1 in the derivation of the prior distribution, then the prior in (2) becomes independent across snapshots and reduces to the prior of vanilla VGAE.</p><p>The inner-product decoder is adopted in VGRNN for the experiments in this paper-? dec in (3)-to clearly demonstrate the advantages of the stochastic recurrent models for the encoder. Potential extensions with other decoders can be integrated with VGRNN if necessary. More specifically, i ? V (t) at time step t. Note the generating distribution can also be conditioned on h t?1 if we want to generate X (t) in addition to the adjacency matrix for other applications. In such cases, ? dec should be a highly flexible neural network instead of a simple inner-product function.</p><formula xml:id="formula_11">p A (t) | Z (t) = Nt i=1 Nt j=1 p (A (t) i,j | z (t) i , z (t) j ; p A (t) i,j = 1 | z (t) i , z (t) j = sigmoid z (t) i (z (t) j ) T ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-implicit VGRNN (SI-VGRNN)</head><p>To further increase the expressive power of the variational posterior of VGRNN, we introduce a SI-VGRNN dynamic node embedding model. We impose a mixing distributions on the variational distribution parameters in <ref type="bibr" target="#b7">(8)</ref> to model the posterior of VGRNN with a semi-implicit hierarchical construction:</p><formula xml:id="formula_12">Z (t) ? q(Z (t) | ? t ), ? t ? q ? (? t | A (?t) , X (?t) , Z (&lt;t) ) = q ? (? t |A (t) , X (t) , h t?1 ). (9)</formula><p>While the variational distribution q(Z (t) | ? t ) is required to be explicit, the mixing distribution, q ? , is not subject to such a constraint, leading to considerably flexible E ?t?q ? (?t|A (t) ,X (t) ,ht?1) (q(z t |? t )).</p><p>More specifically, SI-VGRNN draws samples from q ? by transforming random noise t via a graph neural network, which generally leads to an implicit distribution for q ? .</p><p>Inference. Under the SI-VGRNN construction, the generation, prior and recurrence models are the same as VGRNN (equations (2) to <ref type="formula" target="#formula_7">(5)</ref>). We indeed have updated the encoder functions as follows:</p><formula xml:id="formula_13">(t) j = GNN j (A (t) , CONCAT(h t?1 , (t) j , (t) j?1 )); (t) j ? q j ( ) for j = 1, . . . , L,<label>(t)</label></formula><formula xml:id="formula_14">0 = ? x ? X (t) ? (t) enc (A (t) , X (t) , h t?1 ) = GNN ? (A (t) , (t) L ), ? (t) enc (A (t) , X (t) , h t?1 ) = GNN ? (A (t) , (t) L ), q(Z (t) i | A (t) , X (t) , h t?1 , ? (t) i,enc , ? (t) i,enc ) = N (? (t) i,enc (A (t) , X (t) , h t?1 ), ? (t) i,enc (A (t) , X (t) , h t?1 )),</formula><p>where L is the number of stochastic layers and (t) j is N t -dimensional random noise drawn from a distribution q j with N t denoting number of nodes at time t. Note that given {A (t) , X (t) , h t?1 }, ? i,enc are now random variables rather than analytic and thus the posterior is not Gaussian after marginalizing. Learning. In this construction, because the parameters of the posterior are random variables, the ELBO goes beyond the simple VGRNN in <ref type="bibr" target="#b6">(7)</ref> and can be written as</p><formula xml:id="formula_15">L = T t=1 E ?t?q ? (?t|A (t) ,X (t) ,ht?1) E Z (t) ?q(Z (t) | ?t) log p(A (t) | Z (t) , h t?1 ) ? KL E ?t?q ? (?t|A (t) ,X (t) ,ht?1) q Z (t) | ? t || p(Z (t) | h t?1 ) .<label>(10)</label></formula><p>Direct optimization of the ELBO in SIVI is not tractable <ref type="bibr" target="#b24">[25]</ref>, hence to infer variational parameters of SI-VGRNN, we derive a lower bound for the ELBO as follows (see the supplements for more details.).</p><formula xml:id="formula_16">L = T t=1 E ?t?q ? (?t|A (t) ,X (t) ,ht?1) E Z (t) ?q(Z (t) | ?t) log p(A (t) | Z (t) , h t?1 ) p(Z (t) | h t?1 ) q(Z (t) | ? t ) .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate our proposed methods, VGRNN and SI-VGRNN, and baselines on six real-world dynamic graphs as described in <ref type="table" target="#tab_0">Table 1</ref>. More detailed descriptions of the datasets can be found in the supplement.</p><p>Competing methods. We compare the performance of our proposed methods against four competing node embedding methods, three of which have the capability to model evolving graphs with changing node and edge sets. Among these four, two (DynRNN and DynAERNN <ref type="bibr" target="#b10">[11]</ref>) are based on RNN models. By comparing our models to these methods, we will be able to see how much improvement we may obtain by improving the backbone RNN with our new prior construction compared to these RNNs with deterministic hidden states. We also compare our methods against a deep autoencoder with fully connected layers (DynAE <ref type="bibr" target="#b10">[11]</ref>) to show the advantages of RNN based sequential learning methods. Last but not least, our methods are compared with VGAE <ref type="bibr" target="#b13">[14]</ref>, which is implemented to analyze each snapshot separately, to demonstrate how temporal dependencies captured through hidden states in the backbone GRNN can improve the performance. More detailed descriptions of these selected competing methods are described in the supplements.</p><p>Evaluation tasks. In the dynamic graph embedding literature, the term link prediction has been used with different definitions. While some of the previous works focused on link prediction in a transductive setting and others proposed inductive models, our models are capable of working in both settings. We evaluate our proposed models on three different link prediction tasks that have been widely used in the dynamic graph representation learning studies. More specifically, given partially observed snapshots of a dynamic graph G = {G (1) , . . . , G (T ) } with node attributes X = {X (1) , . . . , X (T ) }, dynamic link prediction problems are defined as follows: 1) dynamic link detection, i.e. detect unobserved edges in G (T ) ; 2) dynamic link prediction, i.e. predict edges in G (T +1) ; 3) dynamic new link prediction, i.e. predict edges in G (T +1) that are not in G (T ) .</p><p>Experimental setups. For performance comparison, we evaluate different methods based on their ability to correctly classify true and false edges. For dynamic link detection problem, we randomly remove 5% and 10% of all edges at each time for validation and test sets, respectively. We also randomly select the equal number of non-links as validation and test sets to compute average precision (AP) and area under the ROC curve (AUC) scores. For dynamic (new) link prediction, all (new) edges are set to be true edges and the same number of non-links are randomly selected to compute AP and AUC scores. In all of our experiments, we test the model on the last three snapshots of dynamic graphs while learning the parameters of the models based on the rest of the snapshots except for HEP-TH where we test the model on the last 10 snapshots. For the datasets without node attributes, we consider the N t -dimensional identity matrix as node attributes at time t. Numbers show mean results and standard error for 10 runs on random datasets splits with random initializations.</p><p>For all datasets, we set up our VGRNN model to have a single recurrent hidden layer with 32 GRU units. All ?'s in equations (3), (4), and (6) are modeled by a 32-dimensional fully-connected layer. We use two 32-dimensional fully-connected layers for ? prior in (2) and 2-layer GCN with sizes equal to <ref type="bibr">[32,</ref><ref type="bibr" target="#b15">16]</ref> to model ? (t) enc and ? (t) enc in <ref type="bibr" target="#b5">(6)</ref>. For SI-VGRNN, a stochastic GCN layer with size 32 and an additional GCN layer of size 16 are used to model the ?. The dimension of injected standard Gaussian noise is 16. The covariance matrix ? is deterministic and is inferred through two layers of GCNs with sizes equal to <ref type="bibr">[32,</ref><ref type="bibr" target="#b15">16]</ref>. For fair comparison, the number of parameters are the same for the competing methods. In all experiments, we train the models for 1500 epochs with the learning rate 0.01. We use the validation set for the early stopping. The supplement contains additional implementation details with hyperparmaeter selection. We implemented (SI-)VGRNN in PyTorch <ref type="bibr" target="#b17">[18]</ref> and the implementation of our proposed models is accessible at https://github.com/VGraphRNN/VGRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and discussion</head><p>Dynamic link detection. <ref type="table" target="#tab_1">Table 2</ref> summarizes the results for inductive link detection in different datasets. Our proposed methods, VGRNN and SI-VGRNN, outperform competing methods across all datasets by large margins. Improvement made by (SI-)VGRNN compared to GRNN and DynAERNN supports our claim that latent random variables carry more information than deterministic hidden states specially for dynamic graphs with complex temporal changes. Comparing the (SI-)VGRNN with VGAE, which is a static graph embedding method, shows that the improvement of the proposed methods is not only because of introducing stochastic latent variables, but also successful modelling of temporal dependencies. We note that methods that take node attributes as input, i.e VGAE, GRNN and (SI-)VGRNN, outperform other competing methods by a larger margin in Cora dataset which includes node attributes.</p><p>Comparing SI-VGRNN with VGRNN shows that the Gaussian latent distribution may not always be the best choice for latent node representations. SI-VGRNN with flexible variational inference can learn more complex latent structures. The results for the Cora dataset, which also includes attributes, clearly magnify the benefits of flexible posterior as SI-VGRNN improves the accuracy by 2% compared to VGRNN. We also note that the improvement made by SI-VGRNN compared to VGRNN is marginal in Facebook dataset. The reason could be that Gaussian latent variables already represent the graph well. Therefore, more flexible posteriors do not enhance the performance significantly.</p><p>Dynamic (new) link prediction. <ref type="table" target="#tab_2">Tables 3 and 4</ref> show the results for link prediction and new link prediction, respectively. Since GRNN is trained as an autoencoder, it cannot predict edges in the next snapshot. However, in (SI-)VGRNN, the prior construction based on previous time steps allows us to predict links in the future. Note that none of the methods can predict new nodes, therefore, HEP-TH, Cora and Citeseer datasets are not evaluated for these tasks. VGRNN and SI-VGRNN outperform the competing methods significantly in both tasks for all of the datasets which proves   that our proposed models have better generalization, which is the result of including random latent variables in our model. We note that our proposed methods improve new link prediction more substantially which shows that they can capture temporal trends better than the competing methods.</p><p>Comparing VGRNN with SI-VGRNN shows that the prediction results are almost the same for all datasets. The reason is that although the posterior is more flexible in SI-VGRNN, the prior on which our predictions are based, is still Gaussian, hence the improvement is marginal. A possible avenue for further improvements is constructing more flexible priors such as semi-implicit priors proposed by Molchanov et al. <ref type="bibr" target="#b16">[17]</ref>, which we leave for future studies.</p><p>To find out when VGRNN and SI-VGRNN show more improvements compared to the baselines, we take a closer look at three of the datasets. <ref type="figure" target="#fig_5">Figure 2</ref> shows the temporal evolution of density and clustering coefficients of COLAB, Enron, and Facebook datasets. Enron shows the highest density and clustering coefficients, indicating that it contains dense clusters who are densely connected with each other. COLAB have low density and high clustering coefficients across time, which means that although it is very sparse but edges are mostly within the clusters. Facebook, which has both low density and clustering coefficients, is very sparse with almost no clusters. Looking back at (new) link prediction results, we see that the improvement margin of (SI-)VGRNN compared to competing methods is more substantial for Facebook. Moreover, the improvement margin diminishes when the graph has more clusters and is more dense. Predicting the evolution very sparse graphs with no clusters is indeed a very difficult task (arguably more difficult than dense graphs), in which our proposed (SI-)VGRNN is very successful. The stochastic latent variables in our models can capture the temporal trend while other methods tend to overfit very few observed links.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Interpretable latent representations</head><p>To show that VGRNN learns more interpretable latent representations, we simulated a dynamic graph with three communities in which a node (red colored node) transfers from one community into another in two time steps <ref type="figure" target="#fig_6">(Figure 3</ref>). We embedded the node into 2-d latent space using VGRNN ( <ref type="figure" target="#fig_7">Figure 4)</ref> and DynAERNN (the best performed baseline; <ref type="figure" target="#fig_1">Figure S1</ref> in the supplementary material). While the advantages of modeling uncertainty for latent representations and its relation to node labels (classes) for static graphs have been discussed in Bojchevski and G?nnemann <ref type="bibr" target="#b1">[2]</ref>, we argue that the uncertainty is also directly related to topological evolution in dynamic graphs.</p><p>More specifically, the variance of the latent variables for the node of interest increases in time (left to right) marked with the red contour. In time steps 2 and 3 (where the node is moving in the graph), the information from previous and current time steps contradicts each other; hence we expect the representation uncertainty to increase. We also plotted the variance of a node whose community doesn't change in time (marked with the green contour). As we expected, the variance of this node does not increase over time. We argue that the uncertainty helps to better encode non-smooth evolution, in particular abrupt changes, in dynamic graphs. Moreover, at time step 2, the moving node have multiple edges with nodes in two communities. Considering the inner-product decoder, which is based on the angle between the latent representations, the moving node can be connected to both of the communities which is consistent with the graph topology. We note that DynAERNN ( <ref type="figure" target="#fig_1">Figure S1</ref>) fails to produce such an interpretable latent representation. We can see that VGRNN can separate the communities in the latent space more distinctively than what DynAERNN does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed VGRNN and SI-VGRNN, the first node embedding methods for dynamic graphs that embed each node to a random vector in the latent space. We argue that adding high level latent variables to graph recurrent neural networks not only increases its expressiveness to better model the complex dynamics of graphs, but also generates interpretable random latent representation for nodes. SI-VGRNN is also developed by combining VGRNN and semi-implicit variational inference for flexible variational inference. We have tested our proposed methods on dynamic link prediction tasks and they outperform competing methods substantially, specially for very sparse graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The presented materials are based upon the research supported by the National Science Foundation under Grants ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641 and IIS-1812699. We also thank Texas A&amp;M High Performance Research Computing and Texas Advanced Computing Center for providing computational resources to perform experiments in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>prior ? R Nt?l denote the parameters of the conditional prior distribution, and ?(t) i,prior and ? (t) i,prior are the i-th row of ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Graphical illustrations of each operation of VGRNN; (a) computing the conditional prior by (2); (b) decoder function (3); (c) updating the GRNN hidden states using (4); and (d) inference of the posterior distribution for latent variables by (3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(t) enc and ?(t) enc denote the parameters of the approximated posterior, and ? (t) i,enc and ? (t) i,enc are the i-th row of ? (t) enc and ? (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>corresponds to the embedding representation of node v (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Evolution of graph statistics through time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Evolution of simulated graph topology through time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Latent representations of the simulated graph in different time steps in 2-d space using VGRNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Metrics</cell><cell>Enron</cell><cell cols="2">COLAB Facebook</cell><cell>HEP-TH</cell><cell>Cora</cell><cell>Social Evolution</cell></row><row><cell>Number of Snapshots</cell><cell>11</cell><cell>10</cell><cell>9</cell><cell>40</cell><cell>11</cell><cell>27</cell></row><row><cell>Number of Nodes</cell><cell>184</cell><cell>315</cell><cell>663</cell><cell cols="2">1199-7623 708-2708</cell><cell>84</cell></row><row><cell>Number of Edges</cell><cell cols="5">115-266 165-308 844-1068 769-34941 406-5278</cell><cell>303-1172</cell></row><row><cell>Average Density</cell><cell cols="2">0.01284 0.00514</cell><cell>0.00591</cell><cell>0.00117</cell><cell>0.00154</cell><cell>0.21740</cell></row><row><cell>Number of Node Attributes</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1433</cell><cell>168</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AUC and AP scores of inductive dynamic link detection on dynamic graphs. ? 1.33 70.49 ? 6.46 80.37 ? 0.12 79.85 ? 0.85 79.31 ? 1.97 87.60 ? 0.54 DynAE 84.06 ? 3.30 66.83 ? 2.62 60.71 ? 1.05 71.41 ? 0.66 63.94 ? 0.18 53.71 ? 0.48 DynRNN 77.74 ? 5.31 68.01 ? 5.50 69.77 ? 2.01 74.13 ? 1.74 72.39 ? 0.63 76.09 ? 0.97 AUC DynAERNN 91.71 ? 0.94 77.38 ? 3.84 81.71 ? 1.51 78.67 ? 1.07 82.01 ? 0.49 74.35 ? 0.85 GRNN 91.09 ? 0.67 86.40 ? 1.48 85.60 ? 0.59 78.27 ? 0.47 89.00 ? 0.46 91.35 ? 0.21 VGRNN 94.41 ? 0.73 88.67 ? 1.57 88.00 ? 0.57 82.69 ? 0.55 91.12 ? 0.71 92.08 ? 0.35 SI-VGRNN 95.03 ? 1.07 89.15? 1.31 88.12 ? 0.83 83.36 ? 0.53 91.05 ? 0.92 94.07 ? 0.44 VGAE 89.95 ? 1.45 73.08 ? 5.70 79.80 ? 0.22 79.41 ? 1.12 81.05 ? 1.53 89.61 ? 0.87 DynAE 86.30 ? 2.43 67.92 ? 2.43 60.83 ? 0.94 70.18 ? 1.98 63.87 ? 0.21 53.84 ? 0.51 DynRNN 81.85 ? 4.44 73.12 ? 3.15 70.63 ? 1.75 72.15 ? 2.30 74.12 ? 0.75 76.54 ? 0.66 AP DynAERNN 93.16 ? 0.88 83.02 ? 2.59 83.36 ? 1.83 77.41 ? 1.47 85.57 ? 0.93 79.34 ? 0.77 GRNN 93.47 ? 0.35 88.21 ? 1.35 84.77 ? 0.62 76.93? 0.35 89.50 ? 0.42 91.37 ? 0.27 VGRNN 95.17 ? 0.41 89.74 ? 1.31 87.32 ? 0.60 81.41 ? 0.53 91.35 ? 0.77 92.92 ? 0.28 SI-VGRNN 96.31 ? 0.72 89.90 ? 1.06 87.69 ? 0.92 83.20? 0.57 91.42 ? 0.86 94.44 ? 0.52</figDesc><table><row><cell>Metrics Methods</cell><cell>Enron</cell><cell>COLAB</cell><cell>Facebook</cell><cell>Social Evo.</cell><cell>HEP-TH</cell><cell>Cora</cell></row><row><cell>VGAE</cell><cell>88.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>AUC and AP scores of dynamic link prediction on real-world dynamic graphs. ? 0.74 63.14 ? 1.30 56.06 ? 0.29 65.50 ? 1.66 DynRNN 86.41 ? 1.36 75.7 ? 1.09 73.18 ? 0.60 71.37 ? 0.72 AUC DynAERNN 87.43 ? 1.19 76.06 ? 1.08 76.02 ? 0.88 73.47 ? 0.49 VGRNN 93.10 ? 0.57 85.95 ? 0.49 89.47 ? 0.37 77.54 ? 1.04 SI-VGRNN 93.93 ? 1.03 85.45 ? 0.91 90.94 ? 0.37 77.84 ? 0.79 DynAE 76.00 ? 0.77 64.02 ? 1.08 56.04 ? 0.37 63.66 ? 2.27 DynRNN 85.61 ? 1.46 78.95 ? 1.55 75.88 ? 0.42 69.02 ? 1.71 AP DynAERNN 89.37 ? 1.17 81.84 ? 0.89 78.55 ? 0.73 71.79 ? 0.81 VGRNN 93.29 ? 0.69 87.77 ? 0.79 89.04 ? 0.33 77.03 ? 0.83 SI-VGRNN 94.44 ? 0.85 88.36 ? 0.73 90.19 ? 0.27 77.40 ? 0.43</figDesc><table><row><cell>Metrics Methods</cell><cell>Enron</cell><cell>COLAB</cell><cell>Facebook</cell><cell>Social Evo.</cell></row><row><cell>DynAE</cell><cell>74.22</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AUC and AP scores of dynamic new link prediction on real-world dynamic graphs. VGRNN 88.60 ? 0.95 77.95 ? 0.41 87.74 ? 0.53 76.45 ? 1.19 DynAE 66.50 ? 1.12 58.82 ? 1.06 54.57 ? 0.20 54.05 ? 1.63 DynRNN 80.96 ? 1.37 75.34 ? 0.67 75.52 ? 0.50 63.47 ? 2.70 AP DynAERNN 85.16 ? 1.04 77.68 ? 0.66 78.70 ? 0.44 65.03 ? 1.74 VGRNN 87.57 ? 0.57 79.63 ? 0.94 86.30 ? 0.29 73.48 ? 1.11 SI-VGRNN 87.88 ? 0.84 81.26 ? 0.38 86.72 ? 0.54 73.85 ? 1.33</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Metrics Methods</cell><cell cols="2">Enron</cell><cell cols="2">COLAB</cell><cell></cell><cell cols="2">Facebook</cell><cell cols="2">Social Evo.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DynAE</cell><cell cols="9">66.10 ? 0.71 58.14 ? 1.16 54.62 ? 0.22 55.25 ? 1.34</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DynRNN</cell><cell cols="9">83.20 ? 1.01 71.71? 0.73 73.32 ? 0.60 65.69 ? 3.11</cell></row><row><cell></cell><cell></cell><cell>AUC</cell><cell cols="10">DynAERNN 83.77 ? 1.65 71.99 ? 1.04 76.35 ? 0.50 66.61 ? 2.18</cell></row><row><cell></cell><cell></cell><cell></cell><cell>VGRNN</cell><cell cols="9">88.43 ? 0.75 77.09 ? 0.23 87.20 ? 0.43 75.00 ? 0.97</cell></row><row><cell>Clustering Coefficient</cell><cell>0.075 0.100 0.125 0.150 0.175 0.200 0.225 0.250</cell><cell cols="2">2 SI-0 4 Snapshot 6 COLAB Enron</cell><cell>8</cell><cell>10 Facebook</cell><cell>Density</cell><cell>0.004 0.006 0.008 0.010 0.012 0.014 0.016</cell><cell>0</cell><cell>2 COLAB</cell><cell cols="2">4 Snapshot 6 Enron</cell><cell>8</cell><cell>10 Facebook</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust negative sampling for network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3191" to="3198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structural node embeddings via diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Donnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>S?ren Kaae S? Nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">D D</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6039-sequential-neural-models-with-stochastic-layers.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?ren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Alias Parth Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6713" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>C?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6713" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dyngem: Deep embedding method for dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11273</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sujit Rokka Chhetri, and Arquimedes Canedo. dyngraph2vec: Capturing network dynamics using dynamic graph representation learning. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attributed network embedding for learning in a dynamic environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valery</forename><surname>Kharitonov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02789</idno>
		<title level="m">Artem Sobolev, and Dmitry Vetrov. Doubly semiimplicit variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shabanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Variational bi-lstms. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dyrep: Learning representations over dynamic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshit</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjeet</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-implicit variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5660" to="5669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic network embedding by modeling triadic closure process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lekui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
