<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation Long-Rang Correlation Local Information Relative Depth Fringe Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
							<email>zhenyuli17@hit.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Zehui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
							<email>jiangjunjun@hit.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Xianming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehui</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation, University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation Long-Rang Correlation Local Information Relative Depth Fringe Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor) 1 https://github.com/zhyever/ Monocular-Depth-Estimation-Toolbox Local Information Long-Rang Correlation RGB Input Estimation Aggregate Features HAHI Neck Transformer Convolution Decoder GT</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Monocular Depth Estimation ? Scene Under- standing ? Deep Learning ? Transformer ? Convolutional Neural Network (CNN)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to address the problem of supervised monocular depth estimation. We start with a meticulous pilot study to demonstrate that the long-range correlation is essential for accurate depth estimation. Therefore, we propose to leverage the Transformer to model this global context with an effective attention mechanism. We also adopt an additional convolution branch to preserve the local information as the Transformer lacks the spatial inductive bias in modeling such contents. However, independent branches lead to a shortage of connections between features. To bridge this gap, we design a hierarchical aggregation and heterogeneous interaction module to enhance the Transformer features via element-wise interaction and model the affinity between the Transformer and the CNN features in a set-to-set translation manner. Due to the unbearable memory cost caused by global attention on high-resolution feature maps, we introduce the deformable scheme to reduce the complexity. Extensive experiments on the KITTI, NYU, and SUN RGB-D datasets demonstrate that our proposed model, termed DepthFormer, surpasses state-of-the-art monocular depth estimation methods with prominent margins. Notably, it achieves the most competitive result on the highly competitive KITTI depth estimation benchmark. Our codes and models are available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Monocular depth estimation plays a critical role in three dimensional reconstruction and perception. Since the groundbreaking work of <ref type="bibr" target="#b16">(He et al., 2016)</ref>, convolutional neural network (CNN) has dominated the primary workhorse for depth estimation, in which the encoder-decoder based architecture is designed <ref type="bibr" target="#b11">(Fu et al., 2018;</ref><ref type="bibr" target="#b27">Lee et al., 2019;</ref><ref type="bibr" target="#b3">Bhat et al., 2021)</ref>. Although there have been numerous work focusing on the decoder design <ref type="bibr" target="#b11">(Fu et al., 2018;</ref><ref type="bibr" target="#b3">Bhat et al., 2021)</ref>, recent studies suggest that the encoder is even more pivotal for accurate depth estimation <ref type="bibr" target="#b27">(Lee et al., 2019;</ref><ref type="bibr" target="#b33">Ranftl et al., 2021)</ref>. Due to the lack of depth cues, fully exploiting both the long-range correlation (i.e., distance relationship among objects) and the local information (i.e., consistency of the same object) are critical capabilities of an effective encoder <ref type="bibr" target="#b35">(Saxena et al., 2005)</ref>. Therefore, the potential bottleneck of current depth estimation methods may lie in the encoder where the convolution operators can scarcely model the long-range correlation with a limited receptive field <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref>.</p><p>In terms of CNN, there have been great efforts to overcome the above limitation, roughly grouped into two cate-  <ref type="figure">Fig. 2</ref> An overview of DepthFormer. It comprises three major components: an encoder consisting of a Transformer branch and a convolution branch, a hierarchical aggregation and heterogeneous interaction (HAHI) module, and a standard decoder. The HAHI enhances the Transformer features F and models the affinity between the Transformer and the convolution features G.</p><p>gories: manipulating the convolution operation and integrating the attention mechanism. The former applies advanced variations, including multi-scale fusion <ref type="bibr" target="#b34">(Ronneberger et al., 2015)</ref>, atrous convolutions <ref type="bibr" target="#b5">(Chen et al., 2017)</ref> and feature pyramids <ref type="bibr" target="#b44">(Zhao et al., 2017)</ref>, to improve the effectiveness of convolution operators. The latter introduces the attention module <ref type="bibr" target="#b40">(Vaswani et al., 2017)</ref> to model the global interactions of all pixels in the feature map. There are also several general approaches <ref type="bibr" target="#b11">(Fu et al., 2018;</ref><ref type="bibr" target="#b27">Lee et al., 2019;</ref><ref type="bibr" target="#b19">Huynh et al., 2020;</ref><ref type="bibr" target="#b3">Bhat et al., 2021)</ref> that explore the combination of both these strategies. Though the performance is improved significantly, the dilemma persists.</p><p>In an alternative to CNN, Vision Transformer (ViT) <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021)</ref>, which achieves tremendous success on image recognition, demonstrates the advantages of serving as the encoder for depth estimation. Benefiting from the attention mechanism, the Transformer is more expert at modeling the long-range correlation with a global receptive field. However, our pilot study (Sec. 3.1) indicates the ViT encoder cannot produce satisfactory performance due to the lack of spatial inductive bias in modeling the local information <ref type="bibr" target="#b42">(Yang et al., 2021)</ref>.</p><p>To mitigate these issues, we propose a novel monocular depth estimation framework, DepthFormer (illustrated in <ref type="figure">Fig. 1</ref>), which boosts model performance by incorporating the advantages from both the Transformer and the CNN. The principle of DepthFormer lies in the fact that the Transformer branch models the long-range correlation while the additional convolution branch preserves the local information. We argue that the integration of these two-type features can help achieve more accurate depth estimation. However, independent branches with late fusion lead to insufficient feature aggregation for the decoder. To bridge this gap, we design the Hierarchical Aggregation and Heterogeneous Interaction (HAHI) module to combine the best part of both branches. Specifically, it consists of a self-attention module to enhance the features among hierarchical layers of the Transformer branch via element-wise interaction and a cross-attention module to model the affinity between 'heterogeneous' fea-tures (i.e., Transformer and CNN features) in a set-to-set translation manner. Since global attention on high-resolution feature maps leads to an unbearable memory cost, we propose to leverage the deformable scheme <ref type="bibr" target="#b8">(Dai et al., 2017;</ref><ref type="bibr" target="#b46">Zhu et al., 2021)</ref> that only attends to a limited set of key sampling vectors in a learnable manner to alleviate this problem.</p><p>The main contributions of this work are three-fold: (1) We apply the Transformer as the image encoder to exploit the long-range correlation and adopt an additional convolution branch to preserve the local information.</p><p>(2) We design the HAHI to enhance features via element-wise interaction and model the affinity in a set-to-set translation manner.</p><p>(3) Our proposed approach DepthFormer significantly outperforms state-of-the-arts with prominent margins on the KITTI <ref type="bibr" target="#b14">(Geiger et al., 2013)</ref>, NYU <ref type="bibr" target="#b36">(Silberman et al., 2012)</ref> and SUN RGB-D <ref type="bibr" target="#b37">(Song et al., 2015)</ref> datasets. Furthermore, it achieves the most competitive result on the highly competitive KITTI depth estimation benchmark 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Estimating depth from RGB images is an ill-posed problem. Lack of cues, scale ambiguities, translucent or reflective materials all leads to ambiguous cases where appearance cannot infer the spatial construction. With the rapid development of deep learning, CNN has become a key component of mainstream methods to provide reasonable depth maps from a single RGB input.</p><p>Monocular depth estimation has drawn much attention in recent years. Among numerous effective methods, we consider DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref>, Adabins <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref> and <ref type="bibr">Transdepth (Yang et al., 2021)</ref> as three the most important competitors.</p><p>DPT proposes to utilize ViT as the encoder and pre-train models on larger-scale depth estimation datasets. Adabins uses adaptive bins that dynamically change depending on RGB BTS <ref type="bibr" target="#b27">(Lee et al., 2019)</ref> Adabins <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref> Ours GT representations of the input scene and proposes to embed the mini-ViT at a high resolution (after the decoder). Transdepth embeds ViT at the bottleneck to avoid the Transformer losing the local information and presents an attention gate decoder to fuse multi-level features. We focus on comparing these (and many other) methods in this paper. Encoder-decoder is commonly used in monocular depth estimation <ref type="bibr" target="#b10">(Eigen et al., 2014;</ref><ref type="bibr" target="#b11">Fu et al., 2018;</ref><ref type="bibr" target="#b17">Hu et al., 2019;</ref><ref type="bibr" target="#b27">Lee et al., 2019;</ref><ref type="bibr" target="#b19">Huynh et al., 2020;</ref><ref type="bibr" target="#b42">Yang et al., 2021;</ref><ref type="bibr" target="#b3">Bhat et al., 2021)</ref>. In terms of the encoder, mainstream feature extractors, including EfficientNet <ref type="bibr" target="#b38">(Tan and Le, 2019)</ref>, ResNet <ref type="bibr" target="#b16">(He et al., 2016)</ref> and DenseNet <ref type="bibr" target="#b18">(Huang et al., 2017)</ref>, are adopted to learn representations. The decoder frequently consists of successive convolutions and upsampling operators to aggregate encoder features in a late fusion manner, recover the spatial resolution and estimate the depth. In this paper, we utilize the baseline decoder architecture in <ref type="bibr" target="#b1">(Alhashim and Wonka, 2018a)</ref>. It allows us to more explicitly study the performance attribution of key contributions of this work, which are independent of the decoder.</p><p>Neck modules between the encoder and the decoder are proposed to enhance features. Many previous methods only focus on the bottleneck feature but ignore the lower-level ones, limiting the effectiveness <ref type="bibr" target="#b11">(Fu et al., 2018;</ref><ref type="bibr" target="#b27">Lee et al., 2019;</ref><ref type="bibr" target="#b19">Huynh et al., 2020;</ref><ref type="bibr" target="#b42">Yang et al., 2021)</ref>. In this work, we propose the HAHI module to enhance all the multi-level hierarchical features. When another branch is available, it can model the affinity between the two-branch features as well, which benefits the decoder to aggregate the heterogeneous information.</p><p>Transformer networks are gaining greater interest in the computer vision community <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b31">Liu et al., 2021;</ref><ref type="bibr" target="#b4">Carion et al., 2020;</ref><ref type="bibr" target="#b45">Zheng et al., 2021)</ref>. Following the success of recent trends that apply the Transformer to solve computer vision tasks, we propose to leverage the Transformer as the encoder to model long-range correlations. In Sec. 3.1, we discuss our motivation and present differences between our method and several related work <ref type="bibr" target="#b33">(Ranftl et al., 2021;</ref><ref type="bibr" target="#b42">Yang et al., 2021;</ref><ref type="bibr" target="#b3">Bhat et al., 2021</ref>) that adopt the Transformer in monocular depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present the motivation of this work and introduce the key components of DepthFormer: (1) an encoder consisting of a Transformer branch and a convolution branch and (2) the hierarchical aggregation and heterogeneous interaction (HAHI) module. An overview of DepthFormer is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>To indicate the necessity of this work, we conduct a meticulous pilot study to investigate the limitations of existing methods that utilize pure CNN or ViT as the encoder in monocular depth estimation.</p><p>Pilot Study: We first present several failure cases of the state-of-the-art CNN-based monocular depth estimation methods on the NYU dataset in <ref type="figure" target="#fig_0">Fig. 3</ref>. The depth results at the wall decorations and carpets are unexpectedly incorrect. Due to the pure convolutional encoder for feature extraction, it is hard for them to model the global context and capture the long-range distance relationship among objects through limited receptive fields. Such large-area counter-intuitive failures severely impair the model performance.</p><p>To solve the above issue, ViT can serve as a proper alternative that is superior in modeling the long-range correlation with a global receptive field. Therefore, we experiment to analyze the performance of ViT-and CNN-based methods on the KITTI dataset. Specifically, based on DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref>, we adopt ViT-Base and ResNet-50 as the encoder to extract features, respectively. The results shown in Tab. 1 prove that the models applying ViT as encoder outperform  those using ResNet-50 on distant object depth estimation. However, opposite results appear on the near objects. Since the depth values exhibit a long tail distribution and there are much more near objects in scenes <ref type="bibr" target="#b22">(Jiao et al., 2018)</ref>, the overall results of the models applying ViT are significantly inferior. More experimental details and results are reported in the experimental section.</p><p>Analysis: In general, it is tougher to estimate the depth of distant objects directly. Benefiting from modeling the longrange correlation, the ViT-based model can be more reliable to accomplish it via reference pixels in a global context. The knowledge of distance relationships among objects results in better performance on distant object depth estimations. As for the inferior near object depth estimation result, there are many potential explanations. We highlight two major concerns: (1) The Transformer lacks spatial inductive bias in modeling the local information <ref type="bibr" target="#b42">(Yang et al., 2021)</ref>. As for depth estimation, the local information is reflected in the detailed context that is crucial for consistent and sharp estimation results. However, these detailed content tends to be lost during the patch-wise interaction of the Transformer.</p><p>Since objects appearing nearer are larger with higher texture quality <ref type="bibr" target="#b35">(Saxena et al., 2005)</ref>, the Transformer will lose more details at these locations, which severely deteriorates the model performance at a near range and leads to unsatisfying results. (2) Visual elements vary substantially in scale <ref type="bibr" target="#b31">(Liu et al., 2021)</ref>. In general, a U-Net <ref type="bibr" target="#b34">(Ronneberger et al., 2015)</ref> shape architecture is applied for depth estimation, where the multi-scale skip connections are pivotal for exploiting multilevel information. Since the tokens in ViT are all of a fixed scale, the consecutive non-hierarchical forward propagation makes the multi-scale property ambiguous, which may also limit the performance.</p><p>In this paper, we propose to leverage an encoder consisting of Transformer and convolution branches to exploit both the long-range correlation and local information. Different from DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref>, which directly utilizes ViT as the encoder, we introduce a convolution branch to make up for the deficiencies of spatial inductive bias in the Transformer branch. Furthermore, we replace ViT with Swin Transformer <ref type="bibr" target="#b31">(Liu et al., 2021)</ref> so that the Transformer encoder can provide hierarchical features and reduce the compu-tational complexity. Unlike previous methods which embed the Transformer into the CNN <ref type="bibr" target="#b3">(Bhat et al., 2021;</ref><ref type="bibr" target="#b42">Yang et al., 2021)</ref>, we adopt the Transformer to encode images directly, which can fully exert the advantages of the Transformer and avoid the CNN discarding crucial information before the global context modeling. Moreover, due to the independence of these two branches, the simple late fusion of the decoder leads to an insufficient feature aggregation and marginal performance improvement. To bridge this gap, we design the HAHI module to enhance features and model affinities via feature interaction, which alleviates the deficiency and helps combine the best part of both branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer and CNN Feature Extraction</head><p>We propose to extract image features via an encoder consisting of a Transformer branch and a light-weight convolution branch, thus fully exploiting the long-range correlation and the local information.</p><p>Transformer branch first splits the input image I into non-overlapping patches by a patch partition module. The initial feature representation of each patch is set as a concatenation of the pixel RGB values. After that, a linear embedding layer is applied to project the initial feature representation to an arbitrary dimension, which is served as the input of the first Transformer layer and denoted as z 0 . After that, L Transformer layers are applied to extract features. In general, each layer consists of a multi-head self-attention (MSA) module, followed by a multi-layer perceptron (MLP). A LayerNorm (LN) is applied before the MSA and the MLP, and a residual connection is utilized for each module. Therefore, the process of layer l is formulated a?</p><formula xml:id="formula_0">z l = MSA LN z l?1 + z l?1 , z l = MLP LN ? l +? l ,<label>(1)</label></formula><p>where? l and z l denote the output features of the MSA module and the MLP module for layer l, respectively. The structure of a Transformer layer is illustrated in the left part of <ref type="figure">Fig. 2</ref>. Following DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref>, we sample and reassemble N feature maps from the N selected Transformer layers as the output of the Transformer branch and symbolize them as F = {f n } N n=1 , where f n ? R Cn?Hn?Wn indicates the n th reassembled feature map.</p><p>Notably, our framework is compatible with a variety of Transformer structures. In this paper, we prefer to utilize Swin Transformer <ref type="bibr" target="#b31">(Liu et al., 2021)</ref> to provide hierarchical representations and reduce the computational complexity. The main differences from the standard Transformer layers lie in the local attention mechanism, the shifted window scheme, and the patch merging strategy.</p><p>Convolution branch contains a standard ResNet encoder to extract the local information, which is commonly used in depth estimation methods. Only the first block of the ResNet is used here to exploit the local information, which avoids the low-level features being washed out by consecutive multiplications <ref type="bibr" target="#b42">(Yang et al., 2021)</ref> and greatly reduces the computational time. The output feature map with C g channels is denoted as G ? R Cg?Hg?Wg .</p><p>Upon acquiring Transformer features F and convolution features G, we feed them to the HAHI module for further processing. Compared to <ref type="bibr">TransDepth (Yang et al., 2021)</ref>, we adopt an additional convolution branch to preserve the local information. It avoids the discarding of crucial information by CNN and enables us to predict sharper depth maps without artifacts, as shown in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HAHI Module</head><p>To alleviate the limitation of insufficient aggregation, we introduce the HAHI module to enhance the Transformer features and further model the affinity of the Transformer and the CNN features in a set-to-set translation manner. It is motivated by Deform-DETR <ref type="bibr" target="#b46">Zhu et al. (2021)</ref> and attempt to apply attention modules to solve the fusion of heterogeneous features.</p><p>We consider a set of hierarchical features F = {f n } N n=1 as the inputs for feature enhancement. Since we use the Swin Transformer layers to extract the features, the reassembled feature maps will exhibit different sizes and channels, as shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. Many previous works have to downsample the multi-level features to the resolution of the bottleneck feature and can only enhance the bottleneck feature with simple concatenation, or latent kernel schemes <ref type="bibr" target="#b42">(Yang et al., 2021;</ref><ref type="bibr" target="#b45">Zheng et al., 2021;</ref><ref type="bibr" target="#b27">Lee et al., 2019)</ref>. Oppositely, we aim to enhance all the features without downsampling operators that may lead to information loss. Specifically, we first utilize 1?1 convolutions to project all the hierarchical features to the same channel C h , denoting as F h = {f n h } N n=1 . Then, we unfold (i.e., flatten and concatenate) the feature maps to a two-dimensional matrix X, where each row is a C h -dimensional feature vector of one pixel from the hierarchical features. After that, we compute the Q (query), K (key) and V (value) by linear projections of X as</p><formula xml:id="formula_1">Q = XP Q , K = XP K , V = XP V ,<label>(2)</label></formula><p>where P Q , P K , and P V are linear projections, respectively. We attempt to apply the self-attention module to enhance the features. However, extremely numerous feature vectors lead to an unbearable memory cost. To alleviate this issue, we propose to adopt a deformable version that only attends to a limited set of key sampling vectors in a learnable manner. It is reasonable for the depth estimation task since several key points that indicate the scene structure are enough for feature  enhancement. Let q and v index a element with representation feature x q ? Q and x v ? V, respectively. p q reprents the location of the query vector x q . The processing can be formulated as</p><formula xml:id="formula_2">F h ? ( ) f 1 f 4 f 2 f 3 o o o o f 1 f 2 f 3 C C C f 4 Query ? 3 ? 3 ? 3 4 ? 4 ? 4 2 ? 2 ? 2 ? 2 ? 2 ? 3 ? 3 ? 4 ? 4 ? ( 1 1 + ??? + 4 4) ? 2 ? 2 ? 3 ? 3 ? 4 ? 4 2 ? 2 ? 2 3 ? 3 ? 3 4 ? 4 ? 4</formula><formula xml:id="formula_3">DAttn(x q , x v , p q ) = k?? k A qk ? x v (p q + ?p qk ) ,<label>(3)</label></formula><p>where the attention weight A qk and the sampling offset ?p qk of the k th sampling point are obtained via linear projection over the query feature x q . A qk are normalized as k?? k A qk = 1. As p q + ?p qk is fractional, bilinear interpolation is applied as in <ref type="bibr" target="#b8">(Dai et al., 2017)</ref> in computing x v (p q + ?p qk ). We also add a hierarchical embedding to identify which feature level each query pixel lies in. The output denoted asX is folded (i.e., split and reshaped) back to the original resolutions to get the hierarchical enhanced features F enh . After fusing F enh and F via channel-wise concatenatetions followed by 1?1 convolutions, we obtain the output F o = {f n o } N n=1 and achieve the feature enhancement. When the additional convolution branch is available, we consider a feature map G as the second input of the HAHI for affinity modeling. Similar to the first input F, G can be any other type of representation. We utilize a 1?1 convolution to project G to G h with a channel dimension C h and then flatten G h to a two-dimensional query matrix Q. Applyin? X as K and V, we calculate the cross-attention to model the affinity. Similarly, the unbearable memory cost still persists. We apply the deformable attention module in Eq. 3 to alleviate this issue, where the reference point locations p q are dynamically predicted from the affinity query embedding via a learnable linear projection followed by a sigmoid function. After reshaping the result to the original resolution to form the attentive representation G att , we fuse G att and G by a channel-wise concatenation and a 1?1 convolution, getting another output of HAHI, denoted as G o . This process achieves the affinity modeling and the feature interaction between the Transformer and the CNN branches.</p><p>All the outputs of the HAHI (i.e., F o and G o ) are sent to the baseline decoder <ref type="bibr" target="#b1">(Alhashim and Wonka, 2018a;</ref><ref type="bibr" target="#b30">Li et al., 2021b</ref>) for depth estimation, which consists of several consecutive UpConv layers that are illustrated in the right part of <ref type="figure">Fig. 2</ref>. The network optimization loss updated from <ref type="bibr" target="#b10">(Eigen et al., 2014)</ref> is:</p><formula xml:id="formula_4">L pixel = ? 1 T i h 2 i ? ? T 2 ( i h i ) 2 ,<label>(4)</label></formula><p>where h i = logd i ?log d i with the ground truth depth d i and predicted depthd i . T denotes the number of pixels having valid ground truth values. Following <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref>, we use ? = 0.85 and ? = 10 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>KITTI is a dataset that provides stereo images and corresponding 3D laser scans of outdoor scenes captured by equipment mounted on a moving vehicle <ref type="bibr" target="#b14">(Geiger et al., 2013)</ref>. The RGB images have a resolution of around 1241 ? 376, while the corresponding ground truth depth maps are of low density. Following the standard Eigen training/testing split <ref type="bibr" target="#b10">(Eigen et al., 2014)</ref>, we use around 26K images from the left view for training and 697 frames for testing. When evaluation, we use the crop as defined by Garg et al. <ref type="bibr" target="#b13">(Garg et al., 2016)</ref> and upsample the prediction to the ground truth resolution. For the online KITTI depth prediction, we use the official benchmark split <ref type="bibr" target="#b39">(Uhrig et al., 2017)</ref>, which contains around 72K training data, 6K selected validation data and 500 test data without the ground truth. NYU-Depth-v2 provides images and depth maps for different indoor scenes captured at a pixel resolution of 640?480 <ref type="bibr" target="#b36">(Silberman et al., 2012)</ref>. Following previous works, we train our network on a 50K RGB-Depth pairs subset. The predicted depth maps of DepthFormer have a resolution of 320 ? 240 and an upper bound of 10 meters. We upsample them by 2? to match the ground truth resolution during both training and testing. We evaluate the results on the predefined center cropping by <ref type="bibr" target="#b10">Eigen et al. (Eigen et al., 2014)</ref>.</p><p>SUN RGB-D is an indoor dataset consisting of around 10K images with high scene diversity collected with four different sensors <ref type="bibr" target="#b37">(Song et al., 2015;</ref><ref type="bibr" target="#b41">Xiao et al., 2013;</ref><ref type="bibr" target="#b20">Janoch et al., 2013)</ref>. We apply this dataset for generalization evaluation. Specifically, we cross-evaluate our NYU pre-trained models on the official test set of 5050 images without further fine-tuning. The depth upper bound is set to 10 meters. Note that this dataset is only for evaluation. We do not train on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>In our experiments, we follow the standard evaluation protocol of the prior work <ref type="bibr" target="#b10">(Eigen et al., 2014)</ref> to confirm the effectiveness of DepthFormer in experiments. For the NYU, KITTI Eigen split and SUN RGB-D dataset, we utilize the accuracy under the threshold (? i &lt; 1.25 i , i = 1, 2, 3), mean absolute relative error (AbsRel), mean squared relative error (SqRel), root mean squared error (RMSE), root mean squared log error (RMSElog), and mean log10 error (log10) to evaluate our methods. In terms of the online KITTI benchmark <ref type="bibr" target="#b39">(Uhrig et al., 2017)</ref>, we use the scale-invariant logarithmic error (SILog), percentage of AbsRel and SqRel (absErrorRel, sqErrorRel), and root mean squared error of the inverse depth (iRMSE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Since we find there is no commonly used codebase for the monocular depth estimation task, we develop a unified bench-mark based on the MMSegmentation <ref type="bibr" target="#b7">(Contributors, 2020)</ref>. We believe it can further boost the development of this field and achieve fair comparisons. We train the entire network with the batch size 2, learning rate 1e ?4 for 38.4k iterations on a single node with 8 NVIDIA V100 32GB GPUs, which takes around 5 hours. The linear learning rate warm-up strategy is applied for the first 30% iterations following <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref>. The cosine annealing learning rate strategy RGB BTS <ref type="bibr" target="#b27">(Lee et al., 2019)</ref> DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref> Ada. <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref> Ours GT <ref type="figure">Fig. 7</ref> Qualitative comparison on the NYU-Depth-v2 dataset.</p><p>Method  <ref type="table">Table 5</ref> Results of models trained on the NYU-Depth-v2 dataset and tested on the SUN RGB-D dataset <ref type="bibr" target="#b37">(Song et al., 2015)</ref> without fine-tuning. The reported numbers are from <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref>.  <ref type="table">Table 6</ref> Ablation study results on the NYU dataset. CB: Convolution Branch. LP: Larger-scale pre-training dataset (22K ImageNet) for boosting the model performance. For fair comparison, we utilize the 22K-ImageNet pre-trained ResNet-50-x3 provided by <ref type="bibr" target="#b24">(Kolesnikov et al., 2020)</ref> to get the results of R-50 (+HAHI+LP). is adopted for the learning rate decay. Following <ref type="bibr" target="#b33">(Ranftl et al., 2021;</ref><ref type="bibr" target="#b31">Liu et al., 2021)</ref>, we sample N = 4 results from the transformer features as the output of the transformer branch. The number of reference points in deformable attention modules and C h is experientially set to 8 and the median value of the channel dimension of F, respectively. Following , we adopt 8 deformable attention heads. The default patch size of ViT-Base and window size of Swin Transformer are 16 and 12, respectively. Following previous works, our encoders are pre-trained on ImageNet dataset <ref type="bibr" target="#b25">(Krizhevsky et al., 2012)</ref> and then fine-tuned on depth datasets. As for the pilot study, the baseline model consists of an encoder and a decoder. We adopt the decoder in (Alhashim and Wonka, 2018a) as a default setting and mainly Input RGB Predicted Depth Reconstructed Point Cloud focus on the influence of encoder choices. In terms of the pure convolution encoder, we utilize the standard ResNet-50 <ref type="bibr" target="#b16">(He et al., 2016)</ref>. For the pure Transformer encoder, we adopt the ViT-B <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021)</ref> following the design of the DPT <ref type="bibr" target="#b33">(Ranftl et al., 2021)</ref> and Swin-T <ref type="bibr" target="#b31">(Liu et al., 2021)</ref>. Notably, our encoders are pre-trained on the ImageNet classification, which is the standard protocol of supervised monocular depth estimation. During training, we adopt the AdamW optimizer. The weight decay is set to 0.01. We experientially use the 1-cycle policy with the learning rate lr = 6e ?5 for the Transformer-based model and lr = 1e ?4 for the ResNet-based model. We also apply a linear warm-up scheduler for the first 500 iterations. The cosine annealing learning rate strategy is adopted for the learning rate decay. When evaluation, we divide the depth range to 0-20m, 20-60m and 60-80m. The results of 0-20m and 60-80m can indicate the model performance predicting the depth of near and distant objects, respectively. We also present more detailed results in <ref type="figure" target="#fig_0">Fig. 13</ref>, where the model performance at each tick is shown in a curve. When visualizing the results,  we utilize the color map of jet and reversed magma for NYU and KITTI, respectively.</p><formula xml:id="formula_5">? 1 ? ? 2 ? ? 3 ? REL? RMS?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to State-of-the-Arts</head><p>We compare the proposed methods with the leading monocular depth estimation models. Primarily, we choose the Adabins <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref> as our main competitor, which is a solid counterpart and achieved state-of-the-art on all of the datasets we consider. We reproduce the codes of Adabins and load the pre-trained models provided by the authors to get the resulting depth images. Other results are from their official codes.</p><p>NYU-Depth-v2: Tab. 2 lists the performance comparison results on the NYU-Depth-v2 dataset. While the performance of the state-of-the-art models tends to approach saturation, DepthFormer outperforms all the competitors with prominent margins in all metrics. It indicates the effectiveness of our proposed methods. Qualitative comparisons can be seen in <ref type="figure">Fig. 7</ref>. DepthFormer achieves more accurate and sharper depth estimation results. We combine camera parameters and predicted depth maps to inv-project the 2D images into the 3D world. As shown in <ref type="figure" target="#fig_4">Fig. 8</ref>, our reconstructed scenes are satisfying with sharp boundaries of objects and reasonable depth estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI:</head><p>We evaluate on the Eigen split <ref type="bibr" target="#b10">(Eigen et al., 2014)</ref> and report the results on the Tab. 3. DepthFormer sig- nificantly outperforms all the leading methods. Qualitative comparisons can be seen in <ref type="figure">Fig. 9</ref>. We then train our model on the training set of the standard KITTI benchmark split and submit the prediction results of the testing set to the online website. We report the results in Tab. 4. While a saturation phenomenon persists in sqErrorRel, DepthFormer still achieves 16% improvement on this metric and achieves the most competitive result on the highly competitive benchmark as the submission time of Nov. 16th, 2021. We report some qualitative comparison results in <ref type="figure">Fig. 10</ref>. SUN RGB-D: Following Adabins <ref type="bibr" target="#b3">(Bhat et al., 2021)</ref>, we conduct a cross-dataset evaluation by training our models on the NYU-Depth-v2 dataset and evaluating them on the test set of the SUN RGB-D dataset without any fine-tuning. As shown in Tab. 5, significant improvements in all the metrics indicate an outstanding generalization performance of DepthFormer. Qualitative results are shown in <ref type="figure">Fig. 11</ref>. It is engaging that DepthFormer presents a strong generalization when cross-dataset evaluation. Especially, our method can predict accurate depth estimation for extremely dark areas which are extremely hard to handle without training on the corresponding dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>For our ablation study, we conduct evaluations with each component of DepthFormer to prove the effectiveness of our method on the NYU and KITTI dataset.</p><p>Effectiveness of key components: We first validate the effectiveness of the key components of DepthFormer. From the baseline network (i.e., ResNet-50, Swin-T), we reinforce the network with our proposed methods and evaluate the improvement of the model performance. The results are reported in the Tab. 6. As the additional convolution branch and the HAHI are adopted, the overall performance is significantly improved, which demonstrates the effectiveness of our methods. Moreover, following previous methods <ref type="bibr" target="#b42">(Yang et al., 2021;</ref><ref type="bibr" target="#b33">Ranftl et al., 2021)</ref>, we utilize larger-scale dataset (i.e., ImageNet-22K) to pre-train our encoder. The results (+LP) indicate that the Transformer encoder can better benefit from the larger model capacity and the larger-scale pre-training dataset compared with the CNN encoder.</p><p>Fine-grained evaluation on convolution branch: The standard CNN encoder can be divided into several sequential blocks. We further scrutinize the influence of different level convolution features on the model performance. Following the default setting, we adopt ResNet-50 as the additional convolution branch. Results are shown in <ref type="figure">Fig. 12</ref>. Interestingly, the model achieves the best performance with only one convolutional block and then downgrades if more blocks are added. A possible explanation for this might be that the consecutive convolutions wash out low-level features, and the gradually reducing spatial resolution discards the finegrained information <ref type="bibr" target="#b42">(Yang et al., 2021)</ref>. Adopting the first block achieves a win-win scenario: it optimizes accuracy by preserving crucial local information while reducing complexity. This can reduce the training time by 2.5? or more and likewise decrease memory consumption, enabling us to easily scale our Transformer branch to large models.</p><p>Fine-grained evaluation on HAHI: Since the HAHI consists of a deformable self-attention module (DSA) for PWA <ref type="bibr" target="#b28">(Lee et al., 2021)</ref> PWA Error <ref type="formula">(</ref>  <ref type="figure">Fig. 12</ref> Effect of the convolution branch block on NYU depth estimation performance. We can observe that behaviour decreases after the first block of R-50 <ref type="bibr" target="#b16">(He et al., 2016).</ref> interaction. We infer the reason that there are large discrepancies between the heterogeneous features. Multi-level DSA achieves the alignment of the features, which propels the affinity modeling. All the results demonstrate the effectiveness of our proposed HAHI module.</p><p>Details about pilot study results: We have discussed that the CNN branch can provide local information lost in the Transformer branch and the HAHI further promotes the depth estimation via feature enhancement and affinity modeling. They improve the model performance, especially on near object depth estimation. Tab. 8 demonstrates the effectiveness of our methods. Moreover, we draw more fine-grained results in <ref type="figure" target="#fig_0">Fig. 13</ref>. Interestingly, Swin-Transformer based model achieves better performance compared with ResNet50-based ones on near object depth estimation and satisfactory results compared with ViT-based ones on distant object depth estimation. We infer that the hierarchical design of the Swin Transformer benefits the extraction of the local information, and the special attention mechanism successfully models the long-range correlation. To compare the model performance in a more direct manner, we also present qualitative comparison results in <ref type="figure" target="#fig_3">Fig. 6</ref>. One can observe sharper and more accurate results can be achieved with our proposed CNN branch and HAHI module.</p><p>Inference time evaluation: Except for the accuracy (? 1 ) of the depth prediction, the inference velocity is of importance as well. We thus evaluate the inference time of Depth-Former w/o HAHI on the KITTI validation set. The reso-Abs Rel (Lower is better) Depth Interval (m) <ref type="figure" target="#fig_0">Fig. 13</ref> Fine-grained quantitative results of our pilot study on KITTI datset. We divide the depth range (0m -80m) into 80 intervals. Point (i, j) in the plot represents the abs rel of the model is j on depth interval (i, i + 1]m. Our method achieves a trade off between long and short range estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented DepthFormer, a novel framework for accurate monocular depth estimation. Our method fully exploits the long-range correlations and the local information by an encoder consisting of a Transformer branch and a CNN branch. Since independent branches with late fusion lead to insufficient feature aggregation for the decoder, we propose the hierarchical aggregation and heterogeneous interaction module to enhance the multi-level features and further model the feature affinity. DepthFormer achieves significant improvements compared with state-of-the-arts in the most popular and challenging datasets. We hope our study can encourage more works applying the Transformer architecture in monocular depth estimation and enlighten the framework design of other tasks.</p><p>Potential impact: Beyond the direct application of our work for autonomous driving or spatial reconstruction, there are several venues that warrant future investigation. For example, the common dense global attention in Transformer might be sumptuous. In terms of depth estimation, several key points that indicate the scene structure could be enough to provide crucial long-range information. Designing a more dedicated attention mechanism would improve the effectiveness of the Transformer branch. Furthermore, the HAHI is input-agnostic, and including other modalities such as sparse LiDAR would enhance performance and generalization. Finally, due to the lack of theoretical guarantees, future work to improve the applicability of DepthFormer might consider challenges of explainability and transparency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>Failure cases of previous methods on NYU dataset caused by a limited receptive field of the convolution operator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>Demonstration of artifacts and the lost of local information. Our method provides consistent and sharp depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Illustration of our proposed HAHI. The deformable self-attention module enhances the input Transformer features F. The deformable cross-attention module models the affinity between the Transformer features F and the convolution features G in a set-to-set translation manner. The output of the HAHI, F o and G o , are sent to the decoder for the final aggregation. Due to the adoption of Swin Transformer layers to extract the hierarchical features, F exhibits different sizes and channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Qualitative comparisons in our pilot study.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8</head><label>8</label><figDesc>Visualization of reconstructed 3D scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 9 Qualitative comparison on the KITTI validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 14</head><label>14</label><figDesc>Accuracy ? 1 vs. Inference Time on KITTI validation set. The speed is measured using a single RTX 3060 GPU. lution of the input images is 352?1216. Results shown in Fig. 14 demonstrate that DepthFormer outperforms current state-of-the-art methods in terms of both accuracy and speed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Feature Map Feature Vector Attention Result Light Color Deep Color</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Hierarchical Aggregation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>F enh</cell><cell></cell><cell>F o</cell><cell></cell></row><row><cell>F</cell><cell>1x1</cell><cell></cell><cell>Unfold</cell><cell>?</cell><cell>Self-Att.</cell><cell>Deformable</cell><cell>?</cell><cell>Fold</cell><cell></cell><cell>1x1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell cols="2">Heterogeneous Interaction</cell><cell></cell><cell></cell><cell></cell><cell>Value</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unfold</cell><cell>Fold</cell></row><row><cell></cell><cell>1x1</cell><cell>h</cell><cell>Unfold</cell><cell>?</cell><cell>Cross-Att.</cell><cell>Deformable</cell><cell>?</cell><cell>Fold</cell><cell>att</cell><cell>1x1</cell><cell>o</cell><cell>1x1 F R</cell><cell>? Concat Flatten and Concat 1x1 Conv Divide and Reshape</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Comparison of performances on the NYU-Depth-v2 dataset. The reported numbers are from the corresponding original papers.</figDesc><table><row><cell>Method</cell><cell>? 1 ?</cell><cell>? 2 ?</cell><cell></cell><cell>? 3 ?</cell><cell>REL ?</cell><cell cols="2">RMS ?</cell><cell>log 10 ?</cell><cell>Reference</cell></row><row><cell>Eigen et al. (Eigen et al., 2014)</cell><cell>0.769</cell><cell>0.950</cell><cell></cell><cell>0.988</cell><cell>0.158</cell><cell cols="2">0.641</cell><cell>-</cell><cell>NIPS2014</cell></row><row><cell>Laina et al. (Laina et al., 2016)</cell><cell>0.811</cell><cell>0.953</cell><cell></cell><cell>0.988</cell><cell>0.127</cell><cell cols="2">0.573</cell><cell>0.055</cell><cell>3DV2016</cell></row><row><cell>StructDepth (Li et al., 2021a)</cell><cell>0.817</cell><cell>0.955</cell><cell></cell><cell>0.988</cell><cell>0.140</cell><cell cols="2">0.534</cell><cell>0.060</cell><cell>ICCV2021</cell></row><row><cell>MonoIndoor (Ji et al., 2021)</cell><cell>0.823</cell><cell>0.958</cell><cell></cell><cell>0.989</cell><cell>0.134</cell><cell cols="2">0.526</cell><cell>-</cell><cell>ICCV2021</cell></row><row><cell>DORN et al. (Fu et al., 2018)</cell><cell>0.828</cell><cell>0.965</cell><cell></cell><cell>0.992</cell><cell>0.115</cell><cell cols="2">0.509</cell><cell>0.051</cell><cell>CVPR2018</cell></row><row><cell>BTS (Lee et al., 2019)</cell><cell>0.885</cell><cell>0.978</cell><cell></cell><cell>0.994</cell><cell>0.110</cell><cell cols="2">0.392</cell><cell>0.047</cell><cell>Arxiv2019</cell></row><row><cell>DAV (Huynh et al., 2020)</cell><cell>0.882</cell><cell>0.980</cell><cell></cell><cell>0.996</cell><cell>0.108</cell><cell cols="2">0.412</cell><cell>-</cell><cell>ECCV2020</cell></row><row><cell>TransDepth (Yang et al., 2021)</cell><cell>0.900</cell><cell>0.983</cell><cell></cell><cell>0.996</cell><cell>0.106</cell><cell cols="2">0.365</cell><cell>0.045</cell><cell>ICCV2021</cell></row><row><cell>DPT (Ranftl et al., 2021)</cell><cell>0.904</cell><cell>0.988</cell><cell></cell><cell>0.998</cell><cell>0.110</cell><cell cols="2">0.357</cell><cell>0.045</cell><cell>ICCV2021</cell></row><row><cell>AdaBins (Bhat et al., 2021)</cell><cell>0.903</cell><cell>0.984</cell><cell></cell><cell>0.997</cell><cell>0.103</cell><cell cols="2">0.364</cell><cell>0.044</cell><cell>CVPR2020</cell></row><row><cell>DepthFormer</cell><cell>0.921</cell><cell>0.989</cell><cell></cell><cell>0.998</cell><cell>0.096</cell><cell cols="2">0.339</cell><cell>0.041</cell><cell>-</cell></row><row><cell>Method</cell><cell cols="2">Backbone</cell><cell></cell><cell>? 1 ?</cell><cell>? 2 ?</cell><cell>? 3 ?</cell><cell>REL ?</cell><cell>Sq ?</cell><cell>RMS ? RMS log ?</cell></row><row><cell>Godard et al. (Godard et al., 2017)</cell><cell cols="2">ResNet-50</cell><cell></cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell></row><row><cell cols="3">Johnston et al. (Johnston and Carneiro, 2020) ResNet-101</cell><cell></cell><cell>0.889</cell><cell>0.962</cell><cell>0.982</cell><cell>0.106</cell><cell>0.861</cell><cell>4.699</cell><cell>0.185</cell></row><row><cell>Gan et al. (Gan et al., 2018)</cell><cell cols="2">ResNet-101</cell><cell></cell><cell>0.890</cell><cell>0.964</cell><cell>0.985</cell><cell>0.098</cell><cell>0.666</cell><cell>3.933</cell><cell>0.173</cell></row><row><cell>DORN et al. (Fu et al., 2018)</cell><cell cols="2">ResNet-101</cell><cell></cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell></row><row><cell>Yin et al. (Yin et al., 2019)</cell><cell cols="2">ResNext-101</cell><cell></cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell></row><row><cell>PGA-Net (Xu et al., 2020)</cell><cell cols="2">ResNet-50</cell><cell></cell><cell>0.952</cell><cell>0.992</cell><cell>0.998</cell><cell>0.063</cell><cell>0.267</cell><cell>2.634</cell><cell>0.101</cell></row><row><cell>BTS (Lee et al., 2019)</cell><cell cols="2">DenseNet-161</cell><cell></cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell><cell>0.059</cell><cell>0.245</cell><cell>2.756</cell><cell>0.096</cell></row><row><cell>TransDepth (Yang et al., 2021)</cell><cell cols="3">ViT-B+ResNet-50</cell><cell>0.956</cell><cell>0.994</cell><cell>0.999</cell><cell>0.064</cell><cell>0.252</cell><cell>2.755</cell><cell>0.098</cell></row><row><cell>DPT-Hybrid (Ranftl et al., 2021)</cell><cell cols="3">ViT-B+R-50-C 1 C 2</cell><cell>0.959</cell><cell>0.995</cell><cell>0.999</cell><cell>0.062</cell><cell>-</cell><cell>2.573</cell><cell>0.092</cell></row><row><cell>AdaBins (Bhat et al., 2021)</cell><cell cols="2">Mini-ViT+E-B5</cell><cell></cell><cell>0.964</cell><cell>0.995</cell><cell>0.999</cell><cell>0.058</cell><cell>0.190</cell><cell>2.360</cell><cell>0.088</cell></row><row><cell>DepthFormer</cell><cell cols="3">Swin-L+R-50-C 1</cell><cell>0.975</cell><cell>0.997</cell><cell>0.999</cell><cell>0.052</cell><cell>0.158</cell><cell>2.143</cell><cell>0.079</cell></row><row><cell>Method</cell><cell></cell><cell>SILog?</cell><cell cols="4">sqErrorRel? absErrorRel?</cell><cell cols="2">iRMSE?</cell><cell>Reference</cell></row><row><cell>DORN (Fu et al., 2018)</cell><cell></cell><cell>11.77</cell><cell></cell><cell>2.23</cell><cell>8.78</cell><cell></cell><cell cols="2">12.98</cell><cell>CVPR2018</cell></row><row><cell>BTS (Lee et al., 2019)</cell><cell></cell><cell>11.67</cell><cell></cell><cell>2.21</cell><cell>9.04</cell><cell></cell><cell cols="2">12.23</cell><cell>Arxiv2019</cell></row><row><cell>BANet (Aich et al., 2020)</cell><cell></cell><cell>11.55</cell><cell></cell><cell>2.31</cell><cell>9.34</cell><cell></cell><cell cols="2">12.17</cell><cell>Arxiv2020</cell></row><row><cell>PWA (Lee et al., 2021)</cell><cell></cell><cell>11.45</cell><cell></cell><cell>2.30</cell><cell>9.05</cell><cell></cell><cell cols="2">12.32</cell><cell>AAAI2021</cell></row><row><cell>ViP-DeepLab (Qiao et al., 2021)</cell><cell></cell><cell>10.80</cell><cell></cell><cell>2.19</cell><cell>8.94</cell><cell></cell><cell cols="2">11.77</cell><cell>CVPR2021</cell></row><row><cell>Ours</cell><cell></cell><cell>10.46</cell><cell></cell><cell>1.82</cell><cell>8.54</cell><cell></cell><cell cols="2">11.17</cell><cell>-</cell></row></table><note>Table 3 Comparison of performances on the KITTI validation dataset. The reported numbers are from the corresponding original papers. Measurements are made for the depth range from 0m to 80m. Best / Second best results are marked bold / underlined. R-50 and E-B5 are short for ResNet-50 and EfficientNet-B5 (Tan and Le, 2019), respectively. C i represents the i th block of the ResNet-50 network.Table 4 Comparison of performances on the KITTI depth estimation benchmark test set. Reported numbers are from the official benchmark website.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>log 10 ?</figDesc><table><row><cell cols="2">Chen et al. (Chen et al., 2019) 0.757 0.943 0.984 0.166</cell><cell>0.494</cell><cell>0.071</cell></row><row><cell>Yin et al. (Yin et al., 2019)</cell><cell>0.696 0.912 0.973 0.183</cell><cell>0.541</cell><cell>0.082</cell></row><row><cell>BTS (Lee et al., 2019)</cell><cell>0.740 0.933 0.980 0.172</cell><cell>0.515</cell><cell>0.075</cell></row><row><cell>Adabins (Bhat et al., 2021)</cell><cell>0.771 0.944 0.983 0.159</cell><cell>0.476</cell><cell>0.068</cell></row><row><cell>Ours</cell><cell>0.815 0.970 0.993 0.137</cell><cell>0.408</cell><cell>0.059</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Ablation study of the HAHI module on NYU dataset. DSA, DCA: Deformable self-attention and deformable cross-attention.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>More detailed ablation quantitative results on KITTI dataset.</figDesc><table><row><cell>Backbone</cell><cell>Various</cell><cell>Range</cell><cell>? 1 ?</cell><cell>? 2 ?</cell><cell>? 2 ?</cell><cell>REL?</cell><cell>RMS?</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.973</cell><cell>0.998</cell><cell>1</cell><cell>0.054</cell><cell>0.985</cell></row><row><cell>ResNet-50 (He et al., 2016)</cell><cell>-</cell><cell>60m-80m</cell><cell>0.600</cell><cell>0.900</cell><cell>0.972</cell><cell>0.188</cell><cell>14.30</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.952</cell><cell>0.994</cell><cell>0.999</cell><cell>0.065</cell><cell>2.596</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.955</cell><cell>0.995</cell><cell>0.999</cell><cell>0.071</cell><cell>1.275</cell></row><row><cell cols="2">ViT-Base (Dosovitskiy et al., 2021) -</cell><cell>60m-80m</cell><cell>0.727</cell><cell>0.936</cell><cell>0.985</cell><cell>0.150</cell><cell>11.86</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.938</cell><cell>0.992</cell><cell>0.999</cell><cell>0.080</cell><cell>2.695</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.960</cell><cell>0.996</cell><cell>0.999</cell><cell>0.067</cell><cell>1.223</cell></row><row><cell cols="2">ViT-Base (Dosovitskiy et al., 2021) +CB</cell><cell>60m-80m</cell><cell>0.725</cell><cell>0.950</cell><cell>0.984</cell><cell>0.147</cell><cell>11.69</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.942</cell><cell>0.994</cell><cell>0.999</cell><cell>0.076</cell><cell>2.644</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.964</cell><cell>0.995</cell><cell>0.999</cell><cell>0.064</cell><cell>1.172</cell></row><row><cell cols="2">ViT-Base (Dosovitskiy et al., 2021) +CB+HAHI</cell><cell>60m-80m</cell><cell>0.712</cell><cell>0.946</cell><cell>0.984</cell><cell>0.150</cell><cell>11.90</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.948</cell><cell>0.993</cell><cell>0.999</cell><cell>0.073</cell><cell>2.596</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.972</cell><cell>0.998</cell><cell>1</cell><cell>0.050</cell><cell>0.948</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>-</cell><cell>60m-80m</cell><cell>0.729</cell><cell>0.941</cell><cell>0.984</cell><cell>0.150</cell><cell>11.85</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.961</cell><cell>0.993</cell><cell>0.999</cell><cell>0.062</cell><cell>2.402</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.979</cell><cell>0.998</cell><cell>1</cell><cell>0.049</cell><cell>0.934</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>+CB</cell><cell>60m-80m</cell><cell>0.726</cell><cell>0.945</cell><cell>0.984</cell><cell>0.149</cell><cell>11.76</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.964</cell><cell>0.995</cell><cell>0.999</cell><cell>0.060</cell><cell>2.310</cell></row><row><cell></cell><cell></cell><cell>0m-20m</cell><cell>0.981</cell><cell>0.998</cell><cell>1</cell><cell>0.049</cell><cell>0.911</cell></row><row><cell>Swin-T (Liu et al., 2021)</cell><cell>+CB+HAHI</cell><cell>60m-80m</cell><cell>0.744</cell><cell>0.939</cell><cell>0.981</cell><cell>0.146</cell><cell>11.61</cell></row><row><cell></cell><cell></cell><cell>Overall</cell><cell>0.966</cell><cell>0.996</cell><cell>0.999</cell><cell>0.059</cell><cell>2.261</cell></row><row><cell>Table 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Qualitative comparison with the state-of-the-art on the KITTI benchmark, better viewed by zooming on screen. Deeper red pixels in the error maps indicate higher errors. Deeper blue means lower errors. The figures are from the official KITTI benchmark website.hierarchical aggregation and a deformable cross-attention module (DCA) for heterogeneous interaction, we conduct more detailed ablation studies on both of these two modules. For fair comparison, we choose the Swin-T with CB as the default backbone. The results are reported in Tab. 7. We propose to apply the attention mechanism on all the hierarchical features (multi-level DSA) for sufficient aggregation. Compared with the one where only each single-layer feature is consid-ered in the attention module, denotes as single-level DSA, the multi-level aggregation strategy get a 4.9% enhancement on RMS. It demonstrates that the multi-level aggregation strategy is much more effective. When DSA is added without the multi-level DSA, the model performance is seriously impaired. However, with the multi-level DSA, DCA achieves a 2.2% improvement on RMS, verifying the importance of both the multi-level DSA and the DCA for heterogeneous Qualitative comparison on the SUN RGB-D dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Lee et al., 2021)</cell></row><row><cell>Input</cell><cell></cell><cell>ViP-DeepLab (Qiao et al., 2021)</cell><cell>ViP-DeepLab Error (Qiao et al., 2021)</cell></row><row><cell></cell><cell></cell><cell>Ours</cell><cell>Ours Error</cell></row><row><cell></cell><cell></cell><cell>PWA (Lee et al., 2021)</cell><cell>PWA Error (Lee et al., 2021)</cell></row><row><cell>Input Input Fig. 11 Abs Rel Error 0.120 0.121 0.122 0.123 0.126 0.124 0.125 0.127 Fig. 10 RGB</cell><cell>w/o CB</cell><cell cols="2">ViP-DeepLab (Qiao et al., 2021) Ours PWA (Lee et al., 2021) ViP-DeepLab (Qiao et al., 2021) Ours Adabins (Bhat et al., 2021) Ours Stage 1 Stage 3 Stage 4 Stage 2 Accuracy ViP-DeepLab Error (Qiao et al., 2021) Ours Error PWA Error (Lee et al., 2021) ViP-DeepLab Error (Qiao et al., 2021) Ours Error GT Abs Rel Error Stage 5 86.3 86.4 86.5 86.6 86.9 86.7 86.8 87.0 Accuracy Stages of Convolutional Branch</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.cvlibs.net/datasets/kitti/eval_ depth.php?benchmark=depth_prediction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bidirectional attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jmu</forename><surname>Vianney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv:200900743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno>arXiv:181211941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">abs/1812.11941:arXiv:1812.11941</idno>
		<idno>1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure-aware residual pyramid network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="694" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A category-level 3d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="141" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Monoindoor: Towards good practice of self-supervised monocular depth estimation for indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12787" to="12796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4756" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno>arXiv:190710326</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Patch-wise attention network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1873" to="1881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structdepth: Leveraging the structural regularities for self-supervised indoor depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12663" to="12673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Simipu: Simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno>arXiv:211204680</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vip-deeplab: Learning visual perception with depth-aware video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3997" to="4008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sun3d: A database of big spaces reconstructed using sfm and object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1625" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic graph attention network with conditional kernels for pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alameda</forename><forename type="middle">-</forename><surname>Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N ;</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
