<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SRM : A Style-based Recalibration Module for Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjae</forename><surname>Lee</surname></persName>
							<email>hjlee@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Eun</forename><surname>Kim</surname></persName>
							<email>hekim@lunit.io</email>
							<affiliation key="aff1">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
							<email>hsnam@lunit.io</email>
							<affiliation key="aff2">
								<orgName type="institution">Lunit Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SRM : A Style-based Recalibration Module for Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The evolution of convolutional neural networks (CNNs) has constantly pushed the boundaries of complex vision tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b1">2]</ref>. Besides their superior performance, a wide investigation has revealed that CNNs are capable of handling not only the content (i.e. shape) but also the style (i.e. texture) of an image. Gatys et al. <ref type="bibr" target="#b5">[6]</ref> discovered that the feature statistics of a CNN effectively encode the style information of an image, which laid the foundation of neural style transfer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13]</ref>. Recent approaches also pointed out that the styles play an unexpectedly significant role in the decision making process by standard CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. Furthermore, Karras et al. <ref type="bibr" target="#b17">[18]</ref> demonstrated that a generative CNN architecture solely based on style manipulation achieves dramatic improvement in terms of realistic image generation.</p><p>Inspired by the tight link between the style and CNN representation, we aim to enhance the utilization of styles in a CNN to boost its representational power. We propose a novel architectural unit, Style-based Recalibration Module (SRM), which explicitly incorporates the styles into CNN representations through a form of feature recalibration. Note that a CNN involves styles with varying levels of significance. While certain styles play an essential role, some are rather a nuisance factor to the task <ref type="bibr" target="#b24">[25]</ref>. SRM dynamically estimates the relative importance of individual styles then reweights the feature maps based on the style importance, which allows the network to focus on meaningful styles while ignoring unnecessary ones.</p><p>The overall structure of SRM is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of two main components: style pooling and style integration. The style pooling operator extracts style features from each channel by summarizing feature responses across spatial dimensions. It is followed by the style integration operator, which produces example-specific style weights by utilizing the style features via channel-wise operation. The style weights finally recalibrate the feature maps to either emphasize or suppress their information. Our proposed module is seamlessly integrated into modern CNN architecture and trained in an end-to-end manner. While SRM only imposes negligible additional parameters and computations, it remarkably improves the performance of the network. Beyond the practical improvements, SRM provides an intuitive interpretation about the effect of channel-wise recalibration: it controls the contribution of styles by adjusting the global statistics of feature responses while maintaining their spatial configuration.</p><p>Our experiments on image recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19]</ref> verify the effectiveness of SRM in general vision tasks. Throughout the experiment, SRM outperforms recent approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> though it requires orders of magnitude less additional parameters. Furthermore, we demonstrate the capability of SRM in arranging the contribution of styles. To this end, we conduct extensive experiments on style-related tasks such as classification with a texture-shape cue conflict <ref type="bibr" target="#b7">[8]</ref>, multidomain classification <ref type="bibr" target="#b31">[32]</ref>, texture recognition <ref type="bibr" target="#b3">[4]</ref>, and style transfer <ref type="bibr" target="#b16">[17]</ref>, where SRM brings exceptional performance improvements. We also provide comprehensive analysis and ablation studies to further investigate the behavior of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRM.</head><p>The main contributions of this paper are as follows:</p><p>? We present a style-based feature recalibration module which enhances the representational capability of a CNN by incorporating the styles into the feature maps.</p><p>? Despite its minimal overhead, the proposed module noticeably improves the performance of a network in general vision tasks as well as style-related tasks.</p><p>? Through in-depth analysis along with ablation study, we examine the internal behavior and validity of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Style Manipulation. Manipulating the style information of CNNs has been widely studied in generative frameworks. The pioneering work by Gatys et al. <ref type="bibr" target="#b6">[7]</ref> presented impressive style transfer results by exploiting the second-order statistics (i.e. the Gram matrix) of convolutional features as style representations. Li et al. <ref type="bibr" target="#b20">[21]</ref> also addressed style transfer by matching a variety of CNN feature statistics such as linear, polynomial and Gaussian kernels. Adaptive instance normalization (AdaIN) <ref type="bibr" target="#b12">[13]</ref> further showed that transferring channel-wise mean and standard deviation can efficiently change image styles. Recent work by Karras et al. <ref type="bibr" target="#b17">[18]</ref> combined AdaIN into generative adversarial networks (GANs) to improve the generator by adjusting styles in intermediate layers.</p><p>The potential of styles in a CNN has been also investigated in discriminative settings. BagNets <ref type="bibr" target="#b0">[1]</ref> demonstrated that a CNN constrained to rely on style information without considering spatial context performs surprisingly well on image classification. Geirhos et al. <ref type="bibr" target="#b7">[8]</ref> discovered that CNNs (e.g. ImageNet-trained ResNet) are highly biased towards styles in their decision making process. Batch-instance normalization <ref type="bibr" target="#b24">[25]</ref> achieved practical performance improvement by controlling styles, which learns static weights for individual styles and selectively normalizes unimportant ones. In this work, we further facilitate the utilization of styles in designing a CNN architecture. Our approach dynamically enriches feature representations by either highlighting or suppressing style regarding its relevance to the task.</p><p>Attention and Feature Recalibration. It is known that human pays attention to important parts of the visual input to better grasp the core information, rather than processing the whole visual signal at once <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref>. This mechanism has been extended to CNNs in a way of refining feature activations and showed effectiveness across a wide range of applications including object classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, multimodal tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref>, video classification <ref type="bibr" target="#b33">[34]</ref>, etc.</p><p>More related to our work, Squeeze-and-Excitation (SE) <ref type="bibr" target="#b11">[12]</ref> proposed a channel-wise recalibration operator that incorporates the interaction between channels. It first aggregates the spatial information with global average pooling and captures the channel dependencies using a fully connected subnetwork. Gather-Excite (GE) <ref type="bibr" target="#b10">[11]</ref> further explored this pipeline for better exploiting the global context with a convolutional aggregator. Convolutional block attention module (CBAM) <ref type="bibr" target="#b34">[35]</ref> also showed that the SE block can be improved by additionally utilizing max-pooled features and combining with a spatial attention module. In contrast to the prior efforts, we reformulate channel-wise recalibration in terms of leveraging style information, without the aid of channel relationship nor spatial attention. We present a style pooling approach which is superior to the standard global average or max pooling in our setting, as well as a channel-independent style integration method which is substantially more lightweight than fully connected counterparts yet more effective in various scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Style-based Recalibration Module</head><p>Given an input tensor X ? R N ?C?H?W , SRM generates channel-wise recalibration weights G ? R N ?C based on the styles of X, where N indicates the number of examples in the mini-batch, C is the number of channels; H and W indicate spatial dimensions. It is divided into two se-quential submodules: style pooling for extracting an intermediate style representation T ? R N ?C?d from X, where d is the number of style features, and style integtration for estimating the style weights G from T. The final outputX is then computed by channel-wise multiplication between G and X. SRM is easily integrated into modern CNN architectures such as ResNets <ref type="bibr" target="#b8">[9]</ref> and trained end-to-end. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the detailed structure of SRM and our configuration of the SRM integrated into a residual block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Style Pooling</head><p>Extracting style information from intermediate convolutional feature maps has been widely studied in style transfer literature. Motivated by <ref type="bibr" target="#b12">[13]</ref>, we adopt the channel-wise statistics-average and standard deviation-of each feature map as style features (i.e. d = 2). Specifically, given input feature maps X ? R N ?C?H?W , the style features T ? R N ?C?2 are calculated by:</p><formula xml:id="formula_0">? nc = 1 HW H h=1 W w=1 x nchw ,<label>(1)</label></formula><formula xml:id="formula_1">? nc = 1 HW H h=1 W w=1 (x nchw ? ? nc ) 2 ,<label>(2)</label></formula><formula xml:id="formula_2">t nc = [? nc , ? nc ].<label>(3)</label></formula><p>The style vector t nc ? R 2 serves as a summary description of the style information for each example n and channel c. Other types of style features such as the correlations between different channels <ref type="bibr" target="#b6">[7]</ref> can be also included in the style vector, but we focus on the channel-wise statistics for efficiency and conceptual clarity. In section 5, we verify the practical benefits of the proposed style pooling compared to other approaches for gathering global information, e.g. using average pooling as in SE <ref type="bibr" target="#b11">[12]</ref> and additionally utilizing max pooling as in CBAM <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Style Integration</head><p>The style features are converted into channel-wise style weights by a style integration operator. The style weights are supposed to model the importance of the styles associated with individual channels so as to emphasize or suppress them accordingly. To achieve this, we adopt a simple combination of a channel-wise fully connected (CFC) layer, a batch normalization (BN) layer, and a sigmoid activation function. Given the style representation T ? R N ?C?2 as an input, the style integration operator performs channelwise encoding using learnable parameters W ? R C?2 :</p><formula xml:id="formula_3">z nc = w c ? t nc<label>(4)</label></formula><p>where Z ? R N ?C represents the encoded style features. This operation can be viewed as a channel-independent fully connected layer with two input nodes and a single output, where the bias term is absorbed into the subsequent BN layer. We then apply BN to facilitate training and a sigmoid function as a gating mechanism:</p><formula xml:id="formula_4">? (z) c = 1 N N n=1 z nc ,<label>(5)</label></formula><formula xml:id="formula_5">? (z) c = 1 N N n=1 (z nc ? ? (z) c ) 2 ,<label>(6)</label></formula><formula xml:id="formula_6">z nc = ? c ( z nc ? ? (z) c ? (z) c ) + ? c ,<label>(7)</label></formula><formula xml:id="formula_7">g nc = 1 1 + e ??nc ,<label>(8)</label></formula><p>where ?, ? ? R C are affine transformation parameters, and G ? R N ?C represents the channel-wise style weights. Note that BN makes use of fixed approximations of mean and variance at inference time, which allows the BN layer to be merged into the preceding CFC layer. Consequently, the style integration for each channel boils down to a single CFC layer f CF C : R 2 ? R followed by an activation function f ACT : R ? [0, 1]. Finally, the original input X is recalibrated by the weights G, so the output X ? R N ?C?H?W is obtained by:</p><formula xml:id="formula_8">x nc = g nc ? x nc .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Parameter and Computational Complexity</head><p>SRM is designed to be lightweight in both terms of memory and computational complexity. We first consider the  additional parameters of SRM which come from the CFC and BN layers. The number of parameters for each term is S s=1 N s ? C s ? 2 and S s=1 N s ? C s ? 4, respectively, where S denotes the number of stages, N s is the the number of repeated blocks in s-th stage, and C s is the dimension of the output channels for s-th stage. We follow the definition of stage in <ref type="bibr" target="#b11">[12]</ref> which refers to a group of convolutions with an identical spatial dimension. In total, the number of extra parameters for SRM is:</p><formula xml:id="formula_9">6 S s=1 N s ? C s ,<label>(10)</label></formula><p>which is typically negligible compared to SE's 2 r S s=1 N s ? C 2 s where r is its reduction ratio. For instance, given ResNet-50 as a baseline architecture, SRM-ResNet-50 requires only 0.06M additional parameters whereas SE-ResNet-50 requires 2.53M.</p><p>In terms of computational complexity, SRM also introduces negligible extra computations to the original architecture. For example, a single forward pass of a 224 ? 224 pixel image for SRM-ResNet-50 requires additional 0.02 GFLOPs to ResNet-50 which requires 3.86 GFLOPs. By adding only 0.52% relative computational burden, SRM increases the top-1 validation accuracy of ResNet-50 from 75.89% to 77.13%, which indicates that SRM offers a good trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we conduct a comprehensive evaluation across a wide range of problems and datasets to verify the effectiveness of SRM. We re-implemented all competitors to compare under consistent settings for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Classification</head><p>We first evaluate SRM on general object classification with ImageNet-1K <ref type="bibr" target="#b27">[28]</ref> and CIFAR-10/100 <ref type="bibr" target="#b18">[19]</ref>, in com-parison with state-of-the-art methods such as Squeeze-and-Excitation (SE) <ref type="bibr" target="#b11">[12]</ref> and Gather-Excite (GE) 1 <ref type="bibr" target="#b10">[11]</ref>. On the extension of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>, which suggest the crucial role of styles in the decision making by standard CNNs, we further demonstrate the potential of styles for improving the general performance of CNNs.</p><p>ImageNet-1K. The ImageNet-1K dataset <ref type="bibr" target="#b27">[28]</ref> consists of 1,000 classes with 1.3 million training and 50,000 validation images. We follow the standard practice for data augmentation and optimization <ref type="bibr" target="#b8">[9]</ref>. The input images are randomly cropped to 224?224 patches and random horizontal flipping is applied. The networks are trained by SGD with a batch size of 256 on 8 GPUs, a momentum of 0.9, and a weight decay of 0.0001. We train the networks for 90 epochs from the scratch with an initial learning rate of 0.1 which is divided by 10 every 30 epochs. Single center crop evaluation is performed on 224?224 patches where each image is first resized so that the shorter side is 256. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the training and validation curves of ResNet-50 with SRM and other feature recalibration methods. Throughout the whole training process, SRM exhibits considerably higher accuracy than SE and GE on both training and validation curves. This implies that utilizing styles with SRM is more effective than modeling channel interdependencies with SE or gathering global context with GE, in both terms of facilitating training and improving generalization. <ref type="table" target="#tab_0">Table 1</ref> also demonstrates that SRM significantly boosts the performance of the baseline architecture (ResNet-50/101) with almost the same number of parameters and computations. On the other hand, due to its tendency of slow convergence as mentioned in <ref type="bibr" target="#b10">[11]</ref>, GE does not exhibit improved performance in a deeper network under a fixed-length training schedule. It is worth noting that SRM outperforms SE and GE with orders of magni-  CIFAR-10/100. We also evaluate the performance of SRM on the CIFAR-10/100 dataset <ref type="bibr" target="#b18">[19]</ref> which consists of 50,000 training and 10,000 test images of 32?32 pixels. On the training phase, each image is zero-padded with 4 pixels then randomly cropped to the original size, and evaluation is performed on the original images. The networks are trained with SGD for 64,000 iterations with a mini-batch size of 128 on a single GPU, a momentum of 0.9, and a weight decay of 0.0001. The initial learning rate is set to 0.2 which is divided by 10 at 32,000 and 48,000 iterations. As presented in <ref type="table" target="#tab_1">Table 2</ref>, SRM considerably improves the accuracy on both CIFAR-10 and 100 with minimal parameter increases, which suggests that the effectiveness of SRM is not constrained to ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Style-Related Classification</head><p>The proposed idea views channel-wise recalibration as an adjustment of intermediate styles, which is achieved by exploiting the global statistics of respective feature maps. This interpretation motivates us to explore the effect of SRM on style-related tasks where explicitly manipulating style information could bring prominent benefits.  Stylized-ImageNet. We first investigate how SRM handles synthetically increased diversity of styles. We employ Stylized-ImageNet introduced by <ref type="bibr" target="#b7">[8]</ref>, which is constructed by transferring each image in ImageNet to the style of a random painting in the Painter by Numbers dataset 2 (total 79,434 paintings). Since the randomly transferred style is irrelevant to the object category, it is a much harder dataset than ImageNet to train on. We train ResNet-50 based networks on Stylized-ImageNet from scratch 3 following the same training policy as the ImageNet experiment, and report the validation accuracy on Stylized-ImageNet and the original ImageNet in <ref type="table" target="#tab_2">Table 3</ref>. SRM not only brings impressive improvements over the baseline and SE on Stylized-ImageNet, but also generalizes better to the original Ima-geNet. This supports our claim that SRM learns to suppress the contribution of nuisance styles, which helps the network to concentrate more on meaningful features.</p><p>Multi-Domain Classification. We also verify the effectiveness of SRM in tackling natural style variations inherent in different input domains. We adopt the Office-Home dataset <ref type="bibr" target="#b31">[32]</ref> which consists of 15,588 images from 65 categories across 4 heterogeneous domains: Art (Ar), Clip-art (Cl), Product (Pr) and Real-world (Rw). We combine all training sets of the 4 domains and train domain-agnostic networks based on ResNet-18, following the same setting as the ImageNet experiment except that the networks are trained with a batch size of 64 on 1 GPU. <ref type="table" target="#tab_3">Table 4</ref> shows the top-1 accuracy averaged over 5-fold cross validation. SRM consistently improves the accuracy with significant margins  across all domains, which indicates the capability of SRM for alleviating the style discrepancy over different domains. It also implies the potential of SRM to be utilized in domain adaptation problems <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> which entail style disparity between the source and target domains.</p><p>Texture Classification. We further evaluate SRM on texture classification using Describable Texture Dataset (DTD) <ref type="bibr" target="#b2">[3]</ref> which comprises 5,640 images across 47 texture categories such as cracked, bubbly, marbled, etc. This task offers to assess a different perspective of the network: the ability to extract most textural patterns that elicit visual impressions prior to recognizing objects in images <ref type="bibr" target="#b3">[4]</ref>. We follow the data processing setting of <ref type="bibr" target="#b25">[26]</ref>, and the same training policy as our CIFAR experiment. The results from 5-fold cross validation with ResNet-32 and ResNet-56 baselines are reported in table 5, in which SRM achieves outstanding performance improvements. It demonstrates that SRM successfully models the importance of individual styles and emphasizes the target textures, enhancing the representational power regarding style attributes.  <ref type="figure" target="#fig_4">Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Style Transfer</head><p>We finally examine the benefit of SRM in a generative problem of style transfer. We utilize a single style feedforward algorithm <ref type="bibr" target="#b16">[17]</ref> implemented in the official PyTorch repository <ref type="bibr" target="#b3">4</ref> . The networks are trained with content images from the MS-COCO dataset <ref type="bibr" target="#b21">[22]</ref>, following the default configurations in the original code. <ref type="figure" target="#fig_5">Figure 5</ref> depicts the training curves of style and content loss with different recalibration methods. As reported in the literature <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>, removing the style from the content image with instance normalization (IN) <ref type="bibr" target="#b29">[30]</ref> brings a huge improvement over using the standard batch normalization (BN) <ref type="bibr" target="#b13">[14]</ref>. Surprisingly, the BN-based network equipped with SRM (BN+SRM) reaches almost the same level of style/content loss with IN, while the network with SE (BN+SE) exhibits much inferior style/content loss. This demonstrates the distinct effect of SRM, which mimics the behavior of IN by dynamically suppressing unnecessary styles from input images. We also show qualitative examples in <ref type="figure" target="#fig_4">Figure 4</ref>. Although BN+SE somewhat improves the stylization quality compared to BN, it is still far behind the performance of IN. In contrast, BN+SRM not only successfully transfers to target style but also better represents the important styles of the content images (e.g. green glass and blue sky), generating competitive results to IN. Overall, the advantage of SRM is not restricted to discriminative tasks but can be extended to generative frameworks, which remains as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study and Analysis</head><p>In this section, we perform ablation experiments to verify the effectiveness of each component in SRM and indepth analysis on the behavior of SRM. As pointed out by Hu et al. <ref type="bibr" target="#b11">[12]</ref>, it remains challenging to perform precise theoretical analysis on the feature representation of CNNs. Instead, we perform an empirical study to gain an insight into the distinguishing role of SRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>Style Pooling. We verify the benefit of the proposed style pooling compared to different pooling options. Throughout the ablation study, we utilize ResNet-50 as a base architecture and address ImageNet classification, following the same procedure as in Section 4.1. <ref type="table" target="#tab_5">Table 6</ref> lists the results of various pooling method fused with style integration operator in our algorithm (except for the baseline). While each pooling component of SRM (i.e. AvgPool and StdPool) brings meaningful performance improvement, the combination of them further boosts the performance. We additionally compare our method with MaxPool and the combination of AvgPool and MaxPool proposed in CBAM <ref type="bibr" target="#b34">[35]</ref>, which are also outperformed by our style pooling approach.  Style Integration. We next examine the style integration module which consists of a channel-wise fully connected layer (CFC) followed by a batch normalization layer (BN). On top of our style pooling operator, we compare CFC with a multi-layer perceptron (MLP) of two fully connected layers (employed in SE) and verify the effect of BN in style integration. To build MLP on style pooling, we concatenate the style features along the channel axis then apply MLP following the default configuration of SE. As shown in Table 7, CFC shows better performance than MLP in spite of its simplicity, which highlights the advantage of utilizing channel-wise styles over modeling channel interdependencies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Channel Pruning</head><p>SRM learns to adaptively predict the channel-wise importance of feature maps. In this regard, we evaluate the validity of the feature importance learned by SRM through channel pruning of ResNet-50 on ImageNet classification. Given an input image in the validation set, we sort the channel weights of each residual block at certain stage in ascending order. Then, we select the channels to be pruned in order according to a prune ratio. Since each pruned channel is filled with zero, the amount of information to be passed decreases as the prune ratio increases. In an extreme case where the prune ratio is equal to one, the input feature maps directly pass through an identity mapping ignoring the residual block.</p><p>We compare the validation accuracy when channel pruning is applied to SE, GE, and SRM at different stages and report the results in <ref type="figure" target="#fig_6">Figure 6</ref>. The accuracy is mostly preserved during the early phase of the pruning process but it quickly drops after a certain prune ratio. Throughout all stages, the accuracy drops noticeably slower in SRM compared to SE and GE, which implies that SRM learns better relative importance of channels than other methods. Note that SRM predicts channel importance solely based on style context, which may provide an insight into how the network utilizes the style of an image in its decision making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Difference between SRM and SE Block</head><p>Although the proposed SRM shares similar aspects of feature recalibration with the SE block, we observe the characteristics of SRM is far distinct from SE throughout the experiments. To further understand their representational difference, we visualize the features learned by each method through seeking the images that leads to the highest channel weights. We record the channel weights for each validation image obtained by SE-ResNet-56 and SRM-ResNet-56 trained on DTD. <ref type="figure">Figure 7</ref> shows the top-activated images for individual channels in conv2-6 among the entire validation set. While SE results in highly overlapped images across channels, SRM yields a greater diversity of topactivated images. This implies SRM allows lower correlation between channel weights compared to the SE block, which leads us to the following exploration. <ref type="figure">Figure 8</ref> depicts the correlation matrix between channel weights produced by SE and SRM. As expected, there exists high correlation between the channel weights in the SE block, but SRM exhibits lower correlation between channels (in terms of the total sum of squared correlation coefficients throughout the whole network, SRM shows almost three times smaller numerical value of 143,909 than SE's 420,509). In addition, the conspicuous grid pattern in SE's correlation matrix implies that groups of channels are turned on or off synchronously, whereas SRM tends to encourage decorrelation between channels. Our comparison between SE and SRM suggests that they target quite different perspectives of feature representations to enhance performance, which is worth future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present Style-based Recalibration Module (SRM), a lightweight architectural unit that dynamically recalibrates feature responses based on style importance. By incorporating the styles into feature maps, it effectively enhances the representational power of a CNN. Our experiments on general object classification demonstrate that simply inserting SRM into standard CNN architectures such as ResNet boosts the performance of network. Furthermore, we verify the significance of SRM in controlling the contribution of styles through various style-related tasks. While most previous works utilized styles in image generation frameworks, SRM is designed to harness the latent ability of style information in more general vision tasks. We hope our work sheds light on better exploiting styles into designing a CNN architecture in a wide range of applications. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A Style-based Recalibration Module (SRM). SRM adaptively recalibrates input feature maps based on the style of an image via channel-independent style pooling and integration operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The schema of (a) SRM and (b) SRM integrated with a residual block. AvgPool : global average pooling, StdPool : global standard deviation pooling, CFC : channelwise fully connected layer, BN : batch normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Training (left) and validation (right) curves on ImageNet-1K with ResNet-50 (baseline) and varying recalibration methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Example style transfer results. While both BN+SRM and BN+SE improve the stylization quality compared to BN, BN+SRM yields much higher quality which is comparable to IN. More examples are provided in Figure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Quantitative comparison of style loss (left) and content loss (right) with a style image of Rain Princess (the first row in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Top-1 validation accuracy of ResNet-50 on ImageNet after pruning channels of each stage according to estimated channel weights. Stage 1 is omitted because it consists of a single convolutional layer where a recalibration module is not applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>The top-activated images for individual channels in conv2-6 (64 channels) of ResNet-56 on DTD. More examples are provided inFigure 10. Visualization of the correlation matrix between the channel weights in conv2-6 (64?64) of ResNet-56 on DTD. More examples are provided inFigure 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Additional examples of style transfer. While BN results in vague boundaries between areas along with severe artifacts and BN+SE alleviates them to some degree, BN+SRM yields considerably higher stylization quality which is comparable to IN. The top-activated images of the first 64 channels in channel weights and the correlation matrix between channel weights of ResNet-56 on Describable Texture Dataset. Each row (from top to bottom) corresponds to conv2 5, conv3 6, conv4 4, conv4 5, and conv4 6, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Top-1 and top-5 accuracy (%) on the ImageNet-1K validation set and complexity comparison.</figDesc><table><row><cell>Model</cell><cell cols="3">Params GFLOPs top-1 top-5</cell></row><row><cell>ResNet-50</cell><cell>25.56M</cell><cell>3.86</cell><cell>75.89 92.85</cell></row><row><cell>SE-ResNet-50</cell><cell>28.09M</cell><cell>3.87</cell><cell>76.80 93.39</cell></row><row><cell>GE-ResNet-50</cell><cell>31.12M</cell><cell>3.87</cell><cell>76.75 93.41</cell></row><row><cell cols="2">SRM-ResNet-50 25.62M</cell><cell>3.88</cell><cell>77.13 93.51</cell></row><row><cell>ResNet-101</cell><cell>44.55M</cell><cell>7.58</cell><cell>77.40 93.59</cell></row><row><cell>SE-ResNet-101</cell><cell>49.33M</cell><cell>7.60</cell><cell>78.08 93.95</cell></row><row><cell>GE-ResNet-101</cell><cell>53.58M</cell><cell>7.60</cell><cell>77.36 93.64</cell></row><row><cell cols="2">SRM-ResNet-101 44.68M</cell><cell>7.62</cell><cell>78.47 94.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Accuracy (%) on the CIFAR-10/100 test sets with</cell></row><row><cell cols="3">a ResNet-56 baseline and complexity comparison.</cell></row><row><cell></cell><cell>CIFAR-10</cell><cell>CIFAR-100</cell></row><row><cell>Model</cell><cell cols="2">Params top-1 Params top-1</cell></row><row><cell cols="3">Baseline 0.87M 93.77 0.89M 74.76</cell></row><row><cell>SE</cell><cell cols="2">0.97M 94.60 0.99M 76.10</cell></row><row><cell>GE</cell><cell cols="2">1.91M 94.32 1.94M 76.02</cell></row><row><cell>SRM</cell><cell cols="2">0.89M 95.05 0.91M 76.93</cell></row><row><cell cols="3">tude less additional parameters. For example, SE-ResNet-</cell></row><row><cell cols="3">50 and GE-ResNet-50 require 2.53M and 5.56M additional</cell></row><row><cell cols="3">parameters to ResNet-50, respectively, but SRM-ResNet-50</cell></row><row><cell cols="3">only requires 0.06M (2.37% of SE and 1.08% of GE) which</cell></row><row><cell cols="3">shows the exceptional parameter efficiency of SRM.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-1 and top-5 accuracy (%) on the validation sets of Stylized-ImageNet and ImageNet with a ResNet-50 baseline, when trained on Stylized-ImageNet.</figDesc><table><row><cell></cell><cell cols="2">Stylized-ImageNet</cell><cell>ImageNet</cell></row><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1 top-5</cell></row><row><cell>Bseline</cell><cell>53.93</cell><cell>76.75</cell><cell>56.11 79.17</cell></row><row><cell>SE</cell><cell>58.31</cell><cell>80.80</cell><cell>60.15 82.54</cell></row><row><cell>SRM</cell><cell>60.69</cell><cell>82.56</cell><cell>62.12 84.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="6">Accuracy (%) on the Office-Home dataset with a</cell></row><row><cell cols="6">ResNet-18 baseline, averaged over 5-fold cross validation.</cell></row><row><cell></cell><cell>Ar</cell><cell>Cl</cell><cell>Pr</cell><cell>Rw</cell><cell>Avg.</cell></row><row><cell cols="6">Baseline 37.49 60.73 72.81 52.12 55.47</cell></row><row><cell>SE</cell><cell cols="5">39.55 62.75 75.60 55.52 58.36</cell></row><row><cell>SRM</cell><cell cols="5">40.50 64.97 76.12 56.30 59.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="3">Top-1 and top-5 accuracy (%) on the Describable</cell></row><row><cell cols="3">Texture Dataset averaged over 5-fold cross validation.</cell></row><row><cell></cell><cell>ResNet-32</cell><cell>ResNet-56</cell></row><row><cell></cell><cell cols="2">top-1 top-5 top-1 top-5</cell></row><row><cell cols="3">Baseline 44.96 73.85 45.46 75.54</cell></row><row><cell>SE</cell><cell cols="2">45.20 75.60 48.63 77.40</cell></row><row><cell>SRM</cell><cell cols="2">46.50 76.63 50.44 79.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of different pooling methods on Ima-geNet validation.</figDesc><table><row><cell>Pooling</cell><cell>top-1 acc.</cell></row><row><cell>ResNet-50 (baseline)</cell><cell>75.89</cell></row><row><cell>ResNet-50 + AvgPool</cell><cell>76.58</cell></row><row><cell>ResNet-50 + StdPool</cell><cell>76.61</cell></row><row><cell>ResNet-50 + MaxPool</cell><cell>75.87</cell></row><row><cell>ResNet-50 + AvgPool + MaxPool</cell><cell>76.35</cell></row><row><cell>ResNet-50 + AvgPool + StdPool (SRM)</cell><cell>77.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of different integration methods on ImageNet validation. SP: style pooling, MLP: multi-layer perceptron, CFC: channel-wise fully connected layer, BN: batch normalization.</figDesc><table><row><cell>Design</cell><cell>top-1 acc.</cell></row><row><cell>ResNet-50 + SP + MLP</cell><cell>76.75</cell></row><row><cell>ResNet-50 + SP + MLP + BN</cell><cell>76.68</cell></row><row><cell>ResNet-50 + SP + CFC</cell><cell>76.91</cell></row><row><cell>ResNet-50 + SP + CFC + BN (SRM)</cell><cell>77.13</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Among the several variants of GE, we compared with GE-? which is mainly explored in their paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/painter-by-numbers/ 3 Although<ref type="bibr" target="#b7">[8]</ref> uses ImageNet pretrained networks, we train networks from scratch to focus on the characteristics on Stylized-ImageNet.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/pytorch/examples/tree/ master/fast_neural_style</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Approximating cnns with bagof-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep filter banks for texture recognition, description, and segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Control of goal-directed and stimulus-driven attention in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gatherexcite: Exploiting feature context in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04948</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch-instance normalization for adaptively style-invariant neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV, 2015. 1, 4</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
