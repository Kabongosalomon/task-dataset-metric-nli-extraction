<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Visual Classification using Self Assessment Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Do</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AIOZ</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AIOZ</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Tjiputra</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AIOZ</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
							<email>quang.tran@aioz.ioanh.nguyen@liverpool.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AIOZ</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Liverpool</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Visual Classification using Self Assessment Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting discriminative features plays a crucial role in the fine-grained visual classification task. Most of the existing methods focus on developing attention or augmentation mechanisms to achieve this goal. However, addressing the ambiguity in the top-k prediction classes is not fully investigated. In this paper, we introduce a Self Assessment Classifier, which simultaneously leverages the representation of the image and top-k prediction classes to reassess the classification results. Our method is inspired by continual learning with coarse-grained and fine-grained classifiers to increase the discrimination of features in the backbone and produce attention maps of informative areas on the image. In practice, our method works as an auxiliary branch and can be easily integrated into different architectures. We show that by effectively addressing the ambiguity in the topk prediction classes, our method achieves new state-of-theart results on CUB200-2011, Stanford Dog, and FGVC Aircraft datasets. Furthermore, our method also consistently improves the accuracy of different existing fine-grained classifiers with a unified setup. Our source code can be found at: https://github.com/aioz-ai/SAC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The fine-grained visual classification task aims to classify images belonging to the same category (e.g., different kinds of birds, aircraft, or flowers). Compared to the ordinary image classification task, classifying fine-grained images is more challenging due to three main reasons: (i) large intra-class difference: objects that belong to the same category present significantly different poses and viewpoints; (ii) subtle inter-class difference: objects that belong to different categories might be very similar apart from some minor differences, e.g., the color styles of a bird's head can usually determine its fine-grained category; (iii) limitation of training data: labeling fine-grained categories usually requires specialized knowledge and a large amount of anno-tation time. Because of these reasons, it is not a trivial task to obtain accurate classification results by using only the state-of-the-art CNN, such as VGG <ref type="bibr" target="#b45">[46]</ref> and ResNet <ref type="bibr" target="#b22">[23]</ref>.</p><p>Recent works show that the key solution for fine-grained classification is to find informative regions in multiple object's parts and extract discriminative features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74]</ref>. A popular approach to learn object's parts is based on human annotations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b68">69]</ref>. However, it is time-consuming to annotate fine-grained regions, hence making this approach impractical. Some improvements utilize unsupervised or weakly-supervised learning to locate the informative object's parts <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b72">73]</ref> or region of interest bounding boxes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>. Although this is a promising approach to overcome the problem of manually labeling finegrained regions, these methods have drawbacks such as low accuracy, costly in training phase/inference phase, or hard to accurately detect separated bounding boxes.</p><p>In this paper, we introduce a Self Assessment Classifier (SAC) method to address the ambiguity in the fine-grained classification task. Intuitively, our method is designed to reassess the top-k prediction results and eliminate the uninformative regions in the input image. This helps to decrease the inter-class ambiguity and allows the backbone to learn more discriminative features. During training, our method also produces the attention maps that focus on informative areas of the input image. By integrating into a backbone network, our method can reduce the wrong classification over top-k ambiguity classes. Note that ambiguity classes are the results of uncertainty in the prediction that can lead to the wrong classification (See <ref type="figure" target="#fig_1">Figure 1</ref> for more details). Our contributions can be summarized as follows.</p><p>? We propose a new self class assessment method that effectively jointly learns the discriminative features and addresses the ambiguity problem in the fine-grained visual classification task.</p><p>? We design a new module that produces attention maps in an unsupervised manner. Based on this attention map, our method can erase the inter-class similar re- <ref type="figure" target="#fig_1">Figure 1</ref>. The visualization of the output confident scores over top-10 classes extracted from the softmax layer of the two models: ResNet-50 without SAC and ResNet-50 with SAC. The red color class label denotes the ground truth. The visualization indicates that the output distributions extracted from ResNet-50 backbone with our Self Assessment Classifier (SAC) significantly reduce the probability and confident score of wrong classes in the top-10 prediction results. Best viewed in color.</p><p>gions that are not useful for classification.</p><p>? We show that our method can be easily integrated into different fine-grained classifiers to achieve new stateof-the-art results. Our source code and trained models will available for further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Fine-grained visual classification involves small diversity within the different classes. Typical fine-grained problems, such as differentiating between animal and plant species, drew much attention from researchers. Since background context acted as a distraction in most cases, many pieces of research focus on improving the attentional and localization capabilities of CNN-based algorithms <ref type="bibr">[6, 17, 28, 30, 45, 53, 55-57, 59, 66, 68, 72, 73, 75, 76]</ref>. Besides, to focus on the informative regions that could distinguish the species between any two images, many methods relied on annotations of parts location or attributes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b59">60]</ref>. Specifically, Part R-CNN <ref type="bibr" target="#b68">[69]</ref> and extended R-CNN <ref type="bibr" target="#b20">[21]</ref> detected objects and localized their parts under a geometric prior. Then, these works predicted a fine-grained category from a pose-normalized representation.</p><p>In practice, it is expensive to acquire pixel-level annotations of the object's parts as ground-truth. Thus, methods that require only image-level annotations draw more attention <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b64">65]</ref>. Lin et al. proposed the bilinear pooling <ref type="bibr" target="#b38">[39]</ref> and its improved version <ref type="bibr" target="#b37">[38]</ref>, where two features were combined at each location using the outer product. In <ref type="bibr" target="#b28">[29]</ref>, the authors introduce the Spatial Transformer Network to achieve accurate classification performance by learning geometric transformations. In <ref type="bibr" target="#b21">[22]</ref>, the authors proposed a lightweight localization module by leveraging global k-max pooling. Recently, Joung et al. <ref type="bibr" target="#b30">[31]</ref> leveraged Canonical 3D Object Representation to cope up with multiple camera viewpoint problem in the fine-grained classification task.</p><p>Many works provided a training routine that maximized the entropy of the output probability distribution for training CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b66">67]</ref>. In <ref type="bibr" target="#b60">[61]</ref>, the authors exploited a regularization between the fine-grained recognition model and the hyper-class recognition model. Sun et al. proposed Multiple Attention Multiple Class loss <ref type="bibr" target="#b47">[48]</ref> that pulled positive features closer to the anchor and pushed negative features away. Dubey et al. proposed PC <ref type="bibr" target="#b13">[14]</ref>, which reduced overfitting by combining the cross-entropy loss with the pairwise confusion loss to learn more discriminative features. In <ref type="bibr" target="#b14">[15]</ref>, by using Maximum-Entropy learning in the context of fine-grained classification, the authors introduced a training routine that maximizes the entropy of the output probability distribution for fine-grained classification task. A triplet loss was used in <ref type="bibr" target="#b53">[54]</ref> to achieve better inter-class separation. Hu et al. <ref type="bibr" target="#b23">[24]</ref> proposed to use attention regularization loss to focus on attention regions between corresponding local features. More recently, in <ref type="bibr" target="#b46">[47]</ref>, diversification block cooperated with gradient-boosting loss had been introduced to maximally separate the highly similar fine-grained classes.</p><p>While it is expensive to acquire annotations of object's parts, unsupervised and weakly supervised methods for identifying informative regions are investigated recently. In SCDA <ref type="bibr" target="#b57">[58]</ref>, an unsupervised method was introduced to lo- cate the informative regions without using any image label or extra annotation. However, it is less accurate when compared with weakly supervised localization methods, which leveraged image-level super-vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b70">71]</ref>. To locate the whole object, Zhang et al. <ref type="bibr" target="#b69">[70]</ref> used Adversarial Complementary Learning which could recognize different object's parts and discover complementary regions that belong to the same object. Recently, authors in <ref type="bibr" target="#b56">[57]</ref> used Gaussian Mixture Model to learn discriminative regions from the image feature maps for fine-grained classification.</p><p>All of the above methods do not focus on the ambiguity prediction classes, which is one of the main reasons that causes wrong classifications. To address this problem, our method is designed to explicitly reduce the effect of the topk ambiguity prediction classes. Furthermore, our method can effectively learn and produce the attention map in an unsupervised manner. In practice, our method can be easily integrated into different fine-grained classifiers to further improve the classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>We propose two main steps in our method: Top-k Coarse-grained Class Search (TCCS) and Self Assessment Classifier (SAC). TCCS works as a coarse-grained classifier to extract visual features from the backbone. The Self Assessment Classifier works as a fine-grained classifier to to reassess the ambiguity classes and eliminate the noninformative regions. Our SAC has four modules: the Top-k Class Embedding module aims to encode the information of the ambiguity class; the Joint Embedding module aims to jointly learn the coarse-grained features and top-k am-biguity classes; the Self Assessment module is designed to differentiate between ambiguity classes; and finally, the Dropping module is a data augmentation method, designed to erase unnecessary inter-class similar regions out of the input image. <ref type="figure" target="#fig_0">Figure 2</ref> shows an overview of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Top-k Coarse-grained Class Search</head><p>The TCCS takes an image as input. Each input image is passed through a Deep CNN to extract feature map F ? R d f ?m?n and the visual feature V ? R dv . m, n, and d f represent the feature map height, width, and the number of channels, respectively; d v denotes the dimension of the visual feature V. In practice, the visual feature V is usually obtained by applying some fully connected layers after the convolutional feature map F.</p><p>The visual features V is used by the 1 st classifier, i.e., the original classifier of the backbone, to obtain the top-k prediction results. Assuming that the fine-grained dataset has N classes. The top-k prediction results C k = {C 1 , ..., C k } is a subset of all prediction classes C N , with k is the number of candidates that have the k-highest confident scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Self Assessment Classifier</head><p>Our Self Assessment Classifier takes the image feature F and top-k prediction C k from TCCS as the input to reassess the fine-grained classification results.</p><p>Top-k Class Embedding. The output of TCCS module C k is passed through top-k class embedding module to output label embedding set</p><formula xml:id="formula_0">E k = {E 1 , ...E i , ..., E k }, i ? {1, 2, ..., k}, E i ? R de .</formula><p>This module contains a word embedding layer <ref type="bibr" target="#b43">[44]</ref> for encoding each word in class labels and a GRU <ref type="bibr" target="#b6">[7]</ref> layer for learning the temporal information in class label names. d e represents the dimension of each class label. It is worth noting that the embedding module is trained end-to-end with the whole model. Hence, the class label representations are learned from scratch without the need of any pre-extracted/pre-trained or transfer learning.</p><p>Given an input class label, we trim the input to a maximum of 4 words. The class label that is shorter than 4 words is zero-padded. Each word is then represented by a 300-D word embedding. This step results in a sequence of word embeddings with a size of 4 ? 300 and denotes as? i of i-th class label in C k class label set. In order to obtain the dependency within the class label name, the? i is passed through a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b6">[7]</ref>, which results in a 1024-D vector representation E i for each input class. Note that, although we use the language modality (i.e., class label name), it is not extra information as the class label name and the class label identity (for calculating the loss) represent the same object category.</p><p>Joint Embedding. This module takes the feature map F and the top-k class embedding E k as the input to produce the joint representation J ? R dj and the attention map. We first flatten F into (d f ? f ), and E k is into (d e ? k). The joint representation J is calculated using two modalities F and E k as follows</p><formula xml:id="formula_1">J T = (T ? 1 vec(F)) ? 2 vec(E k ) (1) where T ? R dF?d E k ?dj is a learnable tensor; d F = (d f ? f ); d E k = (d e ? k); vec() is a vectorization operator; ? i denotes the i-mode tensor product.</formula><p>In practice, the preceding T is too large and infeasible to learn. Thus, we apply decomposition solutions that reduce the size of T but still retain the learning effectiveness. Inspired by <ref type="bibr" target="#b62">[63]</ref> and <ref type="bibr" target="#b33">[34]</ref>, we rely on the idea of the unitary attention mechanism. Specifically, let J p ? R dj be the joint representation of p th couple of channels where each channel in the couple is from a different input. The joint representation J is approximated by using the joint representations of all couples instead of using fully parameterized interaction as in Eq. 1. Hence, we compute J as</p><formula xml:id="formula_2">J = p M p J p<label>(2)</label></formula><p>Note that in Eq. 2, we compute a weighted sum over all possible couples. The p th couple is associated with a scalar weight M p . The set of M p is called as the attention map</p><formula xml:id="formula_3">M, where M ? R f ?k .</formula><p>There are f ?k possible couples over the two modalities. The representation of each channel in a couple is</p><formula xml:id="formula_4">F i , (E k ) j , where i ? [1, f ], j ? [1, k]</formula><p>, respectively. The joint representation J p is then computed as follow</p><formula xml:id="formula_5">J T p = (T u ? 1 F i ) ? 2 (E k ) j<label>(3)</label></formula><p>where T u ? R d f ?de?dj is the learning tensor between channels in the couple.</p><p>From Eq. 2, we can compute the attention map M using the reduced parameterized bilinear interaction over the inputs F and E k . The attention map is computed as</p><formula xml:id="formula_6">M = softmax ((T M ? 1 F) ? 2 E k )<label>(4)</label></formula><p>where T M ? R d f ?de is the learnable tensor. By integrating Eq. 3 and Eq. 4 into Eq. 2, the joint representation J can be rewritten as</p><formula xml:id="formula_7">J T = f i=1 k j=1 M ij (T u ? 1 F i ) ? 2 (E k ) j<label>(5)</label></formula><p>It is also worth noting from Eq. 5 that to compute J, instead of learning the large tensor T ? R d F ?d E k ?dj in Eq. 1, we now only need to learn two smaller tensors</p><formula xml:id="formula_8">T u ? R d f ?de?dj in Eq. 3 and T M ? R d f ?de in Eq. 4.</formula><p>Self Assessment. The joint representation J from the Joint Embedding module is used as the input in the Self Assessment step to obtain the 2 nd top-k predictions C k . Note that C k = {C 1 , ..., C k }. Intuitively, C k is the top-k classification results after self-assessment. This module is a fine-grained classifier that produces the 2 nd predictions to reassess the ambiguity classification results.</p><p>In practice, we train a classifier using a Cross-Entropy loss function to obtain the top-k prediction results. Through back-propagation, the feature map F from the Deep CNN backbone and the joint features J are expected to be more useful for fine-grained classification. By jointly train both the coarse-grained and fine-grained classifiers, our method can increase the discriminative of feature maps. Therefore, we can reduce the the ambiguity classes and improve the overall fine-grained classification accuracy.</p><p>Inspired by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, the contribution of the coarsegrained and fine-grained classifier is calculated by</p><formula xml:id="formula_9">Pr(? = ?i) = ?Pr1(? = ?i) + (1 ? ?)Pr2(? = ?i) (6)</formula><p>where ? is the trade-off hyper-parameter (0 ? ? ? 1). Pr 1 (? = ? i ), Pr 2 (? = ? i ) denote the prediction probabilities for class ? i , from the coarse-grained and fine-grained classifiers, respectively.</p><p>It should be emphasized that in our design, three modules Top-k Class Embedding, Joint Embedding, and Self Assessment have to be utilized simultaneously to produce the final classification predictions.</p><p>Dropping. Although the attention map M can identify the correlation between the image regions and each ambiguity class, unnecessary regions may still exist and affect the fine-grained classification results. To encourage the attention maps to pay more attention to the informative regions which are discriminate between ambiguity classes, we introduce a Dropping mechanism. Our dropping method leverages the current attention map M to eliminate regions which have high correlation scores, i.e., inter-class similar regions. It then recomputes the attention distribution between the remaining regions and the top-k classes. It should be emphasized that the eliminated regions are not backgrounds. They are foregrounds and may also be essential for classification. However, these regions are not discriminative enough for the fine-grained classification task. Correlation scores, which are smaller than ? min , are set to zero.</p><formula xml:id="formula_10">M = (M ? ? min ) * M<label>(7)</label></formula><p>6 To identify the cropped area, we compute top left point P 1 (x 1 , y 1 ) and bottom right point P 2 (x 2 , y 2 ) by using Eq. 8.</p><formula xml:id="formula_11">x 1 = min (IdxRowSelected (M)) y 1 = min (IdxColSelected (M)) x 2 = max (IdxRowSelected (M)) y 2 = max (IdxColSelected (M))<label>(8)</label></formula><p>where IdxRowSelected(M) and IdxColSelected(M) extract the row index list and the column index list of correlation values which are higher than zero in M, respectively. 7 return I c identified by P 1 and P 2 Specifically, for each c-th ambiguity class, we first obtain the dropping mask M Dc ? R f from the attention map M c by setting i-th element M ci which is larger than threshold (d ? * max(M)) to 0, and others to 1. Eq. 9 shows this condition. Note that M c ? R f is the c-th attention map extracted from the attention map M; c ? <ref type="bibr">[1, k]</ref> where k is the number of ambiguity classes.</p><formula xml:id="formula_12">M Dc i = 0, if M ci &gt; d ? * max(M) 1, otherwise<label>(9)</label></formula><p>The dropping masks of ambiguity classes are then integrated into the feature map to remove unnecessary regions. The final feature map F ? R f is calculated as follow</p><formula xml:id="formula_13">F = (M D1 ? M D2 ? ... ? M D k ) F<label>(10)</label></formula><p>where denotes Hadamard product.</p><p>In practice, the Self Assessment Classifier plays an important role during training to learn discriminate features and resolve the ambiguity in the top-k prediction classes. During testing, inspired by <ref type="bibr" target="#b23">[24]</ref>, we can also reuse the attention map M of our Self Assessment Classifier to remove the unnecessary background from the image. We describe this cropping localization step in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset. We evaluate our method on three popular finegrained datasets: CUB-200-2011 <ref type="bibr" target="#b51">[52]</ref>, Stanford Dogs <ref type="bibr" target="#b31">[32]</ref> and FGVC Aircraft <ref type="bibr" target="#b39">[40]</ref>. <ref type="table">Table 1</ref> shows the statistic of these datasets. Note that we do not use any extra bounding box/part annotations in all of our experiments. Implementation. All experiments are conducted on an NVIDIA Titan V GPU with 12GB RAM. The model is trained using Stochastic Gradient Descent with a momentum of 0.9. The maximum number of epochs is set at 80; the weight decay equals 0.00001, and the mini-batch size is 12. Besides, the initial learning rate is set to 0.001, with exponential decay of 0.9 after every two epochs. Based on validation results, the number of top-k ambiguity classes is set to 10, while the parameters d ? , ? are set to 0.1 and 0.5, respectively. The dimension of the embedding of topk classes and the joint representation between top-k class embedding and feature map is set to 1024. The training and testing time of our method depends on the backbone it is being integrated to. However, it is not 1.1 times slower compared to the original baseline.</p><p>Baseline. To validate the effectiveness and generalization of our method, we integrate it into 7 different deep networks, including two popular Deep CNN backbones, Inception-V3 <ref type="bibr" target="#b48">[49]</ref> and ResNet-50 <ref type="bibr" target="#b22">[23]</ref>; and five fine-grained classification methods: WS <ref type="bibr" target="#b24">[25]</ref>, DT <ref type="bibr" target="#b9">[10]</ref>, WS DAN <ref type="bibr" target="#b23">[24]</ref>, MMAL <ref type="bibr" target="#b65">[66]</ref>, and the recent transformer work ViT <ref type="bibr" target="#b8">[9]</ref>. It is worth noting that we only add our Self Assessment Classifier into these works, other setups and hyper-parameters for training are kept unchanged when we compare with original codes. Although our proposed method is integrated into different approaches and tested on different datasets, we use the same parameter setup as described above in all experiments. <ref type="table">Table 2</ref> summarises the contribution of our Self Assessment Classifier (SAC) to the fine-grained classification results of different methods on three datasets CUB-200-2011, Stanford Dogs, and FGVC Aircraft. This table clearly shows that by integrating SAC into different classifiers, the fine-grained classification results are consistently improved. In particular, we observe an average improvement of +1.3, +1.2, and +1.2 in the CUB-200-2011, Stanford Dogs, and FGVC Aircraft datasets, respectively. Our SAC shows a clear improvement when being integrated into Inception-V3 and ResNet-50 on three datasets. It is more challenging to improve the results of existing fine-grained classification methods, however, we still achieve consistent improvement when integrating SAC into WS <ref type="bibr" target="#b24">[25]</ref>, DT <ref type="bibr" target="#b9">[10]</ref>, WS DAN <ref type="bibr" target="#b23">[24]</ref>, MMAL <ref type="bibr" target="#b65">[66]</ref>, and ViT <ref type="bibr" target="#b8">[9]</ref> fine-grained classifiers. <ref type="table">Table 2</ref> illustrates that by integrating with recent fine-grained classifiers, our proposed method achieves new state-of-the-art results in both CUB-200-2011, Stanford Dogs, and FGVC Aircraft datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-grained Classification Results</head><p>To conclude, our proposed method shows a clear and consistent improvement on different classifiers and different fine-grained datasets. In practice, our method can be easily integrated into new state-of-the-art classifiers to further improve the results. The parameter setting for our method is also simple as we obtain good results with one setup across different classifiers on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Module Study Analysis</head><p>Module Contribution. <ref type="table">Table 3</ref> shows the contribution of each module when we integrate our method into ResNet-50 and WS DAN on the CUB-200-2011 dataset. We note that, based on the design of our SAC, three modules Top-k Class Embedding, Joint Embbeding, and Self Assessment must be utilized simultaneously. We denote them as the Auxiliary Classifier in <ref type="table">Table 3</ref>. By adopting the Auxiliary Classifier, both accuracies of ResNet-50 and WS DAN are improved by +0.7% and +0.6%, respectively. This result indicates that the backbone under SAC has learned informative representations for dealing with fine-grained ambiguity classes. We also observe that applying the Dropping mod- ule further improves the performance on both ResNet-50 and WS-DAN. This confirms that by removing inter-class similar regions, the backbone can learn more discriminative information, hence improving the classification results. Table 3 also shows that all components in our Self Assessment Classifier contribute significantly to the final results. Coarse vs. Fine-grained Classifier Analysis. In this work, we consider that both the coarse-grained classifier and the fine-grained classifier are equally important. In practice, we can control the contribution of each classifier by changing the parameter ? in Eq. 6. In all our experiments, the ? parameter is set to 0.5, which means both coarse and fine-grained classifier contributes equally to the classification results. For completeness, <ref type="table" target="#tab_2">Table 4</ref> is provided to validate the effect of this parameter using ResNet-50 and WS DAN on the CUB-200-2011 dataset. This table demonstrates that by fine-tuning the ? parameter, the fine-grained classification results can be slightly improved. However, we can see that the final classification results do not depend too much on this ? parameter.  Complexity Analysis. <ref type="table">Table 6</ref> shows the efficiency of each module of SAC and its complexity indicated by the GPU speed and the number of parameters during inference process, when we integrate SAC into ResNet-50 <ref type="bibr" target="#b22">[23]</ref> and WS DAN <ref type="bibr" target="#b23">[24]</ref> on CUB-200-2011 dataset. By leveraging Auxiliary Classifier and Dropping module, both inference time and the number of parameters of ResNet-50 and WS DAN are unchanged. These results imply that SAC increases the performance without affecting the computational cost of backbones.</p><p>From <ref type="table">Table 6</ref>, we can see that both the network parameters and inference time are increased when we use Localization. However, the increased complexity is an acceptable trade-off to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Number of Top-k Classes Analysis.</head><p>The accuracy of our proposed method depends on the top-k prediction classes extracted dynamically by the coarse-grained classifier. If the coarse-grained classifier has poor performance and the top-k classes value is set at a small number, there may be no ground truth class in any in top-k predictions. In this case, our fine-grained classifier only penalizes the wrong cases. Therefore, the fine-grained classifier can not improve the accuracy of the network. <ref type="table" target="#tab_3">Table 5</ref> shows the effect of the number of top-k ambiguity classes on the classification results in our method. From this table, we can see that if the number of top-k classes is set to a small number, our improvement is minimal. In practice, we recommend setting this parameter to a relatively big number to avoid this problem. We choose k = 10 in all of our experiments with different methods and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Dropping Analysis</head><p>Dropping Comparison. <ref type="table">Table 7</ref> illustrates the performance of our ambiguity class based dropping, in comparison with other augmentation methods. Weakly Supervised learning model (WS <ref type="bibr" target="#b24">[25]</ref>) and Weakly Supervised Data Augmentation Network (WS DAN <ref type="bibr" target="#b23">[24]</ref>) are used as the baselines and also the backbones in our experiment. It is worth noting that we follow the hyper-parameters and evaluation metrics mentioned in <ref type="bibr" target="#b23">[24]</ref> for a fair comparison. Specifically, our proposed Dropping module improves the accuracy of both WS and WS DAN by 2.1 and 0.9, respectively. These values indicate that the dropping module in our method successfully removes ambiguity regions between top-k classes and achieves competitive results with other augmentation methods.</p><p>Dropping Threshold. Dropping is an essential step in our method as well as in other fine-grained works. In practice, dropped regions can enhance the discriminative between the top-k classes, but they also may hurt the original discrimination between the ground truth and the remaining classes. We can control this problem in our method by using the hyper-parameter d ? . <ref type="table">Table 8</ref> shows the result of our method under different values of d ? on ResNet-50 and WS DAN baselines. If d ? is set too small, the effectiveness of the dropping module is not enough for improving the overall performance. If d ? is set too large, the dropping module may mistakenly drop the discriminative features for different images that belong to the same class, hence reducing the accuracy. Through empirical results, we observe that dropping threshold d ? = 0.1 shows the consistent performance across CNN backbones and fine-grained datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization</head><p>Qualitative results for Attention Maps.  indicates that by employing our Self Assessment Classifier, each fine-grained class focuses on different informative regions.</p><p>Qualitative results between Dropping methods. In <ref type="figure" target="#fig_3">Figure 4</ref>, we illustrate the visual comparison between At- <ref type="figure">Figure 3</ref>. The visualization of the attention map between image feature maps and different ambiguity classes from our method. The red-colored class label denotes that the prediction is matched with the ground-truth. Best viewed in color.</p><p>tention Dropping <ref type="bibr" target="#b23">[24]</ref>, Random Dropping <ref type="bibr" target="#b11">[12]</ref>, and our SAC dropping methods. Unlike Random Dropping, which can erase the entire object or just the background, and Attention Dropping, which can erase discriminative informative regions to increase generalization, our proposed dropping method can efficiently identify and erase the informative regions that lead to incorrect classifications.</p><p>Localization Analysis Under the Joint Embedding module, the attention distribution map M contains informative regions used for classification, which can be leveraged to extract image-based object localization. <ref type="figure" target="#fig_4">Figure 5</ref> shows the comparison between Attention-guided localization in- troduced in <ref type="bibr" target="#b23">[24]</ref>, Random localization <ref type="bibr" target="#b24">[25]</ref>, and our SAC localization, to demonstrate the effectiveness of the localization method in Algorithm 1 of SAC. Different from Random localization, which might erase the informative regions of the object out of the image and the Attention-guided localization, which erases the background and keeps the object in the image, SAC localization can efficiently identify the informative regions which are useful for fine-grained classification between hard-to-distinguish classes.</p><formula xml:id="formula_14">(a) (b) (c) (d) (e)</formula><p>Qualitative results of different classification methods. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates the classification results and corresponding localization areas of different methods. In all samples, we can figure out that ResNet-50 do not focus on any specific area during classify process; WS DAN is trained to focus on objects. MMAL and SAC are trained to focus on different specific areas. Through illustration, we can see that ResNet-50 does not show good enough results for classifying finegrained samples. In some cases, focusing on specific object does not make WS DAN to give correct predictions. MMAL achieves good results in most cases, however, if the method focus on wrong areas, the model has high chance to give out wrong prediction. Self-Assessment mechanism in SAC allows it to focus on different areas based on different hard-to-distinguish classes. Thus, the method can focus on more meaningful areas and also ignore unnecessary ones. Hence, SAC achieves good predictions even with hard cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Limitation.</head><p>Our method contains different modules which are specifically designed for the fine-grained classification task. This is also a common problem of many fine-grained approaches since this task requires paying attention at very detailed levels. Based on the design, our method has three main hyperparameters. Fortunately, the experiments show that we can achieve good results with a unified setup. Therefore, there is no need to tune these parameters further. Compared to other approaches, our method utilizes the class name as the input. This is to provide more supervised information during training. While this arguably brings more information, we note that the class name is identical with the class identity, which is used by all methods to calculate the loss. In practice, as we add an extra self assessment classifier to the backbone, our method increases the training time and testing time by approximately 10%. We believe that this is a reasonable trade-off to improve the classification accuracy by 1.2%. Finally, while in theory we can integrate our method to any other fine-grained classifiers, we note that it is not straightforward to integrate our proposed method to multi-step approaches such as Parts Models <ref type="bibr" target="#b18">[19]</ref>. This is because these methods do not allow end-to-end training, hence making the integration more tedious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Broader Impact.</head><p>We have proposed a new fine-grained classification method that significantly improves the current state of the art. More efficient fine-grained methods will have direct impacts on different domains in real-world problems such as crop disease detection. We hope that our method will have broader impacts by enabling the integration into any new fine-grained classifiers in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An overview of our proposed method. The Top-k Coarse-grained Class Search first generates a list of potential top-k prediction candidates using a Deep CNN backbone. The Self Assessment Classifier then reassesses these predictions to improve the fine-grained classification results. The red color class label denotes the ground truth. The black color regions denote dropped regions. Dotted line means the image is used for augmentation. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Region Localization Input: Network model, Input image I Output: Cropped area I c 1 Pass I through Self Assessment Classifier to extract attention map M. 2 Sum up the correlation scores over all top-k class labels in M and then apply bilinear interpolation to get the correlation scores of all regions in the input image. 3 Extract maximum correlation score ? max from M, which denotes the highest correlation score in the attention map M. 4 Compute minimum correlation score ? min = 0.1 * ? max . Only regions which have higher correlation scores than ? min are kept. 5 Update attention map M by using Eq. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3illustrates the visualization of attention maps between image feature maps and each ambiguity class. The visualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of different dropping techniques. (a) Input image, (b) Random Dropping [12], (c) Attention Dropping [24], (d) The distribution maps from our Dropping module, (e) Our Dropping. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The visualization of localization results in images between attention-guided localization, random localization, and SAC localization. Note that (a) are Inputs, (b) shows Localization results with SAC, (c) visualizes the Localization of Attention Cropping [24], and (d) indicates Localization results of Random Cropping [25] The red-colored bounding boxes denote the groundtruth. The green-colored bounding boxes denote the predictions. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative comparison of different classification methods. (a) Input image and its corresponding ground-truth label, (b) ResNet-50 [23], (c) WS DAN [24], (d) MMAL [66], and (e) Our SAC. Boxes are localization areas. Red color indicates wrong classification result. Blue color indicate correct predicted label. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The performance of ResNet-50 + SAC and WS DAN + SAC under different values of parameter ?, which controls the contribution of coarse-grained classifier and fine-grained classifier.</figDesc><table><row><cell>Backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Auxiliary Classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropping</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Localization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ResNet-50 Backbone</cell><cell>86.4 (+0.0)</cell><cell>87.1 (+0.7)</cell><cell>87.6 (+1.2)</cell><cell cols="2">88.0 (+1.6)</cell><cell>88.3 (+1.9)</cell></row><row><cell cols="2">WS DAN Backbone</cell><cell>89.4 (+0.0)</cell><cell>90.0 (+0.6)</cell><cell>90.3 (+0.9)</cell><cell cols="2">90.7 (+1.3)</cell><cell>91.1 (+1.7)</cell></row><row><cell cols="7">Table 3. The performance (% Acc) of each module in our method</cell></row><row><cell cols="7">the on CUB-200-2011 test set. ResNet-50 and WS DAN are used</cell></row><row><cell cols="4">as the backbones in this experiment.</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell cols="2">(1 ? ?)</cell><cell cols="3">ResNet-50</cell><cell>WS DAN</cell></row><row><cell>(coarse-grained)</cell><cell cols="3">(fine-grained)</cell><cell>+SAC</cell><cell></cell><cell>+SAC</cell></row><row><cell>0.5</cell><cell></cell><cell>0.5</cell><cell></cell><cell>88.3</cell><cell></cell><cell>91.1</cell></row><row><cell>0.7</cell><cell></cell><cell>0.3</cell><cell></cell><cell>88.1</cell><cell></cell><cell>91.0</cell></row><row><cell>0.3</cell><cell></cell><cell>0.7</cell><cell></cell><cell>88.3</cell><cell></cell><cell>91.2</cell></row><row><cell>0.9</cell><cell></cell><cell>0.1</cell><cell></cell><cell>88.0</cell><cell></cell><cell>91.0</cell></row><row><cell>0.1</cell><cell></cell><cell>0.9</cell><cell></cell><cell>87.8</cell><cell></cell><cell>90.9</cell></row><row><cell>#Top-k classes</cell><cell cols="3">ResNet-50 + SAC</cell><cell cols="3">WS DAN + SAC</cell></row><row><cell>2</cell><cell></cell><cell>87.2</cell><cell></cell><cell></cell><cell></cell><cell>89.4</cell></row><row><cell>5</cell><cell></cell><cell>88.4</cell><cell></cell><cell></cell><cell></cell><cell>89.6</cell></row><row><cell>10</cell><cell></cell><cell>88.3</cell><cell></cell><cell></cell><cell></cell><cell>91.1</cell></row><row><cell>20</cell><cell></cell><cell>87.7</cell><cell></cell><cell></cell><cell></cell><cell>90.7</cell></row><row><cell>50</cell><cell></cell><cell>85.9</cell><cell></cell><cell></cell><cell></cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The effect of different numbers of top-k classes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .Table 8 .</head><label>68</label><figDesc>Performance and complexity of each module of SAC evaluated on CUB-200-2011 test set. ResNet-50 and WS DAN is used for baselines as well as the backbones. The performance (% Acc) of ResNet-50 + SAC and WS DAN + SAC with different values of dropping threshold d ? .</figDesc><table><row><cell></cell><cell></cell><cell cols="3">#Parameters (M) 25.6</cell><cell>25.6</cell><cell>25.6</cell><cell>25.6</cell><cell>38.4</cell></row><row><cell></cell><cell></cell><cell cols="2">GPU Speed</cell><cell>0.009</cell><cell>0.009</cell><cell>0.009</cell><cell>0.009</cell><cell>0.017</cell></row><row><cell></cell><cell></cell><cell cols="2">(s/sample)</cell><cell>?0.0013</cell><cell>?0.0013</cell><cell>?0.0013</cell><cell>?0.0013</cell><cell>?0.0022</cell></row><row><cell cols="2">WS DAN Backbone</cell><cell cols="3">#Parameters (M) 29.8 GPU Speed 0.121 (s/sample) ?0.0110</cell><cell>29.8 0.121 ?0.0110</cell><cell>29.8 0.121 ?0.0110</cell><cell>29.8 0.121 ?0.0110</cell><cell>49.0 0.201 ?0.0120</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Acc (%)</cell><cell></cell></row><row><cell>WS [25] (Backbone)</cell><cell></cell><cell></cell><cell>86.4</cell><cell></cell><cell></cell></row><row><cell cols="2">WS + Random Cropping [12]</cell><cell></cell><cell>86.8</cell><cell></cell><cell></cell></row><row><cell cols="2">WS + Random Dropping [12]</cell><cell></cell><cell>86.7</cell><cell></cell><cell></cell></row><row><cell cols="3">WS + Random Cropping &amp; Dropping [12]</cell><cell>87.2</cell><cell></cell><cell></cell></row><row><cell cols="2">WS + Attention Cropping [24]</cell><cell></cell><cell>87.8</cell><cell></cell><cell></cell></row><row><cell cols="2">WS + Attention Dropping [24]</cell><cell></cell><cell>87.4</cell><cell></cell><cell></cell></row><row><cell cols="4">WS + Attention Cropping &amp; Dropping [24] 88.4</cell><cell></cell><cell></cell></row><row><cell>WS + Dropping (ours)</cell><cell></cell><cell></cell><cell>88.5</cell><cell></cell><cell></cell></row><row><cell>WS DAN [24] (Backbone)</cell><cell></cell><cell></cell><cell>89.4</cell><cell></cell><cell></cell></row><row><cell cols="2">WS DAN + Dropping (ours)</cell><cell></cell><cell>90.3</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 7. Comparison between our Dropping method and differ-</cell><cell></cell></row><row><cell cols="5">ent augmentation techniques such as Random Dropping [12] and</cell><cell></cell></row><row><cell>Attention Cropping [24].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">d ? (dropping threshold) ResNet-50+SAC WS DAN+SAC</cell><cell></cell></row><row><cell>0.05</cell><cell cols="2">87.6</cell><cell>90.5</cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell cols="2">88.3</cell><cell>91.1</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell cols="2">88.4</cell><cell>90.9</cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell cols="2">83.9</cell><cell>87.0</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">We introduce a Self Assessment Classifier (SAC) which effectively learns the discriminative features in the image and resolves the ambiguity from the top-k prediction classes. Our method generates the attention map and uses this map to dynamically erase unnecessary regions during the training. The intensive experiments on CUB-200-2011, Stanford Dogs, and FGVC Aircraft datasets show that our proposed method can be easily integrated into different finegrained classifiers and clearly improve their accuracy. In the future, we would like to test the effectiveness of our method on more classifiers such as EfficientNet<ref type="bibr" target="#b49">[50]</ref> and bigger datasets such as ImageNet<ref type="bibr" target="#b10">[11]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Archith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heesung</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungtae</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring vision transformers for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerem</forename><surname>Turgutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple meta-model quantifying for medical visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<editor>MIC-CAI</editor>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise confusion for finegrained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ramesh Raskar, and Nikhil Naik. Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Channel interaction networks for finegrained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fine-grained recognition in the wild: A multi-task domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Elope: Fine-grained visual classification with efficient localization, pooling and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Weakly supervised bilinear attention network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Snapmix: Semantically proportional mixing for augmenting finegrained data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic partial swap: Enhanced model generalization and interpretability for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpretable and accurate finegrained recognition via region grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning canonical 3d object representation for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ig-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Twophase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fine-grained recognition as hsnet search for informative image parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Globallocal attention for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhat</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bac</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved bilinear pooling with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep federated learning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang D</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Coarse-to-fine reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Binh X Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02526</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph-based person signature for person re-identifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh X Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Binh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fine-grained recognition: Accounting for subtle differences between similar classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiattention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Light-weight deformable registration using adversarial learning with distilling knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuong</forename><surname>Minh Q Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huy</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erman</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Quang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMI</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Progressive adversarial networks for fine-grained domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph-propagation based correlation learning for weakly supervised fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via guassian mixture model oriented discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Selective convolutional descriptor aggregation for fine-grained image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Mask-cnn: Localizing parts and selecting descriptors for fine-grained bird species categorization. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Wei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hierarchical part matching for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nasoa: Towards faster taskoriented online fine-tuning with a zoo of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanlong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multibranch and multi-scale attention learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisheng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weaklysupervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph-based high-order relation discovery for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
