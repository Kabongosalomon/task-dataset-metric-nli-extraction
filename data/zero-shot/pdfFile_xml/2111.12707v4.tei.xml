<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
							<email>wenhaoli@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<email>hongliu@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<email>hao.tang@vision.ee.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
							<email>pichao.wang@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Lab</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating 3D human poses from monocular videos is a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions (i.e., hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer (MHFormer) that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, the task is decomposed into three stages: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that MHFormer achieves stateof-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and whistles, its performance surpasses the previous best result by a large margin of 3% on Human3.6M. Code and models are available at https://github.com/Vegetebird/MHFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D human pose estimation (HPE) from monocular videos is a fundamental vision task with a wide range of applications, such as action recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">41]</ref>, humancomputer interaction <ref type="bibr" target="#b6">[7]</ref>, and augmented/virtual reality <ref type="bibr" target="#b32">[32]</ref>. This task is typically solved by dividing it into two decoupled subtasks, i.e., 2D pose detection to localize the keypoints <ref type="figure">Figure 1</ref>. Given a frame with occluded body parts (right arm and elbow), a recent state-of-the-art 3D HPE method, PoseFormer <ref type="bibr" target="#b49">[49]</ref>, outputs a single solution that is inconsistent with the 2D input. In contrast, our MHFormer generates multiple plausible hypotheses (different colors) consistent with the 2D evidence and finally synthesizes a more accurate 3D pose (green). For easy comparison, the input frame is shown at a novel viewpoint. on the image plane, followed by 2D-to-3D lifting to infer joint locations in the 3D space from 2D keypoints. Despite their impressive performance <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">36]</ref>, it remains an inherently ill-posed problem because of self-occlusion and depth ambiguity in 2D representations.</p><p>To alleviate such issues, most methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b49">49</ref>] focus on exploring spatial and temporal relationships. They either employ graph convolutional networks to estimate 3D poses with a spatio-temporal graph representation of human skeletons <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40]</ref> or apply a pure Transformer-based model to capture spatial and temporal information from 2D pose sequences <ref type="bibr" target="#b49">[49]</ref>. Yet, the 2D-to-3D lifting from monocular videos is an inverse problem <ref type="bibr" target="#b0">[1]</ref> where multiple feasible solutions (i.e., hypotheses) exist due to its ill-posed nature given the missing depth <ref type="bibr" target="#b17">[18]</ref>. Those approaches ignore this problem and only estimate a single solution, which often leads to unsatisfactory results, especially when the person is severely occluded (see <ref type="figure">Figure 1</ref>).</p><p>Recently, a couple of methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b42">42]</ref> that generate multiple hypotheses have been proposed for the inverse problem. They often rely on the one-to-many mapping by adding multiple output heads to an existing architecture with a shared feature extractor, while failing to build the relationships among the features of different hypotheses. That is an important shortcoming, as such ability is vital to improve the <ref type="bibr">Figure 2</ref>. The proposed MHFormer constructs a three-stage framework by starting from generating multiple initial representations and then communicating among them in both independent and mutual ways to synthesize a more precise estimation. For easy illustration, we only show the process of a single-frame 2D pose as input.</p><p>expressiveness and the performance of the model. In view of the ambiguous inverse problem of 3D HPE, we argue that it is more reasonable to conduct a one-to-many mapping first and then a many-to-one mapping with various intermediate hypotheses, as this way can enrich the diversity of features and produce a better synthesis for the final 3D pose.</p><p>To this end, we present Multi-Hypothesis Transformer (MHFormer), a novel Transformer-based method for 3D HPE from monocular videos. The key insight is to allow the model to learn spatio-temporal representations of diverse pose hypotheses. To accomplish this, we introduce a three-stage framework that starts from generating multiple initial representations and gradually communicates across them to synthesize a more accurate prediction, as shown in <ref type="figure" target="#fig_5">Figure 2</ref>. This framework more effectively models multihypothesis dependencies while also building stronger relationships among hypothesis features. Specifically, in the first stage, a Multi-Hypothesis Generation (MHG) module is built to model the intrinsic structure information of human joints and generate several multi-level features in the spatial domain. Those features contain diverse semantic information in different depths from shallow to deep and hence can be regarded as initial representations of multiple hypotheses.</p><p>Next, we propose two novel modules to model temporal consistencies and enhance those coarse representations in the temporal domain, which have not been explored by the existing works that generate multiple hypotheses. In the second stage, a Self-Hypothesis Refinement (SHR) module is proposed to refine every single-hypothesis feature. The SHR consists of two new blocks. The first block is a multi-hypothesis self-attention (MH-SA) which models single-hypothesis dependencies independently to construct self-hypothesis communication, enabling message passing within each hypothesis for feature enhancement. The second block is a hypothesis-mixing multi-layer perceptron (MLP) that exchanges information across hypotheses. The multiple hypotheses are merged into a single converged representation, and then this representation is partitioned into several diverged hypotheses.</p><p>Although those hypotheses are refined by SHR, the connections across different hypotheses are not strong enough since the MH-SA in the SHR only passes intra-hypothesis information. To address this issue, in the last stage, a Cross-Hypothesis Interaction (CHI) module models interactions among multi-hypothesis features. Its key component is the multi-hypothesis cross-attention (MH-CA), which captures mutual multi-hypothesis correlations to build cross-hypothesis communication, enabling message passing among hypotheses for better interaction modeling. Subsequently, a hypothesis-mixing MLP is used to aggregate the multiple hypotheses to synthesize the final prediction.</p><p>With the proposed MHFormer, multi-hypothesis spatiotemporal feature hierarchies are explicitly incorporated into Transformer models, where the multiple hypothesis information of body joints can be independently and mutually processed in an end-to-end manner. As a result, the representation ability is potentially enhanced and the synthesized pose is much more accurate. Our contributions are summarized as follows:</p><p>? ...</p><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoded Hypothesis</head><p>Features <ref type="table" target="#tab_2">T  T T  T T   1</ref>  types: one-stage approaches and two-stage ones. One-stage approaches directly infer 3D poses from input images without intermediate 2D pose representations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b38">38]</ref>, while two-stage ones first obtain 2D keypoints from pretrained 2D pose detections and then feed them into a 2D-to-3D lifting network to estimate 3D poses. Benefiting from the excellent performance of 2D human pose estimation, this 2D-to-3D pose lifting method can efficiently and accurately regress 3D poses using detected 2D keypoints. For instance, SimpleBaseline <ref type="bibr" target="#b29">[30]</ref> proposes a fully-connected residual network to lift 2D keypoints to 3D joint locations from a single frame. Anatomy3D <ref type="bibr" target="#b3">[4]</ref> decomposes the task into bone direction and bone length predictions to ensure temporal consistency over a sequence. Despite the promising results achieved by using temporal correlations from fully convolutional <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">36]</ref> or graph-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40]</ref> architectures, these methods are less efficient in capturing global-context information across frames.</p><p>Vision Transformers. Recently, Transformer <ref type="bibr" target="#b39">[39]</ref> equipped with the powerfully global self-attention mechanism has received increasingly research interest in the computer vision community <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">46]</ref>. For the basic image classification task, ViT <ref type="bibr" target="#b5">[6]</ref> is proposed to apply a standard Trans-former architecture directly to sequential image patches. For the pose estimation task, PoseFormer <ref type="bibr" target="#b49">[49]</ref> applies a pure Transformer to capture human joint correlations and temporal dependencies. Strided Transformer <ref type="bibr" target="#b20">[21]</ref> introduces a Transformer-based architecture with strided convolutions to lift a long 2D pose sequence to a single 3D pose. Our work is inspired by them and similarly uses the Transformer as the basic architecture. But we do not just utilize a simple architecture with a single representation; instead, the seminal ideas of multi-hypothesis and multi-level feature hierarchies are connected within Transformers, which makes the model not only expressive but also strong. Besides, a cross-attention mechanism is introduced for effective multihypothesis learning.</p><p>Multi-Hypothesis Methods. Single-view 3D HPE is illposed and therefore assuming only a single solution might be sub-optimal. Several works generate diverse hypotheses for the inverse problem and achieve substantial performance gains <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b42">42]</ref>. For example, Jahangiri et al. <ref type="bibr" target="#b13">[14]</ref> generated multiple 3D pose candidates consistent with 2D keypoints via a compositional model and anatomical constraints. Wehrbein et al. <ref type="bibr" target="#b42">[42]</ref> modeled the posterior distribution of 3D pose hypotheses with normalized flows. Unlike these works that focus on a one-to-many mapping, we learn a one-to-many mapping first and then a many-to-one mapping, which allows for the effective modeling of different features corresponding to the various hypotheses to improve the representation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Hypothesis Transformer</head><p>The overview of the proposed MHFormer is depicted in <ref type="figure" target="#fig_0">Figure 3</ref> (a). Given a consecutive 2D pose sequence estimated by an off-the-shelf 2D pose detector from a video, our method aims to reconstruct the 3D pose of the center frame by making full use of spatial and temporal information in the multi-hypothesis feature hierarchies. To achieve our proposed three-stage framework, MHFormer is built upon (i) three major modules: Multi-Hypothesis Generation (MHG), Self-Hypothesis Refinement (SHR), and Cross-Hypothesis Interaction (CHI), and (ii) two auxiliary modules: temporal embedding and regression head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>In this work, we adopt a Transformer-based architecture since it performs well in long-range dependency modeling. We first give a brief description of the basic components in the Transformer <ref type="bibr" target="#b39">[39]</ref>, including a multi-head self-attention (MSA) and a multi-layer perceptron (MLP). MSA. In the MSA, the inputs x?R n?d are linearly mapped to queries Q?R n?d , keys K?R n?d , and values V ?R n?d , where n is the sequence length, and d is the dimension. The scaled dot-product attention can be computed by:</p><formula xml:id="formula_0">Attention(Q, K, V ) = Softmax QK T / ? d V. (1)</formula><p>MSA splits the queries, keys, and values for h times as well as performs the attention in parallel. Then, the outputs of h attention heads are concatenated. MLP. The MLP consists of two linear layers, which are used for non-linearity and feature transformation:</p><formula xml:id="formula_1">MLP(x) = ? (xW 1 + b 1 ) W 2 + b 2 ,<label>(2)</label></formula><p>where ? denotes the GELU activation function, W 1 ?R d?dm and W 2 ?R dm?d are the weights of the two linear layers respectively, and b 1 ?R dm and b 2 ?R d are the bias terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Hypothesis Generation</head><p>In the spatial domain, we address the inverse problem by explicitly designing a cascaded Transformer-based architecture to generate multiple features in different depths of the latent space. To this end, MHG is introduced to model the human joint relations and initialize the multi-hypothesis representations (see <ref type="figure" target="#fig_0">Figure 3</ref> (b)). Suppose there are M different hypotheses and L 1 layers in the MHG, it takes a sequence of 2D poses X?R N ?J?2 with N video frames and J body joints as input and outputs multiple hypothe-</p><formula xml:id="formula_2">ses [X 1 L1 , X 2 L1 , ..., X M L1 ], where X m L1 ?R (J?2)?N is the m-th hypothesis.</formula><p>More specifically, we concatenate the (x, y) coordinates of joints for each frame toX?R (J?2)?N , retain their spatial information of joints via a learnable spatial position embedding E m SP os ?R (J?2)?N , and feed the embedded features into the encoders of the MHG. To encourage gradient propagation, a skip residual connection is applied between the original input and output features from the encoder. These procedures can be formulated as:</p><formula xml:id="formula_3">X m 0 = LN(X m ) + E m SP os , X m l = X m l?1 + MSA m (LN(X m l?1 ), X m l = X m l + MLP m (LN(X m l ), X m L1 = X m + LN(X m L1 ),<label>(3)</label></formula><p>where LN(?) is the LayerNorm layer, l?[1, ..., L 1 ] is the index of MHG layers, X 1 =X, and X m =X m?1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L1</head><p>(m&gt;1). The outputs of the MHG (i.e., X m L1 ) are multi-level features containing diverse semantic information. Therefore, those features can be regarded as initial representations of different pose hypotheses and need to be further enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal Embedding</head><p>The MHG helps to generate initial multi-hypothesis features in the spatial domain, whereas the capabilities of such features are not strong enough. Considering this limitation, we propose to build relationships across hypothesis features and capture temporal dependencies in the temporal domain with two carefully designed modules: an SHR module followed by a CHI module (see <ref type="figure" target="#fig_0">Figure 3</ref> (c) and (d)).</p><p>In order to exploit temporal information, we should first convert the spatial domain into the temporal domain. For this purpose, the encoded hypothesis features X m L1 of each frame are embedded to the high-dimensional features Z m ?R N ?C using a transposition operation and a linear embedding, where C is the embedding dimension. Then, a learnable temporal position embedding E m T P os ?R N ?C is utilized to retain positional information of frames, which can be formulated as:</p><formula xml:id="formula_4">Z m 0 = Z m +E m T P os .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Self-Hypothesis Refinement</head><p>In the temporal domain, we first construct the SHR to refine single-hypothesis features. Each SHR layer consists of a multi-hypothesis self-attention (MH-SA) block and a hypothesis-mixing MLP block. MH-SA. The core of the Transformer model is MSA, through which any two elements can interact with each other, thus modeling long-range dependencies. Instead, our MH-SA aims to capture single-hypothesis dependencies within each hypothesis independently for self-hypothesis communication. Specifically, the embedded features Z m 0 ?R N ?C  of different hypotheses are fed into several parallel MSA blocks, which can be expressed as:</p><formula xml:id="formula_5">Z m l = Z m l?1 + MSA m (LN( Z m l?1 )),<label>(4)</label></formula><p>where l?[1, ..., L 2 ] is the index of SHR layers. Therefore, the message of different hypothesis features can be passed in a self-hypothesis way for feature enhancement.</p><p>Hypothesis-Mixing MLP. The multiple hypotheses are processed independently in the MH-SA, but there is no information exchange across hypotheses. To handle this issue, we add a hypothesis-mixing MLP after the MH-SA. The features of multiple hypotheses are concatenated and fed into the hypothesis-mixing MLP to merge (i.e., converge) themselves. Then, the converged features are evenly partitioned (i.e., diverged) into non-overlapping chunks along the channel dimension, forming refined hypothesis representations. The procedure can be formulated as:</p><formula xml:id="formula_6">Z l = Concat( Z 1 l , ..., Z M l ) ? R N ?(C?M ) , Concat( Z 1 l , ..., Z M l )= Z l + HM-MLP(LN( Z l )),<label>(5)</label></formula><p>where Concat(?) is the concatenation operation and HM-MLP(?) is the function of hypothesis-mixing MLP which shares the same format as Eq. <ref type="formula" target="#formula_1">(2)</ref>. This process explores the relations among channels of different hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Cross-Hypothesis Interaction</head><p>We then model interactions among multi-hypothesis features via the CHI, which contains two blocks: multihypothesis cross-attention (MH-CA) and hypothesis-mixing MLP. MH-CA. The MH-SA lacks connections across hypotheses, which limits its interaction modeling. To capture multihypothesis correlations mutually for cross-hypothesis com-munication, the MH-CA composed of multiple multi-head cross-attention (MCA) elements in parallel is proposed.</p><p>The MCA measures the correlation among crosshypothesis features and has a similar structure to MSA. The common configuration of MCA uses the same input between keys and values <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">44]</ref>. However, an issue with this configuration is that it will result in more blocks (e.g., 6 MCA blocks for 3 hypotheses). Here, we adopt a more efficient strategy, which reduces the number of parameters by using different inputs (only require 3 MCA blocks), as shown in <ref type="figure" target="#fig_1">Figure 4</ref> (Right). The multiple hypotheses Z m are alternately regarded as queries, keys, and values and fed into the MH-CA: </p><formula xml:id="formula_7">Z m l =Z m l?1 + MCA m (LN(Z m1 l?1 ), LN(Z m2 l?1 ), LN(Z m l?1 )),<label>(6)</label></formula><formula xml:id="formula_8">Z l = Concat(Z 1 l , ..., Z M l ) ? R N ?(C?M ) , Concat(Z 1 l , ..., Z M l )=Z l + HM-MLP(LN(Z l )).<label>(7)</label></formula><p>In the hypothesis-mixing MLP of the last CHI layer, the partition operation is not used so that the features of all hypotheses are finally aggregated to synthesize a single hypothesis representation Z L3 ?R N ?(C?M ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Regression Head</head><p>In the regression head, a linear transformation layer is applied on the output Z L3 to perform regression to produce the 3D pose sequence X?R N ?J?3 . Finally, the 3D pose of the center frameX?R J?3 is selected from X as our final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Loss Function</head><p>The entire model is trained in an end-to-end manner with a Mean Squared Error (MSE) loss, which is applied to minimize the error between the estimated and ground truth poses:</p><formula xml:id="formula_9">L = N n=1 J i=1 Y n i ? X n i 2 ,<label>(8)</label></formula><p>where X n i and Y n i represent the predicted and ground truth 3D poses of joint i at frame n, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We evaluate our method on two widely-used datasets for 3D HPE: Human3.6M <ref type="bibr" target="#b12">[13]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b30">[31]</ref>. Human3.6M. The Human3.6M dataset <ref type="bibr" target="#b12">[13]</ref> is the largest and most representative benchmark for 3D HPE. This dataset consists of 3.6 million images captured from four synchronized cameras at 50 Hz. There are 15 daily activities performed by 11 human subjects in an indoor environment. Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40]</ref>, we train a single model on five subjects (S1, S5, S6, S7, S8) and test it on two subjects (S9 and S11). We adopt the most commonly used evaluation protocols: Protocol 1 is the MPJPE which measures the mean Euclidean distance between the ground truth and estimated joints in millimeters; Protocol 2 is the MPJPE after aligning the predicted 3D pose with the ground truth using translation, rotation, and scale (P-MPJPE). MPI-INF-3DHP. The MPI-INF-3DHP <ref type="bibr" target="#b30">[31]</ref> is a large 3D pose dataset in both indoor and outdoor environments. This dataset provides 1.3 million frames, containing more diverse motions than Human3.6M. Following the setting in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">49]</ref>, we report metrics of MPJPE, Percentage of Correct Keypoint (PCK) with the threshold of 150 mm, and Area Under Curve (AUC) for a range of PCK thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>In our implementation, the proposed MHFormer contains L 1 =4 MHG, L 2 =2 SHR, and L 3 =1 CHI layers. The MHFormer model is implemented in PyTorch framework on one GeForce RTX 3090 GPU. We train our model in an end-to-end manner from scratch using Amsgrad optimizer. The initial learning rate is set to 0.001 with a shrink factor of 0.95 applied after each epoch and 0.5 after every 5 epochs. For a fair comparison, the same horizontal flip augmentation is adopted following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b49">49]</ref>. We perform the 2D pose detection using cascaded pyramid network (CPN) <ref type="bibr" target="#b4">[5]</ref> for Human3.6M following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">36]</ref> and ground truth 2D pose for MPI-INF-3DHP following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-Art Methods</head><p>Results on Human3.6M. The proposed MHFormer is compared with the state-of-the-art methods on Human3.6M. The results of our model with a receptive field of 351 frames using 2D detected inputs <ref type="bibr" target="#b4">[5]</ref> are reported in  <ref type="figure" target="#fig_3">Figure 5</ref> shows the qualitative comparison with the PoseFormer and the baseline model (same architecture as ViT <ref type="bibr" target="#b5">[6]</ref>) on some challenging poses. To further explore the lower bound of our method, we compared our MHFormer with the state-of-the-art methods with ground truth 2D poses   Additionally, our method is compared with previous methods of generating multiple 3D pose hypotheses. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. It is noteworthy that these methods report metrics for the best hypothesis due to the adopted oneto-many mapping, while our method reports metrics with a specific solution by learning a deterministic mapping, which is much more practical in reality. Even though we use much fewer hypothesis numbers (3 vs. 200), our proposed method consistently outperforms previous works. Results on MPI-INF-3DHP. To assess the generalization ability, we evaluate our method on MPI-INF-3DHP dataset. Following <ref type="bibr" target="#b49">[49]</ref>, we use 2D pose sequences of 9 frames as our model input due to the fewer samples and shorter sequence lengths of this dataset compared to Human3.6M. The results in <ref type="table" target="#tab_5">Table 3</ref> demonstrate that our method achieves the best performance on all metrics (PCK, AUC, and MPJPE). It emphasizes the effectiveness of our MHFormer in improving performance in outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To verify the impact of each component and design in the proposed model, we conduct extensive ablation experiments Impact of Receptive Fields. For the video-based 3D HPE task, a large receptive field is essential for estimation accuracy. <ref type="table" target="#tab_6">Table 4</ref> shows the results of our method with different input frames. It can be seen that our method obtains larger gains with more frames fed into the model. The error has a great decrease of 16.7% from 9-frames to 351-frames with GT input, which indicates the effectiveness of our method in capturing long-range dependencies across frames with a large receptive field. Next, ablations in the following parts are carried out using a receptive field of 27 frames to balance the computation efficiency and performance. Impact of Parameters in MHG. In the top part of <ref type="table" target="#tab_7">Table 5</ref>, we report the results with different numbers of MHG layers. Experiments show that stacking more layers in MHG can slightly improve the performance with few parameter increases, but the gain disappears when the layer number is larger than 4. Moreover, we investigate the influence of using different numbers of hypotheses in MHG. The results are shown in the bottom part of <ref type="table" target="#tab_7">Table 5</ref>. Increasing the number of hypotheses can improve the result, but the performance saturates when using 3 hypothesis representations. Notably, our model equipped with 3 hypotheses shows significant gains over the single-hypothesis model with 1.7 mm error reduction. This demonstrates that exploiting different representations of multiple pose hypotheses is helpful to improve the performance of the model, validating our motivation. Impact of Parameters in SHR and CHI. <ref type="table" target="#tab_8">Table 6</ref> reports how the different parameters of SHR and CHI impact the performance and computation complexity of our model. The  <ref type="figure">Figure 6</ref>. Diverse 3D pose hypotheses generated by MHFormer. For easy illustration, we color-code the hypotheses to show the difference among them. Green colored 3D pose corresponds to the final synthesized estimation of our method. results show that enlarging the embedding dimension from 256 to 512 can boost the performance, but using dimensions larger than 512 cannot bring further improvements. In addition, we observe no more gains by stacking either more SHR or CHI layers. Therefore, the optimal parameters for our model are L 2 =2, L 3 =1, and C=512. Effect of Model Components. In <ref type="table" target="#tab_9">Table 7</ref>, we carry out experiments to quantify the influence of our proposed components. Firstly, we compare our method with the baseline model. For a fair comparison, the results of the baseline are reported at the same number of layers as MHFormer with different embedding dimensions, since our hypothesis-mixing MLP in MHFormer takes concatenated hypothesis features as inputs (the dimension is 512?3=1536). The results show that the baseline model is prone to overfitting due to the excessive number of parameters, whereas our method performs well. Additionally, it can be seen that our MHFormer built upon MHG, SHR, and CHI outperforms varying variants of baseline models (1.9 mm improvement). Then, when we incorporate multi-hypothesis representations and SHR or CHI within the baseline, the performance has significant gains (-1.3 mm for MHG-SHR and -1.0 mm for MHG-CHI). Besides, we remove the MHG in MHFormer (SHR-CHI). At this point, the model only captures temporal information and its error heavily increases by 1.3 mm. These ablations indicate that learning multi-hypothesis spatio-temporal representations is significant for 3D HPE, and the different hypothesis representations should be modeled in both independent and mutual ways. We also explore the use of multi-level features in MHG by simply building the MHG upon several parallel Transformer encoders (MHFormer * ). As shown in the table, our MHFormer equipped with the multi-level features increases the performance, which indicates that the multi-level features can bring valuable information to the final estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative Results</head><p>Although our method does not aim to produce multiple 3D pose predictions, for better observation, we add additional regression layers and finetune our model to visualize the intermediate hypotheses. The several qualitative results are shown in <ref type="figure">Figure 6</ref>. It can be seen that our method is able to generate different plausible 3D pose solutions, especially for ambiguous body parts with depth ambiguity, self-occlusion, and 2D detector uncertainty. Moreover, the final 3D pose synthesized by aggregating multi-hypothesis information is more reasonable and accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presents Multi-Hypothesis Transformer (MHFormer), a new Transformer-based three-stage framework for the ambiguous inverse problem of 3D HPE from monocular videos. MHFormer first generates initial representations of multiple pose hypotheses in the spatial domain and then communicates across them in both independent and mutual ways in the temporal domain. Extensive experiments show that the proposed MHFormer has a fundamental advantage over single-hypothesis Transformers and achieves state-of-the-art performance on two benchmark datasets. We hope that our approach will foster further research in 2D-to-3D pose lifting considering various ambiguities. Limitation. One limitation of our method is the relatively larger computational complexity. The excellent performance of Transformers comes at a price of high computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material contains the following details: (1) A brief description of multi-head cross-attention. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Head Cross-Attention</head><p>In Sec. 3.1 of our main manuscript, we give a brief description of the multi-head self-attention (MSA) block. Given the inputs x?R n?d , they are linearly mapped to queries Q?R n?d , keys K?R n?d , and values V ?R n?d . The scaled dot-product attention in the MSA can be computed by:</p><formula xml:id="formula_10">Attention(Q, K, V ) = Softmax QK T ? d V. (9)</formula><p>In this section, we further define the multi-head crossattention (MCA) among three tensors, x, y, and z. The inputs x?R n?d , y?R n?d , and z?R n?d are linearly mapped to queries Q x ?R n?d , keys K y ?R n?d , and values V z ?R n?d , respectively. The scaled dot-product attention in the MCA can be computed by:</p><formula xml:id="formula_11">Attention cross (Q x , K y , V z ) = Softmax Q x K T y ? d V z .</formula><p>(10) The common configuration of MCA uses the same input between keys and values <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">44]</ref>, i.e., the inputs x = y = z. Instead, we adopt a more efficient strategy by using different inputs, i.e., the inputs x = y = z. <ref type="table" target="#tab_4">Table 8</ref> shows the results of our proposed MHFormer on Human3.6M under Protocol 2. The input 2D poses are estimated by CPN <ref type="bibr" target="#b4">[5]</ref>. Without bells and whistles, our MHFormer achieves promising results that outperform the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Quantitative Results</head><p>Several methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b50">50]</ref> adopt a pose refinement module, which is first proposed by ST-GCN <ref type="bibr" target="#b1">[2]</ref>, to further improve the estimation accuracy. Following <ref type="bibr" target="#b1">[2]</ref>, we adopt the refine module and the results are shown in <ref type="table" target="#tab_12">Table 9</ref>. It can be seen that our method can use the refine module to improve the performance, achieving an error of 42.4 mm in MPJPE which surpasses all other approaches by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Ablation Studies</head><p>Effect of Model Components. Here, we give more details about how to build the different variants of MHFormer in <ref type="table" target="#tab_9">Table 7</ref> of our main manuscript: MHG, L 2 =2 SHR, and L 3 =1 CHI layers. Please refer to <ref type="figure" target="#fig_0">Figure 3</ref> in our main manuscript. Impact of Configurations in MH-CA. As mentioned in Sec. 3.5 of our main manuscript, the common configuration of MCA uses the same input between keys and values <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">44]</ref>, which will result in more blocks. We adopt a more efficient configuration by using different inputs among queries, keys, and values. The performance and computational complexity of these two configurations are given in <ref type="table" target="#tab_2">Table 10</ref>. We can see that using the same input between keys and values in MH-CA (MH-CA * ) requires more parameters and FLOPs but cannot bring further performance gains. It illustrates the effectiveness of our efficient strategy in MCA. Impact of Receptive Fields. For the video-based 3D human pose estimation task, the number of receptive fields directly influences the estimation results. <ref type="figure">Figure 7 (a)</ref> shows the results of our model with different receptive fields (between 1 and 351) on Human3.6M. Increasing the receptive field can improve the result under both CPN and GT 2D pose inputs, which demonstrates the great power of our method in longrange dependency modeling with a long input sequence. Impact of 2D Detections. To show the effectiveness of our method on different 2D pose detectors, we carry out experiments with the detections from Stack Hourglass (SH) <ref type="bibr" target="#b33">[33]</ref>, Detectron <ref type="bibr" target="#b36">[36]</ref>, and CPN <ref type="bibr" target="#b4">[5]</ref>. In addition, to evaluate the robustness of our method to various levels of noise, we also conduct experiments on 2D ground truth plus different levels of additive Gaussian noise. The results are shown in <ref type="figure">Figure 7</ref> (b). It can be observed that the curve has a nearly linear relationship between MPJPE of 3D poses and two-norm errors of 2D poses. These experiments validate both the effectiveness and robustness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Visualization Results</head><p>3D Reconstruction Visualization. <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref> show qualitative results of our method on Human3.6M dataset, MPI-INF-3DHP dataset, and challenging in-thewild videos. Moreover, <ref type="figure">Figure 10</ref> shows the qualitative comparison with the baseline method and the previous state-   of-the-art method (PoseFormer <ref type="bibr" target="#b49">[49]</ref>) on some wild videos. It can be seen that our method can produce more accurate and reasonable 3D poses, especially when the human action is complex and rare. Hypothesis Visualization. For visualization purposes, we add additional regression layers and finetune our model to output intermediate hypotheses. <ref type="figure">Figure 11</ref> shows the visualization results of intermediate 3D pose hypotheses generated by our proposed method. We can see that our MHFormer can generate different plausible 3D pose solutions, especially for ambiguous body parts with depth ambiguity, self-occlusion, and 2D detector uncertainty. Attention Visualization. Visualization results of the multihead attention maps of the first layers from the Multi-Hypothesis Generation (MHG) module and Self-Hypothesis Refinement (SHR) module (351-frame model with 3 hypotheses) are shown in <ref type="figure" target="#fig_5">Figure 12</ref> and <ref type="figure" target="#fig_0">Figure 13</ref>, respectively. It can be found that the maps of multiple hypotheses contain diverse patterns and semantics. This indicates multiple representations in our method actually learn various modal information of pose hypotheses.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Cross-Attention (MH-CA) Multi-Hypothesis Self-Attention (MH-SA) (a) Overview of the proposed Multi-Hypothesis Transformer (MHFormer). (b) Multi-Hypothesis Generation (MHG) module extracts the intrinsic structure information of human joints within each frame and generates multiple hypothesis representations. N is the number of input frames and T is the matrix transposition. (c) Self-Hypothesis Refinement (SHR) module is used to refine single-hypothesis features. (d) Cross-Hypothesis Interaction (CHI) module following SHR enables interactions among multi-hypothesis features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Left: Multi-head self-attention (MSA). Right: Multihead cross-attention (MCA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where l?[1, ..., L 3 ] is the index of CHI layers, Z m 0 = Z m L2 , m 1 and m 2 are the other two corresponding hypotheses, and MCA(Q, K, V ) denotes the function of the MCA. Thanks to the MH-CA, the message passing can be performed in a crossing way to significantly improve modeling power. Hypothesis-Mixing MLP. The hypothesis-mixing MLP in the CHI serves as the same function as the process in Eq.<ref type="bibr" target="#b4">(5)</ref>. The outputs of the MH-CA are fed into it:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparison among the proposed method (MHFormer), the baseline method, and the previous state-of-the-art method (PoseFormer)<ref type="bibr" target="#b49">[49]</ref> on Human3.6M dataset. Wrong estimations are highlighted by yellow arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 2 )</head><label>2</label><figDesc>Additional quantitative results. (3) Additional ablation studies. (4) Additional visualization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Different receptive fields under MPJPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Different 2D detections under MPJPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .Figure 8 .Figure 9 .Figure 10 .</head><label>78910</label><figDesc>(a) Ablation studies on different receptive fields of our method on Human3.6M under MPJPE metric. (b) The effect of 2D detections on Human3.6M under MPJPE. Here, N (0, ?) represents the Gaussian noise with mean zero and ? is the standard deviation. (CPN) -Cascaded Pyramid Network; (SH) Stack Hourglass; (GT) -2D ground truth. Qualitative results of our proposed method on Human3.6M dataset (first 1 row) and MPI-INF-3DHP dataset (last 2 rows). v t Qualitative results of our proposed method on challenging in-the-wild videos. Qualitative comparison among the proposed method (MHFormer), the baseline method, and the previous state-of-the-art method (PoseFormer) [49] on challenging wild videos. Wrong estimations are highlighted by yellow arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .Figure 12 .Figure 13 .</head><label>111213</label><figDesc>Diverse 3D pose hypotheses generated by MHFormer. For easy illustration, we color-code the hypotheses to show the difference among them, and the hypotheses are shown from two perspectives. Green colored 3D pose corresponds to the final synthesized estimation of our method. Multi-head attention maps (9 heads) from the Multi-Hypothesis Generation (MHG) module of our 351-frame model with 3 different hypotheses. The brighter color indicates a stronger attention value. Multi-head attention maps (8 heads) from the Self-Hypothesis Refinement (SHR) module of our 351-frame model with 3 different hypotheses. The brighter color indicates a stronger attention value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with the state-of-the-art methods on Human3.6M under Protocol 1, using detected 2D poses (top) and ground truth 2D poses (bottom) as inputs. ( ?) -uses temporal information. Blod: best; Underlined: second best. Method Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>Fang et al. (AAAI'18) [8]</cell><cell cols="4">50.1 54.3 57.0 57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell cols="2">72.8 88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>GraphSH (CVPR'21) [45]</cell><cell cols="4">45.2 49.9 47.5 50.9</cell><cell>54.9</cell><cell>66.1</cell><cell>48.5</cell><cell>46.3</cell><cell cols="2">59.7 71.5</cell><cell>51.4</cell><cell>48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1</cell><cell>51.9</cell></row><row><cell>MGCN (ICCV'21) [50]</cell><cell cols="4">45.4 49.2 45.7 49.4</cell><cell>50.4</cell><cell>58.2</cell><cell>47.9</cell><cell>46.0</cell><cell cols="2">57.5 63.0</cell><cell>49.7</cell><cell>46.6</cell><cell>52.2</cell><cell>38.9</cell><cell>40.8</cell><cell>49.4</cell></row><row><cell>ST-GCN (ICCV'19) [2] ( ?)</cell><cell cols="4">44.6 47.4 45.6 48.8</cell><cell>50.8</cell><cell>59.0</cell><cell>47.2</cell><cell>43.9</cell><cell cols="2">57.9 61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell>VPose (CVPR'19) [36] ( ?)</cell><cell cols="4">45.2 46.7 43.3 45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell cols="2">57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>SGNN (ICCV'21) [48] ( ?)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell></row><row><cell>UGCN (ECCV'20) [40] ( ?)</cell><cell cols="4">41.3 43.9 44.0 42.2</cell><cell>48.0</cell><cell>57.1</cell><cell>42.2</cell><cell>43.2</cell><cell cols="2">57.3 61.3</cell><cell>47.0</cell><cell>43.5</cell><cell>47.0</cell><cell>32.6</cell><cell>31.8</cell><cell>45.6</cell></row><row><cell>Liu et al. (CVPR'20) [26] ( ?)</cell><cell cols="4">41.8 44.8 41.1 44.9</cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell cols="2">56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>PoseFormer (ICCV'21) [49] ( ?)</cell><cell cols="4">41.5 44.8 39.8 42.5</cell><cell>46.5</cell><cell>51.6</cell><cell>42.1</cell><cell>42.0</cell><cell cols="2">53.3 60.7</cell><cell>45.5</cell><cell>43.3</cell><cell>46.1</cell><cell>31.8</cell><cell>32.2</cell><cell>44.3</cell></row><row><cell cols="5">Anatomy3D (TCSVT'21) [4] ( ?) 41.4 43.2 40.1 42.9</cell><cell>46.6</cell><cell>51.9</cell><cell>41.7</cell><cell>42.3</cell><cell cols="2">53.9 60.2</cell><cell>45.4</cell><cell>41.7</cell><cell>46.0</cell><cell>31.5</cell><cell>32.7</cell><cell>44.1</cell></row><row><cell>MHFormer (Ours) ( ?)</cell><cell cols="4">39.2 43.1 40.1 40.9</cell><cell>44.9</cell><cell>51.2</cell><cell>40.6</cell><cell>41.3</cell><cell cols="2">53.5 60.3</cell><cell>43.7</cell><cell>41.1</cell><cell>43.8</cell><cell>29.8</cell><cell>30.6</cell><cell>43.0</cell></row><row><cell>Method</cell><cell cols="8">Dir. Disc Eat Greet Phone Photo Pose Purch.</cell><cell>Sit</cell><cell cols="7">SitD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>P-LSTM (ECCV'18) [16] ( ?)</cell><cell cols="4">32.1 36.6 34.3 37.8</cell><cell>44.5</cell><cell>49.9</cell><cell>40.9</cell><cell>36.2</cell><cell cols="2">44.1 45.6</cell><cell>35.3</cell><cell>35.9</cell><cell>30.3</cell><cell>37.6</cell><cell>35.5</cell><cell>38.4</cell></row><row><cell>PoseAug (CVPR'21) [9]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.2</cell></row><row><cell>VPose (CVPR'19) [36] ( ?)</cell><cell cols="4">35.2 40.2 32.7 35.7</cell><cell>38.2</cell><cell>45.5</cell><cell>40.6</cell><cell>36.1</cell><cell cols="2">48.8 47.3</cell><cell>37.8</cell><cell>39.7</cell><cell>38.7</cell><cell>27.8</cell><cell>29.5</cell><cell>37.8</cell></row><row><cell>Liu et al. (CVPR'20) [26] ( ?)</cell><cell cols="4">34.5 37.1 33.6 34.2</cell><cell>32.9</cell><cell>37.1</cell><cell>39.6</cell><cell>35.8</cell><cell cols="2">40.7 41.4</cell><cell>33.0</cell><cell>33.8</cell><cell>33.0</cell><cell>26.6</cell><cell>26.9</cell><cell>34.7</cell></row><row><cell>Anatomy3D (TCSVT'21) [4] ( ?)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.3</cell></row><row><cell>SRNet (ECCV'20) [47] ( ?)</cell><cell cols="4">34.8 32.1 28.5 30.7</cell><cell>31.4</cell><cell>36.9</cell><cell>35.6</cell><cell>30.5</cell><cell cols="2">38.9 40.5</cell><cell>32.5</cell><cell>31.0</cell><cell>29.9</cell><cell>22.5</cell><cell>24.5</cell><cell>32.0</cell></row><row><cell>PoseFormer (ICCV'21) [49] ( ?)</cell><cell cols="4">30.0 33.6 29.9 31.0</cell><cell>30.2</cell><cell>33.3</cell><cell>34.8</cell><cell>31.4</cell><cell cols="2">37.8 38.6</cell><cell>31.7</cell><cell>31.5</cell><cell>29.0</cell><cell>23.3</cell><cell>23.1</cell><cell>31.3</cell></row><row><cell>MHFormer (Ours) ( ?)</cell><cell cols="4">27.7 32.1 29.1 28.9</cell><cell>30.0</cell><cell>33.9</cell><cell>33.0</cell><cell>31.2</cell><cell cols="2">37.0 39.3</cell><cell>30.0</cell><cell>31.0</cell><cell>29.4</cell><cell>22.2</cell><cell>23.0</cell><cell>30.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>M</cell><cell>MPJPE (mm)</cell></row><row><cell>Li et al. (CVPR'19) [17]</cell><cell>5</cell><cell>52.7</cell></row><row><cell>Sharma et al. (ICCV'19) [37]</cell><cell>200</cell><cell>46.8</cell></row><row><cell>Oikarinen (IJCNN'21) [34]</cell><cell>200</cell><cell>46.2</cell></row><row><cell>Wehrbein et al. (ICCV'21) [42]</cell><cell>200</cell><cell>44.3</cell></row><row><cell>MHFormer (Ours)</cell><cell>3</cell><cell>43.0</cell></row></table><note>Comparison with the methods of generating multiple 3D pose hypotheses on Human3.6M. The number of hypotheses is denoted as M . Blod: best; Underlined: second best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 (</head><label>8</label><figDesc>top). Without bells and whistles, our MHFormer outperforms all previous state-of-the-art methods by a large margin under both Protocol 1 (43.0 mm) and Protocol 2 (34.4 mm, see supplemental material). Compared to the very recent Transformerbased method, i.e., PoseFormer [49], MHFormer noticeably surpasses it by 1.3 mm in MPJPE (relative 3% improvement).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison with the state-of-the-art methods on MPI-INF-3DHP. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>PCK ?</cell><cell>AUC ?</cell><cell>MPJPE ?</cell></row><row><cell>Mehta et al. (3DV'17) [31]</cell><cell>75.7</cell><cell>39.3</cell><cell>117.6</cell></row><row><cell>Lin et al. (BMVC'19) [22]</cell><cell>83.6</cell><cell>51.4</cell><cell>79.8</cell></row><row><cell>VPose (CVPR'19) [36]</cell><cell>86.0</cell><cell>51.9</cell><cell>84.0</cell></row><row><cell>Li et al. (CVPR'20) [20]</cell><cell>81.2</cell><cell>46.1</cell><cell>99.7</cell></row><row><cell>Anatomy3D (TCSVT'21) [4]</cell><cell>87.9</cell><cell>54.0</cell><cell>78.8</cell></row><row><cell>PoseFormer (ICCV'21) [49]</cell><cell>88.6</cell><cell>56.4</cell><cell>77.1</cell></row><row><cell>MHFormer (Ours)</cell><cell>93.8</cell><cell>63.3</cell><cell>58.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on different receptive fields with MPJPE (mm). CPN -cascaded pyramid network; GT -2D ground truth.</figDesc><table><row><cell></cell><cell>9</cell><cell>27</cell><cell>81</cell><cell>243</cell><cell>351</cell></row><row><cell>CPN</cell><cell>47.8</cell><cell>45.9</cell><cell>44.5</cell><cell>43.2</cell><cell>43.0</cell></row><row><cell>GT</cell><cell>36.6</cell><cell>34.3</cell><cell>32.7</cell><cell>30.9</cell><cell>30.5</cell></row></table><note>as inputs. The results are shown in Table 8 (bottom). It can be seen that our method achieves the best performance (30.5 mm in MPJPE), outperforming all other methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on different parameters of MHG. Here, L1 is the number of MHG layers and M is the hypothesis number.</figDesc><table><row><cell>M</cell><cell>L 1</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>MPJPE (mm)</cell></row><row><cell>3</cell><cell>2</cell><cell>18.91</cell><cell>1.03</cell><cell>46.4</cell></row><row><cell>3</cell><cell>3</cell><cell>18.92</cell><cell>1.03</cell><cell>46.3</cell></row><row><cell>3</cell><cell>4</cell><cell>18.92</cell><cell>1.03</cell><cell>45.9</cell></row><row><cell>3</cell><cell>5</cell><cell>18.93</cell><cell>1.04</cell><cell>46.1</cell></row><row><cell>1</cell><cell>4</cell><cell>6.32</cell><cell>0.34</cell><cell>47.6</cell></row><row><cell>2</cell><cell>4</cell><cell>12.61</cell><cell>0.69</cell><cell>46.7</cell></row><row><cell>3</cell><cell>4</cell><cell>18.92</cell><cell>1.03</cell><cell>45.9</cell></row><row><cell>4</cell><cell>4</cell><cell>25.22</cell><cell>1.38</cell><cell>46.9</cell></row></table><note>on Human3.6M dataset under Protocol 1 with MPJPE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Ablation study on different parameters of SHR and CHI. Here, L2 and L3 indicate the number of SHR and CHI layers, respectively. C is the embedding dimension.</figDesc><table><row><cell>L 2</cell><cell>L 3</cell><cell>C</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>MPJPE (mm)</cell></row><row><cell>2</cell><cell>1</cell><cell>256</cell><cell>4.72</cell><cell>0.26</cell><cell>47.2</cell></row><row><cell>2</cell><cell>1</cell><cell>384</cell><cell>10.65</cell><cell>0.58</cell><cell>46.4</cell></row><row><cell>2</cell><cell>1</cell><cell>512</cell><cell>18.92</cell><cell>1.03</cell><cell>45.9</cell></row><row><cell>2</cell><cell>1</cell><cell>768</cell><cell>42.50</cell><cell>2.31</cell><cell>47.4</cell></row><row><cell>2</cell><cell>1</cell><cell>512</cell><cell>18.92</cell><cell>1.03</cell><cell>45.9</cell></row><row><cell>1</cell><cell>3</cell><cell>512</cell><cell>25.20</cell><cell>1.38</cell><cell>46.7</cell></row><row><cell>2</cell><cell>2</cell><cell>512</cell><cell>25.20</cell><cell>1.38</cell><cell>46.8</cell></row><row><cell>3</cell><cell>1</cell><cell>512</cell><cell>25.20</cell><cell>1.38</cell><cell>46.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on different components of our MHFormer. Here, * means no multi-level features in MHG.</figDesc><table><row><cell>Method</cell><cell>MHG</cell><cell>SHR</cell><cell>CHI</cell><cell>MPJPE (mm)</cell></row><row><cell>Baseline (C=256)</cell><cell></cell><cell></cell><cell></cell><cell>49.9</cell></row><row><cell>Baseline (C=512)</cell><cell></cell><cell></cell><cell></cell><cell>47.8</cell></row><row><cell>Baseline (C=1536)</cell><cell></cell><cell></cell><cell></cell><cell>50.4</cell></row><row><cell>SHR-CHI</cell><cell></cell><cell></cell><cell></cell><cell>47.2</cell></row><row><cell>MHG-SHR</cell><cell></cell><cell></cell><cell></cell><cell>46.5</cell></row><row><cell>MHG-CHI</cell><cell></cell><cell></cell><cell></cell><cell>46.8</cell></row><row><cell>MHFormer  *</cell><cell></cell><cell></cell><cell></cell><cell>46.5</cell></row><row><cell>MHFormer (Ours)</cell><cell></cell><cell></cell><cell></cell><cell>45.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>?</head><label></label><figDesc>Baseline: The baseline model contains 3 layers for standard Transformer encoder (same architecture as ViT [6]). ? SHR-CHI: We remove the MHG module. SHR-CHI contains L 2 =2 SHR and L 3 =1 CHI layers. ? MHG-SHR: We replace the CHI layers in MHFormer with SHR layers. MHG-SHR contains L 1 =4 MHG and L 3 =3 SHR layers. ? MHG-CHI: We replace the SHR layers in MHFormer with CHI layers. SHR-CHI contains L 1 =4 MHG and L 3 =3 CHI layers. ? MHFormer * : The MHG in MHFormer is simply built upon several parallel Transformer encoders. ? MHFormer: Our proposed method that contains L 1 =4</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Quantitative comparison with the state-of-the-art methods on Human3.6M under Protocol 2. ( ?) -uses temporal information. Blod: best; Underlined: second best.</figDesc><table><row><cell>MHFormer (Ours) ( ?)</cell><cell>31.5 34.9 32.8 33.6</cell><cell>35.3</cell><cell>39.6</cell><cell>32.0</cell><cell>32.2</cell><cell>43.5 48.7</cell><cell>36.4</cell><cell>32.6</cell><cell>34.3</cell><cell>23.9</cell><cell>25.1</cell><cell>34.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Quantitative comparison on Human3.6M under MPJPE. Blod: best; Underlined: second best.</figDesc><table><row><cell>Method</cell><cell>Refine module</cell><cell>MPJPE (mm)</cell></row><row><cell>MGCN (ICCV'21) [50]</cell><cell></cell><cell>49.4</cell></row><row><cell>ST-GCN (ICCV'19) [2]</cell><cell></cell><cell>48.8</cell></row><row><cell>UGCN (ECCV'20) [40]</cell><cell></cell><cell>45.6</cell></row><row><cell>UGCN (ECCV'20) [40]</cell><cell></cell><cell>44.5</cell></row><row><cell>MHFormer (Ours)</cell><cell></cell><cell>43.0</cell></row><row><cell>MHFormer (Ours)</cell><cell></cell><cell>42.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Ablation study on different configurations of MH-CA on Human3.6M under MPJPE. Here, * means using the same input between keys and values in MH-CA.</figDesc><table><row><cell>Method</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>MPJPE (mm)</cell></row><row><cell>MH-CA  *</cell><cell>22.07</cell><cell>1.21</cell><cell>46.1</cell></row><row><cell>MH-CA</cell><cell>18.92</cell><cell>1.03</cell><cell>45.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Aston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CrossViT: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anatomy-aware 3D human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human-computer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Errity</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">An Introduction to Cyberpsychology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="263" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PoseAug: A differentiable pose augmentation framework for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8575" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15908" to="15919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TransReID: Transformer-based object Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15013" to="15022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional directed graph convolution for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (ACMMM)</title>
		<meeting>the ACM International Conference on Multimedia (ACMMM)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="602" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating multiple diverse hypotheses for human 3D pose consistent with 2D joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Jahangiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops (ICCVW)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-instance pose networks: Rethinking top-down pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rawal</forename><surname>Khirodkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visesh</forename><surname>Chari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3122" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Propagating LSTM: 3D pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3D human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised generative network for multiple 3D human pose hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3D human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting temporal contexts with strided transformer for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Trajectory space factorization for deep video-based 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen-Ching</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A video is worth three views: Trigeminal transformers for video-based person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01745</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context modeling in 3D human pose estimation: A unified perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6238" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">VNect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GraphMDN: Leveraging graph structure and deep learning to solve inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Oikarinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohrob</forename><surname>Kazerounian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><forename type="middle">Teja</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashast</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Motion guided 3D pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Depth pooling based large-scale 3-d action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1051" to="1061" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probabilistic monocular 3D human pose estimation with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wehrbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11199" to="11208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CDTrans: Cross-domain transformer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongkun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16105" to="16114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transformer-based attention networks for continuous pixel-wise prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16269" to="16279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SRNet: Improving generalization in 3D human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11436" to="11445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">3D human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modulated graph convolutional network for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11477" to="11487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Disc Eat Greet Phone Photo Pose Purch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Method</forename><surname>Dir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simplebaseline</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vpose</surname></persName>
		</author>
		<idno>CVPR&apos;19) [36] ( ?) 34</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
