<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMANTIC ROLE AWARE CORRELATION TRANSFORMER FOR TEXT TO VIDEO RETRIEVAL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Satar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Hongyuan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
							<email>xavier@nus.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><forename type="middle">Hwee</forename><surname>Lim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute for Infocomm Research</orgName>
								<orgName type="department" key="dep2">A*STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEMANTIC ROLE AWARE CORRELATION TRANSFORMER FOR TEXT TO VIDEO RETRIEVAL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video understanding</term>
					<term>text-to-video re- trieval</term>
					<term>transformer</term>
					<term>multi-modal</term>
					<term>hierarchical</term>
					<term>cross-modal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the emergence of social media, voluminous video clips are uploaded every day, and retrieving the most relevant visual content with a language query becomes critical. Most approaches aim to learn a joint embedding space for plain textual and visual contents without adequately exploiting their intra-modality structures and inter-modality correlations. This paper proposes a novel transformer that explicitly disentangles the text and video into semantic roles of objects, spatial contexts and temporal contexts with an attention scheme to learn the intra-and inter-role correlations among the three roles to discover discriminative features for matching at different levels. The preliminary results on popular YouCook2 indicate that our approach surpasses a current state-of-the-art method, with a high margin in all metrics. It also overpasses two SOTA methods in terms of two metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the popularity of TikTok, Youtube and Instagram, millions of videos are uploaded every minute, making video retrieval an important function for users to find relevant content. Conventional models are based on keywords query <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, unstructured keywords are limited and insufficient to retrieve fine-grained and compositional events. Therefore, communities are shifting attention to cross-modal video text retrieval <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> which can retrieve videos using natural language descriptions that includes more structured details.</p><p>Most existing cross-modal text video retrieval maps each modality into a joint embedding space to measure their similarities. Some works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9]</ref> directly embeds whole videos and texts into flat vectors for matching, however losing finegrained details in texts and videos. To avoid losing those This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project A18A2b0046). Xavier Bresson is supported by the NRF Fellowship NRFF2017-10. Correspondence to Dr. <ref type="bibr">Zhu Hongyuan.</ref> details, some other works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> align a sequence of frames and words in texts to compute overall similarities. Although these approaches have achieved certain progress, aligning video and text is still an open problem, given a huge semantic gap between video and text.</p><p>Recently, there are different attempts to resolve the videotext semantic gap. <ref type="bibr" target="#b2">[3]</ref> propose decomposing text into three semantic roles (events, actions and entities) and then embedding 2D video features into these three spaces accordingly for matching. Another line of research uses a BERT-like transformer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> to learn the text-video correspondence, based on recent mixture-of-expert embedding <ref type="bibr" target="#b4">[5]</ref> which requires large-scale dataset for pre-training.</p><p>We propose a novel transformer architecture for videotext matching inspired by <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Different from <ref type="bibr" target="#b2">[3]</ref>, which only considers multi-head embedding of the spatial frame and ignores the interaction between different visual contexts, our method explicitly considers more finegrained visual encoding of object contexts, spatial contexts and temporal contexts by embedding RoI regions, 2D frames and video sequences into the corresponding space with their interactions. Different from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which only uses self-attention to discover modality-specific information, our method uses a self-attention scheme to discover modalityspecific discriminative features. Moreover, our model utilizes cross-modal attention to consider the interactions between object, spatial and temporal contexts to discover modalitycomplement features for better align video and text.</p><p>We experimented with the YouCook2 dataset for the textto-video retrieval task. The results show that our approach surpasses a recent SOTA with a high margin. While our approach gives better results over other approaches in terms of two parameters(R@1, R@5), it falls a close behind them in terms of the other two parameters(R@10, MedR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The text-video retrieval task is challenging because there are significant semantic and structure gap between videos and <ref type="figure">Fig. 1</ref>. Overview of our model on text-to-video retrieval, where the textual input is a caption of a video clip, and the visual input are the visual expert features from RoI, 2D Frames and 3D clips. It computes the similarity between the caption and a candidate video clip in the three embedding spaces correspond to object contexts, spatial contexts and temporal contexts by embracing transformer with self-attention and cross-modal attention that can capture the specific and complement information within three semantic role modalities. In the cross-modal matching unit, a matching score is calculated for three-level of embeddings. Then, we average the similarities and utilize contrastive ranking loss as a training objective.</p><p>text. Mithun et al. <ref type="bibr" target="#b3">[4]</ref> employ multimodal image, motion, audio modalities in video. Liu et al. <ref type="bibr" target="#b7">[8]</ref> further utilize all modalities that can be extracted from videos such as speech contents and scene texts for video encoding. Dong et al. <ref type="bibr" target="#b8">[9]</ref> use biGRU and CNN to encode sequential videos and texts. Yu et al. <ref type="bibr" target="#b10">[11]</ref> propose to fuse the sequential interaction of videos and texts for video text retrieval. Song et al. <ref type="bibr" target="#b9">[10]</ref> employ to align encoded video and text elements for matching. Tan et al. <ref type="bibr" target="#b14">[15]</ref> use LSTM and Graph Neural Networks to capture cross-modal relation. Chen et al. <ref type="bibr" target="#b2">[3]</ref> disentangle phrases into different part-of-speech such as verbs and nouns for fine-grained retrieval, and also it uses GNNs to encode the interactions between the roles in text modalities. Our work complements Chen et al. <ref type="bibr" target="#b2">[3]</ref> by focusing on video encoding parts using a transformer to learn interactions between object contexts, spatial contexts and video contexts. Some methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> extend BERT-like models with self-supervised learning to have better video-text representation. For example, while Sun et al. <ref type="bibr" target="#b15">[16]</ref> build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens. Zhu et al. <ref type="bibr" target="#b16">[17]</ref> aim to encode global actions, local regions, objects, and linguistic descriptions with self-supervised learning without considering the linguistic and visual structures. Our work aims to exploit transformer like architecture to learn interactions for better object contexts, spatial context and video context encoding.</p><p>Most recent papers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> use complex datasets such as <ref type="bibr" target="#b5">[6]</ref> for pre-training to have a more general textual and visual embeddings. Alayrac et al. <ref type="bibr" target="#b17">[18]</ref> even use an audio-based dataset <ref type="bibr" target="#b18">[19]</ref> to have audio embedding. In this current work, we have yet to exploit pre-training on a huge dataset moti-vated by studying the computational time and the effects of pre-training absence, although our work can incorporate pretrained backbones. <ref type="figure">Figure 1</ref> is an illustration of our semantic role aware correlation transformer model, which includes three main blocks: textual encoding, visual encoding, and cross-modal matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Textual Encoding</head><p>Text description internally has a hierarchical structure. For example, the whole sentence can describe global contexts; nouns and verbs can define object and actions. We use an offthe-shelf toolkit <ref type="bibr" target="#b19">[20]</ref> for semantic role labelling to parse these components and the semantic relationship between them in a graph structure. Verb nodes are directly connected with the global node by showing temporal relations of various actions. Noun nodes are connected with verb nodes by defining the objects. r ij edges show the semantic relationship between verb nodes i and object nodes j. For graph representation, we follow the method proposed by Chen et al. <ref type="bibr" target="#b2">[3]</ref>. This approach, which is shown in Eq. 1, uses factorized weights in GCN, where W t is a transformation matrix and shared among all relationship types. W t denotes a unique matrix for different semantic roles. g a and g b denote node embeddings which can be for sentences, verbs or objects. ? is the outcome after attention is applied at the nodes.</p><formula xml:id="formula_0">g l i = g l?1 i + j?Ni (? ij (W l t W r r ij )g j )<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Encoding</head><p>Parsing videos into hierarchical semantic features is more challenging than texts since it includes detection, action segmentation and so on. We extract spatial context features F S with 2D CNNs by ResNet-152 <ref type="bibr" target="#b20">[21]</ref>, pre-trained on Imagenet <ref type="bibr" target="#b21">[22]</ref>. The temporal context features F T are extracted with 3D CNNs by ResNeXt-101 <ref type="bibr" target="#b22">[23]</ref> pre-trained on Kinetics <ref type="bibr" target="#b23">[24]</ref>, then we perform temporal max-pooling on feature maps. We extract object context features F O by using Faster R-CNN <ref type="bibr" target="#b24">[25]</ref>, pre-trained on MS COCO <ref type="bibr" target="#b25">[26]</ref>, and the backbone is ResNet-101 <ref type="bibr" target="#b20">[21]</ref>. We follow the settings of Zhu et al. <ref type="bibr" target="#b16">[17]</ref> by extracting the features at 1 FPS after RoI-pooling happens. Confidence threshold is set as 0.4, while each frame could contain up to ten boxes. Dimension for all three expert embedding is 2048.</p><p>To capture the modality-complement and specific information, we adapt a transformer with attention modules <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. For example, to generate spatial context-specific feature s e , the temporal context features F T , and object context features F O are concatenated in feature dimension and fed into an encoder with a multi-head self-attention layer and a feed-forward linear layer FF, as shown in Eq. 2. </p><formula xml:id="formula_1">Head i = Attention(QW Q i , KW K i , V W V i ) Attention(Q, K, V ) = ? QK T ? d V<label>(3)</label></formula><p>Eq. 4 shows the formula of generating spatial contextcomplement feature. The spatial context feature F S is first fed into the self-attention layer to encode into z s , then with crossmodal attention is conditioned on s e and z s to generate complement feature c e . Next, it is encoded through feed-forward linear layer FF and together with the modality-specific feature s e to deliver final embeddings for the spatial context E S . 'Norm' refers to layer normalization. </p><p>The same process is applied to generate the final embeddings E T and E O for temporal context feature F T and object context feature F O , respectively, for matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-modal Matching</head><p>We utilize cosine similarity to calculate the cross-modal matching score for each level by corresponding visual and textual embeddings.</p><formula xml:id="formula_3">s(V, C) = &lt; v, c &gt; ||v|| 2 ||c|| 2<label>(5)</label></formula><p>Then, we average the similarities and utilize contrastive ranking loss as a training objective. Our aim is to have such positive pair (v p , c p ) pushed away from the negative pairs (v p , c n ) and (v n , c p ) than a set margin. v and c refer to the video and textual representation, respectively. This matching applies to three levels of embeddings. ? represents the predefined margin, which is for training with contrastive loss.</p><formula xml:id="formula_4">L(v p , c p ) = [? + s(v p , c n ) ? s(v p , c p )]+ [? + s(v n , c p ) ? s(v p , c p )]<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We compare our model with state-of-the-art methods on textto-video retrieval task, shown in <ref type="table" target="#tab_0">Table 1</ref>. We also share an ablation study using various expert embeddings in different hierarchical levels, shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metrics</head><p>Dataset. We evaluate our model on YouCook2 <ref type="bibr" target="#b28">[29]</ref>. It is a video dataset on cooking gathered from YouTube. The videos have a diverse number of cooking styles and methods. It includes 89 recipe types and 14k video clips correlated by imperative English captions defining the action. Note that since annotations for the test set is not published yet, we evaluate the task on validation clips which is around 3.5k totally. Evaluation metrics. The task is retrieving video clips based on text queries. We evaluate our model using common metrics on recall at various sets and median rank. R@1, R@5, R@10 gives the number of accurately retrieved clips in the ranking list's top associated rates. MedR gives the median rank of correct clips in the ranking list. While higher is better for recall metrics, lower is better for the median rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Training. We use Glove embeddings <ref type="bibr" target="#b29">[30]</ref> by setting the word embedding size as 300 for the text encoding. We adopt the text encoding approach of <ref type="bibr" target="#b2">[3]</ref> to disentangle the text embeddings. Graph convolutions have two layers, and it outputs features with 1024 dimensions. A linear layer is applied to  <ref type="table">Table 2</ref>. Ablation studies on YouCook2 dataset to investigate the contributions of various feature experts at different levels.</p><p>The same ablation is also done on HGR method <ref type="bibr" target="#b2">[3]</ref> since it is a strong baseline. On 2D + 3D visual features setting, when the feature dimension is 4096, concatenation is done on dimension one; otherwise is done on dimension zero. Our model surpasses HGR with the same hierarchical features with a high margin by using cross-modal attention. each visual expert embeddings to transform their dimensions into 1024. We use a cross-modal attention mechanism for each level in visual encoding. For training, the margin is set to ? = 0.2; the epoch is 100 for each experiment. The minibatch size is 32. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison with SOTA methods. While 'Visual Backbone' in the table only refers to 3D CNNs Features, ResNet-based models are also used in all the models as 2D CNNs backbone. We focus on training only with YouCook2 dataset; however, we also add the methods when using pre-training for reference. 'FT' denotes fine-tuning on YouCook2 dataset. We see that the accelerator type, which directly defines the maximum batch size, and pre-training usage affects the result sharply. There are even differences with the same model when training on different accelerators, epoch, and batch sizes; which can be found on corresponding papers. For example, the MIL-NCE method reaches 50% of its accuracy when trained with less batch size and epochs. Our method surpasses the HGR method <ref type="bibr" target="#b2">[3]</ref> with a high margin for all metrics. We also outperform the other two SOTA methods: Miech et al. <ref type="bibr" target="#b5">[6]</ref>, and HGLMM <ref type="bibr" target="#b27">[28]</ref>, in terms of the first two parameters. We think that modality-specific and modality-complement features improve accuracy at R@1 and R@5, which are more demanding and useful for real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Since we adopt our model from HGR <ref type="bibr" target="#b2">[3]</ref>, we implement extensive ablation to show the improvement in our approach on the visual encoding part. We aim to find how the feature expert combination affects the result. Our model falls short when we feed three levels with only 2D features. The same decrease continues with the concatenated 2D and 3D features at dimension zero as well as the concatenated 2D and 3D features at dimension one. However, when we feed them with 2D, 3D, and RoI features respectively, while the HGR model shows a slight decrease, our model reaches a better result with a high margin. This confirms our insight that inter-modal correlation can be exploited with our proposed cross-modal attention mechanism to achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>Retrieving related video on a textual query gets harder since the number of videos on the internet increases. Most works use one joint embedding space for text-to-video retrieval task without fully exploiting cross-modal features. We propose a hierarchical model representing complex textual and visual features with three joint embedding spaces by utilizing selfattention and cross-modal attention to exploit the modalityspecific and modality-complement visual embeddings. Our model surpasses a strong baseline with a high margin, and it also overpasses other SOTA methods in R@1, R@5 metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 )</head><label>2</label><figDesc>f e = Concat(F T , F O ) z e = Norm MultiHead(f e , f e , f e ) + f e s e = Norm FF(z e ) + z e (The attention layers are multi-headed as Vaswani et al. [12]'s dot-product attention, and each layer also follows layer normalization and residual connection, as shown in Eq. 3. All the W matrices are trainable parameters. MultiHead(Q, K, V ) = Concat(Head 1 , ..., Head h )W O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>z s = Norm MultiHead(F S , F S , F S ) + F S c e = Norm MultiHead(s e , z s , z s ) + z s E S = Norm FF(c e ) + c e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Text-to-video retrieval comparison with SOTA approaches on YouCook2 validation set. 'Visual Backbone' only refers to 3D CNNs Features. Our method surpasses the SOTA methods in the first two parameters when without pre-training.</figDesc><table><row><cell>Method</cell><cell>Pre-training</cell><cell cols="6">Visual Backbone Batch Size R@1? R@5? R@10? MedR?</cell></row><row><cell>Random</cell><cell>No</cell><cell>-</cell><cell>-</cell><cell>0.03</cell><cell>0.15</cell><cell>0.3</cell><cell>1675</cell></row><row><cell>Miech et al [6]</cell><cell>No</cell><cell>ResNeXt-101</cell><cell>-</cell><cell>4.2</cell><cell>13.7</cell><cell>21.5</cell><cell>65</cell></row><row><cell>HGLMM [28]</cell><cell>No</cell><cell>-</cell><cell>-</cell><cell>4.6</cell><cell>14.3</cell><cell>21.6</cell><cell>75</cell></row><row><cell>HGR [3]</cell><cell>No</cell><cell>ResNeXt-101</cell><cell>32</cell><cell>4.7</cell><cell>14.1</cell><cell>20.0</cell><cell>87</cell></row><row><cell>Ours</cell><cell>No</cell><cell>ResNeXt-101</cell><cell>32</cell><cell>5.3</cell><cell>14.5</cell><cell>20.8</cell><cell>77</cell></row><row><cell>Miech et al+FT [6]</cell><cell>HowTo100M</cell><cell>ResNeXt-101</cell><cell>-</cell><cell>8.2</cell><cell>24.5</cell><cell>35.3</cell><cell>24</cell></row><row><cell>ActBert [17]</cell><cell>HowTo100M</cell><cell>ResNet-3D</cell><cell>-</cell><cell>9.6</cell><cell>26.7</cell><cell>38.0</cell><cell>19</cell></row><row><cell>MMV FAC [18]</cell><cell>HowTo100M+AudioSet</cell><cell>TSM-50</cell><cell>4096</cell><cell>11.5</cell><cell>30.2</cell><cell>41.5</cell><cell>16</cell></row><row><cell>MIL-NCE [7]</cell><cell>HowTo100M</cell><cell>S3D</cell><cell>8192</cell><cell>15.1</cell><cell>38.0</cell><cell>51.2</cell><cell>10</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic concept discovery for large-scale zero-shot event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Composite concept discovery for zero-shot video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ICMR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on ICMR</title>
		<meeting>the 2018 ACM on ICMR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a textvideo embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in arXiv</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9338" to="9347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Polysemous visualsemantic embedding for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal Transformer for Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">wman: Weakly-supervised moment alignment network for textbased video segment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple bert models for relation extraction and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li F.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ms coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<biblScope unit="page" from="7590" to="7598" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
