<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Multimodal Fusion</term>
					<term>Multitask Learning</term>
					<term>Channel Exchanging</term>
					<term>Semantic Segmentation</term>
					<term>Image-to-Image Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal fusion and multitask learning are two vital topics in machine learning. Despite the fruitful progress, existing methods for both problems are still brittle to the same challenge-it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task). Besides, while they are actually closely related to each other, multimodal fusion and multitask learning are rarely explored within the same methodological framework before. In this paper, we propose Channel-Exchanging-Network (CEN) which is self-adaptive, parameter-free, and more importantly, applicable for multimodal and multitask dense image prediction. At its core, CEN adaptively exchanges channels between subnetworks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. For the application of dense image prediction, the validity of CEN is tested by four different scenarios: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of CEN compared to state-of-the-art methods. Detailed ablation studies have also been carried out, which demonstrate the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>E NCOURAGED by the growing availability of low-cost sensors, multimodal fusion that takes advantage of multiple data sources for classification or regression becomes one of the central problems in machine learning <ref type="bibr" target="#b0">[1]</ref>. Joining the success of deep learning, multimodal fusion is recently specified as deep multimodal fusion by introducing end-to-end neural integration of multiple modalities <ref type="bibr" target="#b1">[2]</ref>, and it has exhibited remarkable benefits against the unimodal paradigm in semantic segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, action recognition <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[6]</ref>, <ref type="bibr" target="#b21">[7]</ref>, visual question answering <ref type="bibr" target="#b22">[8]</ref>, <ref type="bibr" target="#b23">[9]</ref>, and many others <ref type="bibr" target="#b24">[10]</ref>, <ref type="bibr" target="#b25">[11]</ref>, <ref type="bibr" target="#b26">[12]</ref>. Multitask learning <ref type="bibr" target="#b27">[13]</ref> is another crucial topic in machine learning. It aims to seek models to solve multiple tasks simultaneously, which enjoys the benefit of model generation and data efficiency against the methods that learn each task independently. Similar to multimodal fusion, multitask learning has also been developed from previously shallow methods <ref type="bibr" target="#b28">[14]</ref> to deep variants <ref type="bibr" target="#b29">[15]</ref>, <ref type="bibr" target="#b30">[16]</ref>, <ref type="bibr" target="#b31">[17]</ref>, <ref type="bibr" target="#b32">[18]</ref>, <ref type="bibr" target="#b33">[19]</ref> by taking advantage of deep learning. The successful applications of multitask learning include navigation <ref type="bibr" target="#b34">[20]</ref>, robot manipulation <ref type="bibr" target="#b35">[21]</ref>, etc.</p><p>In general, dense image prediction could be a collection of computer vision tasks that aim at classifying (e.g., segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b36">[22]</ref>, <ref type="bibr" target="#b37">[23]</ref>, <ref type="bibr" target="#b38">[24]</ref>) or regressing (e.g., imageto-image translation <ref type="bibr" target="#b39">[25]</ref>, <ref type="bibr" target="#b40">[26]</ref>, <ref type="bibr" target="#b41">[27]</ref>, <ref type="bibr" target="#b42">[28]</ref>) every pixel in an image, namely, producing pixel-wise output based on the given input pixels. The learning pipeline for dense prediction is usually expected to capture rich spatial details or Y. <ref type="bibr">Wang</ref>  strong semantics, which also benefits greatly from multimodal data sources or the multitask joint training. A variety of works tailored for dense image prediction have been done towards multimodal fusion and multitask learning. For multimodal fusion, regarding the type of how they fuse, existing methods are generally categorized into aggregationbased fusion <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b43">[29]</ref>, <ref type="bibr" target="#b44">[30]</ref>, alignment-based fusion <ref type="bibr" target="#b21">[7]</ref>, <ref type="bibr" target="#b45">[31]</ref>, and the mixture of them <ref type="bibr" target="#b0">[1]</ref>. As for multitask learning, in the context of deep learning, two types of contemporary techniques are identified: hard parameter-sharing <ref type="bibr" target="#b46">[32]</ref>, <ref type="bibr" target="#b47">[33]</ref> and soft parameter-sharing <ref type="bibr" target="#b29">[15]</ref>, <ref type="bibr" target="#b48">[34]</ref>. Despite the fruitful progress, existing methods for both problems are still brittle to the same challenge-it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task) for multimodal fusion (resp. multitask learning). To be more specific, for multimodal fusion, the aggregation-based fusion is prone to underestimating the intra-modal propagation, whereas the alignment-based fusion mostly delivers ineffective inter-modal fusion owing to the weak message exchanging by solely training alignment losses <ref type="bibr" target="#b44">[30]</ref>, <ref type="bibr" target="#b49">[35]</ref>, <ref type="bibr" target="#b50">[36]</ref>. A similar issue exists in multitask learning. Current hard/soft parameter sharing schemes could be vulnerable to the negative transfer issue across different tasks owing to the insufficient balance between inter-task knowledge sharing and intra-task information processing <ref type="bibr" target="#b51">[37]</ref>. When focusing on dense image prediction, multimodal fusion and multitask learning can also be regarded as the dual problem of each other. As will be described in ? 3, multimodal fusion corresponds to the multiple-input-singleoutput problem while multitask learning, inversely, is of the single-input-multiple-output formulation. Yet, most previous works study these two problems separately without revealing their common property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2112.02252v2 [cs.CV] 4 Oct 2022</head><p>In this paper, we propose Channel-Exchanging-Network (CEN) which is self-adaptive, parameter-free, and applicable for multimodal and multitask dense image prediction. For unification, we refer to both the modality-specific network in multimodal fusion and the task-specific network in multitask learning as a subnetwork. To enable message passing among different modalities/tasks, CEN adaptively exchanges the channels between subnetworks. The core of CEN lies in its smaller-norm-less-informative assumption inspired by network pruning <ref type="bibr" target="#b52">[38]</ref>, <ref type="bibr" target="#b53">[39]</ref>. To be specific, we utilize the scaling factor (i.e., ?) of Batch-Normalization (BN) <ref type="bibr" target="#b54">[40]</ref> or Instance-Normalization (IN) <ref type="bibr" target="#b55">[41]</ref> as the importance measurement for each corresponding channel, and replace the channels associated with close-to-zero factors of each subnetwork with the mean of other subnetworks. Such message exchanging is self-adaptive in determining when to exchange, and hence it is capable of accomplishing better trade-off between inter-subnetwork knowledge sharing and intra-subnetwork information processing, in contrast to conventional multimodal and multitask learning methods. Further, the channel exchanging operation itself is parameter-free, making CEN less prone to overfitting, while, for example, the attention-based fusion <ref type="bibr" target="#b3">[4]</ref> needs extra parameters to adjust the importance of each subnetwork. Another hallmark of CEN is that the encoder parameters except for BN layers of all subnetworks are shared with each other ( ? 3.2). Apart from compacting the model size, we apply the idea here to serve specific purposes in CEN: by using private BNs, we can determine the channel importance for each individual modality; by sharing convolutional filters, the corresponding channels among different modalities are embedded with the same mapping, thus more capable of modelling the modality-common statistic.</p><p>CEN is generally powerful, capable of addressing four different problems in image dense prediction: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. For multimodal fusion, we conduct channel exchanging on the encoder side to allow information integration between different input modalities. We also design cycle multimodal fusion to reuse the knowledge among different generation flows, which can promote performance for each flow. As natural extensions, channel exchanging could be applied to the decoder side or both the decoder and encoder to exchange task-specific information for multitask learning or for multimodal multitask learning. These details will be provided in ? 3.</p><p>To sum up, our contributions are as follows:</p><p>? We propose CEN for message fusion, which is selfadaptive and parameter-free. The core of CEN is to replace the channels associated with close-to-zero BN or IN scaling factors of each subnetwork with the mean of others.</p><p>? CEN is generally powerful and is applied to multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. To the best of our knowledge, it is the first time that one single technique is explicitly employed to address multimodal fusion, multitask learning, or both, particularly on dense image prediction.</p><p>? Experimental evaluations are conducted on semantic segmentation via RGB-D data <ref type="bibr" target="#b56">[42]</ref>, <ref type="bibr" target="#b57">[43]</ref> and image translation through multi-domain input <ref type="bibr" target="#b58">[44]</ref>. It demonstrates that CEN yields remarkably superior performance to various kinds of multimodal fusion methods and multitask learning methods under a fair condition of comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We introduce the methods of deep multimodal fusion and deep multitask learning, especially using dense image prediction as examples. We also discuss other related concepts. Deep multimodal fusion. Regarding dense image prediction, deep multimodal fusion uses multiple data sources to enhance pixel-level semantics and fine-grained details against the single-modality counterpart. To this end, related methods toward dense image prediction are basically categorized into aggregation-based fusion and alignment-based fusion. Aggregation-based fusion methods apply a certain operation (e.g., averaging <ref type="bibr" target="#b43">[29]</ref>, concatenation <ref type="bibr" target="#b44">[30]</ref>, <ref type="bibr" target="#b59">[45]</ref>, and attention-based modules <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b60">[46]</ref>) to fuse high-resolution feature maps and combine multimodal subnetworks into a single network. For example, U2Fusion <ref type="bibr" target="#b61">[47]</ref> concatenates source images and puts forward the information measurement for unsupervised learning. RDFNet <ref type="bibr" target="#b50">[36]</ref> adopts multilayer fusion and iteratively refines fused features with additional convolutional blocks for aggregation. Due to the weakness in intra-modal processing, recent aggregationbased works perform feature fusion while still maintaining the subnetworks of all modalities <ref type="bibr" target="#b49">[35]</ref>, <ref type="bibr" target="#b62">[48]</ref>. Alignmentbased fusion methods <ref type="bibr" target="#b21">[7]</ref>, <ref type="bibr" target="#b45">[31]</ref>, instead, adopt regulation losses to align the embedding of subnetworks while keeping full propagation for each of them. These methods align multimodal features by applying the similarity regulation, where Maximum-Mean-Discrepancy (MMD) <ref type="bibr" target="#b63">[49]</ref> is usually adopted for the measurement. However, simply focusing on unifying the whole distribution may overlook the specific patterns in each domain/modality <ref type="bibr" target="#b21">[7]</ref>, <ref type="bibr" target="#b64">[50]</ref>. Hence, <ref type="bibr" target="#b45">[31]</ref> provides a way that might alleviate this issue, which correlates modality-common features while simultaneously maintaining modality-specific information. Another categorization of multimodal fusion towards dense prediction could be generally specified as early, middle, and late fusion, depending on when to fuse, which have been discussed in earlier works <ref type="bibr" target="#b65">[51]</ref>, <ref type="bibr" target="#b66">[52]</ref>, <ref type="bibr" target="#b67">[53]</ref>, <ref type="bibr" target="#b68">[54]</ref> and also in the current deep learning literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b69">[55]</ref>, <ref type="bibr" target="#b70">[56]</ref>, <ref type="bibr" target="#b71">[57]</ref>. Besides, evaluations in <ref type="bibr" target="#b50">[36]</ref> indicate that the single-layer fusion can not effectively exploit multimodal features, especially for addressing highresolution predictions torward dense image prediction. <ref type="bibr" target="#b43">[29]</ref> points out that the performance of dense feature fusion is highly affected by the choice of which layer to fuse. Beyond dense image prediction, there are other portions of the multimodal learning literature, e.g., based on modulation <ref type="bibr" target="#b71">[57]</ref>, <ref type="bibr" target="#b72">[58]</ref>, <ref type="bibr" target="#b73">[59]</ref>. Different from these categories of fusion methods, we propose a new fusion method by channel exchanging, which potentially enjoys the guarantee of both sufficient inter-model interactions and intra-modal learning.</p><p>Deep multitask learning. In general, multitask visual perception predicts multiple output domains based on one same vision domain. Typical approaches could include designing hard parameter-sharing and soft parameter-sharing.</p><p>Specifically, hard parameter-sharing imposes a fixed subset of hidden layers to be shared across tasks and other layers to be task-specific, for example, UberNet <ref type="bibr" target="#b46">[32]</ref>, U2Fusion <ref type="bibr" target="#b61">[47]</ref>, and others <ref type="bibr" target="#b47">[33]</ref>, <ref type="bibr" target="#b74">[60]</ref>, <ref type="bibr" target="#b75">[61]</ref>. Differently, for soft (or partial) parameter-sharing, there could be a separate set (or a significant fraction) of parameters per task, and models are correlated either by adaptive feature sharing or by aligning parameters to be similar, for example, Cross-stitch <ref type="bibr" target="#b29">[15]</ref>, Sluice <ref type="bibr" target="#b48">[34]</ref>, and NDDR <ref type="bibr" target="#b76">[62]</ref>. Yet, compared with the learning upon single modalities, multitask learning is not always beneficial, since the performance might be harmed by the negative transfer (negative knowledge transfer across tasks), which is discussed in <ref type="bibr" target="#b77">[63]</ref>, <ref type="bibr" target="#b78">[64]</ref>, <ref type="bibr" target="#b79">[65]</ref>. In addition, many multitask learning methods are specifically designed for dense image prediction, which is also the main focus of this paper. For example, MTI-Net <ref type="bibr" target="#b80">[66]</ref> distills dense features across different tasks with multimodal feature aggregation. <ref type="bibr" target="#b79">[65]</ref>, <ref type="bibr" target="#b81">[67]</ref> explicitly enforce cycle-based consistency between domains to improve performance and generalization. U2Fusion <ref type="bibr" target="#b61">[47]</ref> develops joint training and sequential training that leverages a shared model to handle multitask learning for imageto-image translation. In this paper, we integrate the benefits of both hard parameter-sharing and soft parameter-sharing. Specifically, for multitask learning, we share the parameters of encoders for all tasks (hard parameter-sharing) and then conduct CEN on decoders (soft parameter-sharing).</p><p>Other related concepts. The idea of using the BN scaling factor to evaluate the importance of CNN channels has been studied in network pruning <ref type="bibr" target="#b52">[38]</ref>, <ref type="bibr" target="#b53">[39]</ref> and representation learning <ref type="bibr" target="#b82">[68]</ref>. Moreover, <ref type="bibr" target="#b52">[38]</ref> enforces 1 norm penalty on the scaling factors and explicitly prunes out filters meeting sparsity criteria. Here, we apply this idea as an adaptive tool to determine where to exchange and fuse. CBN <ref type="bibr" target="#b71">[57]</ref> performs cross-modal message passing by modulating BN of one modality conditional on the other, which is different from our method that directly exchanges channels across modalities for fusion. ShuffleNet <ref type="bibr" target="#b83">[69]</ref> proposes to shuffle a portion of channels among multiple groups for efficient propagation in light-weight networks, which is similar to our idea of exchanging channels for message fusion. Yet, while the motivation of our paper is highly different, the exchanging process is self-determined by the BN scaling factors, instead of the random exchanging in ShuffleNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CHANNEL EXCHANGING NETWORKS</head><p>We first introduce the general formulation of CEN, and then follow it up by specifying the design of four different settings: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The general mechanism</head><p>For either multimodal or multitask learning, we are interested in studying the relationship between subnetworks on different streams of input-output pairs. Suppose we have the data of M streams {(x m , y m )} M m=1 , where x m and y m represent the input data point and output label, respectively. The subnetwork of the m-th stream is dubbed as f m . The notion of "stream" can be flexibly specified: for multimodal fusion, a different stream corresponds to a different modality where x m varies but y m keeps unchanged in terms of different m; for multitask learning, on the contrary, a different stream implies a different task, where x m usually keeps the same and y m represents the label for task m.</p><p>A trivial training paradigm is minimizing the loss of each subnetwork f m independently, which leads to the loss between the prediction? m := f m (x m ) and the label y m 1 ,</p><formula xml:id="formula_0">min f 1:M M m=1 L f m (x m ), y m .<label>(1)</label></formula><p>However, the independent training strategy fails to characterize the affinity between different streams, limiting the expressivity of multimodal information fusion or multitask knowledge transfer.</p><p>In this work, we propose CEN that adaptively exchanges the knowledge between different subnetworks in an end-toend manner. In form, the training objective in Eq. 1 can be rewritten as</p><formula xml:id="formula_1">min f 1:M M m=1 L f m (x 1:M ), y m + ? ? m 1<label>(2)</label></formula><p>where,</p><p>? The subnetwork f m (x 1:M ) (instead of f m (x m ) in Eq. 1) fuses multimodal information by channel exchanging from other subnetworks to the m-th subnetwork, as we will detail later;</p><p>? Each subnetwork is equipped with BN layers containing the scaling factors ? m , and we will penalize the 1 norm of their certain portion? m for sparsity. The 1 norm is uniformly applied to all BN layers.</p><p>Here, we omit the layer index for simplicity.</p><p>Prior to introducing the mechanism of channel exchanging, we first review the Batch-Normalization (BN) layer <ref type="bibr" target="#b54">[40]</ref>, which is used widely in deep learning to eliminate covariate shift and improve generalization. For a certain BN layer, we denote by x m the feature map of the m-th subnetwork, and by x m,c the c-th channel. The BN layer performs a normalization of x m followed by an affine transformation, namely,</p><formula xml:id="formula_2">x m,c = ? m,c x m,c ? ? m,c ? 2 m,c + + ? m,c ,<label>(3)</label></formula><p>where, ? m,c and ? m,c compute the mean and the standard deviation, respectively, of all activations over all pixel locations (H and W ) for the current mini-batch data; ? m,c and ? m,c are the trainable scaling factor and offset, respectively; is a small constant to avoid divisions by zero. The following layer takes {x m,c } c as input after a non-linear function.</p><p>The factor ? m,c in Eq. 3 evaluates the correlation between the input x m,c and the output x m,c during training. The gradient of the loss w.r.t. x m,c will approach 0 if ? m,c ? 0 at one training step, implying that x m,c will almost lose its influence to the final prediction and become redundant thereby at this traing step.</p><p>In addition, as will be shown in <ref type="figure">Fig. 8 (a)</ref>, if the scaling factor of one channel (with sparsity constraints) is lower than the small threshold at one training step, this channel 1. Note that this loss should be summed over all data points in real implementation. Here we consider a single data point throughout the paper for simplicity.  <ref type="figure">Fig. 1</ref>: An illustration of CEN. The sparsity constraints on scaling factors are applied to disjoint channel regions of different modalities. A feature map will be replaced by that of other modalities at the same position, if its scaling factor is lower than a threshold.</p><p>will hardly recover and almost become redundant during the later training process.</p><p>It motivates us to replace the channels of small scaling factors with the ones of other subnetworks, since those channels potentially are redundant. To do so, we have</p><formula xml:id="formula_3">x m,c = ? ? ? ? ? ? m,c xm,c??m,c ? ? 2 m,c + + ? m,c , if ? m,c &gt; ?; 1 M ?1 M m =m ? m ,c x m ,c ?? m ,c ? 2 m ,c + + ? m ,c , else;<label>(4)</label></formula><p>where the current channel is replaced with the mean of other channels if its scaling factor is smaller than a certain threshold ? ? 0 + . In a nutshell, if one channel of one modality has little impact on the final prediction, then we replace it with the mean of other modalities. We apply Eq. 4 for each modality before feeding them into the nonlinear activation followed by the convolutions in the next layer. Gradients are detached from the replaced channel and backpropagated through the new ones. <ref type="figure">Fig. 11</ref> illustrates our channel exchanging process in each of the layers. In order to In our implementation, we equally divide the whole channels into M sub-parts and only perform the channel exchanging in each corresponding subpart for each modality. This is mainly to avoid a portion of channels being redundant w.r.t. all modalities. More detailed reasons are described in ? 4.5. We denote the scaling factors that are allowed to be replaced as? m . We further impose the sparsity constraint on? m in Eq. 2 to discover unnecessary channels. As the exchanging in Eq. 4 is a directed process within only one sub-part of channels, it hopefully can not only retain modal-specific propagation in the other M ? 1 sub-parts but also avoid unavailing exchanging since ? m ,c , different from? m,c , is out of the sparsity constraint.</p><p>Regarding specific tasks where Instance-Normalizations (INs) are used for normalization instead of BNs, the sparsity constraints are similarly applied to scaling factors of INs, and the channel exchanging design (Eq. 4) is still applicable.</p><p>We summarize the advantages of our CEN below:</p><p>? Prameter-free. As specified in Eq. 4, CEN involves no additional parameter and applies BN scaling factors to control the exchanging process.</p><p>? Self-adaptive. The channel exchanging could take place at every layer throughout the encoder or/and decoder. BN scaling factors are learned from the data, which adaptively balances the inter-subnetwork processing and inter-subnetwork fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal fusion via CEN on encoders</head><p>In this part, we focus particularly on multimodal fusion {x m } M m=1 ? y, where x m denotes the m-th input modality, and all subnetworks generate the same output y, i.e., y m = y, ?m = 1, ? ? ? , M . Given that this paper mainly copes with dense prediction problems (such as depth estimation or semantic segmentation), the subnetwork f m is of the encoder-decoder style. The goal of multimodal fusion is to effectively fuse the information of all modalities to improve the prediction accuracy for the target output. It is thus natural to fix the same decoder for all subnetworks and conduct CEN between their encoders. The architecture of multimodal fusion is depicted in <ref type="figure">Fig. 2 (a)</ref>.</p><p>We first carry out sparsity penalty on BN scaling factors for the m-th encoder following Eq. 2, and then perform channel exchanging. Besides, the final output of the decoder is an ensemble of all modalities associated with the decision scores {? m } M m=1 2 ; in our implementation, these decision scores are learned by an additional softmax output to meet the simplex constraint M m=1 ? m = 1. It is known in <ref type="bibr" target="#b84">[70]</ref> that leveraging individual BN layers characterizes the traits of different domains or modalities. In our method, specifically, different scaling factors (Eq. 3) evaluate the importance of the channels of different modalities, and they should be decoupled. With the exception of BN (or IN) layers, all subnetworks share all parameters (e.g. convolutional filters 3 ) in the encoder with each other. The hope is that we can further reduce the network complexity and therefore improve the predictive generalization. Rather, considering the specific design of our framework, sharing convolutional filters is able to capture the common patterns in different modalities, which is a crucial purpose of multimodal fusion. This design further compacts the multimodal architecture to be almost as small as the unimodal one, as will be evaluated in <ref type="table" target="#tab_4">Table 2</ref>. In our experiments, we conduct multimodal fusion on RGB-D images or on other domains of images corresponding to the same image content. In this scenario, all modalities are homogeneous in the sense that they are just different views of the same input. Thus, sharing parameters between different subnetworks still yields promisingly expressive power. Nevertheless, when we are dealing with heterogeneous modalities (e.g., images and text sequences), it would impede the expressive power of the subnetworks if keeping sharing their parameters, hence a more dexterous mechanism is suggested, and the discussion of which is left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cycle multimodal fusion via CEN on encoders</head><p>In the previous section ( ? 3.2), we have introduced how to apply CEN on multimodal fusion. Here, we discuss a more complicated setting: cycle multimodal fusion. Assuming</p><formula xml:id="formula_4">we have {x m } M m=1 ? x M +1 ,</formula><p>where the output is specified as the (M + 1)-th modality for consistent denotation. Note that such learning task is related to a different task 2. The decision scores are learnable scalars, optimized by comparing ensembled outputs with labels while temporally freezing (detaching) the subnetworks. The decision scores are fixed during inference.</p><p>3. If the input channels of different modalities are different (e.g., RGB and depth), we will broaden their sizes to be the same as their Least Common Multiple (LCM).  <ref type="figure">Fig. 2</ref>: Structures of CENs for multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. For cycle multimodal learning, given the case with three modalities, only two of the three forward passes are performed at each time. Here, "conv" and "indiv" are abbreviations for "convolutional" and "individual", respectively. L and L denote layer numbers of the encoder and the decoder, respectively.</p><formula xml:id="formula_5">{x m } M +1</formula><p>m=1,m =j ? x j , which, inversely, uses modality M + 1 along with the remaining modalities to generate modality j. Actually, we can go through all the M + 1 cases by cycling different output modality, which leads to a set of cycle multimodal fusion tasks</p><formula xml:id="formula_6">{T j := {x m } M +1 m=1,m =j ? x j } M +1 j=1</formula><p>. By ? 3.2, a straightforward way is applying CEN independently to each multimodal fusion task T j for fusing the input modalities. Nevertheless, such an independent learning fashion is unable to reveal the relationships between T j s. Although different tasks conduct different generation directions, these tasks are tackling overlapping modalities, hence potentially, their learning knowledge might be reused and the learning processes could be coupled. Towards this purpose, we enforce all T j s to share the same encoder except the BN parameters. Specifically, for each task T j , we utilize distinct sets of BN parameters for different input modalities, giving rise to the total number of BN parameter sets for all tasks as M (M + 1). With the separated BNs, we then carry out CEN on the encoder for multimodal fusion for each task T j . The sketched pipeline is illustrated in <ref type="figure">Fig. 2</ref> (b). Note that for the case with three modalities, channels are still divided into two parts, since for cycle multimodal fusion, only two of the three modalities are sent to the encoder at each time.</p><p>Obviously, cycle multimodal fusion is a multitask generalization of the multimodal fusion in ? 3.2. The key benefit is that it simultaneously addresses all combinations of the cycling generation tasks with only one single pair of the encoder and decoder, which dramatically decreases the model complexity. More interestingly, as we will demonstrate in our experiments, the cycle multimodal fusion can improve each of the single-task multimodal fusion, probably thanks to the knowledge transfer by parameter sharing and joint training. We will provide more details and evaluations for cycle multimodal fusion in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multitask learning via CEN on decoders</head><p>Different from multimodal fusion, multitask learning requires to predict different labels for different subnetworks:</p><formula xml:id="formula_7">x ? {y m } M m=1</formula><p>, where we assume all tasks have the same input, i.e., x m = x, ?m = 1, ? ? ? , M and the output label is y m for the task m. The advantage of multitask learning is to improve model generalization and data efficiency, by sharing task-common knowledge while retaining taskspecific information. One of the widely-used methods is employing the hard parameter-sharing mechanism [71] that shares the encoder and uses task-specific decoders. Despite its popularity in previous applications, modelling the multitask relationship by solely sharing the encoder is insufficient in characterizing high-level patterns, particularly the related features across decoders.</p><p>To address the aforementioned issues, we propose to perform channel exchanging on the decoders. Our goal of employing CEN on decoders lies in adaptively discovering the redundant channels in decoders and compensating for the information from the channels of other tasks. The methodology is illustrated is in <ref type="figure">Fig. 2 (c)</ref>. Specifically, the sparsity penalty of BN (or IN) scaling factors is added to the decoder part. Accordingly, for the m-th subnetwork, channel exchanging is conducted from other decoders to the m-th decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multimodal multitask learning via CEN on both encoders and decoders</head><p>It could be straightforward to combine the designs in ? 3.2 and ? 3.4 to handle multimodal multitask learning tasks, with multiple input and output modalities, as illustrated in <ref type="figure">Fig. 2 (d)</ref>. It requires to address {{x m1 } M1 m1=1 ? y m2 } M2 m2=1 , where M 1 and M 2 are the numbers of input and output modalities, respectively. To enable simultaneous multimodal fusion and multitask learning, we perform CEN on both encoders and decoders. The input for each decoder is given by CEN on all encoders. In this case, we share the convolutional layers at the encoder part and privatize M 1 M 2 groups of BN (or IN) parameters. Similarly, for the m 2 -th task/decoder (where m 2 = 1, ? ? ? , M 2 ), we adopt {? m2 m1 } M1 m1=1 as decision scores for ensemble that meet M1 m1=1 ? m2 m1 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We contrast the performance of CEN against existing methods on the four problems in <ref type="figure">Fig. 2</ref>. For multimodal fusion, we conduct experiments on the two tasks: semantic segmentation and image-to-image translation. For the other three problems, we evaluate the performance mainly on imageto-image translation, since this task contains a rich number of image modalities and is suitable for evaluations under various settings. The datasets and implementation details for semantic segmentation and image-to-image translation are provided below. Semantic segmentation. We evaluate our method on two public datasets NYUDv2 <ref type="bibr" target="#b56">[42]</ref> and SUN RGB-D <ref type="bibr" target="#b57">[43]</ref>, which consider RGB and depth as input. Regarding NYUDv2, we follow the standard settings and adopt the split of 795 images for training and 654 for testing, predicting standard 40 classes <ref type="bibr" target="#b86">[72]</ref>. SUN RGB-D is one of the most challenging large-scale benchmarks for indoor semantic segmentation, containing 10,335 RGB-D images of 37 semantic classes. We use the public train-test split (5,285 vs 5,050). We consider RefineNet <ref type="bibr" target="#b2">[3]</ref>/PSPNet <ref type="bibr" target="#b87">[73]</ref> as our segmentation framework whose backbone is implemented by ResNet <ref type="bibr" target="#b88">[74]</ref> pretrained from ImageNet dataset <ref type="bibr" target="#b89">[75]</ref>. The initial learning rates are set to 5 ? 10 ?4 for the encoder and 3 ? 10 ?3 for the decoder, respectively, both of which are reduced to their halves every 100/150 epochs (of total epochs 300/450) on NYUDv2 with ResNet101/ResNet152 and every 20 epochs (of total epochs 60) on SUN RGB-D. The mini-batch size, momentum, and weight decay are selected as 6, 0.9, and 10 ?5 , respectively, on both datasets. We set ? = 5 ? 10 ?3 in Eq. 2 and the threshold to ? = 2 ? 10 ?2 in Eq. 4. Unless otherwise specified, we adopt the multi-scale strategy <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b50">[36]</ref> during the test time. We employ common evaluation metrics including Mean IoU, Pixel Accuracy, and Mean Accuracy <ref type="bibr" target="#b2">[3]</ref>. Full implementation details are provided in the appendix.</p><p>Image-to-image translation. We adopt Taskonomy <ref type="bibr" target="#b58">[44]</ref>, a dataset with 4 million images of indoor scenes gathered from about 600 buildings. Each image in Taskonomy has more than 10 multimodal representations, including depth (euclidean/zbuffer), shade, normal, texture, edge, principal curvature, etc. For efficiency, we sample 1,000 high-quality multimodal images for training, and 500 for validation. We also provide experiments with 15,000 sampled images for training in the appendix. Following Pix2pix <ref type="bibr" target="#b39">[25]</ref>, we adopt the U-Net-256 structure for image translation with the consistent setups with <ref type="bibr" target="#b39">[25]</ref>. The BN computations are replaced with Instance Normalization layers (INs), and our method (Eq. 4) is still applicable. We adopt individual INs in the encoder, and share all other parameters including INs in the decoder. We set ? to 10 ?3 for sparsity constraints and the threshold ? to 10 ?2 . FID <ref type="bibr" target="#b90">[76]</ref> and KID <ref type="bibr" target="#b91">[77]</ref> are adopted as evaluation metrics, as will be introduced in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluations on multimodal fusion</head><p>We first assess the importance of each component in CEN solely on the semantic segmentation dataset NYUDv2, and then compare the performance with other multimodal fusion baselines and SOTA methods on semantic segmentation and image-to-image translation. It thus verifies the effectiveness of our proposed mechanism on this task.</p><p>? Note that the channel exchanging is only available on a certain portion of each layer, i.e., exchanging only half of the channels in the two-modal case. When we remove this constraint and allow all channels to be exchanged by Eq. 4, the accuracy decreases, which we conjecture is owing to the detriment by impeding modal-specific propagation, if all channels are engaged in cross-modal fusion.</p><p>After training CEN (with sparsity constraints on disjoint channel regions, as illustrated in <ref type="figure">Fig. 11</ref>), each certain channel belongs to one of the three categories:  will not be (? rgb ? 0, ? depth ? 0) since we apply sparsity constraints on disjoint channels. To further explain why channel exchanging works, <ref type="figure" target="#fig_0">Fig. 3</ref> displays the averaged feature maps of RGB and Depth. Here, "averaged" means: Firstly, extracting feature maps at all specific channels (in a layer) that belong to the same (aforementioned) category; Secondly, averaging these feature maps along the channels. We observe from <ref type="figure" target="#fig_0">Fig. 3</ref> that RGB channels with non-zero scaling factors mainly characterize the texture, while Depth channels with non-zero factors focus more on the boundary; in this sense, performing channel exchanging can better combine the complementary properties of both modalities. Comparison with fusion baselines. In <ref type="table" target="#tab_4">Table 2</ref>, we report comparison results of our CEN with two aggregation-based methods: concatenation <ref type="bibr" target="#b44">[30]</ref> and self-attention <ref type="bibr" target="#b3">[4]</ref>, and one alignment-based approach <ref type="bibr" target="#b45">[31]</ref>, using the same backbone. All baselines are implemented with the early, middle, late, and all stage fusion. For a more fair comparison, all base-lines are further conducted under the same setting (except channel exchanging) with ours, namely, sharing convolutions with individual BNs, and preserving the propagation of all subnetworks (with also the ensemble). Full details are provided in the appendix. It demonstrates that, in both settings, our method always outperforms others by an average improvement of larger than 2%. We also report the parameters used for fusion, e.g. the aggregation weights of two modalities in concatenation. While self-attention (allstage) attains the closest performance to ours (49.1 vs 51.1), its parameters used for fusion are considerable, whereas our fusion is parameter-free.</p><formula xml:id="formula_8">(? rgb ? 0, ? depth &gt; 0), (? rgb &gt; 0, ? depth ? 0), and (? rgb &gt; 0, ? depth &gt; 0). There replaced replaced RGB Depth ? "#$ ? "#$%&amp; ? 0 &gt; 0 , ? "#$ ? "#$%&amp; &gt; 0 ? 0 , ? "#$ ? "#$%&amp; &gt; 0 &gt; 0,</formula><p>Visualizations are provided in <ref type="figure">Fig. 4</ref>. We choose the hard cases including the images containing tables and chairs, as well as those with low/high light intensity. We observe that the concatenation method is more sensitive to noises in the depth input. Both concatenation and self-attention methods are weak in predicting thin objects, e.g.,  legs. These objects are usually missed in the depth input, which may hinder the prediction results after fusion. On the contrary, the prediction results of our method preserve more details and are more robust to the light intensity. Comparison with SOTAs. In <ref type="table" target="#tab_6">Table 3</ref>, we contrast our method against a wide range of state-of-the-art methods. Their results are directly copied from previous papers if provided or re-implemented by us otherwise, as marked with annotations. Results conclude that our method equipped with PSPNet (ResNet152) achieves new records remarkably superior to previous methods in terms of all metrics on both datasets. In particular, given the same backbone, our method is still much better than RDFNet <ref type="bibr" target="#b50">[36]</ref>. To isolate the contribution of RefineNet in our method, <ref type="table" target="#tab_6">Table 3</ref> also provides the uni-modal results, where we observe a clear advantage of multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Image-to-Image Translation</head><p>Comparison with baseline fusion methods. We then evaluate the performance given five specific translation cases, including Shade+Texture?RGB, Depth+Normal?RGB, RGB+Shade?Normal, RGB+Normal?Shade and RGB+ Edge ?Depth. In addition to the three baselines used in semantic segmentation (Concat, Self-attention, Align), we conduct an extra aggregation-based method by using the average operation. All baselines perform fusion under four different kinds of strategies: early (at the 1st Conv-layer), middle (the 4th Conv-layer), late (the 8th Conv-layer), and  all-layer fusion. Our method yields much lower FID/KID or MAE/MSE than others, especially when predicting the RGB modality, as detailed in <ref type="table" target="#tab_3">Table 15</ref>. These results support the benefit of our proposed idea once again.</p><p>Main visualizations are provided in <ref type="figure" target="#fig_1">Fig. 5</ref>. We observe that when predicting RGB given texture and shade, the prediction solely predicted from the texture is vague at boundary lines, while the prediction solely from the shade misses some opponents, e.g. the pendant lamp, and is weak in predicting handrails. When fusing both input modalities, the concatenation method is uncertain in the regions where both modalities have disagreements. Alignment and selfattention are still weak in combining both modalities at details. Our results are clear at boundaries and fine-grained details. When predicting depth given RGB and edge, it is straightforward to find the benefits of multimodal fusion in this figure. The depth predicted by RGB is good at predicting numerical values, but is weak in capturing boundaries, which results in vague and curving boundaries. Oppositely, the depth predicted by the edge well captures boundaries, but is relatively weak in determining numerical values. The alignment fusion method is still weak in capturing boundaries. Both concatenation and self-attention methods are able to combine the advantages of both modalities, but numerical values are still obviously lower than the ground truth. All illustrations verify that our CEN achieves better performance compared to baseline methods. More visualizations and baseline settings are provided in the appendix.</p><p>Considering more input modalities. We test whether our method is applicable to the case with more than two modalities. For this purpose, <ref type="table" target="#tab_9">Table 5</ref> presents the results of image translation to RGB by inputting from one to four modalities of Depth, Normal, Texture, and Shade. It is observed that increasing the number of modalities improves the performance consistently, suggesting much potential of applying our method towards various cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluations on cycle multimodal fusion</head><p>In this subsection, we evaluate CEN-cycle, a cycle multimodal fusion mode of CEN to simultaneously tackle three generation flows with a compact structure. As described in ? 3.3, in cycle multimodal fusion, we go through all 6 flows where each flow contains two input modalities and one output modality. The subnetwork is trained with all the   three flows at each step. For each flow, our default setting is employing the encoders and decoders with shared convolution parameters but unshared INs. To demonstrate the benefit of CEN-cycle, we also implement these baselines in <ref type="table" target="#tab_10">Table 6</ref>: independent CEN that trains each flow separately, CEN-random that randomly samples one of the three flows per training step, and CEN-cycle with unshared decoders. We observe that compared with independent CEN, CENcycle with unshared decoders not only compacts the overall model but also achieves provably better prediction performance. By further sharing the decoders, CEN-cycle further reduces the model size (needing about 1/3 parameter) and still yields better results than independent CENs. In addition, CEN-random is inferior to independent CEN, probably because it is ineffective to balance the training between different flows if only one flow is trainable per step. In summary, the results here support that performing CEN-cycle is valuable, and it is able to reuse the information in different generation flows that involve overlapping input/output modalities by parameter sharing and joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluations on multitask learning</head><p>This subsection evaluates multitask learning which adopts a single modality as input and simultaneously predicts two  or three different modalities. As introduced in ? 3.4, CEN is conducted on the decoder side, abbreviated as CEN-dec. <ref type="table" target="#tab_11">Table 7</ref> reports the case of predicting two modalities. Besides individual training with shared/unshared encoders, we consider a stronger baseline named Cross-Task Consistency (X-TC) <ref type="bibr" target="#b79">[65]</ref> under the triangle loss setting. X-TC basically enforces an addition supervision to let one predicted modality generate the other one. As observed, CEN-dec outperforms individual learning and X-TC in all tasks, and its performance is further promoted if used along with X-TC, showing the compatibility between CEN-dec and X-TC.</p><p>In <ref type="figure" target="#fig_2">Fig. 6</ref>, we further provide visualizations of multitask learning. We observe that by simultaneously predicting RGB and depth from texture, our CEN-dec predicts noticeably better results. By simultaneously normal and the principle curve from RGB, predicted normal boundaries of the table and wall are more accurate with CEN-dec.</p><p>We also consider the case of predicting three modalities. To this end, we implement two recent popular methods including AdaShare (AS) <ref type="bibr" target="#b95">[81]</ref> and Task-Grouping (TG) <ref type="bibr" target="#b78">[64]</ref> which consider multitask learning by parameter sharing. Table 8 summarizes the experimental results. We find that both AS and TG usually achieve better accuracy than individual learning on some tasks (for example RGB?Depth) but at the sacrifice of other tasks (RGB?SemSeg), probably owing to the negative transfer. Yet, our CEN-dec, which simply shares the encoders with individual INs and performs channel exchanging in the decoder, outperforms all methods by noticeable margins in all tasks, supporting the superiority of channel exchanging for message fusion between different tasks. Interestingly, when combined with TG, the performance of CEN-dec is boosted remarkably, implying the flexibility of integrating our method with other techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluations on multimodal multitask learning</head><p>We evaluate our multimodal multitask CEN as a combination of multimodal fusion and multitask learning, as shown  <ref type="table" target="#tab_11">Table 7</ref>. AdaShare (AS) <ref type="bibr" target="#b95">[81]</ref> and Taskgrouping (TG) <ref type="bibr" target="#b78">[64]</ref> are additionally served as baselines.  in <ref type="table" target="#tab_14">Table 9</ref>. We compare four different settings including individual training (Indiv), CEN on the encoder (CENenc), CEN on the decoder (CEN-dec), and CEN on both the encoder and decoder (CEN-enc &amp; dec). All the settings maintain four individual INs that correspond to the four different input-output combinations, respectively. Results indicate that performing CEN either on the encoder or decoder is beneficial compared with the individual training baseline. Generally speaking, CEN-enc obtains more benefits compared with CEN-dec. This is natural as each input modality contains complementary information for predicting each output modality, hence CEN-enc is particularly advantageous. But different output modalities might not be necessarily related, and as a result, CEN-dec gains smaller improvement. As expected, combining CENenc and CEN-dec can further improve each of them and delivers the best performance in all considered cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head><p>Why dividing channels into M sub-parts. We describe in ? 3.1 and <ref type="figure">Fig. 11</ref> that we evenly divide the whole channels into M sub-parts (where M is the number of input modalities), and apply sparsity constraints only to one sub-part for each modality. Otherwise, if we do not divide channels and apply sparsity constraints to all scaling factors for each  modality, there is likely to be a portion of channels with close-to-zero scaling factors w.r.t. all modalities. We provide the illustration in <ref type="figure" target="#fig_8">Fig. 12</ref>. We observe that with sparsity constraints on all channels, <ref type="figure" target="#fig_8">Fig. 12</ref> (middle) has a number of channels with small scaling factors, which are thus considered to be redundant w.r.t. all modalities, which might lead to the decline of model capacity. Besides, it is hard to decide the exchanging direction on these redundant channels based on Eq. 4. We provide corresponding experimental results in <ref type="table" target="#tab_3">Table 1</ref> and the appendix <ref type="table" target="#tab_3">(Table 13)</ref>.</p><p>Typical values of scaling factors. <ref type="figure">Fig. 8</ref> demonstrates typical values of BN scaling factors vs training steps, consisting of four combinations: within/beyond sparsity constraints, and with/without channel exchanging. Experimental details are provided in the caption of <ref type="figure">Fig. 8</ref>. From the first two subfigures, we observe that whether applying channel exchanging or not, scaling factors that are close to zero can hardly recover (in the later training process). In addition, according to the last two subfigures, it seems channel exchanging increases the learning speed of a portion of scaling factors without sparsity constraints, probably due to the accumulated gradient on both the RGB branch and the depth branch by channel exchanging (Eq. 4).</p><p>Effect of zeroing out channels and channel exchanging. This part provides the sensitivity analysis for two essential hyper-parameters of CEN, including the weight ? (Eq. 2) of sparsity constraint, and the threshold ? (Eq. 4) that identifies close-to-zero scaling factors. Experimental details are provided in the caption of <ref type="figure">Fig. 9</ref>. To isolate the advantage of channel exchanging, <ref type="figure">Fig. 9 (a)</ref> indicates that by zeroing out channels with small scaling factors (instead of channel exchanging), the performance slightly drops with the increase of ? or ? since the percentage of zeroing out channels increases accordingly. Nevertheless, such a drop is moderate, given that under the sparsity constraints, the zeroedout channels are less influential (as analyzed in ? 3.1). <ref type="figure">Fig. 9 (b)</ref> provides the sensitivity analysis of our channel exchanging. We observe that both hyper-parameters ? and ? are not sensitive around their default settings. It is also noticeable that without channel exchanging, simply zeroing out channels reaches much inferior performance.</p><p>Importance of the exchanging process. We provide additional experiments in the appendix <ref type="table" target="#tab_3">(Table 12)</ref>, to evaluate the importance of the exchanging process. We try other approaches to replace zeroed-out channels with: concatenated multimodal features (followed by a Conv-layer) instead of the average, evenly spaced channels from the same modality or other modalities, channels with the largest scaling factors, etc. Results indicate the superiority of our current design. In summary, albeit the simplicity of using the average of other modalities in CEN, it is also effective and competitive.</p><p>Evaluation for the unsupervised learning. In a part of multimodal fusion tasks, there is no ground truth during training <ref type="bibr" target="#b41">[27]</ref>, <ref type="bibr" target="#b61">[47]</ref>, <ref type="bibr" target="#b96">[82]</ref>. As a general multimodal/multitask method, CEN is also potentially applicable to unsupervised learning tasks. For example, we apply CEN to the Saliency Network in <ref type="bibr" target="#b96">[82]</ref> for RGB-D unsupervised saliency detection, a dense image prediction task aiming to effectively find and segment the most distinctive objects in a scene. Quantitive results and visualizations are provided in the appendix ( <ref type="figure" target="#fig_1">Fig.  15</ref> and <ref type="table" target="#tab_3">Table 14)</ref>, where improvements are also achieved, indicating the effectiveness of CEN in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose Channel-Exchanging-Network (CEN), a novel framework for multimodal fusion and multitask learning, which is parameter-free and self-adaptive. The motivation and ? = 10 ?2 . The left y-axis indicates the metric (mIoU ? or FID score ?). The right y-axis indicates the percentage of channels that are lower than ? and these channels will be replaced by zeros (left group) or by cross-modal channels (right group). Metric results at default settings are marked.</p><p>behind this is to boost inter-subnetwork fusion while simultaneously keeping sufficient intra-subnetwork processing. The channel exchanging is self-guided by channel impor-tance measured by individual BNs, making our framework self-adaptive and compact. Extensive evaluations in four cases (multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning) verify the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A IMPLEMENTATION DETAILS</head><p>In our experiments, we adopt ResNet101 and ResNet152 for semantic segmentation, and U-Net-256 for image-toimage translation. We use an NVIDIA Tesla V100 with 32GB for each experiment. Regarding both ResNet structures, we apply sparsity constraints on Batch-Normalization (BN) scaling factors w.r.t. each Convolutional-layer (Conv-layer) with 3 ? 3 kernels. These scaling factors further guide the channel exchanging process that exchanges a portion of feature maps after BN. For the Conv-layer with 7?7 kernels at the beginning of ResNet, and all other Conv-layers with 1?1 kernels, we do not apply sparsity constraints or channel exchanging. For U-Net, we apply sparsity constraints on Instance-Normalization (IN) scaling factors w.r.t. all Convlayers (eight layers in total) in the encoder of the generator, and each is followed by channel exchanging. We mainly adopt three multimodal fusion baselines in our paper, including concatenation, alignment, and selfattention. Regarding the concatenation method, we stack multimodal feature maps along the channel, and then add a 1 ? 1 convolutional layer to reduce the number of channels back to the original number. The alignment fusion method is a re-implementation of <ref type="bibr" target="#b45">[31]</ref>, and we follow its default settings for hyper-parameter, e.g. using 11 kernel functions for the multiple kernel Maximum Mean Discrepancy. The self-attention method is a re-implementation of the SSMA block proposed in <ref type="bibr" target="#b3">[4]</ref>, where we also follow the default settings, e.g. setting the channel reduction ratio ? to <ref type="bibr">16.</ref> In <ref type="table" target="#tab_4">Table 2</ref> of our main paper, we adopt early, middle, late and all-stage fusion for each baseline method. In ResNet101, there are four stages with 3, 4, 23, and 3 blocks, respectively. The early fusion, middle fusion, and late fusion refer to fusing after the 2nd stage, 3rd stage, and 4th stage respectively. All-stage fusion refers to fusing after the four stages.</p><p>Assumed zeroed-out channel to be replaced   <ref type="table" target="#tab_3">Table 12</ref>.</p><p>We now introduce the metrics (including FID and KID) used in our image-to-image translation task.</p><p>Firstly, Fr?chet-Inception-Distance (FID) <ref type="bibr" target="#b90">[76]</ref> mainly contrasts the statistics of generated samples against real samples. FID fits a Gaussian distribution to the hidden activations of InceptionNet for each compared image set and then computes the Fr?chet distance (also known as the Wasserstein-2 distance) between those Gaussians. Lower FID is better, indicating that the generated images are more similar to the real ones.  Secondly, Kernel-Inception-Distance (KID) <ref type="bibr" target="#b91">[77]</ref> is a metric similar to the FID score but uses the squared Maximum-Mean-Discrepancy (MMD) between Inception representations with a polynomial kernel. Unlike FID, KID has a simple unbiased estimator, making it more reliable especially when there are much more inception feature channels than image numbers. Lower KID indicates more visual similarity between real and generated images. Regarding our implementation of KID, the hidden representations are derived from the Inception-v3 pool3 layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ADDITIONAL DISCUSSIONS AND RESULTS</head><p>Dividing channels into sub-parts. Here we provide additional descriptions of why dividing channels into M subparts and individually applying sparsity constraints. We first use the case with 2 modalities for example. As shown in <ref type="figure">Fig. 11</ref>, we divide channels into 2 disjoint sub-parts and apply sparsity constraints. During training, each certain channel belongs to one of the three categories: A (where  <ref type="figure">Fig. 11</ref>: An illustration of CEN. The sparsity constraints on scaling factors are applied to disjoint channel regions of different modalities. As annotated, each channel is categorized to A, B, or C, based on its ? rgb and ? depth .  ? rgb ? 0, ? depth &gt; 0), B (where ? rgb &gt; 0, ? depth ? 0), and C (where ? rgb &gt; 0, ? depth &gt; 0). There won't be (? rgb ? 0, ? depth ? 0) as we apply sparsity constraints on disjoint sub-parts. However, if we apply sparsity constraints on all scaling factors for each modality (without dividing 2 sub-parts), there is likely be a portion of channels with close-to-zero scaling factors w.r.t. both modalities, i.e., (? rgb ? 0, ? depth ? 0). These channels are considered to be unimportant/redundant for both modalities. Regarding multimodal fusion, it is kind of waste of channels, which might lead to the decline of model capacity. Besides, it is hard to decide the exchanging direction on these channels according to Eq. 4 (main paper). Similarly, when there are 3 (or M ) modalities as input, dividing the whole channels into 3 (or M ) sub-parts avoids a channel from being redundant for all modalities. As a result, we divide channels into M sub-parts and apply sparsity constraints on one sub-part for each modality. As an example, for Shade+Texture+Depth?RGB image-to-image translation with shared Convs and unshared INs, channels are evenly divided into three sub-parts. We plot the proportion of IN scaling factors at each Conv-layer in the encoder of U-Net in <ref type="figure" target="#fig_8">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B B B A A C C C C C C C</head><p>Comparison results to support the channel dividing have been provided in <ref type="table" target="#tab_3">Table 1</ref> of our main paper: semantic segmentation "All-channel" (49.8) vs "Half-channel" (51.1). Additional results for image-to-image translation are shown in <ref type="table" target="#tab_3">Table 13</ref>. All these results indicate the superiority of applying sparsity constraints on sub-parts of channels.</p><p>Effect of network sharing. In <ref type="table" target="#tab_3">Table 10</ref>, we verify that sharing convolutional layers (Convs) but using individual Instance-Normalization layers (INs) allows 2?4 modalities trained in a single network, and even achieve better performance than training with individual networks. Again, if we further share INs, there will be an obvious performance drop. More detailed comparison is provided in <ref type="table" target="#tab_3">Table 11</ref>.</p><p>Visualization of indoor experiments. We provide additional visualizations of the image-to-image translation task in <ref type="figure" target="#fig_0">Fig. 13</ref>, as a complement to <ref type="figure" target="#fig_1">Fig. 5 (main paper)</ref>. Regarding baseline implementation in all these visualizations, we adopt all-layer fusion (fusion at all eight Conv-layers in the encoder) for concatenation and self-attention methods, and  adopt middle fusion (fusion at the 4th Conv-layer) for the alignment method. These settings achieve relatively high performance regarding baseline methods according to their numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of outdoor experiments.</head><p>In this part, we additionally conduct outdoor semantic segmentation experiments on the Cityscapes dataset <ref type="bibr" target="#b97">[83]</ref> and provide the visualization comparison. Cityscapes is an outdoor dataset containing images from 27 cities in Germany and neighboring countries. The dataset contains 2,975 training, 500 validation and 1,525 test images. There are 20,000 additional coarse annotations provided by the dataset, which are not used for training in our experiments. All results are obtained with the backbone PSPNet (ResNet101) of single-scale evaluation for test. These visualizations are provided in <ref type="figure">Fig 14.</ref> Evaluation for unsupervised learning. Apart from the common supervised training settings for dense image prediction <ref type="bibr" target="#b113">[99]</ref>, <ref type="bibr" target="#b114">[100]</ref>, we show the potential of CEN for unsupervised learning, e.g., <ref type="bibr" target="#b41">[27]</ref>, <ref type="bibr" target="#b61">[47]</ref>, <ref type="bibr" target="#b96">[82]</ref>. As an example shown in <ref type="figure" target="#fig_1">Fig. 15</ref> and <ref type="table" target="#tab_3">Table 14</ref>, we apply CEN to the Saliency Network in <ref type="bibr" target="#b96">[82]</ref> for RGB-Depth unsupervised saliency detection, which also achieves promising results, indicating Quantitave results of applying CEN to the unsupervised RGB-D saliency detection. We follow the training settings in <ref type="bibr" target="#b96">[82]</ref>. Evaluation datasets include NJUD <ref type="bibr" target="#b98">[84]</ref>, NLPR <ref type="bibr" target="#b99">[85]</ref>, STERE <ref type="bibr" target="#b100">[86]</ref>, and DUTLF-Depth <ref type="bibr" target="#b101">[87]</ref>. We adopt Mean Absolute Error (MAE) <ref type="bibr" target="#b102">[88]</ref> as the evaluation metric following <ref type="bibr" target="#b96">[82]</ref>. Lower values indicate better performance. <ref type="bibr">Method</ref> NJUD NLPR STERE DUTLF-Depth MST <ref type="bibr" target="#b103">[89]</ref> .281 .199 .269 .279 BSCA <ref type="bibr" target="#b104">[90]</ref> .216 .178 .179 .181 GP <ref type="bibr" target="#b105">[91]</ref> .204 .144 .182 -CDB <ref type="bibr" target="#b106">[92]</ref> .200 .108 .166 -SE <ref type="bibr" target="#b107">[93]</ref> .164 .085 .143 .196 DCMC <ref type="bibr" target="#b108">[94]</ref> .167 .196 .148 .243 MB <ref type="bibr" target="#b109">[95]</ref> .202 .089 .178 .156 CDCP <ref type="bibr" target="#b110">[96]</ref> .181 .114 .149 .159 USD <ref type="bibr" target="#b111">[97]</ref> .163 .119 .146 .157 DeepUSPS <ref type="bibr" target="#b112">[98]</ref> .  <ref type="figure">Fig. 14:</ref> Visualization for the semantic segmentation on Cityscapes <ref type="bibr" target="#b97">[83]</ref>. For the baseline methods, we use white frames to highlight the regions with poor prediction results. We observe that when the light intensity is high, baseline methods are weak in capturing the boundary between the sky and buildings using the depth information. Besides, the concatenation and self-attention methods do not preserve fine-grained objects, e.g. traffic signs, and are sensitive to noises of the depth input (see the rightmost vehicle in the last group). In contrast, the prediction of our CEN is better in these aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB input Depth input RGB-D fusion by PS</head><p>RGB-D fusion by our CEN Ground truth <ref type="figure" target="#fig_1">Fig. 15</ref>: Visualization of applying CEN to the unsupervised RGB-D saliency detection. We compare our method with another RGB-D-based method Promoting Saliency (PS) <ref type="bibr" target="#b96">[82]</ref> which recently achieves SOTA.</p><p>promising results. Here, we enlarge the sampled set with 15,000 training images and conduct the experiments. Since the training cost becomes quite large given the 15? expansion of the default sampled training dataset, we choose typical experiments to evaluate our CEN. Results provided in <ref type="table" target="#tab_3">Table 15</ref> include multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning based on 15,000 sampled training images. By comparison, we observe that mostly, training with 1,000 images and training with 15,000 images achieve similar relative improvements of CEN over baselines. These results indicate that using 1,000 images for training already demonstrates the general advantages of CEN. Comparison on multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning with 15,000 sampled training images (instead of 1,000 for other experiments). Evaluation metrics are FID/KID (?10 ?2 ) for RGB predictions and MAE (?10 ?2 )/MSE (?10 ?2 ) for other predictions. Lower values indicate better performance for all metrics. "Curve", "SemSeg", and "X-TC" are abbreviations for the principle curve, semantic segmentation, and Cross-Task Consistency <ref type="bibr" target="#b79">[65]</ref> respectively. More abbreviation details follow the captions of <ref type="table" target="#tab_8">Table 4</ref>, 6, 7, 9 in our paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal fusion</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of the averaged feature maps for RGB and Depth. From left to right: the input images, the channels of (? rgb ? 0, ? depth &gt; 0), (? rgb &gt; 0, ? depth ? 0), and (? rgb &gt; 0, ? depth &gt; 0). The feature maps are collected in a single layer, specifically, the 9th layer of ResNet, i.e. the 2nd layer of the 3rd stage (with 256 channels) of ResNet. Values under color bars correspond to the actual values of averaged feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization results of multimodal image-to-image translation including Texture+Shade?RGB (top group), RGB+ Edge?Depth (middle group), and RGB+Shade?Normal (bottom group), respectively. The resolution of each predicted image is 256 ? 256. More visualizations are provided in the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Visualization results of multitask image-to-image translation including Texture?RGB+Depth (left group) and RGB? Normal+Curve (right group), respectively. "Curve" is the abbreviation for the principle curve modality. We compare the individual (Indiv) baseline with unshared encoders and our CEN-dec. The resolution of each predicted image is 256 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Without sparsity constraints With sparsity constraints on all channels With sparsity constraints on disjoint channels (default of CEN) Depth Shade Texture Channel Proportion of IN scaling factors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>We adopt shared Convs and unshared INs, and plot the proportion of scaling factors for each modality at the 7th Conv-layer, i.e. ? m,c /(? 1,c + ? 2,c + ? 3,c ), where m = 1, 2, 3 being Shade, Texture and Depth, respectively. Note that we use the white space to represent a channel c if all of the three scaling factors (? 1,c , ? 2,c , ? 3,c ) are less than the threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Illustrations of channels as a complement to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>We adopt shared Convs and unshared INs, and plot the proportion of scaling factors for each modality at each Conv-layer, i.e. ? m,c /(? 1,c + ? 2,c + ? 3,c ), where m = 1, 2, 3 being Shade, Texture and Depth, respectively.Proportion of scaling factors in all Conv-layers, and sparsity constraints are applied on disjoint channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Additional visualization results of image-to-image translation including Texture+Shade?RGB (top group), RGB+Edge?Depth (middle group), and RGB+Normal?Shade (bottom group), respectively. The resolution of each predicted image is 256 ? 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and F. Sun are with Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Department of Computer Science and Technology, Tsinghua University. W. Huang is with Gaoling School of Artificial Intelligence, Renmin University of China and Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China. F. He and D. Tao are with JD Explore Academy, JD.com Inc.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 :</head><label>1</label><figDesc>Detailed results for different versions of our CEN on NYUDv2. All results are obtained with the backbone RefineNet (ResNet101) of single-scale evaluation for test. "Ens." is the abbreviation for "Ensemble".Compared to the unshared baseline, sharing convolutional parameters greatly boosts the performance, particularly on the Depth modality(35.8 vs 38.4). Yet, the performance will encounter a clear drop if we additionally share the BN layers. This observation is consistent with our analyses in ? 3.2 due to the different roles of convolutional filters and BN parameters.As 1 enables the discovery of unnecessary channels, naively exchanging channels with a fixed portion (without using 1 ) could not reach good performance. For example, exchanging a fixed portion of 30% channels (close to the averaged number of exchanged channels in CEN) only gets IoU 47.2. Besides, we try to exchange channels randomly like ShuffleNet or directly discard unimportant channels without channel exchanging, the IoUs of which are 46.8 and 47.5, respectively.After carrying out directed channel exchanging under the 1 regulation, our model gains a huge improvement on both modalities, i.e. from 46.0 to 49.7 on RGB, and from 38.1 to 45.1 on Depth, and finally increases the ensemble Mean IoU from 47.6 to 51.1.</figDesc><table><row><cell>Convs</cell><cell>BNs</cell><cell>1 Regulation</cell><cell>Exchange</cell><cell cols="3">Mean IoU (%) RGB Depth Ens.</cell></row><row><cell cols="2">Unshared Unshared</cell><cell>?</cell><cell>?</cell><cell>45.5</cell><cell>35.8</cell><cell>47.6</cell></row><row><cell>Shared</cell><cell>Shared</cell><cell>?</cell><cell>?</cell><cell>43.7</cell><cell>35.5</cell><cell>45.2</cell></row><row><cell>Shared</cell><cell>Unshared</cell><cell>?</cell><cell>?</cell><cell>46.2</cell><cell>38.4</cell><cell>48.0</cell></row><row><cell>Shared</cell><cell>Unshared</cell><cell>?</cell><cell cols="2">(fixed 30%) 44.9</cell><cell>40.3</cell><cell>47.2</cell></row><row><cell>Shared</cell><cell>Unshared</cell><cell>?</cell><cell>(random)</cell><cell>44.2</cell><cell>40.5</cell><cell>46.8</cell></row><row><cell cols="3">Unshared Unshared All-channel</cell><cell>?</cell><cell>44.6</cell><cell>35.3</cell><cell>46.6</cell></row><row><cell cols="3">Unshared Unshared All-channel</cell><cell></cell><cell>46.8</cell><cell>41.7</cell><cell>49.1</cell></row><row><cell>Shared</cell><cell cols="2">Unshared All-channel</cell><cell>?</cell><cell>46.1</cell><cell>37.9</cell><cell>47.5</cell></row><row><cell>Shared</cell><cell cols="2">Unshared All-channel</cell><cell></cell><cell>48.6</cell><cell>39.0</cell><cell>49.8</cell></row><row><cell cols="3">Unshared Unshared Half-channel</cell><cell>?</cell><cell>45.1</cell><cell>35.5</cell><cell>47.3</cell></row><row><cell cols="3">Unshared Unshared Half-channel</cell><cell></cell><cell>46.5</cell><cell>41.6</cell><cell>48.5</cell></row><row><cell>Shared</cell><cell cols="2">Unshared Half-channel</cell><cell>?</cell><cell>46.0</cell><cell>38.1</cell><cell>47.7</cell></row><row><cell>Shared</cell><cell cols="2">Unshared Half-channel</cell><cell></cell><cell>49.7</cell><cell>45.1</cell><cell>51.1</cell></row><row><cell cols="3">4.1.1 Semantic Segmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">The validity of each proposed component. Table 1 summa-</cell></row><row><cell cols="7">rizes the results of different variants of CEN on NYUDv2.</cell></row><row><cell cols="4">We have the following observations:</cell><cell></cell><cell></cell><cell></cell></row></table><note>???</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Modality Approach</cell><cell cols="2">Commonly-used setting Params. Mean IoU (%) in total (M)</cell><cell cols="2">Same with our setting Mean IoU (%) Params. RGB / Depth / Ens. in total (M)</cell><cell>Params. used for fusion (M)</cell></row><row><cell>RGB</cell><cell>Uni-modal</cell><cell>45.5</cell><cell>118.1</cell><cell>45.5 / -/ -</cell><cell>118.1</cell><cell>-</cell></row><row><cell>Depth</cell><cell>Uni-modal</cell><cell>35.8</cell><cell>118.1</cell><cell>-/ 35.8 / -</cell><cell>118.1</cell><cell>-</cell></row><row><cell></cell><cell>Concat (early)</cell><cell>47.2</cell><cell>120.1</cell><cell>47.0 / 37.5 / 47.6</cell><cell>118.8</cell><cell>0.6</cell></row><row><cell></cell><cell>Concat (middle)</cell><cell>46.7</cell><cell>147.7</cell><cell>46.6 / 37.0 / 47.4</cell><cell>120.3</cell><cell>2.1</cell></row><row><cell></cell><cell>Concat (late)</cell><cell>46.3</cell><cell>169.0</cell><cell>46.3 / 37.2 / 46.9</cell><cell>126.6</cell><cell>8.4</cell></row><row><cell></cell><cell>Concat (all-stage)</cell><cell>47.5</cell><cell>171.7</cell><cell>47.8 / 36.9 / 48.3</cell><cell>129.4</cell><cell>11.2</cell></row><row><cell></cell><cell>Align (early)</cell><cell>46.4</cell><cell>238.8</cell><cell>46.3 / 35.8 / 46.7</cell><cell>120.8</cell><cell>2.6</cell></row><row><cell></cell><cell>Align (middle)</cell><cell>47.9</cell><cell>246.7</cell><cell>47.7 / 36.0 / 48.1</cell><cell>128.7</cell><cell>10.5</cell></row><row><cell>RGB-D</cell><cell>Align (late) Align (all-stage)</cell><cell>47.6 46.8</cell><cell>278.1 291.9</cell><cell>47.3 / 35.4 / 47.6 46.6 / 35.5 / 47.0</cell><cell>160.1 173.9</cell><cell>41.9 55.7</cell></row><row><cell></cell><cell>Self-att. (early)</cell><cell>47.8</cell><cell>124.9</cell><cell>47.7 / 38.3 / 48.2</cell><cell>123.6</cell><cell>5.4</cell></row><row><cell></cell><cell>Self-att. (middle)</cell><cell>48.3</cell><cell>166.9</cell><cell>48.0 / 38.1 / 48.7</cell><cell>139.4</cell><cell>21.2</cell></row><row><cell></cell><cell>Self-att. (late)</cell><cell>47.5</cell><cell>245.5</cell><cell>47.6 / 38.1 / 48.3</cell><cell>203.2</cell><cell>84.9</cell></row><row><cell></cell><cell>Self-att. (all-stage)</cell><cell>48.7</cell><cell>272.3</cell><cell>48.5 / 37.7 / 49.1</cell><cell>231.0</cell><cell>112.8</cell></row><row><cell></cell><cell>Our CEN</cell><cell>-</cell><cell>-</cell><cell>49.7 / 45.1 / 51.1</cell><cell>118.2</cell><cell>0.0</cell></row></table><note>Comparison with three typical fusion methods including concatenation (concat), fusion by alignment (align), and self-attention (self-att.) on NYUDv2. All results are obtained with RefineNet (ResNet101) of single-scale evaluation for test. "Ens." is the abbreviation for "Ensemble".</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>table legs and chair</figDesc><table><row><cell>RGB (input)</cell><cell>Depth (input)</cell><cell>Concat</cell><cell>Align</cell><cell>Self-att.</cell><cell>Our CEN</cell><cell>Ground truth</cell></row></table><note>Fig. 4: Visualization results of semantic segmentation. Images are collected from NYUDv2 and SUN RGB-D datasets. All results are obtained with the backbone RefineNet (ResNet101) of single-scale evaluation for test.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="2">Modality Approach</cell><cell>Backbone Network</cell><cell>Pixel Acc. (%)</cell><cell>NYUDv2 Mean Acc. (%)</cell><cell>Mean IoU (%)</cell><cell>Pixel Acc. (%)</cell><cell>SUN RGB-D Mean Acc. (%)</cell><cell>Mean IoU (%)</cell></row><row><cell></cell><cell>FCN-32s [22]</cell><cell>VGG16</cell><cell>60.0</cell><cell>42.2</cell><cell>29.2</cell><cell>68.4</cell><cell>41.1</cell><cell>29.0</cell></row><row><cell>RGB</cell><cell>RefineNet [3]</cell><cell>ResNet101</cell><cell>73.8</cell><cell>58.8</cell><cell>46.4</cell><cell>80.8</cell><cell>57.3</cell><cell>46.3</cell></row><row><cell></cell><cell>RefineNet [3]</cell><cell>ResNet152</cell><cell>74.4</cell><cell>59.6</cell><cell>47.6</cell><cell>81.1</cell><cell>57.7</cell><cell>47.0</cell></row><row><cell></cell><cell>FuseNet [29]</cell><cell>VGG16</cell><cell>68.1</cell><cell>50.4</cell><cell>37.9</cell><cell>76.3</cell><cell>48.3</cell><cell>37.3</cell></row><row><cell></cell><cell>ACNet [78]</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>48.1</cell></row><row><cell></cell><cell>SSMA [4]</cell><cell>ResNet50</cell><cell>75.2</cell><cell>60.5</cell><cell>48.7</cell><cell>81.0</cell><cell>58.1</cell><cell>45.7</cell></row><row><cell></cell><cell>SSMA [4]  ?</cell><cell>ResNet101</cell><cell>75.8</cell><cell>62.3</cell><cell>49.6</cell><cell>81.6</cell><cell>60.4</cell><cell>47.9</cell></row><row><cell></cell><cell>CBN [57]  ?</cell><cell>ResNet101</cell><cell>75.5</cell><cell>61.2</cell><cell>48.9</cell><cell>81.5</cell><cell>59.8</cell><cell>47.4</cell></row><row><cell></cell><cell>3DGNN [79]</cell><cell>ResNet101</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>57.0</cell><cell>45.9</cell></row><row><cell></cell><cell>SCN [80]</cell><cell>ResNet152</cell><cell>-</cell><cell>-</cell><cell>49.6</cell><cell>-</cell><cell>-</cell><cell>50.7</cell></row><row><cell>RGB-D</cell><cell>CFN [48]</cell><cell>ResNet152</cell><cell>-</cell><cell>-</cell><cell>47.7</cell><cell>-</cell><cell>-</cell><cell>48.1</cell></row><row><cell></cell><cell>RDFNet [36]</cell><cell>ResNet101</cell><cell>75.6</cell><cell>62.2</cell><cell>49.1</cell><cell>80.9</cell><cell>59.6</cell><cell>47.2</cell></row><row><cell></cell><cell>RDFNet [36]</cell><cell>ResNet152</cell><cell>76.0</cell><cell>62.8</cell><cell>50.1</cell><cell>81.5</cell><cell>60.1</cell><cell>47.7</cell></row><row><cell></cell><cell cols="2">Ours-RefineNet (single-scale) ResNet101</cell><cell>76.2</cell><cell>62.8</cell><cell>51.1</cell><cell>82.0</cell><cell>60.9</cell><cell>49.6</cell></row><row><cell></cell><cell>Ours-RefineNet</cell><cell>ResNet101</cell><cell>77.2</cell><cell>63.7</cell><cell>51.7</cell><cell>82.8</cell><cell>61.9</cell><cell>50.2</cell></row><row><cell></cell><cell cols="2">Ours-RefineNet (single-scale) ResNet152</cell><cell>77.0</cell><cell>64.4</cell><cell>51.6</cell><cell>82.3</cell><cell>61.7</cell><cell>50.0</cell></row><row><cell></cell><cell>Ours-RefineNet</cell><cell>ResNet152</cell><cell>77.4</cell><cell>64.8</cell><cell>52.2</cell><cell>83.2</cell><cell>62.5</cell><cell>50.8</cell></row><row><cell></cell><cell>Ours-PSPNet</cell><cell>ResNet152</cell><cell>77.7</cell><cell>65.0</cell><cell>52.5</cell><cell>83.5</cell><cell>63.2</cell><cell>51.1</cell></row></table><note>Comparison with SOTA semantic segmentation methods on NYUDv2 and SUN RGB-D datasets. ? indicates our implemented results. Evaluation metrics include Pixel Accuracy, Mean Accuracy and Mean IoU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison on multimodal image-to-image translation task. Evaluation metrics are FID/KID (?10 ?2 ) for RGB predictions and MAE (?10 ?2 )/MSE (?10 ?2 ) for other predictions. Lower values indicate better performance for all metrics.</figDesc><table><row><cell>Modality</cell><cell>Our CEN</cell><cell>Baseline</cell><cell>Early</cell><cell>Middle</cell><cell>Late</cell><cell>All-layer</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>87.46 / 3.64</cell><cell>95.16 / 4.67</cell><cell>122.47 / 6.56</cell><cell>78.82 / 3.13</cell></row><row><cell>Shade+Texture ?RGB</cell><cell>62.63 / 1.65</cell><cell>Average Align</cell><cell>93.72 / 4.22 99.68 / 4.93</cell><cell>93.91 / 4.27 95.52 / 4.75</cell><cell>126.74 / 7.10 98.33 / 4.70</cell><cell>80.64 / 3.24 92.30 / 4.20</cell></row><row><cell></cell><cell></cell><cell>Self-att.</cell><cell>83.60 / 3.38</cell><cell>90.79 / 3.92</cell><cell>105.62 / 5.42</cell><cell>73.87 / 2.46</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>105.17 / 5.15</cell><cell>100.29 / 3.37</cell><cell>116.51 / 5.74</cell><cell>99.08 / 4.28</cell></row><row><cell>Depth+Normal ?RGB</cell><cell>84.33 / 2.70</cell><cell>Average Align</cell><cell>109.25 / 5.50 111.65 / 5.53</cell><cell>104.95 / 4.98 108.92 / 5.26</cell><cell>122.42 / 6.76 105.85 / 4.98</cell><cell>99.63 / 4.41 105.03 / 4.91</cell></row><row><cell></cell><cell></cell><cell>Self-att.</cell><cell>100.70 / 4.47</cell><cell>98.63 / 4.35</cell><cell>108.02 / 5.09</cell><cell>96.73 / 3.95</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>13.34 / 28.27</cell><cell>12.15 / 26.54</cell><cell>13.93 / 28.80</cell><cell>13.36 / 28.51</cell></row><row><cell>RGB+Shade ?Normal</cell><cell>11.23 / 25.09</cell><cell>Average Align</cell><cell>14.24 / 30.47 14.50 / 31.07</cell><cell>12.62 / 27.02 13.92 / 29.34</cell><cell>14.01 / 28.95 12.81 / 27.55</cell><cell>12.82 / 28.28 15.18 / 32.50</cell></row><row><cell></cell><cell></cell><cell>Self-att.</cell><cell>12.99 / 28.21</cell><cell>11.75 / 25.86</cell><cell>14.22 / 29.07</cell><cell>12.63 / 27.61</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>15.62 / 24.49</cell><cell>13.81 / 21.24</cell><cell>12.62 / 19.17</cell><cell>12.83 / 20.18</cell></row><row><cell>RGB+Normal ?Shade</cell><cell>11.03 / 17.16</cell><cell>Average Align</cell><cell>14.63 / 22.88 13.88 / 22.62</cell><cell>12.83 / 20.42 13.16 / 21.55</cell><cell>15.11 / 23.92 12.73 / 20.41</cell><cell>12.28 / 18.64 14.09 / 22.05</cell></row><row><cell></cell><cell></cell><cell>Self-att.</cell><cell>12.14 / 18.26</cell><cell>11.52 / 17.33</cell><cell>14.47 / 22.82</cell><cell>11.79 / 17.62</cell></row><row><cell></cell><cell></cell><cell>Concat</cell><cell>3.43 / 7.53</cell><cell>3.17 / 7.39</cell><cell>3.82 / 7.87</cell><cell>3.25 / 7.46</cell></row><row><cell>RGB+Edge ?Depth</cell><cell>2.75 / 6.60</cell><cell>Average Align</cell><cell>3.62 / 7.78 4.38 / 8.93</cell><cell>3.41 / 7.64 3.86 / 8.16</cell><cell>3.56 / 7.73 4.19 / 8.61</cell><cell>3.30 / 7.44 4.38 / 9.03</cell></row><row><cell></cell><cell></cell><cell>Self-att.</cell><cell>3.03 / 7.05</cell><cell>3.32 / 7.29</cell><cell>3.40 / 7.47</cell><cell>3.01 / 6.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Modality</cell><cell cols="3">Depth Normal Texture</cell><cell>Shade</cell><cell>Depth+Normal</cell><cell>Depth+Normal +Texture</cell><cell>Depth+Normal +Texture+Shade</cell></row><row><cell>FID</cell><cell>113.91</cell><cell>108.20</cell><cell>97.51</cell><cell>100.96</cell><cell>84.33</cell><cell>60.90</cell><cell>57.19</cell></row><row><cell>KID (?10 ?2 )</cell><cell>5.68</cell><cell>5.42</cell><cell>4.82</cell><cell>5.17</cell><cell>2.70</cell><cell>1.56</cell><cell>1.33</cell></row></table><note>Multimodal fusion on image translation (to RGB) with 1 ? 4 input modalities.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Modality</cell><cell>CEN (IN?6, enc?3, dec?3)</cell><cell>CEN-random (IN?6, enc?1, dec?3)</cell><cell>CEN-cycle (IN?6, enc?1, dec?3)</cell><cell>CEN-cycle (IN?6, enc?1, dec?1)</cell></row><row><cell>RGB+Shade ? Texture</cell><cell>1.74 / 3.05</cell><cell>2.17 / 4.53</cell><cell>1.54 / 2.56</cell><cell>1.62 / 2.81</cell></row><row><cell>RGB+Texture ? Shade</cell><cell>16.53 / 25.07</cell><cell>18.26 / 28.60</cell><cell>15.53 / 23.77</cell><cell>16.10 / 24.36</cell></row><row><cell>Shade+Texture ? RGB</cell><cell>62.63 / 1.65</cell><cell>73.27 / 2.33</cell><cell>61.03 / 1.50</cell><cell>61.25 / 1.60</cell></row><row><cell>RGB+Depth ? SemSeg</cell><cell>21.52 / 36.24</cell><cell>22.80 / 37.09</cell><cell>18.57 / 33.29</cell><cell>18.71 / 33.56</cell></row><row><cell>RGB+SemSeg ? Depth</cell><cell>4.63 / 8.59</cell><cell>5.03 / 8.81</cell><cell>4.02 / 7.90</cell><cell>4.27 / 8.26</cell></row><row><cell>Depth+SemSeg ? RGB</cell><cell>99.60 / 4.18</cell><cell>102.97 / 4.31</cell><cell>96.13 / 3.66</cell><cell>97.01 / 3.94</cell></row><row><cell>RGB+Depth ? Normal</cell><cell>13.03 / 28.75</cell><cell>15.72 / 31.15</cell><cell>12.26 / 27.12</cell><cell>11.94 / 26.79</cell></row><row><cell>RGB+Normal ? Depth</cell><cell>3.34 / 5.22</cell><cell>4.67 / 6.73</cell><cell>2.63 / 4.70</cell><cell>2.57 / 4.45</cell></row><row><cell>Depth+Normal ? RGB</cell><cell>84.33 / 2.70</cell><cell>90.49 / 3.73</cell><cell>82.81 / 2.64</cell><cell>83.73 / 2.66</cell></row><row><cell>RGB+Depth ? Curve</cell><cell>5.42 / 15.09</cell><cell>5.73 / 16.08</cell><cell>4.83 / 13.71</cell><cell>5.03 / 14.15</cell></row><row><cell>RGB+Curve ? Depth</cell><cell>2.62 / 3.87</cell><cell>2.82 / 4.23</cell><cell>2.14 / 3.47</cell><cell>2.25 / 3.67</cell></row><row><cell>Depth+Curve ? RGB</cell><cell>85.13 / 2.82</cell><cell>88.69 / 3.39</cell><cell>83.85 / 2.42</cell><cell>84.52 / 2.64</cell></row><row><cell>Depth+Normal ? Shade</cell><cell>7.10 / 11.22</cell><cell>7.47 / 11.45</cell><cell>7.03 / 10.65</cell><cell>6.60 / 10.31</cell></row><row><cell>Shade+Depth ? Normal</cell><cell>13.11 / 31.57</cell><cell>13.74 / 32.20</cell><cell>13.12 / 31.65</cell><cell>12.92 / 31.30</cell></row><row><cell>Shade+Normal ? Depth</cell><cell>1.62 / 2.91</cell><cell>1.92 / 3.18</cell><cell>1.56 / 2.94</cell><cell>1.50 / 2.87</cell></row><row><cell>Total params. (M)</cell><cell>Gen: 163.3; Dis: 8.3</cell><cell>Gen: 124.2; Dis: 8.3</cell><cell>Gen: 124.2; Dis: 8.3</cell><cell>Gen: 54.5; Dis: 8.3</cell></row></table><note>Experimental results of cycle multimodal fusion. Evaluation metrics are FID/KID (?10 ?2 ) for RGB predictions and MAE (?10 ?2 )/MSE (?10 ?2 ) for other predictions. Lower values indicate better performance for all these metrics. "Curve" and "SemSeg" are abbreviations for the principle curve and semantic segmentation, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Modality</cell><cell>Indiv (IN?2, enc?2, dec?2)</cell><cell>Indiv (IN?2, enc?1, dec?2)</cell><cell>X-TC [65] (IN?2, enc?1, dec?2)</cell><cell>CEN-dec (IN?2, enc?1, dec?2)</cell><cell>CEN-dec + X-TC [65] (IN?2, enc?1, dec?2)</cell></row><row><cell>RGB ?</cell><cell cols="2">SemSeg Depth</cell><cell>26.71 / 40.15 5.35 / 9.13</cell><cell>27.14 / 41.90 5.51 / 9.42</cell><cell>23.83 / 38.10 5.22 / 8.98</cell><cell>23.02 / 37.54 4.82 / 8.50</cell><cell>21.78 / 37.32 4.76 / 8.43</cell></row><row><cell>RGB ?</cell><cell cols="2">Normal Curve</cell><cell>18.74 / 37.24 6.24 / 16.97</cell><cell>18.15 / 36.82 6.02 / 16.70</cell><cell>18.18 / 36.49 5.33 / 14.76</cell><cell>16.74 / 32.26 4.92 / 14.08</cell><cell>14.85 / 29.33 4.50 / 13.81</cell></row><row><cell>RGB ?</cell><cell cols="2">Shade Texture</cell><cell>24.04 / 33.85 2.40 / 4.93</cell><cell>23.63 / 32.92 2.19 / 4.66</cell><cell>19.04 / 29.87 2.33 / 4.85</cell><cell>18.77 / 27.94 1.83 / 3.67</cell><cell>17.07 / 27.10 1.64 / 2.99</cell></row><row><cell>Texture ?</cell><cell></cell><cell>RGB Depth</cell><cell>97.51 / 4.82 4.20 / 8.16</cell><cell>96.81 / 4.57 4.05 / 7.94</cell><cell>95.81 / 3.94 3.54 / 6.07</cell><cell>92.92 / 3.25 3.19 / 5.05</cell><cell>90.85 / 2.81 2.90 / 4.87</cell></row><row><cell cols="2">Normal ?</cell><cell>Depth Shade</cell><cell>2.59 / 3.92 8.08 / 12.40</cell><cell>2.72 / 4.16 7.90 / 12.03</cell><cell>2.20 / 3.54 7.26 / 11.52</cell><cell>1.97 / 3.30 7.09 / 11.14</cell><cell>1.85 / 3.04 6.94 / 10.88</cell></row><row><cell cols="7">Total params. (M) Gen: 108.7; Dis: 8.3 Gen: 89.3; Dis: 8.3 Gen: 89.3; Dis: 8.3 Gen: 89.3; Dis: 8.3</cell><cell>Gen: 89.3; Dis: 8.3</cell></row></table><note>Experimental results of multitask learning. Evaluation metrics are FID/KID (?10 ?2 ) for RGB predictions and MAE (?10 ?2 )/MSE (?10 ?2 ) for other predictions. Lower values indicate better performance. Individual (Indiv) learning and Cross-Task Consistency (X-TC) [65] are served as baselines. We provide numbers of groups for instance normalization (IN), encoder (enc) and decoder (dec), and the total parameters (params.) in generator (Gen) and discriminator (Dis), respectively. "Curve" and "SemSeg" are abbreviations for the principle curve and semantic segmentation, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 :</head><label>8</label><figDesc>Experimental results of simultaneously predicting three tasks. Evaluation metrics and abbreviations follow</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 9 :</head><label>9</label><figDesc></figDesc><table><row><cell cols="7">Experimental results of multimodal multitask learning. Evaluation metrics are FID/KID (?10 ?2 ) for RGB</cell></row><row><cell cols="7">predictions and MAE (?10 ?2 )/MSE (?10 ?2 ) for other predictions. Lower values indicate better performance. Individual</cell></row><row><cell cols="7">(Indiv) learning is served as the baseline. We provide numbers of groups for instance normalization (IN), encoder (enc)</cell></row><row><cell cols="7">and decoder (dec), and the total parameters (params.) in generator (Gen) and discriminator (Dis). "Curve" and "SemSeg"</cell></row><row><cell cols="6">are abbreviations for the principle curve and semantic segmentation, respectively.</cell><cell></cell></row><row><cell cols="3">Modality</cell><cell>Indiv (IN?4, enc?2, dec?2)</cell><cell>CEN-enc (IN?4, enc?1, dec?2)</cell><cell>CEN-dec (IN?4, enc?1, dec?2)</cell><cell>CEN-enc &amp; dec (IN?4, enc?1, dec?2)</cell></row><row><cell>RGB Depth</cell><cell>?</cell><cell>SemSeg Curve</cell><cell>26.86 / 40.24 5.97 / 16.51</cell><cell>21.17 / 36.05 5.49 / 15.30</cell><cell>25.22 / 39.36 5.76 / 16.04</cell><cell>20.25 / 35.17 5.27 / 14.93</cell></row><row><cell>RGB Depth</cell><cell>?</cell><cell>Nomal Shade</cell><cell>18.68 / 37.11 8.62 / 12.76</cell><cell>13.54 / 29.03 7.37 / 11.09</cell><cell>16.81 / 32.75 8.20 / 12.14</cell><cell>12.23 / 27.39 7.08 / 10.91</cell></row><row><cell>RGB Edge</cell><cell>?</cell><cell>Depth Normal</cell><cell>4.49 / 9.80 16.56 / 33.40</cell><cell>2.81 / 6.77 13.28 / 29.32</cell><cell>4.02 / 8.53 15.14 / 32.72</cell><cell>2.47 / 6.33 12.62 / 28.71</cell></row><row><cell>Texture Shade</cell><cell>?</cell><cell>RGB Depth</cell><cell>97.31 / 4.76 2.66 / 4.20</cell><cell>62.47 / 1.63 1.64 / 3.03</cell><cell>87.50 / 3.72 2.18 / 3.77</cell><cell>60.26 / 1.57 1.58 / 2.94</cell></row><row><cell cols="3">Total params. (M)</cell><cell>Gen: 108.7; Dis: 8.3</cell><cell>Gen: 89.3; Dis: 8.3</cell><cell>Gen: 89.3; Dis: 8.3</cell><cell>Gen: 89.3; Dis: 8.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>-Net-256. Dataset: Taskonomy.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scaling factor !</cell><cell>0.06 0.08 0.10 0.12 0.04</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Threshold</cell><cell>Scaling factor !</cell><cell>0.04 0.06 0.08 0.10 0.12</cell><cell></cell><cell></cell><cell></cell><cell>Threshold</cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.00</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000 1200</cell><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000 1200</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training step</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Training step</cell></row><row><cell></cell><cell cols="6">(a) Scaling factors of the first 128 channels</cell><cell></cell><cell cols="5">(b) Scaling factors of the first 128 channels</cell></row><row><cell></cell><cell cols="6">(within sparsity constraints) when channel</cell><cell></cell><cell cols="5">(within sparsity constraints) when channel</cell></row><row><cell></cell><cell></cell><cell cols="5">exchanging is NOT applied</cell><cell></cell><cell></cell><cell cols="4">exchanging is applied</cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.12</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scaling factor !</cell><cell>0.04 0.06 0.08 0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Scaling factor !</cell><cell>0.04 0.06 0.08 0.10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000 1200</cell><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000 1200</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training step</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Training step</cell></row><row><cell></cell><cell cols="6">(c) Scaling factors of the last 128 channels</cell><cell></cell><cell cols="5">(d) Scaling factors of the last 128 channels</cell></row><row><cell></cell><cell cols="6">(beyond sparsity constraints) when channel</cell><cell></cell><cell cols="5">(beyond sparsity constraints) when channel</cell></row><row><cell></cell><cell></cell><cell cols="5">exchanging is NOT applied</cell><cell></cell><cell></cell><cell cols="4">exchanging is applied</cell></row><row><cell cols="13">Fig. 8: Typical values of BN scaling factors (w.r.t. the RGB</cell></row><row><cell cols="13">modality) within/beyond sparsity constraints vs training</cell></row><row><cell cols="13">steps. We compare circumstances when channel exchang-</cell></row><row><cell cols="13">ing is and is not applied. Experiments are conducted on</cell></row><row><cell cols="13">NYUDv2 with RefineNet (ResNet101). We choose the 8th</cell></row><row><cell cols="13">layer of convolutional layers that have 3 ? 3 kernels, and</cell></row><row><cell cols="13">there are 256 channels. Regarding RGB, sparsity constraints</cell></row><row><cell cols="13">to scaling factors are applied on the first 128 channels.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Backbone: ResNet-101. Dataset: NYUDv2.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">47.5?0.4</cell><cell></cell><cell></cell><cell></cell><cell>47.5?0.4</cell><cell></cell><cell></cell><cell>51.0?0.3</cell><cell></cell><cell></cell><cell>51.0?0.3</cell></row><row><cell cols="13">(a) Sensitivity analysis when simply zeroing out channels with small scaling factors (channel exchanging is NOT applied) 97.3?0.6 97.3?0.6 Backbone: UFig. 9: Effect when zeroing out channels (without channel (b) Sensitivity analysis when channel exchanging is applied on channels with small scaling factors 63.2?0.6 63.2?0.6</cell></row><row><cell cols="13">exchanging) as well as the sensitivity analysis for ? and ?.</cell></row><row><cell cols="13">Experiments include RGB-D semantic segmentation (Sem-</cell></row><row><cell cols="13">Seg) on NYUDv2 (top group) and Texture+Shade?RGB on</cell></row><row><cell cols="13">Taskonomy (bottom group). We conduct five experiments</cell></row><row><cell cols="13">for each parameter setting. Default settings are ? = 10 ?3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 10 :</head><label>10</label><figDesc>We compare training multimodal features in a parallel manner with different parameter sharing settings. Results of the proposed fusion method are reported in the last column. Evaluation metrics are FID/KID (?10 ?2 ). We observe that the convolutional layers can be shared as long as we leave individual INs for different modalities, achieving even better performance.</figDesc><table><row><cell>Modality</cell><cell>Network stream</cell><cell>Unshared Convs unshared INs</cell><cell>Shared Convs shared INs</cell><cell>Shared Convs unshared INs</cell><cell>Multi-modal fusion</cell></row><row><cell>Shade +Texture</cell><cell>Shade Texture</cell><cell>102.21 / 5.25 98.19 / 4.83</cell><cell cols="3">112.40 / 5.58 100.69 / 4.51 72.07 / 2.32 102.28 / 5.22 93.40 / 4.18 65.60 / 1.82</cell></row><row><cell>?RGB</cell><cell>Ensemble</cell><cell>92.72 / 4.15</cell><cell>96.31 / 4.36</cell><cell cols="2">87.91 / 3.73 62.63 / 1.65</cell></row><row><cell>Shade +Texture +Depth</cell><cell>Shade Texture Depth</cell><cell>101.86 / 5.18 98.60 / 4.89 114.18 / 5.71</cell><cell cols="3">115.51 / 5.77 104.39 / 4.54 121.40 / 6.23 107.07 / 5.19 71.61 / 2.27 98.49 / 4.07 69.37 / 2.21 95.87 / 4.27 64.70 / 1.73</cell></row><row><cell>?RGB</cell><cell>Ensemble</cell><cell>91.30 / 3.92</cell><cell>100.41 / 4.73</cell><cell cols="2">84.39 / 3.45 58.35 / 1.42</cell></row><row><cell>Shade +Texture +Depth</cell><cell>Shade Texture Depth</cell><cell>100.83 / 5.06 97.34 / 4.77 114.50 / 5.83</cell><cell cols="3">131.74 / 7.48 109.45 / 4.86 125.54 / 6.48 109.93 / 5.41 70.47 / 2.09 96.98 / 4.23 68.70 / 2.14 94.64 / 4.22 63.26 / 1.69</cell></row><row><cell>+Normal</cell><cell>Normal</cell><cell>108.65 / 5.45</cell><cell>113.15 / 5.72</cell><cell cols="2">99.38 / 4.45 67.73 / 1.98</cell></row><row><cell>?RGB</cell><cell>Ensemble</cell><cell>89.52 / 3.80</cell><cell>102.78 / 4.67</cell><cell cols="2">86.76 / 3.63 57.19 / 1.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE 11 :</head><label>11</label><figDesc>An Instance-Normalization layer consists of four components, including scaling factors ?, offsets ?, running mean ? and variance ?. FollowingTable 5, we further compare the evaluation results with only unshared ?, ?, or only unshared ?, ?. Evaluation metrics are FID/KID (?10 ?2 ). We observe that using unshared scaling factors and offsets seems to be more important. 1 regulation and channel exchanging are not applied throughout these experiments.</figDesc><table><row><cell>Modality</cell><cell>Network stream</cell><cell>Unshared Convs unshared INs</cell><cell>Shared Convs unshared INs</cell><cell>Shared Convs,?,? unshared ?,?</cell><cell>Shared Convs,?,? unshared ?,?</cell></row><row><cell>Shade +Texture +Depth</cell><cell>Shade Texture Depth</cell><cell>101.86 / 5.18 98.60 / 4.89 114.18 / 5.71</cell><cell>98.49 / 4.07 95.87 / 4.27 102.07 / 4.89</cell><cell>107.86 / 5.53 105.46 / 5.25 118.35 / 6.07</cell><cell>105.29 / 5.29 102.90 / 5.06 114.35 / 5.80</cell></row><row><cell>?RGB</cell><cell>Ensemble</cell><cell>91.30 / 3.92</cell><cell>84.39 / 3.45</cell><cell>96.30 / 4.41</cell><cell>92.25 / 4.02</cell></row><row><cell>Shade +Texture +Depth</cell><cell>Shade Texture Depth</cell><cell>100.83 / 5.06 97.34 / 4.77 114.50 / 5.83</cell><cell>96.98 / 4.23 94.64 / 4.22 109.93 / 5.41</cell><cell>113.56 / 5.65 105.36 / 5.32 119.31 / 6.20</cell><cell>102.74 / 5.17 97.53 / 4.56 112.73 / 5.60</cell></row><row><cell>+Normal</cell><cell>Normal</cell><cell>108.65 / 5.45</cell><cell>99.38 / 4.45</cell><cell>108.01 / 5.06</cell><cell>100.34 / 4.53</cell></row><row><cell>?RGB</cell><cell>Ensemble</cell><cell>89.52 / 3.80</cell><cell>86.76 / 3.63</cell><cell>95.56 / 4.64</cell><cell>89.26 / 3.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>TABLE 12 :</head><label>12</label><figDesc>Additional experiments on the NYUDv2 dataset based on RefineNet (ResNet101) to evaluate the importance of the exchanging process. Results include multimodal fusion on image translation (to RGB) with 2 ? 4 input modalities. Evaluation metrics are FID/KID (?10 ?2 ) and lower values indicate better performance.For modality m, replacing (the feature map) at a zeroed-out channel (channel index i) with: Depth+Normal Depth+Normal +Texture Depth+Normal +Texture+Shade A zero embedding (only fusion by ensemble) 107.32 / 5.39 96.90 / 4.75 95.50 / 4.68 The i-th channel from another one modality m = m Same with CEN 63.14 / 1.73 62.76 / 1.69 Average of evenly spaced channels [a] (Fig. 10) (beyond sparsity constraints) from the same modality m 106.62 / 5.29 95.63 / 4.64 95.90 / 4.71 One random channel in the region [b](Fig. 10) (beyond sparsity constraints)from the same modality m Average of unused channels with the largest scaling factors from other modalities ?m = m 88.61 / 3.13 68.09 / 2.10 68.87 / 2.15 Weighted average of the i-th channels [d] (Fig. 10) from other modalities ?m = m Same with CEN 61.07 / 1.59 58.26 / 1.40 Concatenation (followed by a 1 ? 1 Conv) of the i-th channels [d] (Fig. 10) from other modalities ?m = m Same with CEN 63.32 / 1.73 61.70 / 1.64 Average of the i-th channels [d] (Fig. 10) from other modalities ?m = m (our CEN)</figDesc><table><row><cell>109.71 / 5.62</cell><cell>97.06 / 4.92</cell><cell>96.64 / 4.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 13 :</head><label>13</label><figDesc>Multimodal fusion on image translation (to RGB) with or without (w/o) dividing channels into M sub-parts. Evaluation metrics are FID/KID (?10 ?2 ). Lower values indicate better performance for both metrics.</figDesc><table><row><cell>Method</cell><cell>Depth+Normal</cell><cell>Depth+Normal +Texture</cell><cell>Depth+Normal +Texture+Shade</cell></row><row><cell cols="2">Dividing M sub-parts (default) 84.33 / 2.70</cell><cell>60.90 / 1.56</cell><cell>57.19 / 1.33</cell></row><row><cell>W/o dividing M sub-parts</cell><cell>87.63 / 3.49</cell><cell>65.12 / 1.90</cell><cell>64.87 / 1.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 14 :</head><label>14</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>In our image-to-image translation experiments on the Taskonomy dataset, we find that CEN with 1,000 training images already achieves</figDesc><table><row><cell>RGB</cell><cell>Depth</cell><cell>Ground truth</cell><cell></cell><cell></cell></row><row><cell>Concat</cell><cell>Align</cell><cell>Self-att.</cell><cell></cell><cell>Ours Our CEN</cell></row><row><cell>RGB</cell><cell>Depth</cell><cell>Ground truth</cell><cell></cell><cell></cell></row><row><cell>Concat</cell><cell>Align</cell><cell>Self-att.</cell><cell></cell><cell>Ours Our CEN</cell></row><row><cell>RGB</cell><cell>Depth</cell><cell>Ground truth</cell><cell></cell><cell></cell></row><row><cell>Concat</cell><cell>Align</cell><cell>Self-att.</cell><cell></cell><cell>Ours Our CEN</cell></row><row><cell></cell><cell></cell><cell></cell><cell>159</cell><cell>.088</cell><cell>.124</cell><cell>.149</cell></row><row><cell></cell><cell></cell><cell>SP [82] (RGB-D)</cell><cell>.135</cell><cell>.065</cell><cell>.099</cell><cell>.107</cell></row><row><cell></cell><cell></cell><cell>CEN (RGB-D)</cell><cell>.132</cell><cell>.059</cell><cell>.095</cell><cell>.103</cell></row></table><note>the effectiveness of CEN in this case. Further discussion of unsupervised learning is left for future exploration. Enlarging the sampled dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>TABLE 15 :</head><label>15</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>RGB 75.39 / 2.77 75.46 / 2.82 86.20 / 3.92 68.65 / 2.23 56.94 / 1.45</figDesc><table><row><cell></cell><cell>Concat</cell><cell>Average</cell><cell>Align</cell><cell>Self-att.</cell><cell>Our CEN</cell></row><row><cell>Shade+Texture ? Depth+Normal ? RGB</cell><cell>91.64 / 3.38</cell><cell>93.81 / 3.50</cell><cell>97.05 / 3.99</cell><cell>88.60 / 3.02</cell><cell>79.68 / 2.59</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Refinenet: Multipath refinement networks for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised model adaptation for multimodal semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">IJCV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Rgb+depth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semseg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<idno>124.2; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<idno>124.2; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<idno>54.5; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multitask learning Indiv (IN?2, enc?2, dec?2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X-Tc</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>65] (IN?2, enc?1, dec?2) CEN-dec (IN?2, enc?1, dec?2</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cen-Dec + X-Tc</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>65] (IN?2, enc?1, dec?2</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Multimodal multitask learning Indiv (IN?4, enc?2, dec?2) CEN-enc (IN?4, enc?1, dec?2) CEN-dec (IN?4, enc?1, dec?2) CEN-enc &amp; dec</title>
		<imprint/>
	</monogr>
	<note>IN?4, enc?1, dec?2</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<idno>89.3; Dis: 8.3</idno>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modality distillation with multiple stream networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modality compensation network: Cross-modal adaptation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal learning and reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pose guided RGBD feature learning for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning multimodal graph-to-graph translation for molecule optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust multi-modality multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-task multiview learning based on cooperative multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end multi-task learning with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to branch for multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Which tasks should be learned together in multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adashare: Learning what to share for efficient deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modular multitask reinforcement learning with policy sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rahmatizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abolghasemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>B?l?ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fapn: Feature-aligned pyramid network for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion methods and applications: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">DRIT++: diverse image-to-image translation via disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>IJCV, 2020. 1</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based CNN architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep surface normal estimation with hierarchical RGB-D fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning common and specific features for RGB-D semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multinet++: Multi-stream feature aggregation and geometric loss strategy for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawashdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Latent multitask architecture learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Translate-torecognize networks for RGB-D scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Rdfnet: RGB-D multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04650</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking the smaller-normless-informative assumption in channel pruning of convolution layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Tsnet: Three-stream selfattention network for RGB-D indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">U2fusion: A unified unsupervised image fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Multimedia systems</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An introduction to multisensor data fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning deep multimodal feature representation with asymmetric multi-layer fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Feature-wise transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Learning multiple tasks with multilinear relationship networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02117</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Regularizing deep multi-task networks using orthogonal gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suteu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06844</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A survey on multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Which tasks should be learned together in multitask learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Standley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>in ICML, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cheerla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>Robust learning through cross-task consistency,&quot; in CVPR, 2020. 3, 11</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Mti-net: Multiscale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2017. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Channel equilibrium networks for learning deep representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Imagenet large scale visual recognition challenge,&quot; IJCV, 2015. 6</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Demystifying MMD gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">ACNET: attention based network to exploit complementary features for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">3d graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">SCN: switchable context network for semantic segmentation of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Adashare: Learning what to share for efficient deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Promoting saliency from depth: Deep unsupervised RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2022. 13</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">RGBD salient object detection: a benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Real-time salient object detection with a minimum spanning tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Exploiting global priors for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Ying</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Stereoscopic saliency model using contrast and depth-guidedbackground prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image via saliency evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A multilayer backpropagation saliency detection algorithm based on depth mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCAIP</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">DeepUSPS: Deep robust unsupervised saliency prediction via self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multimodal token fusion for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
