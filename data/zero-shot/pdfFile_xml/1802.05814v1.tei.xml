<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2018">2018. 2018. 2018. 2018. April 23-27. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
							<email>dliang@netflix.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
							<email>rahulgk@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
							<email>mhoffman@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CA</roleName><forename type="first">San</forename><surname>Francisco</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
							<email>tjebara@netflix.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Netflix Los Gatos</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Netflix</orgName>
								<address>
									<settlement>Los Gatos</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of The</title>
						<meeting>The <address><addrLine>New York, NY, USA; Lyon, France</addrLine></address>
						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="volume">10</biblScope>
							<date type="published" when="2018">2018. 2018. 2018. 2018. April 23-27. 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3178876.3186150</idno>
					<note>ACM Reference Format: This paper is published under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 International (CC BY-NC-ND 4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender systems</term>
					<term>collaborative filtering</term>
					<term>implicit feedback</term>
					<term>variational autoencoder</term>
					<term>Bayesian models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We extend variational autoencoders (vaes) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research. We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems are an integral component of the web. In a typical recommendation system, we observe how a set of users interacts with a set of items. Using this data, we seek to show users a set of previously unseen items they will like. As the web grows in size, good recommendation systems will play an important part in helping users interact more effectively with larger amounts of content.</p><p>Collaborative filtering is among the most widely applied approaches in recommender systems. Collaborative filtering predicts what items a user will prefer by discovering and exploiting the similarity patterns across users and items. Latent factor models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref> still largely dominate the collaborative filtering research literature due to their simplicity and effectiveness. However, these models are inherently linear, which limits their modeling capacity. Previous work <ref type="bibr" target="#b26">[27]</ref> has demonstrated that adding carefully crafted non-linear features into the linear latent factor models can significantly boost recommendation performance. Recently, a growing body of work involves applying neural networks to the collaborative filtering setting with promising results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Here, we extend variational autoencoders (vaes) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> to collaborative filtering for implicit feedback. Vaes generalize linear latent-factor models and enable us to explore non-linear probabilistic latent-variable models, powered by neural networks, on large-scale recommendation datasets. We propose a neural generative model with multinomial conditional likelihood. Despite being widely used in language modeling and economics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30]</ref>, multinomial likelihoods appear less studied in the collaborative filtering literature, particularly within the context of latent-factor models. Recommender systems are often evaluated using rankingbased measures, such as mean average precision and normalized discounted cumulative gain <ref type="bibr" target="#b20">[21]</ref>. Top-N ranking loss is difficult to optimize directly and previous work on direct ranking loss minimization resorts to relaxations and approximations <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. Here, we show that the multinomial likelihoods are well-suited for modeling implicit feedback data, and are a closer proxy to the ranking loss relative to more popular likelihood functions such as Gaussian and logistic.</p><p>Though recommendation is often considered a big-data problem (due to the huge numbers of users and items typically present in a recommender system), we argue that, in contrast, it represents a uniquely challenging "small-data" problem: most users only interact with a tiny proportion of the items and our goal is to collectively make informed inference about each user's preference. To make use of the sparse signals from users and avoid overfitting, we build a probabilistic latent-variable model that shares statistical strength among users and items. Empirically, we show that employing a principled Bayesian approach is more robust regardless of the scarcity of the data.</p><p>Although vaes have been extensively studied for image modeling and generation, there is surprisingly little work applying vaes to recommender systems. We find that two adjustments are essential to getting state-of-the-art results with vaes on this task:</p><p>? First, we use a multinomial likelihood for the data distribution. We show that this simple choice realizes models that outperform the more commonly used Gaussian and logistic likelihoods. ? Second, we reinterpret and adjust the standard vae objective, which we argue is over-regularized. We draw connections between the learning algorithm resulting from our proposed regularization and the information-bottleneck principle and maximum-entropy discrimination.</p><p>The result is a recipe that makes vaes practical solutions to this important problem. Empirically, our methods significantly outperform state-of-the-art baselines on several real-world datasets, including two recently proposed neural-network approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>We use u ? {1, . . . , U } to index users and i ? {1, . . . , I } to index items. In this work, we consider learning with implicit feedback <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. The user-by-item interaction matrix is the click 1 matrix X ? N U ?I . The lower case x u = [x u1 , . . . , x uI ] ? ? N I is a bag-ofwords vector with the number of clicks for each item from user u. For simplicity, we binarize the click matrix. It is straightforward to extend it to general count data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>The generative process we consider in this paper is similar to the deep latent Gaussian model <ref type="bibr" target="#b36">[37]</ref>. For each user u, the model starts by sampling a K-dimensional latent representation z u from a standard Gaussian prior. The latent representation z u is transformed via a non-linear function f ? (?) ? R I to produce a probability distribution over I items ? (z u ) from which the click history x u is assumed to have been drawn:</p><formula xml:id="formula_0">z u ? N (0, I K ), ? (z u ) ? exp{ f ? (z u )}, x u ? Mult(N u , ? (z u )).<label>(1)</label></formula><p>The non-linear function f ? (?) is a multilayer perceptron with parameters ? . The output of this transformation is normalized via a softmax function to produce a probability vector ? (z u ) ? S I ?1 (an (I ? 1)-simplex) over the entire item set. Given the total number of clicks N u = i x ui from user u, the observed bag-of-words vector x u is assumed to be sampled from a multinomial distribution with probability ? (z u ). This generative model generalizes the latentfactor model -we can recover classical matrix factorization <ref type="bibr" target="#b37">[38]</ref> by setting f ? (?) to be linear and using a Gaussian likelihood.</p><p>The log-likelihood for user u (conditioned on the latent representation) is:</p><formula xml:id="formula_1">log p ? (x u | z u ) c = i x ui log ? i (z u ).<label>(2)</label></formula><p>This multinomial likelihood is commonly used in language models, e.g., latent Dirichlet allocation <ref type="bibr" target="#b4">[5]</ref>, and economics, e.g., multinomial logit choice model <ref type="bibr" target="#b29">[30]</ref>. It is also used in the cross-entropy loss 2 for multi-class classification. For example, it has been used in recurrent neural networks for session-based sequential recommendation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> and in feedward neural networks applied to Youtube recommendation <ref type="bibr" target="#b8">[9]</ref>. The multinomial likelihood is less well studied in the context of latent-factor models such as matrix factorization and autoencoders. A notable exception is the collaborative competitive filtering (CCF) model <ref type="bibr" target="#b52">[53]</ref> and its successors, which take advantage of more fine-grained information about what options were presented to which users. (If such information is available, it can also be incorporated into our vae-based approach.) We believe the multinomial distribution is well suited to modeling click data. The likelihood of the click matrix (Eq. 2) rewards the model for putting probability mass on the non-zero entries in x u . But the model has a limited budget of probability mass, since ? (z u ) must sum to 1; the items must compete for this limited budget <ref type="bibr" target="#b52">[53]</ref>. The model should therefore assign more probability mass to items that are more likely to be clicked. To the extent that it can, it will perform well under the top-N ranking loss that recommender systems are commonly evaluated on.</p><p>By way of comparison, we present two popular choices of likelihood functions used in latent-factor collaborative filtering: Gaussian and logistic likelihoods. Define f ? (z u ) ? [f u1 , . . . , f uI ] ? as the output of the generative function f ? (?). The Gaussian log-likelihood for user u is</p><formula xml:id="formula_2">log p ? (x u | z u ) c = ? i c ui 2 (x ui ? f ui ) 2 .<label>(3)</label></formula><p>We adopt the convention in Hu et al. <ref type="bibr" target="#b18">[19]</ref> and introduce a "confidence" weight c x ui ? c ui where c 1 &gt; c 0 to balance the unobserved 0's which far outnumber the observed 1's in most click data. This is also equivalent to training the model with unweighted Gaussian likelihood and negative sampling. The logistic log-likelihood 3 for user u is</p><formula xml:id="formula_3">log p ? (x u | z u ) = i x ui log ? (f ui ) + (1 ?x ui ) log(1 ? ? (f ui )),<label>(4)</label></formula><p>where ? (x) = 1/(1 + exp(?x)) is the logistic function. We compare multinomial likelihood with Gaussian and logistic in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational inference</head><p>To learn the generative model in Eq. 1, we are interested in estimating ? (the parameters of f ? (?)). To do so, for each data point we need to approximate the intractable posterior distribution p(z u | x u ).</p><p>We resort to variational inference <ref type="bibr" target="#b21">[22]</ref>. Variational inference approximates the true intractable posterior with a simpler variational distribution q(z u ). We set q(z u ) to be a fully factorized (diagonal) Gaussian distribution:</p><formula xml:id="formula_4">q(z u ) = N (? u , diag{? 2 u }).</formula><p>The objective of variational inference is to optimize the free variational parameters {? u , ? 2 u } so that the Kullback-Leiber divergence KL(q(z u )?p(z u |x u )) is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Amortized inference and the variational autoencoder:</head><p>With variational inference the number of parameters to optimize {? u , ? 2 u } grows with the number of users and items in the dataset. This can become a bottleneck for commercial recommender systems with millions of users and items. The variational autoencoder (vae) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> replaces individual variational parameters with a data-dependent function (commonly called an inference model):</p><formula xml:id="formula_5">? ? (x u ) ? [? ? (x u ), ? ? (x u )] ? R 2K</formula><p>parametrized by ? with both ? ? (x u ) and ? ? (x u ) being K-vectors and sets the variational distribution as follows:</p><formula xml:id="formula_6">q ? (z u | x u ) = N (? ? (x u ), diag{? 2 ? (x u )}).</formula><p>That is, using the observed data x u as input, the inference model outputs the corresponding variational parameters of variational distribution q ? (z u | x u ), which, when optimized, approximates the intractable posterior p(z u | x u ). <ref type="bibr" target="#b3">4</ref> Putting q ? (z u | x u ) and the generative model p ? (x u | z u ) together in <ref type="figure">Figure 2c</ref>, we end up with a neural architecture that resembles an autoencoder -hence the name variational autoencoder. Vaes make use of amortized inference <ref type="bibr" target="#b11">[12]</ref>: they flexibly reuse inferences to answer related new problems. This is well aligned with the ethos of collaborative filtering: analyze user preferences by exploiting the similarity patterns inferred from past experiences. In Section 2.4, we discuss how this enables us to perform prediction efficiently.</p><p>Learning vaes: As is standard when learning latent-variable models with variational inference <ref type="bibr" target="#b3">[4]</ref>, we can lower-bound the log marginal likelihood of the data. This forms the objective we seek to maximize for user u (the objective function of the dataset is obtained by averaging the objective function over all the users):</p><formula xml:id="formula_7">log p(x u ; ? ) ? E q ? (z u | x u ) [log p ? (x u | z u )] ? KL(q ? (z u | x u )?p(z u )) ? L(x u ; ?, ?)<label>(5)</label></formula><p>This is commonly known as the evidence lower bound (elbo). Note that the elbo is a function of both ? and ?. We can obtain an unbiased estimate of elbo by sampling z u ? q ? and perform stochastic gradient ascent to optimize it. However, the challenge is that we cannot trivially take gradients with respect to ? through this sampling process. The reparametrization trick <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> sidesteps this issue: we sample ? ? N (0, I K ) and reparametrize z u = ? ? (x u ) + ? ? ? ? (x u ). By doing so, the stochasticity in the sampling process is isolated and the gradient with respect to ? can be back-propagated through the sampled z u . The vae training procedure is summarized in Algorithm 1. We can view elbo defined in Eq. 5 from a different perspective: the first term can be interpreted as (negative) reconstruction error, while the second KL term can be viewed as regularization. It is this perspective we work with because it allows us to make a trade-off that forms the crux of our method. From this perspective, it is natural to extend the elbo by introducing a parameter ? to control the strength of regularization:</p><formula xml:id="formula_8">L ? (x u ; ?, ?) ? E q ? (z u | x u ) [ log p ? (x u | z u )] ?? ? KL(q ? (z u | x u )?p(z u )).<label>(6)</label></formula><p>While the original vae (trained with elbo in Eq. 5) is a powerful generative model; we might ask whether we need all the statistical properties of a generative model for tackling problems in recommender systems. In particular, if we are willing to sacrifice the ability to perform ancestral sampling, can we improve our performance? The regularization view of the elbo (Eq. 6) introduces a trade-off between how well we can fit the data and how close the approximate posterior stays to the prior during learning.</p><p>We propose using ? 1. This means we are no longer optimizing a lower bound on the log marginal likelihood. If ? &lt; 1, then we are also weakening the influence of the prior constraint <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the basic idea (we observe the same trend consistently across datasets). Here we plot the validation ranking metric without KL annealing (blue solid) and with KL annealing all the way to ? = 1 (green dashed, ? reaches 1 at around 80 epochs). As we can see, the performance is poor without any KL annealing. With annealing, the validation performance first increases as the training proceeds and then drops as ? gets close to 1 to a value that is only slightly better than doing no annealing at all.</p><p>Having identified the best ? based on the peak validation metric, we can retrain the model with the same annealing schedule, but stop increasing ? after reaching that value (shown as red dot-dashed in <ref type="figure" target="#fig_1">Figure 1</ref>). <ref type="bibr" target="#b4">5</ref> This might be sub-optimal compared to a thorough grid search. However, it is much more efficient, and gives us competitive empirical performance. If the computational budget is scarce, then within a single run, we can stop increasing ? when we notice the validation metric dropping. Such a procedure incurs no additional runtime to learning a standard vae. We denote this partially regularized vae with multinomial likelihood as Mult-vae pr .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Computational Burden.</head><p>Previous collaborative filtering models with neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51]</ref> are trained with stochastic gradient descent where in each step a single (user, item) entry from the click matrix is randomly sampled to perform a gradient update. In Algorithm 1 we subsample users and take their entire click history (complete rows of the click matrix) to update model parameters. This eliminates the necessity of negative sampling (and consequently the hyperparameter tuning for picking the number of negative examples), commonly used in the (user, item) entry subsampling scheme.</p><p>A computational challenge that comes with our approach, however, is that when the number of items is huge, computing the multinomial probability ? (z u ) could be computationally expensive, since it requires computing the predictions for all the items for normalization. This is a common challenge for language modeling where the size of the vocabulary is in the order of millions or more <ref type="bibr" target="#b31">[32]</ref>. In our experiments on some medium-to-large datasets with less than 50K items (Section 4.1), this has not yet come up as a computational bottleneck. If this becomes a bottleneck when working with larger item sets, one can easily apply the simple and <ref type="bibr" target="#b4">5</ref> We found this to give slightly better results than keeping ? at the best value throughout the training. effective method proposed by Botev et al. <ref type="bibr" target="#b5">[6]</ref> to approximate the normalization factor for ? (z u ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A taxonomy of autoencoders</head><p>In Section 2.2, we introduced maximum marginal likelihood estimation of vaes using approximate Bayesian inference under a non-linear generative model (Eq. 1). We now describe our work from the perspective of learning autoencoders. Maximum-likelihood estimation in a regular autoencoder takes the following form:</p><formula xml:id="formula_9">? AE , ? AE = arg max ?,? u E ? (z u ?? ? (x u )) [log p ? (x u | z u )] = arg max ?,? u log p ? (x u | ? ? (x u ))<label>(7)</label></formula><p>There are two key distinctions of note: (1) The autoencoder (and denoising autoencoder) effectively optimizes the first term in the vae objective (Eq. 5 and Eq. 6) using a delta variational distribution</p><formula xml:id="formula_10">q ? (z u | x u ) = ? (z u ? ? ? (x u )) -it does not regularize q ? (z u | x u ) towards any prior distribution as the vae does. (2) the ? (z u ?? ? (x u ))</formula><p>is a ? distribution with mass only at the output of ? ? (x u ). Contrast this to the vae, where the learning is done using a variational distribution, i.e., ? ? (x u ) outputs the parameters (mean and variance) of a Gaussian distribution. This means that vae has the ability to capture per-data-point variances in the latent state z u .</p><p>In practice, we find that learning autoencoders is extremely prone to overfitting as the network learns to put all the probability mass to the non-zero entries in x u . By introducing dropout <ref type="bibr" target="#b42">[43]</ref> at the input layer, the denoising autoencoder (dae) is less prone to overfitting and we find that it also gives competitive empirical results. In addition to the Mult-vae pr , we also study a denoising autoencoder with a multinomial likelihood. We denote this model Mult-dae. In Section 4 we characterize the tradeoffs in what is gained and lost by explicitly parameterizing the per-user variance with Mult-vae pr versus using a point-estimation in Mult-dae.</p><p>To provide a unified view of different variants of autoencoders and clarify where our work stands, we depict variants of autoencoders commonly found in the literature in <ref type="figure">Figure 2</ref>. For each one, we specify the model (dotted arrows denote a sampling operation) and describe the training objective used in parameter estimation.</p><p>In <ref type="figure">Figure 2a</ref> we have autoencoder. It is trained to reconstruct input with the same objective as in Eq. 7. Adding noise to the input (or the intermediate hidden representation) of an autoencoder yields the denoising autoencoder in <ref type="figure">Figure 2b</ref>. The training objective is the same as that of an autoencoder. Mult-dae belongs to this model class. Collaborative denoising autoencoder <ref type="bibr" target="#b50">[51]</ref> is a variant of this model class. The vae is depicted in <ref type="figure">Figure 2c</ref>. Rather than using a delta variational distribution, it uses an inference model parametrized by ? to produce the mean and variance of the approximating variational distribution. The training objective of the vae is given in Eq. 6. Setting ? to 1 recovers the original vae formulation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref>. Higgins et al. <ref type="bibr" target="#b16">[17]</ref> study the case where ? &gt; 1. Our model, Mult-vae pr corresponds to learning vaes with ? ? [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Prediction</head><p>We now describe how we make predictions given a trained generative model of the form Eq. or Mult-dae (Section 2.3), we make predictions in the same way. Given a user's click history x, we rank all the items based on the un-normalized predicted multinomial probability f ? (z). The latent representation z for x is constructed as follows: For Mult-vae pr , we simply take the mean of the variational distribution z = ? ? (x); for Mult-dae, we take the output z = ? ? (x).</p><p>It is easy to see the advantage of using autoencoders. We can effectively make predictions for users by evaluating two functions -the inference model (encoder) ? ? (?) and the generative model (decoder) f ? (?). For most of the latent factor collaborative filtering model, e.g., matrix factorization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>, when given the click history of a user that is not present in the training data, normally we need to perform some form of optimization to obtain the latent factor for this user. This makes the use of autoencoders particularly attractive in industrial applications, where it is important that predictions be made cheaply and with low latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>Vaes on sparse data. Variational autoencoders (vaes) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> have seen much application to images since their inception. Doersch <ref type="bibr" target="#b9">[10]</ref> presents a review on different applications of vae to image data. Miao et al. <ref type="bibr" target="#b30">[31]</ref> study vaes on text data. More recent results from Krishnan et al. <ref type="bibr" target="#b24">[25]</ref> find that vaes (trained with Eq. 5) suffer from underfitting when modeling large, sparse, high-dimensional data. We notice similar issues when fitting vae without annealing ( <ref type="figure" target="#fig_1">Figure 1</ref>) or annealing to ? = 1. By giving up the ability to perform ancestral sampling in the model, and setting ? ? 1, the resulting model is no longer a proper generative model though for collaborative filtering tasks we always make predictions conditional on users' click history.</p><p>Information-theoretic connection with vae. The regularization view of the elbo in Eq. 6 resembles maximum-entropy discrimination <ref type="bibr" target="#b19">[20]</ref>. Maximum-entropy discrimination attempts to combine discriminative estimation with Bayesian inference and generative modeling. In our case, in Eq. 6, ? acts as a knob to balance discriminative and generative aspects of the model.</p><p>The procedure in Eq. 6 has information-theoretic connections described in Alemi et al. <ref type="bibr" target="#b0">[1]</ref>. The authors propose the deep variational information bottleneck, which is a variational approximation to the information bottleneck principle <ref type="bibr" target="#b45">[46]</ref>. They show that as a special case they can recover the learning objective used by vaes. They report more robust supervised classification performance with ? &lt; 1. This is consistent with our findings as well. Higgins et al. <ref type="bibr" target="#b16">[17]</ref> proposed ?-vae, which leads to the same objective as Eq. 6.</p><p>They motivate ?-vae for the goal of learning disentangled representations from images (basic visual concepts, such as shape, scale, and color). Their work, however, sets ? ? 1, effectively imposing a stronger independent prior assumption on the latent code z. While their motivations are quite different from ours, it is interesting to note orthogonal lines of research emerging from exploring the full spectrum of values for ?.</p><p>Neural networks for collaborative filtering. Early work on neural-network-based collaborative filtering models focus on explicit feedback data and evaluates on the task of rating predictions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54]</ref>. The importance of implicit feedback has been gradually recognized, and consequently most recent research, such as this work, has focused on it. The two papers that are most closely related to our approaches are collaborative denoising autoencoder <ref type="bibr" target="#b50">[51]</ref> and neural collaborative filtering <ref type="bibr" target="#b13">[14]</ref>.</p><p>Collaborative denoising autoencoder (cdae) <ref type="bibr" target="#b50">[51]</ref> augments the standard denoising autoencoder, described in Section 2.3, by adding a per-user latent factor to the input. The number of parameters of the cdae model grows linearly with both the number of users as well as the number of items, making it more prone to overfitting. In contrast, the number of parameters in the vae grows linearly with the number of items. The cdae also requires additional optimization to obtain the latent factor for unseen users to make predicion. In the paper, the authors investigate the Gaussian and logistic likelihood loss functions -as we show, the multinomial likelihood is significantly more robust for use in recommender systems. Neural collaborative filtering (ncf) <ref type="bibr" target="#b13">[14]</ref> explore a model with non-linear interactions between the user and item latent factors rather than the commonly used dot product. The authors demonstrate improvements of ncf over standard baselines on two small datasets. Similar to cdae, the number of parameters of ncf also grows linearly with both the number of users as well as items. We find that this becomes problematic for much larger datasets. We compare with both cdae and ncf in Section 4.</p><p>Asymmetric matrix factorization <ref type="bibr" target="#b34">[35]</ref> may also be interpreted as an autoencoder, as elaborated in Steck <ref type="bibr" target="#b43">[44]</ref>. We can recover this work by setting both f ? (?) and ? ? (?) to be linear.</p><p>Besides being applied in session-based sequential recommendation (see Section 2.1), various approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> have applied neural networks to incorporate side information into collaborative filtering models to better handle the cold-start problem. These approaches are complementary to ours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL STUDY</head><p>We evaluate the performance of Mult-vae pr and Mult-dae. We provide insights into their performance by exploring the resulting fits. We highlight the following results:</p><p>? Mult-vae pr achieves state-of-the-art results on three realworld datasets when compared with various baselines, including recently proposed neural-network-based collaborative filtering models. ? For the denoising and variational autoencoder, the multinomial likelihood compares favorably over the more common Gaussian and logistic likelihoods. ? Both Mult-vae pr and Mult-dae produce competitive empirical results. We identify when parameterizing the uncertainty explicitly as in Mult-vae pr does better/worse than the point estimate used by Mult-dae and list pros and cons for both approaches.</p><p>The source code to reproduce the experimental results is available on GitHub 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We study three medium-to large-scale user-item consumption datasets from various domains:</p><p>MovieLens-20M (ML-20M): These are user-movie ratings collected from a movie recommendation service. We binarize the explicit data by keeping ratings of four or higher and interpret them as implicit feedback. We only keep users who have watched at least five movies.</p><p>Netflix Prize (Netflix): This is the user-movie ratings data from the Netflix Prize 7 . Similar to ML-20M, we binarize explicit data by keeping ratings of four or higher. We only keep users who have watched at least five movies.</p><p>Million Song Dataset (MSD): This data contains the user-song play counts released as part of the Million Song Dataset <ref type="bibr" target="#b2">[3]</ref>. We binarize play counts and interpret them as implicit preference data. We only keep users with at least 20 songs in their listening history and songs that are listened to by at least 200 users. <ref type="table" target="#tab_1">Table 1</ref> summarizes the dimensions of all the datasets after preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We use two ranking-based metrics: Recall@R and the truncated normalized discounted cumulative gain (NDCG@R). For each user, both metrics compare the predicted rank of the held-out items with their true rank. For both Mult-vae pr and Mult-dae, we get the predicted rank by sorting the un-normalized multinomial probability f ? (z). While Recall@R considers all items ranked within the first R to be equally important, NDCG@R uses a monotonically increasing discount to emphasize the importance of higher ranks versus lower ones. Formally, define ?(r ) as the item at rank r , I[?] is the indicator function, and I u is the set of held-out items that user u clicked on.</p><p>Recall@R for user u is</p><formula xml:id="formula_11">Recall@R(u, ?) := R r =1 I[?(r ) ? I u ] min(M, |I u |) .</formula><p>The expression in the denominator is the minimum of R and the number of items clicked on by user u. This normalizes Recall@R to have a maximum of 1, which corresponds to ranking all relevant items in the top R positions.</p><p>Truncated discounted cumulative gain (DCG@R) is</p><formula xml:id="formula_12">DCG@R(u, ?) := R r =1 2 I[?(r )?I u ] ? 1 log(r + 1) .</formula><p>NDCG@R is the DCG@R linearly normalized to [0, 1] after dividing by the best possible DCG@R, where all the held-out items are ranked at the top.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental setup</head><p>We study the performance of various models under strong generalization <ref type="bibr" target="#b28">[29]</ref>: We split all users into training/validation/test sets. We train models using the entire click history of the training users. To evaluate, we take part of the click history from held-out (validation and test) users to learn the necessary user-level representations for the model and then compute metrics by looking at how well the model ranks the rest of the unseen click history from the held-out users.</p><p>This is relatively more difficult than weak generalization where the user's click history can appear during both training and evaluation. We consider it more realistic and robust as well. In the last row of <ref type="table" target="#tab_1">Table 1</ref>, we list the number of held-out users (we use the same number of users for validation and test). For each held-out user, we randomly choose 80% of the click history as the "fold-in" set to learn the necessary user-level representation and report metrics on the remaining 20% of the click history.</p><p>We select model hyperparameters and architectures by evaluating NDCG@100 on the validation users. For both Mult-vae pr and Mult-dae, we keep the architecture for the generative model f ? (?) and the inference model ? ? (?) symmetrical and explore multilayer perceptron (mlp) with 0, 1, and 2 hidden layers. We set the dimension of the latent representation K to 200 and any hidden layer to 600. As a concrete example, recall I is the total number of items, the overall architecture for a Mult-vae pr /Mult-dae with 1-hidden-layer mlp generative model would be [I ? 600 ? 200 ? 600 ? I ]. We find that going deeper does not improve performance. The best performing architectures are mlps with either 0 or 1 hidden layers. We use a tanh non-linearity as the activation function between layers.</p><p>Note that for Mult-vae pr , since the output of ? ? (?) is used as the mean and variance of a Gaussian random variable, we do not apply an activation function to it. Thus, the Mult-vae pr with 0-hiddenlayer mlp is effectively a log-linear model. We tune the regularization parameter ? for Mult-vae pr following the procedure described in Section 2.2.2. We anneal the Kullback-Leibler term linearly for 200,000 gradient updates. For both Mult-vae pr and Mult-dae, we apply dropout at the input layer with probability 0.5. We apply a weight decay of 0.01 for Mult-dae. We do not apply weight decay for any vae models. We train both Mult-vae pr and Mult-dae using Adam <ref type="bibr" target="#b22">[23]</ref> with batch size of 500 users. For ML-20M, we train for 200 epochs. We train for 100 epochs on the other two datasets. We keep the model with the best validation NDCG@100 and report test set metrics with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines</head><p>We compare results with the following standard state-of-the-art collaborative filtering models, both linear and non-linear:</p><p>Weighted matrix factorization (wmf) <ref type="bibr" target="#b18">[19]</ref>: a linear low-rank factorization model. We train wmf with alternating least squares; this generally leads to better performance than with SGD. We set the weights on all the 0's to 1 and tune the weights on all the 1's in the click matrix among {2, 5, 10, 30, 50, 100}, as well as the latent representation dimension K ? {100, 200} by evaluating NDCG@100 on validation users.</p><p>Slim <ref type="bibr" target="#b32">[33]</ref>: a linear model which learns a sparse item-to-item similarity matrix by solving a constrained ? 1 -regularized optimization problem. We grid-search both of the regularization parameters over {0.1, 0.5, 1, 5} and report the setting with the best NDCG@100 on validation users. We did not evaluate Slim on MSD because the dataset is too large for it to finish in a reasonable amount of time (for the Netflix dataset, the parallelized grid search took about two weeks). We also found that the faster approximation of Slim <ref type="bibr" target="#b25">[26]</ref> did not yield competitive performance.</p><p>Collaborative denoising autoencoder (cdae) <ref type="bibr" target="#b50">[51]</ref>: augments the standard denoising autoencoder by adding a per-user latent factor to the input. We change the (user, item) entry subsampling strategy in SGD training in the original paper to the user-level subsampling as we did with Mult-vae pr and Mult-dae. We generally find that this leads to more stable convergence and better performance. We set the dimension of the bottleneck layer to 200, and use a weighted square loss, equivalent to what the square loss with negative sampling used in the original paper. We apply tanh activation at both the bottleneck layer as well as the output layer. <ref type="bibr" target="#b7">8</ref> We use Adam with a batch size of 500 users. As mentioned in Section 3, the number of parameters for cdae grows linearly with the number of users and items. Thus, it is crucial to control overfitting by applying weight decay. We select the weight decay parameter over {0.01, 0.1, ? ? ? , 100} by examining the validation NDCG@100.</p><p>Neural collaborative filtering (ncf) <ref type="bibr" target="#b13">[14]</ref>: explores non-linear interactions (via a neural network) between the user and item latent factors. Similar to cdae, the number of parameters for ncf grows linearly with the number of users and items. We use the publicly available source code provided by the authors, yet cannot obtain competitive performance on the datasets used in this paper -the validation metrics drop within the first few epochs over a wide range of regularization parameters. The authors kindly provided the two datasets (ML-1M and Pinterest) used in the original paper, as well as the training/test split, therefore we separately compare with ncf on these two relatively smaller datasets in the empirical study. In particular, we compare with the hybrid NeuCF model which gives the best performance in He et al. <ref type="bibr" target="#b13">[14]</ref>, both with and without pre-training.</p><p>We also experiment with Bayesian personalized ranking (bpr) <ref type="bibr" target="#b35">[36]</ref>. However, the performance is not on par with the other baselines above. This is consistent with some other studies with similar baselines <ref type="bibr" target="#b39">[40]</ref>. Therefore, we do not include bpr in the following results and analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental results and analysis</head><p>In this section, we quantitatively compare our proposed methods with various baselines. In addition, we aim to answer the following two questions:</p><p>1. How does multinomial likelihood compare with other commonly used likelihood models for collaborative filtering? 2. When does Mult-vae pr perform better/worse than Mult-dae?</p><p>Quantitative results. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results between our proposed methods and various baselines. Each metric is averaged across all test users. Both Mult-vae pr and Mult-dae significantly outperform the baselines across datasets and metrics. Multvae pr significantly outperforms Mult-dae on ML-20M and Netflix data-sets. In most of the cases, non-linear models (Mult-vae pr , Multdae, and cdae) prove to be more powerful collaborative filtering models than state-of-the-art linear models. The inferior results of cdae on MSD are possibly due to overfitting with the huge number of users and items, as validation metrics drop within the first few epochs even though the training objective continues improving.</p><p>We compare with ncf on the two relatively smaller datasets used in Hu et al. <ref type="bibr" target="#b18">[19]</ref>: ML-1M (6,040 users, 3,704 items, 4.47% density) and Pinterest (55,187 users, 9,916 items, 0.27% density). Because of the size of these two datasets, we use Mult-dae with a 0-hidden-layer mlp generative model -the overall architecture is [I ? 200 ? I ].</p><p>(Recall Mult-vae pr with a 0-hidden-layer mlp generative model is effectively a log-linear model with limited modeling capacity.) <ref type="table" target="#tab_3">Table 3</ref> summarizes the results between Mult-dae and ncf. Multdae significantly outperforms ncf without pre-training on both datasets. On the larger Pinterest dataset, Mult-dae even improves over the pre-trained ncf model by a big margin.</p><p>How well does multinomial likelihood perform? Despite being commonly used in language models, multinomial likelihoods have typically received less attention in the collaborative filtering literature, especially with latent-factor models. Most previous work builds on Gaussian likelihoods (square loss, Eq. 3) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51]</ref> or logistic likelihood (log loss, Eq. 4) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51]</ref> instead. We argue in Section 2.1 that multinomial likelihood is in fact a good proxy for the top-N ranking loss and is well-suited for implicit feedback data. To demonstrate the effectiveness of multinomial likelihood, we take the best-performing Mult-vae pr and Mult-dae model on each dataset and swap the likelihood distribution model for the data while keeping everything else exactly the same.    <ref type="table" target="#tab_4">Table 4</ref> summarizes the results of different likelihoods on ML-20M (the results on the other two datasets are similar.) We tune the hyperparameters for each likelihood separately. <ref type="bibr" target="#b8">9</ref> The multinomial likelihood performs better than the other likelihoods. The gap between logistic and multinomial likelihood is closer -this is understandable since multinomial likelihood can be approximated by individual binary logistic likelihood, a strategy commonly adopted in language modeling <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>We wish to emphasize that the choice of likelihood remains datadependent. For the task of collaborative filtering, the multinomial likelihood achieves excellent empirical results. The methodology behind the partial regularization in Mult-vae pr , however, is a technique we hypothesize will generalize to other domains.</p><p>When does Mult-vae pr perform better/worse than Multdae? In <ref type="table" target="#tab_2">Table 2</ref> we can see that both Mult-vae pr and Mult-dae produce competitive empirical results with Mult-vae pr being comparably better. It is natural to wonder when a variational Bayesian inference approach (Mult-vae pr ) will win over using a point estimate (Mult-dae) and vice versa.</p><p>Intuitively, Mult-vae pr imposes stronger modeling assumptions and therefore could be more robust when user-item interaction data is scarce. To study this, we considered two datasets: ML-20M where Mult-vae pr has the largest margin over Mult-dae and MSD where Mult-vae pr and Mult-dae have roughly similar performance. The results on the Netflix dataset are similar to ML-20M. We break down test users into quintiles based on their activity level in the fold-in set which is provided as input to the inference model ? ? (?) to make prediction. The activity level is simply the number of items each user has clicked on. We compute NDCG@100 for each group of users using both Mult-vae pr and Mult-dae and plot results in <ref type="figure">Figure 3</ref>. This summarizes how performance differs across users with various levels of activity.</p><p>In <ref type="figure">Figure 3</ref>, we show performance across increasing user activity. Error bars represents one standard error. For each subplot, a paired t-test is performed and statistical significance is marked. Although there are slight variations across datasets, Mult-vae pr consistently improves recommendation performance for users who have only clicked on a small number of items. This is particularly prominent for ML-20M <ref type="figure">(Figure 3a)</ref>. Interestingly, Mult-dae actually <ref type="bibr" target="#b8">9</ref> Surprisingly, partial regularization seems less effective for Gaussian and logistic.  <ref type="figure">Figure 3</ref>: NDCG@100 breakdown for users with increasing levels of activity (starting from 0%), measured by how many items a user clicked on in the fold-in set. The error bars represents one standard error. For each subplot, a paired t-test is performed and * indicates statistical significance at ? = 0.05 level, ** at ? = 0.01 level, and *** at ? = 0.001 level. Although details vary across datasets, Mult-vae pr consistently improves recommendation performance for users who have only clicked on a small number of items.</p><p>outperforms Mult-vae pr on the most active users. This indicates the stronger prior assumption could potentially hurt the performance when a lot of data is available for a user. For MSD <ref type="figure">(Figure 3b</ref>), the least-active users have similar performance under both Multvae pr and Mult-dae. However, as we described in Section 4.1, MSD is pre-processed so that a user has at least listened to 20 songs. Meanwhile for ML-20M, each user has to watch at least 5 movies. This means that the first bin of ML-20M has much lower user activity than the first bin of MSD.</p><p>Overall, we find that Mult-vae pr , which may be viewed under the lens of a principled Bayesian inference approach, is more robust than the point estimate approach of Mult-dae, regardless of the scarcity of the data. More importantly, the Mult-vae pr is less sensitive to the choice of hyperparameters -weight decay is important for Mult-dae to achieve competitive performance, yet it is not required for Mult-vae pr . On the other hand, Mult-dae also has advantages: it requires fewer parameters in the bottleneck layer -Mult-vae pr requires two sets of parameters to obtain the latent representation z: one set for the variational mean ? ? (?) and another for the variational variance ? ? (?) -and Mult-dae is conceptually simpler for practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we develop a variant of vae for collaborative filtering on implicit feedback data. This enables us to go beyond linear factor models with limited modeling capacity.</p><p>We introduce a generative model with a multinomial likelihood function parameterized by neural network. We show that multinomial likelihood is particularly well suited to modeling user-item implicit feedback data.</p><p>Based on an alternative interpretation of the vae objective, we introduce an additional regularization parameter to partially regularize a vae (Mult-vae pr ). We also provide a practical and efficient way to tune the additional parameter introduced using KL annealing. We compare the results obtained against a denoising autoencoder (Mult-dae).</p><p>Empirically, we show that the both Mult-vae pr and Mult-dae provide competitive performance with Mult-vae pr significantly outperforming the state-of-the-art baselines on several real-world datasets, including two recently proposed neural-network-based approaches. Finally, we identify the pros and cons of both Mult-vae pr and Multdae and show that employing a principled Bayesian approach is more robust.</p><p>In future work, we would like to futher investigate the tradeoff introduced by the additional regularization parameter ? and gain more theoretical insight into why it works so well. Extending Mult-vae pr by condition on side information might also be a way to improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Validation ranking metrics with different annealing configurations. For the green dashed curve, ? reaches 1 at around 80 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .Figure 2 :</head><label>12</label><figDesc>For both, Mult-vae pr (Section 2.2) A taxonomy of autoencoders. The dotted arrows denote a sampling operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1: VAE-SGD Training collaborative filtering vae with stochastic gradient descent.</figDesc><table /><note>Input: Click matrix X ? R U ?I Randomly initialize ? , ? while not converged do Sample a batch of users U forall u ? U do Sample ? ? N (0, I K ) and compute z u via reparametrization trick Compute noisy gradient ? ? L and ? ? L with zu end Average noisy gradients from batch Update ? and ? by taking stochastic gradient steps end return ? , ? 2.2.2 Alternative interpretation of elbo.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Attributes of datasets after preprocessing. Interactions are non-zero entries. % of interactions refers to the density of the user-item click matrix X. # of the held-out users is the number of validation/test users out of the total number of users in the first row.</figDesc><table><row><cell></cell><cell cols="2">ML-20M Netflix</cell><cell>MSD</cell></row><row><cell># of users</cell><cell>136,677</cell><cell cols="2">463,435 571,355</cell></row><row><cell># of items</cell><cell>20,108</cell><cell>17,769</cell><cell>41,140</cell></row><row><cell># of interactions</cell><cell>10.0M</cell><cell>56.9M</cell><cell>33.6M</cell></row><row><cell>% of interactions</cell><cell>0.36%</cell><cell>0.69%</cell><cell>0.14%</cell></row><row><cell># of held-out users</cell><cell>10,000</cell><cell>40,000</cell><cell>50,000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between various baselines and our proposed methods. Standard errors are around 0.002 for ML-20M and 0.001 for Netflix and MSD. Both Mult-vae pr and Mult-dae significantly outperform the baselines across datasets and metrics. We could not finish Slim within a reasonable amount of time on MSD.</figDesc><table><row><cell></cell><cell cols="2">(a) ML-20M</cell><cell></cell></row><row><cell></cell><cell cols="3">Recall@20 Recall@50 NDCG@100</cell></row><row><cell>Mult-vae pr</cell><cell>0.395</cell><cell>0.537</cell><cell>0.426</cell></row><row><cell>Mult-dae</cell><cell>0.387</cell><cell>0.524</cell><cell>0.419</cell></row><row><cell>wmf</cell><cell>0.360</cell><cell>0.498</cell><cell>0.386</cell></row><row><cell>Slim</cell><cell>0.370</cell><cell>0.495</cell><cell>0.401</cell></row><row><cell>cdae</cell><cell>0.391</cell><cell>0.523</cell><cell>0.418</cell></row><row><cell></cell><cell cols="2">(b) Netflix</cell><cell></cell></row><row><cell></cell><cell cols="3">Recall@20 Recall@50 NDCG@100</cell></row><row><cell>Mult-vae pr</cell><cell>0.351</cell><cell>0.444</cell><cell>0.386</cell></row><row><cell>Mult-dae</cell><cell>0.344</cell><cell>0.438</cell><cell>0.380</cell></row><row><cell>wmf</cell><cell>0.316</cell><cell>0.404</cell><cell>0.351</cell></row><row><cell>Slim</cell><cell>0.347</cell><cell>0.428</cell><cell>0.379</cell></row><row><cell>cdae</cell><cell>0.343</cell><cell>0.428</cell><cell>0.376</cell></row><row><cell></cell><cell></cell><cell>(c) MSD</cell><cell></cell></row><row><cell></cell><cell cols="3">Recall@20 Recall@50 NDCG@100</cell></row><row><cell>Mult-vae pr</cell><cell>0.266</cell><cell>0.364</cell><cell>0.316</cell></row><row><cell>Mult-dae</cell><cell>0.266</cell><cell>0.363</cell><cell>0.313</cell></row><row><cell>wmf</cell><cell>0.211</cell><cell>0.312</cell><cell>0.257</cell></row><row><cell>Slim</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cdae</cell><cell>0.188</cell><cell>0.283</cell><cell>0.237</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between ncf and Mult-dae with [I ? 200 ? I ] architecture. We take the results of ncf from He et al.<ref type="bibr" target="#b13">[14]</ref>. Mult-dae model significantly outperforms ncf without pre-training on both datasets and further improves on Pinterest even comparing with pre-trained ncf.</figDesc><table><row><cell cols="2">(a) ML-1M</cell><cell></cell></row><row><cell cols="3">ncf ncf (pre-train) Mult-dae</cell></row><row><cell>Recall@10 0.705</cell><cell>0.730</cell><cell>0.722</cell></row><row><cell>NDCG@10 0.426</cell><cell>0.447</cell><cell>0.446</cell></row><row><cell cols="2">(b) Pinterest</cell><cell></cell></row><row><cell cols="3">ncf ncf (pre-train) Mult-dae</cell></row><row><cell>Recall@10 0.872</cell><cell>0.880</cell><cell>0.886</cell></row><row><cell>NDCG@10 0.551</cell><cell>0.558</cell><cell>0.580</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of Mult-vae pr and Mult-dae with different likelihood functions at the output layer on ML-20M. The standard error is around 0.002 (the results on the other two datasets are similar.) The multinomial likelihood performs better than the other two commonly-used likelihoods from the collaborative filtering literature.</figDesc><table><row><cell></cell><cell cols="3">Recall@20 Recall@50 NDCG@100</cell></row><row><cell>Mult-vae pr</cell><cell>0.395</cell><cell>0.537</cell><cell>0.426</cell></row><row><cell>Gaussian-vae pr</cell><cell>0.383</cell><cell>0.523</cell><cell>0.415</cell></row><row><cell>Logistic-vae pr</cell><cell>0.388</cell><cell>0.523</cell><cell>0.419</cell></row><row><cell>Mult-dae</cell><cell>0.387</cell><cell>0.524</cell><cell>0.419</cell></row><row><cell>Gaussian-dae</cell><cell>0.376</cell><cell>0.515</cell><cell>0.409</cell></row><row><cell>Logistic-dae</cell><cell>0.381</cell><cell>0.516</cell><cell>0.414</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the verb "click" for concreteness; this can be any type of interaction, including "watch", "purchase", or "listen".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The cross-entropy loss for multi-class classification is a multinomial likelihood under a single draw from the distribution.<ref type="bibr" target="#b2">3</ref> Logistic likelihood is also cross-entropy loss for binary classification.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In the implementation, the inference model will output the log of the variance of the variational distribution. We continue to use ? ? (x u ) for notational brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">U u q(z | x u ) ? p(z) = N (z; 0, I K )<ref type="bibr" target="#b17">[18]</ref>; this means that the model is less able to generate novel user histories by ancestral sampling.But ultimately our goal is to make good recommendations, not to maximize likelihood or generate imagined user histories. Treating ? as a free regularization parameter therefore costs us nothing, and, as we will see, yields significant improvements in performance.Selecting ?: We propose a simple heuristic for setting ?: we start training with ? = 0, and gradually increase ? to 1. We linearly anneal the KL term slowly over a large number of gradient updates to ?, ? and record the best ? when its performance reaches the peak. We found this method to work well and it does not require the need for training multiple models with different values of ?, which can be time-consuming. Our procedure is inspired by KL annealing<ref type="bibr" target="#b6">[7]</ref>, a common heuristic used for training vaes when there is concern that the model is being underutilized.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/dawenl/vae_cf 7 http://www.netflixprize.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Wu et al.<ref type="bibr" target="#b50">[51]</ref> used sigmoid activation function but mentioned tanh gave similar results. We use tanh to be consistent with our models.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Variational Information Bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning distributed representations from reviews for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
		<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Million Song Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thierry Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Complementary Sum Sampling for Likelihood Approximation in Large Scale Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent Latent Variable Networks for Session-Based Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Andreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 2nd Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1148" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cognitive Science Society</title>
		<meeting>the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable Recommendation with Hierarchical Poisson Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03847</idno>
		<title level="m">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linas Baltrunas, and Domonkos Tikk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
	</analytic>
	<monogr>
		<title level="m">Session-based recommendations with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shakir Mohamed, and Alexander Lerchner. 2017. ? -VAE: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ELBO surgery: yet another way to carve up the variational evidence lower bound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop in Advances in Approximate Bayesian Inference, NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08. Eighth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum entropy discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="470" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of IR techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>J?rvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kek?l?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the challenges of learning with inference networks on sparse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06085</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">high-dimensional data. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient top-n recommendation by linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys Large Scale Recommender Systems Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Content-Aware Collaborative Music Recommendation Using Pre-trained Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minshu</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Collaborative filtering: A machine learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin Marlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional logit analysis of qualitative choice behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mcfadden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="105" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Slim: Sparse linear methods for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajan</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving regularized singular value decomposition for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Paterek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Restricted Boltzmann machines for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="791" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On the Effectiveness of Linear Models for One-Class Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Braziunas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Autorec: Autoencoders meet collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvash</forename><surname>Sedhain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Sanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="111" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contextual Sequence Modeling for Recommendation with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Smirnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavian</forename><surname>Vasile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 2nd Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gaussian ranking by matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Steck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
		<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved recurrent neural networks for session-based recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxing</forename><surname>Yong Kiam Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<idno>physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2643" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collaborative deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1235" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cofi rank-maximum margin matrix factorization for collaborative ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2764" to="2770" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient subsampling for training complex language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asela</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1128" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collaborative competitive filtering: learning recommender using context of user choice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Neural Autoregressive Approach to Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
