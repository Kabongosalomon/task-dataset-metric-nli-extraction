<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEMANTIC EMBEDDING SPACE FOR ZERO-SHOT ACTION RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEMANTIC EMBEDDING SPACE FOR ZERO-SHOT ACTION RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Index Terms-action recognition, zero-shot learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The number of categories for action recognition is growing rapidly. It is thus becoming increasingly hard to collect sufficient training data to learn conventional models for each category. This issue may be ameliorated by the increasingly popular "zero-shot learning" (ZSL) paradigm. In this framework a mapping is constructed between visual features and a human interpretable semantic description of each category, allowing categories to be recognised in the absence of any training data. Existing ZSL studies focus primarily on image data, and attribute-based semantic representations. In this paper, we address zero-shot recognition in contemporary video action recognition tasks, using semantic word vector space as the common space to embed videos and category labels. This is more challenging because the mapping between the semantic space and space-time features of videos containing complex actions is more complex and harder to learn. We demonstrate that a simple self-training and data augmentation strategy can significantly improve the efficacy of this mapping. Experiments on human action datasets including HMDB51 and UCF101 demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The number and complexity of action categories of interest to be recognised in videos is growing rapidly. A consequence of the growing complexity of actions to be recognised is that more training data per category is required to learn sufficiently strong models for complex actions. Meanwhile, the growing number of categories means that it will become increasingly difficult to collect sufficient annotated training data for each. Moreover the annotation of space-time segments of video to train action recognition is more difficult and costly than annotating static images. The "zero-shot learning" (ZSL) paradigm has the potential to ameliorate these issues by respectively sharing information across categories; and crucially by allowing recognisers for novel categories to be constructed based on a human description of the action, rather than an extensive collection of training data.</p><p>The ZSL paradigm is most commonly been realised by using attributes <ref type="bibr" target="#b0">[1]</ref> to bridge the semantic gap between low-level features (e.g., MBH or HOG) and human class descriptions. Visual to attribute classifiers are learned on an auxiliary dataset, and then novel categories are specified by a human in terms of their attributes -thus enabling recognition in absence of training data for the new categories. With a few exceptions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, this paradigm has primarily been applied to images rather than video action recognition.</p><p>An emerging alternative paradigm to the attribute-centric strategy to bridging the semantic gap for ZSL is that of semantic embedding spaces (SES) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. In this case a distributed representation of text words is generated by a model such as an unsupervised neural network <ref type="bibr" target="#b6">[7]</ref> trained on a large text corpus. This neural network is used to map the text string of a category name into a vector space. Regressors (contrast classifiers in the attribute case) are then used to map videos into this word vector space. Zero-shot recognition is then enabled by mapping novel category instances (via regression), and novel class names (via the neural network) to this common space and performing similarity matching. The key advantage of SES over attribute-centric approaches is that new categories can be defined trivially by naming them, without the requirement to exhaustively define each class in terms of a list of attributes -which grows non-scaleably as the breadth of classes to recognise grows. Moreover it allows information sharing across categories (via the common regressor), and can even be used to improve conventional supervised recognition if training samples are sparse <ref type="bibr" target="#b5">[6]</ref>.</p><p>Although SES-based ZSL is a very attractive paradigm for the mentioned reasons, it has not previously been demonstrated in zero-shot video action recognition. This is for two reasons: (i) for many classes of complex actions, the mapping from low-level features to semantic embedding space is very complex and hard to learn reliably, and (ii) a heavy burden is placed on the generalisation capability of these regressors which need to learn a single visual to semantic embedding space mapping that is general enough to cover all action categories including unseen action categories. This can be seen as the pervasive issue <ref type="bibr" target="#b4">[5]</ref> of domain shift between the categories on which the semantic embedding is trained and the disjoint set of categories on which it is applied for zero-shot recognition.</p><p>In this paper we show how to use simple data augmentation and self-training strategies to ameliorate these issues and achieve state of the art ZSL performance on contemporary video action datasets, HMDB51 and UCF101. Our framework also achieves action recognition accuracy comparable to the state of the art in the conventional supervised settings. The processing pipeline of our framework is illustrated in <ref type="figure">Fig.1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic Embedding Space</head><p>To formalise the problem, we have a task T = {X, Y } where X = {x i } i=1???n is the set of d x dimensional low-level spacetemporal visual feature representations (e.g., MBH and HOG) of n videos, including n tr and n ts training and testing samples. Y = {y i } i=1???n is the class names/labels of each instance (e.g. "brush hair" and "handwalk"). We want to establish a semantic embedding space Z to connect the lowlevel visual features and class labels. In particular we use the word2vector neural network <ref type="bibr" target="#b6">[7]</ref> trained on a 100 billion word corpus to realise a mapping g : y ? z that produces a unique d z dimensional encoding of each word in the english dictionary and thus any class name of interest in Y . For multi-word class names, such as "brush hair" or "ride horse" we generate a single vector z by averaging the unique words {y j } j=1???N in the description <ref type="bibr" target="#b7">[8]</ref></p><formula xml:id="formula_0">: z = 1 N ? N j=1 g(y j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Visual to Semantic Mapping</head><p>In order to map videos into the semantic embedding space constructed above, we train a regression model f : x ? z from d x dimensional low-level space-time visual feature space to the d z dimensional embedding space. The regression is trained using training instances X tr = {x i } i=1???ntr and the corresponding embedding Z tr = g(Y tr ) of the instance class name y as the target value. Various methods have previously been used for this task including linear support vector regression (SVR) <ref type="bibr" target="#b4">[5]</ref> and more complex multi-layer neural networks <ref type="bibr" target="#b3">[4]</ref>. Considering the trade-off between accuracy and complexity, we choose non-linear SVR with RBF-? 2 kernel defined by:</p><formula xml:id="formula_1">K(x i , x j ) = exp(?? ? D(x i , x j ))<label>(1)</label></formula><p>where D(x i , x j ) is the ? 2 distance between histogram based representation x i and x j <ref type="bibr" target="#b8">[9]</ref>. This kernel is effective for histogram-based low-level space-time feature representations [10] that we use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-shot and Zero-shot Learning</head><p>Distances in semantic embedding spaces have been shown to be best measured using the cosine metric <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref>. Thus we normalise each data point in this space with L2, making euclidean distance comparisons effectively correspond to cosine distance d(z i , z j ) = 1 ? &lt;zi,zj &gt; zi ? zj in this space. Multi-shot learning For conventional multi-shot learning, we map all data instances X into the semantic space using projection Z = f (X), and then simply train SVM classifiers with RBF kernel using the L2 normalised projections f (X) as data. Zero-shot learning For zero-shot learning, test instances and classes X * and Y * are disjoint from training classes. I.e., no instances of test classes Y * occur in training data Y . For each unique test category y * ? Y * , we obtain its semantic space projection g(y * ). Then the embedding f (x * ) of each test instance X * is generated via support vector regressor f as described earlier. To classify instances x * of novel categories, we use nearest neighbour matching:</p><formula xml:id="formula_2">y * = arg min y * ?Y * f (x * ) ? g(y * )<label>(2)</label></formula><p>The projections g(y * ) can be seen as class prototypes in the semantic space. Data instances f (x * ) can be directly matched against prototypes in this common space. Self-training for domain adaptation The change in statistics induced by the disjointness of the training and zero-shot testing categories Y and Y * means that regressor f trained on X will not be well adapted at zero-shot test time for X * , and thus perform poorly <ref type="bibr" target="#b4">[5]</ref>. To ameliorate this domain shift, we apply transductive self-training (Eq. 3) to adjust unseen class prototypes to be more comparable to the projected data points. For each category prototype g(y * ) we search for the K nearest neighbours among the unlabelled test instance projections, and re-define the adapted prototypeg(y * ) as the average of the K neighbours. Thus if N N K (g(y * )) denotes the set of K nearest neighbours of g(y * ), we have:</p><formula xml:id="formula_3">g(y * ) := 1 K K f (x * )?N N K (g(y * )) f (x * )<label>(3)</label></formula><p>The adapted prototypesg(y * ) are now more directly comparable with the test data for matching using Eq. (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Data Augmentation</head><p>The approach outlined so far relies heavily on the efficacy of the low-level feature to semantic space mapping f (x). As discussed, the mapping is hard to learn well because: (i) actions are visually complex and ambiguous, and (ii) even a mapping well learned for training categories may not generalise well to testing categories as required by ZSL, because the volume of training data is small compared to the complexity of a general visual to semantic space mapping. The self-training mechanism above addresses the latter to some extent. Another way to further mitigate both of these problems is via augmentation with any available auxiliary dataset T aux = {X aux , Y aux }, which need not contain classes in common with the target dataset T trg . This will provide more data to learn a better and more generalisable regressor z = f (x). The auxiliary dataset class names Y aux are also projected into the embedding space with the neural network g(Y aux ). The auxiliary instances X aux are aggregated with the target training  <ref type="figure">Fig. 1</ref>. Illustration of our framework's processing pipeline. We start by exploiting word2vector g(?) to project textual labels Y into semantic embedding space Z. Then we learn a Support Vector Regression f (?) to map low-level visual features X into the semantic spaceZ. By augmenting target training data X trg tr with auxiliary data X aux tr we can achieve the state-of-the-art performance for Zero-shot Learning and competitive performance on Multi-shot Learning data X tr = [X trg tr , X aux ], Z tr = [g(Y trg tr ), g(Y aux )] and together to train the regressor f . Although the auxiliary data is disjoint from both the target training or zero-shot classes, it helps to both better learn the complex visual-semantic space mapping and to learn a more generalisable mapping that better applies to the held-out zero-shot classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-shot Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot Action Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Data Semantic Embedding Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Recognition Tasks</head><formula xml:id="formula_4">) ( ? ) ( ? ? ? ? ? ? min ] [ ] [ ? ? ) min( : ) ( ? ) ( ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Datasets: Experiments are performed on HMDB51 <ref type="bibr" target="#b10">[11]</ref> and UCF101 <ref type="bibr" target="#b11">[12]</ref>, two of the largest and most challenging action recognition datasets available. HMDB51 has 6766 videos with 51 categories of actions. UCF101 has 13320 videos with 101 categories of actions. Visual Feature Encoding: For each video we extract dense trajectory descriptors using <ref type="bibr" target="#b9">[10]</ref> and encode Bag of Words features. We first compute dense trajectory descriptors (DenseTrajectory, HOG, HOF and MBH) then we randomly sample 10,000 descriptors from all videos and learn the BoW codebook with K-means using K=4000. Thus d x = 4000. Semantic Embedding Space: We adopted the skip-gram neural network model <ref type="bibr" target="#b6">[7]</ref> trained on the Google News dataset (about 100 billion words). This neural network can then encode any of approximately 3 million unique worlds as a d z = 300 dimension vector. Visual to Semantic Mapping: The SVR from visual feature X to semantic space Z is learned from training data with RBF-? 2 kernel. The ? parameter for the kernel is set as ? =</p><formula xml:id="formula_5">1 1 N ? i,j D(xi,xj ) where D(x i , x j )</formula><p>is the ? 2 distance function. Slack parameter C for SVR was set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Zero-shot Learning</head><p>Data Split: Because there is no existing zero-shot learning evaluation protocol for HMDB51 and UCF101 action datasets, we propose our own split 1 . For each dataset, we use a 50/50 category split. Semantic space mappings are trained on the 50% training categories, and the other 50% are held <ref type="bibr" target="#b0">1</ref> The data split will be released on our website out unseen until test time. We randomly generate 30 independent splits and take the average mean accuracy and standard deviation for fair evaluation. Alternative Approaches: We compare our model with 3 alternatives: (1) Random Guess -A lower bound that randomly guesses the class label of unseen test samples. (2) Attribute Based -the classic Direct Attribute Prediction (DAP) <ref type="bibr" target="#b0">[1]</ref> zeroshot recognition strategy. (3) Attribute Based -Indirect Attribute Prediction (IAP) <ref type="bibr" target="#b0">[1]</ref>. Note that because attribute annotations are only available for UCF dataset, the two attribute methods are only tested on UCF. (4) Vanilla semantic word vector embedding with Nearest Neighbour (NN) -This is the simplest variant of ZSL using the same embedding space as our model. Projection f is learned as for our model, and then NN matching is applied to classify test instances using the unseen class prototypes. The results are presented in Tab. 1. All methods are much better than random chance, demonstrating successful ZSL. A direct application of the embedding space (NN) is reasonable, suggesting that the semantic space is effective as a representation: Videos are successfully mapped near to the correct prototypes in the semantic space. Although NN is not clearly better than the attribute-based approaches <ref type="bibr" target="#b0">[1]</ref>, it does not require the latter's extensive and costly attribute annotation. Finally, our self-training approach performs best, suggesting that our strategy ameliorates some of the domain-shift between training and testing categories compared to vanilla NN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Zero-shot Learning with Data Augmentation</head><p>Semantic embedding space as an intermediate representation enables exploiting multiple datasets to improve the projection f as explained in Sec 2.4. We next investigate the effect of data augmentation across HMDB51 and UCF101. Zero-shot Learning with Data Augmentation: We follow the same zero-shot learning protocol as Sec. 3.1, but augment the HMDB51 regressor training with data from UCF101 and The performance of only data augmentation without self-training (NN+Aux) and our full model including both self-training and data augmentation (NN+ST+Aux) are shown in Tab. 1. Overall the both strategies (NN+Aux and NN+ST+Aux) significantly outperform their respective baselines (NN and NN+ST) and the full model clearly beats the classic attribute-based approaches (DAP and IAP). This is attributed to learning a more accurate and generalisable regressor for mapping videos into the semantic space for classification. Note that NN roughly corresponds to the embedding space strategy of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b12">[13]</ref>. Qualitative Illustration: We give insight into our selftraining and data-augmentation contributions in <ref type="figure" target="#fig_3">Fig. 2</ref>. We randomly sample 5 unseen classes from HMDB and project all samples from these classes into the semantic space by (a) regression trained on target seen class data alone; (b) regression trained on target seen data augmented with auxiliary (UCF101) data. The results are visualised in 2D with t-SNE <ref type="bibr" target="#b13">[14]</ref>. Data instances are shown as dots, prototypes as diamonds, and self-training adapted prototypes as stars. Colours indicate category.</p><p>There are two main observations: (i) Comparing <ref type="figure" target="#fig_3">Fig. 2</ref>(a) and (b), we can see that regression trained without auxiliary data yields a less accurate projection of unseen data, as instances are projected further from the prototypes: reducing NN matching accuracy. (ii) Self-training is effective as the adapted prototypes (stars) are closer to the center of the corresponding samples (dots) than the original prototypes (diamonds). These observations explain our final model's ZSL accuracy improvement on conventional approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-shot Learning</head><p>We finally validate our representation on standard supervised (multi-shot) action recognition. We use the standard data splits for both HMDB51 <ref type="bibr" target="#b10">[11]</ref> and UCF101 <ref type="bibr" target="#b11">[12]</ref>. The action recognition accuracy is the average of each fold. Alternatives: We compare our approach to: (i) the state of the art results based on low-level features <ref type="bibr" target="#b9">[10]</ref>, (ii) an alternative semantic space using attributes. To realise the latter we use Human-Labelled Attribute (HLA) <ref type="bibr" target="#b14">[15]</ref>. We train binary SVM classifier with RBF-? 2 kernel for attribute detection and   use the concatenation of attribute scores as semantic attribute space representation. A SVM classifier with RBF kernel is trained on attribute representation to predict final labels.</p><p>The resulting accuracies are shown in Tab. 2. We observe that our semantic embedding is comparable to the state of the art low-level feature-based classification and better than the conventional attribute-based intermediate representation. This may be due to the attribute-space being less discriminative than our semantic word space, or due to the reliance on human annotation: some annotated attributes may not be detectable, or may be detectable but not discriminative for class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper we investigated semantic-embedding space representations for video action recognition for the first time. This representation enables projecting visual instances and category prototypes into the same space for zero-shot recognition, however it possess serious challenges of projection complexity and generalisation across domain-shift. We show that simple self-training and data augmentation strategies can address these challenges and achieve the state of the art results for zero-shot action recognition in video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 5 )</head><label>5</label><figDesc>Our zero-shot learning approach (Sec. 2.3), including both Nearest Neighbour + Self-Training (NN+ST). (Investigation of Data Augmentation is in the following Sec. 3.2.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Regression trained on target data augmented with auxiliary data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>A qualitative illustration of ZSL with semantic space representation: self-training and data augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Zero-shot action recognition performance (average % accuracy ? standard deviation).</figDesc><table><row><cell>Method</cell><cell>HMDB51</cell><cell>UCF101</cell></row><row><cell>Random Guess</cell><cell>4.0</cell><cell>2.0</cell></row><row><cell>DAP[1]</cell><cell>-</cell><cell>14.3 ? 1.9</cell></row><row><cell>IAP[1]</cell><cell>-</cell><cell>12.8 ? 2.0</cell></row><row><cell>NN</cell><cell>13.0 ? 2.7</cell><cell>10.9 ? 1.5</cell></row><row><cell>NN + ST</cell><cell>15.0 ? 3.0</cell><cell>15.8 ? 2.3</cell></row><row><cell>NN + Aux</cell><cell>18.0 ? 3.0</cell><cell>12.7 ? 1.6</cell></row><row><cell cols="3">NN + ST + Aux 21.2 ? 3.0 18.6 ? 2.2</cell></row><row><cell>vice versa.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Standard supervised action recognition mean accuracy in % on HMDB51 and UCF101 .</figDesc><table><row><cell>indicates our imple-</cell></row></table><note>*</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning multimodal latent attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">VideoStory: A New Multimedia Embedding for Few-Example Recognition and Translation of Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habibian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cgm</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating neural word representations in tensor-based compositional settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">ICCV</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ucf101 : A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Submodular Attribute Selection for Action Recognition in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
