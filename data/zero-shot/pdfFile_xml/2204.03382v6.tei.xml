<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HunYuan_tvr for Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong-Cheng</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dihong</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent Data Platform</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HunYuan_tvr for Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-Video Retrieval plays an important role in multi-modal understanding and has attracted increasing attention in recent years. Most existing methods focus on constructing contrastive pairs between whole videos and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short clips and phrases or single frame and word. In this paper, we propose a novel method, named HunYuan_tvr, to explore hierarchical cross-modal interactions by simultaneously exploring video-sentence, clip-phrase, and frame-word relationships. Considering intrinsic semantic relations between frames, HunYuan_tvr first performs self-attention to explore frame-wise correlations and adaptively clusters correlated frames into clip-level representations. Then, the clip-wise correlation is explored to aggregate clip representations into a compact one to describe the video globally. In this way, we can construct hierarchical video representations for frame-clip-video granularities, and also explore word-wise correlations to form word-phrase-sentence embeddings for the text modality. Finally, hierarchical contrastive learning is designed to explore cross-modal relationships, i.e., frame-word, clip-phrase, and video-sentence, which enables HunYuan_tvr to achieve a comprehensive multi-modal understanding. Further boosted by adaptive label denoising and marginal sample enhancement, HunYuan_tvr obtains new state-of-the-art results on various benchmarks, e.g., Rank@1 of 55.0%, 58.2%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet, respectively. * Corresponding authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-Video Retrieval (TVR) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref> has made significant progress in recent years, which is an important proxy task to understand video contents. However, a natural semantic gap between two modalities, i.e., video and text, raises a great challenge, which hinders industrial-level applications of TVR. To this end, recent methods target to distill cross-modal knowledge from largescale pretraining experts <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref> and leverage cross-modal contrastive learning to explore both intra-modal representation and cross-modal interaction.</p><p>A pioneering vision-language pretraining work is the Contrastive Language-Image Pretraining (CLIP) <ref type="bibr" target="#b25">[26]</ref>, which collects 400 million image-text pairs to learn general multi-modal knowledge. Based on CLIP, recent methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5]</ref> aim to transfer the well-pretrained image-text knowledge from text-image to text-video. For example, CLIP4Clip <ref type="bibr" target="#b22">[23]</ref> develops temporal ensemble modules to aggregate sequential CLIP frame features into a global one. Besides, massive matched and unmatched text-video pairs are constructed to learn video representations and text embeddings via cross-modal contrastive learning. By finetuning on text-video data, CLIP4Clip achieves great success in TVR. Different from learning global video representations and sentence embeddings, DCR <ref type="bibr" target="#b28">[29]</ref> proves that the token-wise interaction between frames and words reveals more fine-grained cross-modal knowledge. Specifically, DCR constructs a similarity matrix between different frame representations and word embeddings, and then infers a comprehensive text-video matching score by considering dense frame-word correlations. However, these methods only consider single cross-modal interaction from either the video-sentence level or the frame-word level, which results in biased retrieval. In the human sense, we recognize a video-text pair by simultaneously analyzing video-sentence, clip-phrase, and frame-word interaction, due to the intrinsic hierarchical semantic structure in video and text data, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In this paper, we propose a novel method, named HunYuan_tvr, that hierarchically explores videosentence, clip-phrase, and frame-word interactions to understand text-video contents comprehensively.</p><p>To utilize large-scale pretraining knowledge, HunYuan_tvr leverages CLIP as our initial visual and text encoders, similar to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref>. To explore hierarchical cross-modal interactions, HunYuan_tvr first constructs hierarchical visual representations and text embeddings for respective frame-clip-video and word-phrase-sentence granularities, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Taking visual modality as an example, we perform self-attention to gather correlated frames and aggregate them via adaptive weights. This can cluster semantic-related frames into an integral one that forms the clip concept. Similarly, by aggregating all clip representations, we can obtain the global video representation, which is different from summing up frame representations in CLIP4Clip. Then, with hierarchical frame-clip-video representations and word-phrase-sentence embeddings, cross-modal contrastive learning is designed to learn inter-modal knowledge at different granularities. Specifically, at frame-word and clip-frame granularities, token-wise interaction is used to capture dense correlations between multi-tokens. Finally, we argue that it is unreasonable to regard all videos as individual categories and repel a text embedding from all other video representations with different sample IDs, which is, however, widely used in cross-modal contrastive learning. For example, the text of a man is running' should not be a negative pair of other videos containing 'man running' content. Thus, HunYuan_tvr designs an adaptive label denoising strategy to discover potential positive text-video pairs even with different sample IDs to avoid confusing updating, and proposes a marginal sample enhancement strategy to improve feature discrimination. Consequently, HunYuan_tvr obtains new state-of-the-art performance on various benchmarks.</p><p>Our contributions are three-fold: a) we propose a novel method, named HunYuan_tvr, that hierarchically explores video-sentence, clip-phrase, and frame-word interactions to understand text-video contents comprehensively; b) we design adaptive label denoising and marginal sample enhancement strategies to discover potential positive pairs and enlarge hard sample margins, which can avoid noisy gradients and improve feature discrimination; c) HunYuan_tvr obtains new state-of-the-art Rank@1 retrieval results of 55.0%, 58.2%, 29.7%, 52.1%, 57.3% on MSR-VTT, MSVD, LSMDC, DiDemo, and ActivityNet, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, a plenty of pre-trained model based methods have dominated the leaderboard of the video-text retrieval. In general, these methods can be roughly divided into two categories: videosentence-interaction-based and frame-word-interaction-based methods.</p><p>The video-sentence-interaction-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, whose retrieval process is extremely concise and efficient, employ the separate text and video embedding extractors to map the texts and videos into a common feature space, and then directly conduct the retrieval task based on the cosine similarities between their feature representations. For example, FROZEN <ref type="bibr" target="#b1">[2]</ref> treats an image as a single-frame video and designs a curriculum learning schedule to train the model on both image and video datasets. Different from the FROZEN pretraining a new model on video-text retrieval, the previous SOTO method CLIP4clip <ref type="bibr" target="#b22">[23]</ref> transfers the knowledge from the image-text pre-trained model CLIP <ref type="bibr" target="#b25">[26]</ref> to solve the video-text retrieval task. Also based on the pre-trained CLIP model, CAMoE <ref type="bibr" target="#b4">[5]</ref> proposes a multi-stream Corpus Alignment network with single gate Mixture-of-Experts (CAMoE) and a novel Dual Softmax Loss (DSL) to further improve the retrieval performance.</p><p>The frame-word-interaction-based methods first utilize embedding extractors to transform each text (video) into a sequence of token (frame) embeddings, and then an interaction module is used to capture the fine-grained clues between the token and frame embeddings. The state-of-the-art method, DRL <ref type="bibr" target="#b28">[29]</ref>, proposes a Weighted Token-wise Interaction (WTI) to explore the fine-grained clues between sentence tokens and video frame embeddings and a Channel DeCorrelation Regularization (CDCR) to reduce feature redundancy from a micro-view.</p><p>However, almost all of the existing methods only consider single cross-modal interaction from either the video-sentence level or the frame-word level. Few of them explore the intrinsic hierarchical semantic structure in the video and text data. Hence, in this paper, we propose a novel method, dubbed HunYuan_tvr, that hierarchically explores video-sentence, clip-phrase, and frame-word interactions to understand text-video contents comprehensively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Given a set of videos v = {v i } N i=0 and corresponding captions t = {t i } N i=0 , our core motivation is to learn a visual encoder f v (?) and a text encoder f t (?) that well capture visual and text semantics. Here, we denote V f i ? R N f ?D as the N f frame representations extracted from f v (v i ), and T w i ? R Nt?D as the N t word embeddings extracted from f t (t i ). D is the feature dimension. The overall pipeline of HunYuan_tvr is given in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Cross-Modal Interaction</head><p>V f i and T w i contain frame-level and word-level information, which depict fine-grained contents of video and text, respectively. To further extract features that capture temporal visual information and long-term word dependence, HunYuan_tvr leverages self-attention to aggregate semantic-related frames and words. Taking visual modality as an example, the aggregation function g v (?) can be defined as:</p><formula xml:id="formula_0">V c i = g v (V f i ) = sof tmax(V f i W ) T h(V f i ) W ? R D?Nc ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">sof tmax(V f i W ) T projects V f i ? R N f ?D into normalized frame weights, which has the dimension of R N f ?Nc . h(?) is a two-layers FC-ReLU layer with channel changes D ? 2D ? D. Thus, V c i ? R Nc?D aggregates N f frame representations into N c clip representations, where N c &lt; N f . Denote V c i = {V c i,1 ? ? ? V c i,Nc }. V c i,j</formula><p>aggregates several frame representations, thereby containing clip information. Also, the text aggregation function g t (?) can be given as:</p><formula xml:id="formula_2">T p i = g t (T w i ) = sof tmax(T w i W ) T h(T w i ) W ? R D?Nw ,<label>(2)</label></formula><p>Video Input</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Input</head><p>Visual Encoder</p><p>Text Encoder</p><formula xml:id="formula_3">Hierarchical Visual Representation { ! " , ! # , ! $ } Hierarchical Text Representation { ! % , ! &amp; , ! ' } Adaptive Label Denoising Hierarchical</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Interaction</head><p>Batch Size</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Marginal Sample Enhancement</head><p>Hard Negative Sample + <ref type="figure">Figure 2</ref>: The pipeline of HunYuan_tvr. Given hierarchical visual and text features, HunYuan_tvr first produces text-video pair labels via the adaptive label denoising strategy. Besides, the hard negative samples will be selected according to text-video matching scores. Then, the produced text-video labels and hard negative samples will be sent to the hierarchical cross-modal contrastive loss and triplet loss, respectively.</p><p>where T p i ? R Np?D contains phrase information. Similar to g v (?) and g t (?), V c i and T p i can be further aggregated into the video-level representation V v i ? R 1?D and the sentence-level embedding</p><formula xml:id="formula_4">T s i ? R 1?D . Notably, {V f i , V c i , V v i } and {T w i , T p i , T s i } make up video</formula><p>representations for frame-clip-video granularities and text embeddings for word-phrase-sentence granularities, respectively. Next, we design hierarchical contrastive learning for different granularities.</p><p>For {V f i , T w i }, the token-wise interaction function is given by:</p><formula xml:id="formula_5">L f ?w = ? 1 N N i (log exp(T I(V f i , T w i )) N j exp(T I(V f i , T w j )) + log exp(T I(T w i , V f i )) N j exp(T I(T w i , V f j )) )/2, (3) T I(V f i , T w i ) = 1 N t Nt n=1 max Nv m=1 &lt; T w i,n , V f i,m &gt; + 1 N v Nv n=1 max Nt m=1 &lt; V f i,n , T w i,m &gt; /2,<label>(4)</label></formula><p>where &lt; ?, ? &gt; is the dot product function. T I(V f i , T w i ) first calculates a similarity matrix between different frames and words, and then aggregates all token-wise similarities into an overall score. L f ?w is a symmetric cross-modal contrastive loss.</p><formula xml:id="formula_6">For {V c i , T p i }, L c?p is given by: L c?p = ? 1 N N i (log exp(T I(V c i , T p i )) N j exp(T I(V c i , T p j )) + log exp(T I(T p i , V c i )) N j exp(T I(T p i , V c j )) )/2.<label>(5)</label></formula><p>For</p><formula xml:id="formula_7">{V v i , T s i }, L v?s is given by: L v?s = ? 1 N N i (log exp(&lt; V v i , T s i &gt;) N j exp(&lt; V v i , T s j &gt;) + log exp(&lt; T s i , V v i &gt;) N j exp(&lt; T s i , V v j &gt;) )/2.<label>(6)</label></formula><p>Finally, the loss function for hierarchical cross-modal interaction is:</p><formula xml:id="formula_8">L hci = L f ?w + ?L c?p + ?L v?s ,<label>(7)</label></formula><p>where ? and ? balance different terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Label Denoising</head><p>Furthermore, most existing methods simply treat all video-text pairs with different sample IDs as negative pairs. For example, a text has only one positive video that is from the same data pair, and all the videos from other data pairs are treated as negative datapoints. However, in fact, one text usually has other similar video descriptions among the dataset, and if taking such datapoints as the negative ones, it will harm the retrieval performance of the model. To tackle the above issue, we propose a novel adaptive label denoising way to discover such potential similar datapoints.</p><p>First, for one video v i , we obtain two views v 1 i and v 2 i of it by randomly sampling its frames twice. Then the feature embedding of these two views v 1 i and v 2 i may be different, but they denote the same video, i.e., they are similar. Similarly, suppose there are two views v 1 j and v 2 j for the video v j , then the video v j can be treated similar to v i , when the following inequality holds:</p><formula xml:id="formula_9">1 4 k,l?{1,2} cos(V vk i , V vl j ) ? cos(V v1 i , V v2 i ),<label>(8)</label></formula><p>where cos(?, ?) denotes the cosine similarity between the inputs, and V vk i represents the video level feature of the k th view of video v i . Furthermore, considering that the datapoints in the matching video-caption data pair (v i , t i ) are similar, then when the videos v i and v j are defined as similar through Formula <ref type="formula" target="#formula_9">(8)</ref>, all the datapoints in the two data pairs (v i , t i ) and (v j , t j ) are similar.</p><p>Finally, Formulas <ref type="formula">(3)</ref>, <ref type="formula" target="#formula_6">(5)</ref> and <ref type="formula" target="#formula_7">(6)</ref> can be reformulated as follows:</p><formula xml:id="formula_10">L f ?w = ? 1 N N i ( k?N + i log exp(T I(V f i , T w k )) exp(T I(V f i , T w k )) + j?N ? i exp(T I(V f i , T w j )) + k?N + i log exp(T I(T w i , V f k )) exp(T I(T w i , V f k )) + j?N ? i exp(T I(T w i , V f j )) )/2,<label>(9)</label></formula><formula xml:id="formula_11">L c?p = ? 1 N N i ( k?N + i log exp(T I(V c i , T p k )) exp(T I(V c i , T p k )) + j?N ? i exp(T I(V c i , T p j )) + k?N + i log exp(T I(T p i , V c k )) exp(T I(T p i , V c k )) + j?N ? i exp(T I(T p i , V c j )) )/2,<label>(10)</label></formula><formula xml:id="formula_12">L v?s = ? 1 N N i ( k?N + i log exp(&lt; V v i , T s k &gt;) exp(&lt; V v i , T s k &gt;) + j?N ? i exp(&lt; V v i , T s j &gt;) + k?N + i log exp(&lt; T s i , V v k &gt;) exp(&lt; T s i , V v k &gt; + j?N ? i exp(&lt; T s i , V v j &gt;) )/2,<label>(11)</label></formula><p>where N + i denotes indexes of data pairs similar to the data pair (v i , t i ); N ? i denotes the indexes of data pairs dissimilar to (v i , t i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Marginal Sample Enhancement</head><p>To fully explore cross-modal interactions, we design a marginal sample enhancement strategy to boost the performance. Concretely, for a video V v i , we select the hardest text sample T s j according to L hci and vice versa. Then, a triplet loss is applied to</p><formula xml:id="formula_13">{V v i , T s i , T s j } and {T s i , V v i , V v k } as: L hsm = (max(&lt; V v i , T s j &gt; ? &lt; V v i , T s i &gt; +?, 0) + max(&lt; T s i , V v k &gt; ? &lt; T s i , V v i &gt; +?, 0))/2,<label>(12)</label></formula><p>where ? is a margin coefficient. Here, we only use {V v i , T s i } at the video-sentence granularity, as the gradients will be back-propagated to {V f i , T w i } and {V c i , T p i }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Scalability</head><p>Existing vision-language pre-training methods have proved that multi-modal content understanding benefits greatly from large model scalability. In this paper, we explore the model scalability in the text-video retrieval task. Inspired by Switch Transformer [], we utilize Sparse Mix-of-Expert (SMoE) to expand the model scalability in HunYuan_tvr.</p><p>In <ref type="figure">Figure 2</ref>, we replace the dense feed forward (FFN) layers in both visual and text encoders, i.e., Transformer, with SMoE layers in []. Specifically, a SMoE layer is consisted of a gating function G(?) and several experts E i (?). G(?) determines which experts are used for an input token. Given an input token x, the gate-value of G(?) for expert E i (?) is:</p><formula xml:id="formula_14">p i (x) = e G(x)i Ne j e G(x)j ,<label>(13)</label></formula><p>where N j is the number of experts and p i (x) is the gate-value for i ? th expert. Then, the top-k values p i (x) are selected to route x. For examples, T is the selected index set by G(?). The output token y is obtained by weighted combination of selected experts outputs, which is defined by:</p><formula xml:id="formula_15">y = i?T p i (x)E i (x).<label>(14)</label></formula><p>Usually, we set k N e to achieve sparse routing.</p><p>In Eq. <ref type="formula" target="#formula_0">(14)</ref>, we select appropriate experts for an input token, adaptively. Since k N e , not all experts are loaded and updated for a batch data, thus we can train a very large model with limited GPU resources. After traversing the whole dataset, all the experts can be well-trained and are skilled at tackling different data samples. This enables us to explore the power of large-scale model, efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Overall Objective</head><p>The overall objective function of HunYuan_tvr is</p><formula xml:id="formula_16">L = L hci + ?L hsm ,<label>(15)</label></formula><p>where ? controls the balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>HunYuan_tvr is evaluated on five public benchmarks:</p><p>MSR-VTT <ref type="bibr" target="#b31">[32]</ref> is the most popular TVR benchmark, which contains 10,000 videos with 20 captions. We report the results on the standard full split.</p><p>MSVD <ref type="bibr" target="#b29">[30]</ref> contains 1,970 videos with 80,000 captions. We report the standard split.</p><p>DiDeMo <ref type="bibr" target="#b26">[27]</ref> contains 10,000 videos with 40,000 sentences. Following <ref type="bibr" target="#b22">[23]</ref>, all captions are concatenated into a single query for text-video retrieval.</p><p>LSMDC <ref type="bibr" target="#b0">[1]</ref> contains 118,081 videos with an equal number of caption sentences from 202 movies. We adopt the standard split for training and testing.</p><p>ActivityNet <ref type="bibr" target="#b3">[4]</ref> contains 20,000 YouTube videos. Following <ref type="bibr" target="#b22">[23]</ref>, we concatenate all captions of a video as a single query.</p><p>Following <ref type="bibr" target="#b22">[23]</ref>, we report experiments under the standard TVR metrics which contain recall at rank-1, rank-5, rank-10, median rank, and mean rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The basic visual and text encoders adopt the pretrained weights in CLIP <ref type="bibr" target="#b25">[26]</ref>, which include ViT-B/16 and ViT-L/14 architectures <ref type="bibr" target="#b6">[7]</ref>. Besides, a 4-layer temporal transformer is added to capture the  temporal information on the top of the visual encoder, similar to CLIP4Clip <ref type="bibr" target="#b22">[23]</ref>, of which the parameters are initialized from the text encoder. The frame length N v and word length N w are 12 and 32 for MSR-VTT, MSVD, LSMDC, and 64 and 64 for DiDeMo and ActivityNet, respectively. The network is optimized by Adam with batch-size 128 and epoch 5. The initial learning rate is 1e ? 7 for the clip parameters and 1e ? 4 for the non-clip parameters, respectively. The hyper-parameters are set as N c = N p = 6, ? = 0.5, ? = 0.1, ? = ? = 0.1.</p><p>For the HunYuan_tvr with SMoE, we explore the best setting for different datasets using 64 A100 GPUs. Here, we utlize the ViT-L/14 as the basic model, which has aroud 450M parameters. Then, we replace the FFN layers in ViT-L/14 with SMoE layers to expand the model scale. For MSR-VTT, LSMDC and MSVD with 12 frames and 32 words, we use 64 experts in each SMoE layer, which produces about 17B parameters of the whole model. For ActivityNet and DiDeMo with 32 frames and 64 words, we use 8 experts in each SMoE layer to fill up the GPUs. Finally, after experiments, We find that k = 2 gives the best results for MSR-VTT and LSMDC, and k = 4 is the best for MSVD. For ActivityNet and DiDeMo, k = 1 gives the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We conducted experiments on MSR-VTT to evaluate the effects of each module and method in HunYuan_tvr. 'Base' indicates the baseline CLIP4Clip model with a temporal transformer and ViT-B/16. 'GDP' indicates the global dot product interaction between video representations and sentence embeddings, which is used in the CLIP4Clip method. 'TWI' indicates using token-wise interaction in <ref type="bibr" target="#b28">[29]</ref>, which explores a dense relationship between different frames and words. 'HCI' indicates the hierarchical cross-modal interaction in HunYuan_tvr, which explores frame-word, clip-phase, and video-word relationships, simultaneously. 'Denoise' and 'MSE' indicate adding adaptive label denoising and marginal sample enhancement strategies. 'Dual' indicates using the dual-softmax inference proposed in <ref type="bibr" target="#b4">[5]</ref>. The results are given in <ref type="table" target="#tab_0">Table 1</ref>. It can be seen that, compared to the global dot product interaction in 'Base', our hierarchical cross-modal interaction ('HCI') brings a significant <ref type="table">Table 3</ref>: Retrieval results on MSVD. * indicates that the method uses post-processing operations, e.g., DSL <ref type="bibr" target="#b4">[5]</ref> or QB-Norm <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Text-Video Video-Text R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR CE <ref type="bibr" target="#b20">[21]</ref> 19.8 49.0 63.8 6.0 ------SUPPORT <ref type="bibr" target="#b23">[24]</ref> 28  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Art Methods</head><p>In this part, we compare our HunYuan_tvr with state-of-the-art methods on MSR-VTT, MSVD, LSMDC, DiDeMo, and ActivityNet benchmarks.</p><p>The MSR-VTT results are given in <ref type="table" target="#tab_1">Table 2</ref>. It can be seen that our HunYuan_tvr significantly surpasses CLIP4Clip by 10.4% R@1 and outperforms the brand-new method DCR by 1.6%, which proves the effectiveness of our method. The reason of weak performance of ViT-L/14 is that a large-scale model tends to overfit to a small-scale dataset. From the results of MSVD and LSMDC in <ref type="table">Table 3</ref> and 4, HunYuan_tvr with ViT-L/14 obtains an obvious gain over previous methods. Notably, MDMMT-2 also adopts ViT-L/14 as its backbone, and LSMDC has over 100K videos. These results prove that the large-scale model can benefit cross-modal knowledge when having enough data for training.</p><p>Besides dense model, we further expand HunYuan_tvr to huge model with about 17B parameters, which is denoted as HunYuan_tvr (hube) in the Tables. Especially on MSR-VTT and LSMDC with large-scale videos, HunYuan_tvr shows dominated superiority over dense models. This proves the potential of large-scale models on tackling multi-modal tasks.</p><p>In summary, HunYuan_tvr obtains new state-of-the-art performance on popular TVR benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we revisited recent Text-Video Retrieval (TVR) methods and analyzed their pros and cons. Considering comprehensive interaction between two modalities in human perception, we proposed a novel method, named HunYuan_tvr, that hierarchically explores video-sentence, <ref type="table">Table 5</ref>: Retrieval results on DiDeMo. * indicates that the method uses post-processing operations, e.g., DSL <ref type="bibr" target="#b4">[5]</ref> or QB-Norm <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Text-Video Video-Text R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR CE <ref type="bibr" target="#b20">[21]</ref> 15.6 40.  <ref type="table">Table 6</ref>: Retrieval results on ActivityNet. * indicates that the method uses post-processing operations, e.g., DSL <ref type="bibr" target="#b4">[5]</ref> or QB-Norm <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Text-Video Video-Text R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR CE <ref type="bibr" target="#b20">[21]</ref> 17.7 46.6 -6.0 ------SUPPORT <ref type="bibr" target="#b23">[24]</ref> 28.7 60. clip-phrase, and frame-word interactions to understand text-video contents. Besides, two boosting strategies, e.g., adaptive label denoising and marginal sample enhancement, were designed to further improve the performance. Consequently, HunYuan_tvr has been demonstrated to surpass the existing TVR methods on MSR-VTT, MSVD, LSMDC, DiDeMo, and ActivityNet by a notable margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>HunYuan_tvr simultaneously considers frame-word, clip-phrase, and video-sentence granularities to explore the hierarchical cross-modal relationship.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The ablation study results on MSR-VTT 1k.</figDesc><table><row><cell>Base GDP TWI HCI Denoise MSE Dual</cell><cell cols="5">Text-Video Retrieval R@1 R@5 R@10 MdR MnR</cell></row><row><cell></cell><cell>44.6</cell><cell>72.0</cell><cell>82.3</cell><cell>2.0</cell><cell>13.6</cell></row><row><cell></cell><cell>48.3</cell><cell>74.9</cell><cell>83.8</cell><cell>2.0</cell><cell>12.4</cell></row><row><cell></cell><cell>49.0</cell><cell>76.5</cell><cell>84.3</cell><cell>2.0</cell><cell>11.8</cell></row><row><cell></cell><cell>49.5</cell><cell>76.9</cell><cell>84.2</cell><cell>2.0</cell><cell>12.2</cell></row><row><cell></cell><cell>49.7</cell><cell>75.0</cell><cell>83.5</cell><cell>2.0</cell><cell>11.4</cell></row><row><cell></cell><cell>54.9</cell><cell>79.5</cell><cell>87.0</cell><cell>1.0</cell><cell>10.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Retrieval results on MSR-VTT 1K. * indicates that the method uses post-processing operations, e.g., DSL<ref type="bibr" target="#b4">[5]</ref> or QB-Norm<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell>Methods</cell><cell cols="10">Text-Video R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR Video-Text</cell></row><row><cell>HERO[18]</cell><cell>16.8</cell><cell>43.4</cell><cell>57.7</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UniVL[22]</cell><cell>21.2</cell><cell>49.6</cell><cell>63.1</cell><cell>6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ClipBERT[16]</cell><cell>22.0</cell><cell>46.8</cell><cell>59.9</cell><cell>6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MDMMT[8]</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4.0</cell><cell>-</cell><cell>27.0</cell><cell>57.5</cell><cell>69.7</cell><cell>3.7</cell><cell>-</cell></row><row><cell>SUPPORT[24]</cell><cell>27.4</cell><cell>56.3</cell><cell>67.7</cell><cell>3.0</cell><cell>-</cell><cell>26.6</cell><cell>55.1</cell><cell>67.5</cell><cell>3.0</cell><cell>-</cell></row><row><cell>FROZEN[2]</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP4Clip[23]</cell><cell>44.5</cell><cell>71.4</cell><cell>81.6</cell><cell>2.0</cell><cell>15.3</cell><cell>42.7</cell><cell>70.9</cell><cell>80.6</cell><cell>2.0</cell><cell>-</cell></row><row><cell>CLIP2Video[9]</cell><cell>45.6</cell><cell>72.6</cell><cell>81.7</cell><cell>2.0</cell><cell>14.6</cell><cell>43.5</cell><cell>72.3</cell><cell>82.1</cell><cell>2.0</cell><cell>10.2</cell></row><row><cell>CAMoE*[5]</cell><cell>48.8</cell><cell>75.6</cell><cell>85.3</cell><cell>2.0</cell><cell>12.4</cell><cell>50.3</cell><cell>74.6</cell><cell>83.8</cell><cell>2.0</cell><cell>9.9</cell></row><row><cell>CLIP2TV*[11]</cell><cell>52.9</cell><cell>78.5</cell><cell>86.5</cell><cell>1.0</cell><cell>12.8</cell><cell>54.1</cell><cell>77.4</cell><cell>85.7</cell><cell>1.0</cell><cell>9.0</cell></row><row><cell>MDMMT-2[15]</cell><cell>48.8</cell><cell>75.7</cell><cell>84.4</cell><cell>2.0</cell><cell>13.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DCR*[29]</cell><cell>53.3</cell><cell>80.3</cell><cell>87.6</cell><cell>1.0</cell><cell>-</cell><cell cols="2">56.2 79.9</cell><cell>87.4</cell><cell>1.0</cell><cell>-</cell></row><row><cell>Ours +B/16</cell><cell>49.7</cell><cell>75.0</cell><cell>83.5</cell><cell>2.0</cell><cell>11.4</cell><cell>47.4</cell><cell>76.1</cell><cell>85.2</cell><cell>2.0</cell><cell>8.1</cell></row><row><cell>Ours +L/14</cell><cell>49.5</cell><cell>74.2</cell><cell>83.9</cell><cell>2.0</cell><cell>12.1</cell><cell>46.8</cell><cell>75.1</cell><cell>84.0</cell><cell>2.0</cell><cell>9.7</cell></row><row><cell>Ours +B/16+DSL</cell><cell cols="2">55.0 80.4</cell><cell>86.8</cell><cell>1.0</cell><cell>10.3</cell><cell>55.5</cell><cell>78.4</cell><cell>85.8</cell><cell>1.0</cell><cell>7.7</cell></row><row><cell cols="3">HunYuan_tvr (huge)* 62.9 84.5</cell><cell>90.8</cell><cell>1.0</cell><cell>9.3</cell><cell>64.8</cell><cell>84.9</cell><cell>91.1</cell><cell>1.0</cell><cell>5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Retrieval results on LSMDC. * indicates that the method uses post-processing operations, e.g., DSL<ref type="bibr" target="#b4">[5]</ref> or QB-Norm<ref type="bibr" target="#b2">[3]</ref>. e.g., 4.9% improvement on R@1. Compared to the token-wise interaction 'TWI', 'TCI' also shows obvious superiority, due to extra clip-phrase and video-sentence interactions. Finally, our HunYuan_tvr is made up of Base+HCI+Denoise+MSE+Dual.</figDesc><table><row><cell>Methods</cell><cell cols="10">Text-Video R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR Video-Text</cell></row><row><cell>CE[21]</cell><cell>11.2</cell><cell>26.9</cell><cell>34.8</cell><cell>25.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP4Clip[23]</cell><cell>22.6</cell><cell>41.0</cell><cell>49.1</cell><cell>11.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CAMoE*[5]</cell><cell>25.9</cell><cell>46.1</cell><cell>53.7</cell><cell>-</cell><cell>54.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MDMMT-2[15]</cell><cell>26.9</cell><cell>46.7</cell><cell>55.9</cell><cell>6.7</cell><cell>48.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DCR*[29]</cell><cell>26.5</cell><cell>47.6</cell><cell>56.8</cell><cell>7.0</cell><cell>-</cell><cell>27.0</cell><cell>45.7</cell><cell>55.4</cell><cell>8.0</cell><cell>-</cell></row><row><cell>Ours +B/16</cell><cell>24.5</cell><cell>44.1</cell><cell>53.9</cell><cell>9.0</cell><cell>52.5</cell><cell>24.6</cell><cell>43.6</cell><cell>51.8</cell><cell>9.0</cell><cell>47.8</cell></row><row><cell>Ours +L/14</cell><cell>27.1</cell><cell>45.1</cell><cell>53.4</cell><cell>8.5</cell><cell>60.3</cell><cell>26.1</cell><cell>42.5</cell><cell>52.0</cell><cell>9.0</cell><cell>54.5</cell></row><row><cell>Ours +L/14+DSL</cell><cell cols="2">29.7 46.4</cell><cell>55.4</cell><cell>7.0</cell><cell cols="3">56.4 30.1 47.5</cell><cell>55.7</cell><cell>7.0</cell><cell>48.9</cell></row><row><cell cols="3">HunYuan_tvr (huge) 40.4 80.1</cell><cell>92.8</cell><cell>2.0</cell><cell>3.9</cell><cell cols="2">34.6 71.8</cell><cell>91.8</cell><cell>2.0</cell><cell>4.3</cell></row><row><cell>gain,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12777</idno>
		<title level="m">Cross modal retrieval with querybank normalisation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<title level="m">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-bert: Aligning image pixels with text by deep multi-modal transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mdmmt-2: Multidomain multimodal transformer for video retrieval, one more step towards generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kunitsyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivaniuta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07086</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Align and prompt: Video-and-language pretraining with entity prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09583</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11915" to="11925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07111</idno>
		<title level="m">Disentangled representation learning for text-video retrieval</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for video classification and captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers of multimedia research</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14084</idno>
		<title level="m">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
