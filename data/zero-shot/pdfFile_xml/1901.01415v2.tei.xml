<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Luc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">C</forename><surname>Sakaridis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">S</forename><surname>Hecker</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">L</forename><surname>Van Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Z?rich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Switzerland</forename><forename type="middle">L</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Van Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Leuven, Belgium</roleName><surname>Leuven</surname></persName>
						</author>
						<title level="a" type="main">Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic foggy scene understanding ? Fog simulation ? Learning with synthetic and real data ? Curriculum model adaptation ? Network distillation ? Adverse weather conditions</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work addresses the problem of semantic scene understanding under fog. Although marked progress has been made in semantic scene understanding, it is mainly concentrated on clear-weather scenes. Extending semantic segmentation methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both labeled synthetic foggy data and unlabeled real foggy data. The method is based on the fact that the results of semantic segmentation in moderately adverse conditions (light fog) can be bootstrapped to solve the same problem in highly adverse conditions (dense fog). CMAda is extensible to other adverse conditions and provides a new paradigm for learning with synthetic data and unlabeled real data. In addition, we present four other main stand-alone contributions: 1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; 2) a new fog density estimator; 3) a novel fog densification method for real foggy scenes without known depth; and 4) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 40 images with dense fog. Our experiments show that 1) our fog simulation and fog density estimator outperform their state-of-the-art counterparts with respect to the task of semantic foggy scene understanding (SFSU); 2) CMAda improves the performance of state-ofthe-art models for SFSU significantly, benefiting both from our synthetic and real foggy data. The foggy datasets and code are publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Adverse weather or illumination conditions create visibility problems for both people and the sensors that power automated systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b58">59]</ref>. While sensors and the downstream vision algorithms are constantly getting better, their performance is mainly benchmarked on clear-weather images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Many outdoor applications, however, cannot escape from "bad" weather <ref type="bibr" target="#b43">[44]</ref>. One typical example of adverse weather conditions is fog, which degrades the visibility of a scene significantly <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b63">64]</ref>. The denser the fog is, the more severe this problem becomes.</p><p>During the past years, the community has made a tremendous progress in image dehazing (defogging) to increase the visibility in foggy images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b68">69]</ref>. The last few years have also witnessed a leap in object recognition. A great deal of effort is made specifically in semantic road scene understanding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. However, the extension of these techniques to other weather/illumination conditions has not received due attention, despite its importance in outdoor applications. For example, an automated car still needs to detect other traffic agents and traffic control devices in the presence of fog or rain. This work investigates the problem of semantic foggy scene understanding (SFSU).</p><p>The current "standard" policy for addressing semantic scene understanding is to train a neural network with numerous annotated real images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">57]</ref>. While this trend of creating and using more human annotations may still continue, extending the same protocol to all conditions seems to be problematic, as the manual annotation part is hard to <ref type="figure">Fig. 1</ref> The illustrative pipeline of a two-stage instantation of CMAda for semantic scene understanding under dense fog scale. The problem is more pronounced for adverse weather conditions, as the difficulty of data collection and annotation increases significantly. To overcome this problem, a few streams of research have gained extensive attention: learning with limited, weak supervision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43]</ref>, transfer learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>, and learning with synthetic data <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Our method falls into the middle ground, and aims to combine the strength of these two kinds of methods. In particular, our method is developed to learn from 1) a dataset with high-quality synthetic fog and the corresponding human annotations, and 2) a dataset with a large number of unlabeled images with real fog. The goal of our method is to improve the performance of SFSU without requiring extra human annotations for foggy images.</p><p>To this end, this work proposes a novel fog simulator to add high-quality synthetic fog to real images of clearweather outdoor scenes, and then leverage these partially synthetic foggy images for SFSU. Our fog simulator builds on the recent work of Sakaridis et al. <ref type="bibr" target="#b58">[59]</ref>, by introducing a semantic-aware filter to exploit the structures of object instances. We show that learning with our synthetic foggy data improves the performance for SFSU. Furthermore, we learn a fog density estimator from synthetic images of varying fog density, and order unlabeled real images by increasing fog density. This ordering forms the foundation of our novel learning method Curriculum Model Adaptation (CMAda) to gradually adapt a semantic segmentation model from clear weather to dense fog, through light fog. CMAda is based on the fact that recognition in moderately adverse conditions (light fog) is easier and its results can be re-used via knowledge distillation to solve a harder problem, i.e. recognition in highly adverse conditions (dense fog).</p><p>CMAda is iterative by nature and can be implemented for different numbers of steps. The pipeline of a two-step implementation of CMAda is shown in <ref type="figure">Figure 1</ref>. CMAda has the potential to be used for other adverse weather conditions, and opens a new avenue for learning with synthetic data and unlabeled real data in general. Experiments show that CMAda yields the best results on two datasets with dense real fog as well as a dataset with real fog of varying density.</p><p>A shorter version of this work has been published to European Conference on Computer Vision <ref type="bibr" target="#b57">[58]</ref>. Compared to the conference version, this paper makes the following six additional contributions:</p><p>1. An extension of the formulation of CMAda to accommodate multiple adaptation steps instead of only two steps, leading to improved performance over the conference paper as well. 2. A novel fog densification method for real foggy scenes.</p><p>The fog densification method can close the domain gap between light real fog and dense real fog; using it in CMAda significantly increases the performance for SFSU. 3. A method named Model Selection for the task of semantic scene understanding in multiple weather conditions where test images are a mixture of clear-weather images and foggy images. This extension is important for real world applications, as weather conditions change constantly. Semantic scene understanding methods need to be robust to such changes.</p><p>4. An enlarged annotated dense foggy set for our Foggy Zurich dataset, increasing its size from 16 to 40 images. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>More extensive experiments to diagnose the contribution of each component of the CMAda pipeline, to compare with more competing methods, and to comprehensively study the usefulness of image dehazing for SFSU. <ref type="bibr" target="#b5">6</ref>. Other sections are also enhanced, including related work as well as dataset collection and annotation.</p><p>The paper is structured as follows. Section 2 presents the related work. Section 3 is devoted to our method for simulating synthetic fog, which is followed by Section 4 for our learning approach. Section 5 summarizes our data collection and annotation. Finally, Section 6 presents our experimental results and Section 7 concludes this paper. Our foggy datasets and fog simulation code are publicly available at https://www.vision.ee.ethz.ch/ csakarid/Model_adaptation_SFSU_dense/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is relevant to image defogging, joint image filtering, foggy scene understanding, and domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Defogging/Dehazing</head><p>Fog fades the color of observed objects and reduces their contrast. Extensive research has been conducted on image defogging (dehazing) to increase the visibility of foggy scenes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b63">64]</ref>. Certain works focus particularly on enhancing foggy road scenes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b65">66]</ref>. Recent approaches also rely on trainable architectures <ref type="bibr" target="#b64">[65]</ref>, which have evolved to end-to-end models <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b72">73]</ref>. For a comprehensive overview of defogging/dehazing algorithms, we point the reader to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b70">71]</ref>. Our work is complementary and mainly focuses on SFSU, while it also investigates the usefulness of image dehazing in the context of SFSU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Image Filtering</head><p>Using additional images as input for filtering a target image has been originally studied in settings where the target image has low photometric quality <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52]</ref> or low resolution <ref type="bibr" target="#b34">[35]</ref>. Compared to the bilateral filtering formulation of these approaches, subsequent works propose alternative formulations, such as the guided filter <ref type="bibr" target="#b28">[29]</ref> and mutual structure filtering <ref type="bibr" target="#b60">[61]</ref>, for better incorporating the reference image into the filtering process. In comparison, we extend the classical cross-bilateral filter to a dual-reference cross-bilateral filter by accepting two reference images, one of which is a discrete label image that helps our filter adhere to the semantics of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Foggy Scene Understanding</head><p>Typical examples in this line include road and lane detection <ref type="bibr" target="#b3">[4]</ref>, traffic light detection <ref type="bibr" target="#b33">[34]</ref>, car and pedestrian detection <ref type="bibr" target="#b23">[24]</ref>, and a dense, pixel-level segmentation of road scenes into most of the relevant semantic classes <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. While deep recognition networks have been developed <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b74">75]</ref> and large-scale datasets have been presented <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, that research mainly focused on clear weather. There is also a large body of work on fog detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b62">63]</ref>. Classification of scenes into foggy and fog-free has been tackled as well <ref type="bibr" target="#b50">[51]</ref>. In addition, visibility estimation has been extensively studied for both daytime <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67]</ref> and nighttime <ref type="bibr" target="#b21">[22]</ref>, in the context of assisted and autonomous driving. The closest of these works to ours is <ref type="bibr" target="#b66">[67]</ref>, in which synthetic fog is generated and foggy images are segmented to free-space area and vertical objects. Our work differs in that our semantic scene understanding task is more complex and we tackle the problem from a different route by learning jointly from synthetic fog and real fog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Domain Adaptation</head><p>Our work bears resemblance to transfer learning and model adaptation. Model adaptation across weather conditions to semantically segment simple road scenes is studied in <ref type="bibr" target="#b37">[38]</ref>. More recently, domain adversarial based approaches were proposed to adapt semantic segmentation models both at pixel level and feature level from simulated to real environments <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70]</ref>. Most of these works are based on adversarial domain adaptation. Our work is complementary to methods in this vein; we adapt the model parameters with carefully generated data, leading to an algorithm whose behavior is easy to understand and whose performance is more predictable. Combining our method and adversarial domain adaptation is a promising direction. Our work also shares similarity to <ref type="bibr" target="#b73">[74]</ref> in applying the general idea of curriculum learning to domain adaptation. The concurrent work in <ref type="bibr" target="#b14">[15]</ref> on adaptation of semantic segmentation models from daytime to nighttime using solely real data, which was preceded by the conference version of this paper, shows that real images captured at twilight are helpful for supervision transfer from daytime to nighttime. CMAda constitutes a more complex framework, since it leverages both synthetic foggy data and real foggy data jointly for adapting semantic segmentation models to fog, whereas the method in <ref type="bibr" target="#b14">[15]</ref> uses solely real data for the adaptation. Moreover, the assignment of real foggy images to the correct target foggy domain through fog density estimation is another crucial and nontrivial component of CMAda and it is a prerequisite for using these real images as training data in the method. By contrast, the partition of the real dataset in <ref type="bibr" target="#b14">[15]</ref> into subsets that correspond to different times of day from daytime to nighttime is trivially performed by using the time of capture of the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fog Simulation on Real Scenes Using Semantics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>We drive our motivation for fog simulation on real scenes using semantic input from the pipeline that was used in <ref type="bibr" target="#b58">[59]</ref> to generate the Foggy Cityscapes dataset, which primarily focuses on depth denoising and completion. This pipeline is denoted in <ref type="figure">Figure 2</ref> with thin gray arrows and consists of three main steps: depth outlier detection, robust depth plane fitting at the level of SLIC superpixels <ref type="bibr" target="#b1">[2]</ref> using RANSAC, and postprocessing of the completed depth map with guided image filtering <ref type="bibr" target="#b28">[29]</ref>. Our approach adopts the general configuration of this pipeline, but aims to improve its postprocessing step by leveraging the semantic annotation of the scene as additional reference for filtering, which is indicated in <ref type="figure">Figure 2</ref> with the thick blue arrow.</p><p>The guided filtering step in <ref type="bibr" target="#b58">[59]</ref> uses the clear-weather color image as guidance to filter the depth map. However, as previous works on image filtering <ref type="bibr" target="#b60">[61]</ref> have shown, guided filtering and similar joint filtering methods such as crossbilateral filtering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b51">52]</ref> transfer every structure that is present in the guidance/reference image to the output target image. Thus, any structure that is specific to the reference image but irrelevant for the target image is transferred to the latter erroneously.</p><p>Whereas previous approaches such as mutual-structure filtering <ref type="bibr" target="#b60">[61]</ref> attempt to estimate the common structure between reference and target images, we identify this common structure with the structure that is present in the ground-truth semantic labeling of the image. In other words, we assume that edges which are shared by the color image and the depth map generally coincide with semantic edges, i.e. locations in the image where the semantic classes of adjacent pixels are different. Under this assumption, the semantic labeling can be used directly as the reference image in a classical crossbilateral filtering setting, since it contains exactly the mutual structure between the color image and the depth map. In practice, however, the boundaries drawn by humans when creating semantic annotations are not pixel-accurate, and using the color image as additional reference helps to capture the precise location and orientation of edges better. As a result, we formulate the postprocessing step of the completed depth map in our fog simulation as a dual-reference crossbilateral filter, with color and semantic reference.</p><p>Before delving into the formulation of our filter, we briefly argue against alternative usage cases of semantic annotations in our fog simulation pipeline which might seem attractive at first sight. First, replacing SLIC superpixels with superpixels induced by the semantic labeling for the depth plane fitting step is not viable, because it induces very large superpixels, for which the planarity assumption breaks completely. Second, we have experimented with omitting the robust depth plane fitting step altogether and applying our dual-reference cross-bilateral filter directly on the incomplete depth map which is output from the outlier detection step. This approach, however, is highly sensitive to outliers that have not been detected and invalidated in the preceding step. By contrast, these remaining outliers are handled successfully by robust RANSAC-based depth plane fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-reference Cross-bilateral Filter Using Color and Semantics</head><p>Let us denote the RGB image of the clear-weather scene by R and its CIELAB counterpart by J. We consider CIELAB, as it has been designed to increase perceptual uniformity and gives better results for bilateral filtering of color images <ref type="bibr" target="#b48">[49]</ref>. The input image to be filtered in the postprocessing step of our pipeline constitutes a scalar-valued transmittance mapt. We provide more details on this transmittance map in Section 3.3. Last, we are given a labeling function h : P ? {1, . . . , C}</p><p>which maps pixels to semantic labels, where P is the discrete domain of pixel positions and C is the total number of semantic classes in the scene. We define our dual-reference cross-bilateral filter with color and semantic reference as</p><formula xml:id="formula_1">t(p) = ? ? ? q?N (p) G ?s ( q ? p ) [?(h(q) ? h(p)) + ?G ?c ( J(q) ? J(p) )]t(q) ? ? ? ? ? ? q?N (p) G ?s ( q ? p ) [?(h(q) ? h(p)) + ?G ?c ( J(q) ? J(p) )] ? ? ? ,<label>(2)</label></formula><p>where p and q denote pixel positions, N (p) is the neighborhood of p, ? denotes the Kronecker delta, G ?s is the  <ref type="figure">Fig. 2</ref> The pipeline of our fog simulation using semantics spatial Gaussian kernel, G ?c is the color-domain Gaussian kernel and ? is a positive constant. The novel dual reference is demonstrated in the second factor of the filter weights, which constitutes a sum of the terms ?(h(q) ? h(p)) for semantic reference and G ?c ( J(q) ? J(p) ) for color reference, weighted by ?. The formulation of the semantic term implies that only pixels q with the same semantic label as the examined pixel p contribute to the output at p through this term, which prevents blurring of semantic edges. At the same time, the color term helps to better preserve true depth edges that do not coincide with any semantic boundary but are present in J, e.g. due to self-occlusion of an object. The formulation of (2) enables an efficient implementation of our filter based on the bilateral grid <ref type="bibr" target="#b48">[49]</ref>. More specifically, we construct two separate bilateral grids that correspond to the semantic and color domains respectively and operate separately on each grid to perform filtering, combining the results in the end. In this way, we handle a 3D bilateral grid for the semantic domain and a 5D grid for the color domain instead of a single joint 6D grid that would dramatically increase computation time <ref type="bibr" target="#b48">[49]</ref>.</p><p>In our experiments, we set ? = 5, ? s = 20, and ? c = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Remaining Steps</head><p>Here we outline the rest parts of our fog simulation pipeline of <ref type="figure">Figure 2</ref>. For more details, we refer the reader to <ref type="bibr" target="#b58">[59]</ref>, with which most parts of the pipeline are common. The standard optical model for fog that forms the basis of our fog simulation was introduced in <ref type="bibr" target="#b35">[36]</ref> and is expressed as</p><formula xml:id="formula_2">I(x) = R(x)t(x) + L(1 ? t(x)),<label>(3)</label></formula><p>where I(x) is the observed foggy image at pixel x, R(x) is the clear scene radiance and L is the atmospheric light, which is assumed to be globally constant. The transmittance t(x) determines the amount of scene radiance that reaches the camera. For homogeneous fog, transmittance depends on the distance (x) of the scene from the camera through</p><formula xml:id="formula_3">t(x) = exp (?? (x)) .<label>(4)</label></formula><p>The attenuation coefficient ? controls the density of the fog: larger values of ? mean denser fog. Fog decreases the meteorological optical range (MOR), also known as visibility, to less than 1 km by definition <ref type="bibr" target="#b0">[1]</ref>. For homogeneous fog MOR = 2.996/?, which implies</p><formula xml:id="formula_4">? ? 2.996 ? 10 ?3 m ?1 ,<label>(5)</label></formula><p>where the lower bound corresponds to the lightest fog configuration. In our fog simulation, the value that is used for ? always obeys <ref type="bibr" target="#b4">(5)</ref>. The required inputs for fog simulation with (3) are the image R of the original clear scene, atmospheric light L and a complete transmittance map t. We use the same approach for atmospheric light estimation as that in <ref type="bibr" target="#b58">[59]</ref>. Moreover, we adopt the stereoscopic inpainting method of <ref type="bibr" target="#b58">[59]</ref> for depth denoising and completion to obtain an initial complete transmittance mapt from a noisy and incomplete input disparity map D, using the recommended parameters. We filter t with our dual-reference cross-bilateral filter (2) to compute the final transmittance map t, which is used in (3) to synthesize the foggy image I.</p><p>Results of the presented pipeline for fog simulation on example images from Cityscapes <ref type="bibr" target="#b11">[12]</ref> are provided in <ref type="figure">Fig</ref>  150m. We specifically leverage the instance-level semantic annotations that are provided in Cityscapes and set the labeling h of (1) to a different value for each distinct instance of the same semantic class in order to distinguish adjacent instances. We compare our synthetic foggy images against the respective images of Foggy Cityscapes that were generated with the approach of <ref type="bibr" target="#b58">[59]</ref>. Our synthetic foggy images generally preserve the edges between adjacent objects with large discrepancy in depth better than the images in Foggy Cityscapes, because our approach utilizes semantic boundaries, which usually encompass these edges. The incorrect structure transfer of color textures to the transmittance map, which deteriorates the quality of Foggy Cityscapes, is also reduced with our method. We have applied our fog simulation using semantics to the entire Cityscapes dataset. The resulting foggy dataset is named Foggy Cityscapes-DBF (Dual-reference cross-Bilateral Filter). Foggy Cityscapes-DBF is publicly available at the Cityscapes website https://www. cityscapes-dataset.com.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Foggy Scene Understanding</head><p>In this section, we first present a standard supervised learning approach for semantic segmentation under dense fog using our synthetic foggy data with the novel fog simulation of Section 3, and then elaborate on our novel CMAda approach which uses both synthetic and real foggy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning with Synthetic Fog</head><p>Generating synthetic fog from real clear-weather scenes grants the potential of inheriting the existing human annotations of these scenes, such as those from the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>. This is a significant asset that enables training of standard segmentation models. Therefore, an effective way of evaluating the merit of a fog simulator is to adapt a segmentation model originally trained on clear weather to the synthesized foggy images and then evaluate the adapted model against the original one on real foggy images. The primary goal is to verify that the standard learning methods for semantic segmentation can benefit from our simulated fog in the challenging scenario of real fog. This evaluation policy has been proposed in <ref type="bibr" target="#b58">[59]</ref>. We adopt this policy and fine-tune the RefineNet model <ref type="bibr" target="#b39">[40]</ref> on synthetic foggy images from our Foggy Cityscapes-DBF dataset. The performance of our adapted models on real fog is compared to that of the original clear-weather model as well as the models that are adapted on Foggy Cityscapes <ref type="bibr" target="#b58">[59]</ref>, providing an objective comparison of our simulation method against <ref type="bibr" target="#b58">[59]</ref>.</p><p>The learned model can be used as a standalone approach for semantic foggy scene understanding as shown in <ref type="bibr" target="#b58">[59]</ref>, or it can be used as an initialization step for our CMAda method, which is described next and learns both from synthetic and real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Curriculum Model Adaptation (CMAda)</head><p>In the previous section, the proposed method learns to adapt semantic segmentation models from the domain of clear weather to the domain of foggy weather in a single step. While considerable improvement can be achieved (as shown in Section 6.1.1), the method falls short when it is presented with dense fog. This is because domain discrepancies become more accentuated for denser fog: 1) the domain discrepancy between synthetic foggy images and real foggy images increases with fog density; and 2) the domain discrepancy between real clear-weather images and real foggy images increases with fog density. This section presents a method to gradually adapt the semantic segmentation model which was originally trained with clear-weather images to images with dense fog by using both labeled synthetic foggy images and unlabeled real foggy images. The method, which we term Curriculum Model Adaptation (CMAda), uses synthetic fog with a range of varying fog density-from light fog to dense fog-and a large dataset of unlabeled real foggy scenes with variable, unknown fog density. The goal is to improve the performance of state-of-the-art semantic segmentation models on dense foggy scenes without using any human annotations of foggy scenes. Below, we first present our fog density estimator and our method for densification of fog in real foggy images without depth information, and then proceed to the complete learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Fog Density Estimation</head><p>Fog density is usually determined by the visibility of the foggy scene. An accurate estimate of fog density can benefit many applications, such as image defogging <ref type="bibr" target="#b10">[11]</ref>. Since annotating images in a fine-grained manner regarding fog density is very challenging, previous methods are trained on a few hundreds of images divided into only two classes: foggy and fog-free <ref type="bibr" target="#b10">[11]</ref>. The performance of the system, however, is affected by the small amount of training data and the coarse class granularity.</p><p>In this paper, we leverage our fog simulation applied to Cityscapes <ref type="bibr" target="#b11">[12]</ref> for fog density estimation. Since simulated fog density is directly controlled through ?, we generate several versions of Foggy Cityscapes-DBF with varying ? ? {0, 0.005, 0.01, 0.02} and train AlexNet <ref type="bibr" target="#b36">[37]</ref> to regress the value of ? for each image, lifting the need to handcraft features relevant to fog and to collect human annotations as <ref type="bibr" target="#b10">[11]</ref> did. The predicted fog density with our method on real images correlates well with human judgments of fog density, based on a user study conducted on our large real Foggy Zurich dataset via Amazon Mechanical Turk (cf. Section 6.1.2 for results). The fog density estimator is used to order images in Foggy Zurich according to fog density, paving the way for our curriculum adaptation which learns from images with progressively denser fog. We denote the estimator by f : x ? R + , where x is an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">CMAda with Synthetic and Real Fog</head><p>The CMAda algorithm has a source domain denoted by S, an ultimate target domain denoted by T , and an ordered sequence of intermediate target domains indicated by (? 1 , ...,? K ) with K being the number of intermediate domains. In this work, S is clear weather, T is dense fog, an? T k 's correspond to fog density that increases with k, ranging between the density of S (zero) and T . Our method adapts semantic segmentation models through the sequence of domains (S,? 1 ,? 2 , . . . ,? K , T ). The intermediate target do-mains? k 's are optional; when K = 0, the method reduces to a single-stage adaptation as presented in Section 4.1. Similarly, K = 1 leads to a two-stage adaptation approach as presented in the conference version of this paper <ref type="bibr" target="#b57">[58]</ref>, K = 2 to a three-stage adaptation approach, and so on. We abbreviate these instantiations of CMAda as CMAda1 (K = 0), CMAda2 (K = 1), CMAda3 (K = 2), and so on.</p><p>Let us denote by z ? {1, ..., Z} the domain index in the above ordered sequence (S,? 1 ,? 2 , . . . ,? K , T ), with Z = K + 2. In this work, the sequence of domains is sorted in ascending order with respect to fog density. For instance, it could be (clear weather, light fog, dense fog), with clear weather being the source domain, dense fog the ultimate target domain and light fog the intermediate target domain. The approach proceeds progressively and adapts the semantic segmentation model from the current domain (fog density) to the subsequent one by learning from the corresponding synthetic foggy dataset and the corresponding real foggy dataset. Once the model for the subsequent domain has been trained, its knowledge is distilled on unlabeled real foggy images from that domain, and then used along with a denser version of synthetic foggy data to adapt this model to the next domain (i.e. the immediately higher fog density).</p><p>Since the method proceeds in an iterative manner, we only present the algorithmic details for model adaptation from z ? 1 to z. Let us use ? z to indicate the fog density for domain z, represented as the attenuation coefficient. In order to adapt the semantic segmentation model ? z?1 from the previous domain z ? 1 to the current domain z, we generate synthetic fog of the exact fog density ? z and inherit the human annotations of the original clear-weather images. Thus, the synthetic foggy dataset for adapting to z is</p><formula xml:id="formula_5">D z syn = {(x ?z m , y 1 m )} M m=1 ,<label>(6)</label></formula><p>where M is the total number of synthetic foggy images, y 1 m (i, j) ? {1, ..., C} is the label of pixel (i, j) of the clearweather image x ?1 m (? 1 = 0), and C is the total number of classes.</p><p>For real foggy images, since no human annotations are available, we rely on a strategy of self-learning or curriculum learning. Objects in lighter fog are easier to recognize than in denser fog, hence models trained for lighter fog are more generalizable to real data. The model ? z?1 for the previous domain z ? 1 can be applied to all real foggy images with fog density less than ? z?1 in order to generate supervisory labels for training model ? z for domain z. Specifically, the real foggy dataset for adapting to z is</p><formula xml:id="formula_6">D z real = {(x n ,? z?1 n ) | f (x n ) ? ? z?1 } N n=1 ,<label>(7)</label></formula><p>where? z?1 n = ? z?1 (x n ) denotes the predicted labels of image x n using the model ? z?1 .</p><p>Once the two training sets are formed, the aim is to learn ? z from D z syn and D z real . The proposed scheme balances the contributions of both the synthetic foggy dataset D z syn from domain z with human annotations and the real foggy dataset D z real from domain z ? 1 with labels inferred using model</p><formula xml:id="formula_7">? z?1 : min ? z (x ,y ) ?D z syn L(? z (x ), y ) + ? (x ,y ) ?D z real L(? z (x ), y ) ,<label>(8)</label></formula><p>where L(., .) is the cross entropy loss function and ? = w R M is a hyper-parameter balancing the weights of the two datasets, with w serving as the relative weight of each real noisily labeled image compared to each synthetic labeled one and R being the number of images in D z real . We empirically set w = 1 in our experiments, but an optimal value can be obtained via cross-validation if needed. The optimization of (8) is implemented by generating a hybrid data stream and feeding it to a CNN for standard supervised training. More specifically, during training, training images are fetched from the randomly shuffled D z syn and D z real with a ratio of 1 : w.</p><p>We now describe the initialization stage of our method, which is also a variant of our method when no intermediate target domains are used. When z = 1, we are in the clearweather domain and the model ? 1 is directly trained on a labeled real dataset, so no adaptation is required. For the case z = 2, there are no real foggy images falling into the domain z ? 1 = 1 which is the clear-weather domain. In this case, the model ? 2 is trained with the synthetic dataset D 2 syn only, as specified in Section 4.1. For the remaining steps from z = 3 on, we iteratively apply the adaptation approach introduced above to adapt to domain Z, which constitutes the ultimate target domain T . In this work, we have experimented with three instantiations of our method for Z = {2, 3, 4}, which we name CMAda1, CMAda2 and CMAda3 respectively. The sequences of attenuation coefficients (fog densities) for the three versions are (0, 0.01), (0, 0.005, 0.01) and (0, 0.0025, 0.005, 0.01) respectively. <ref type="figure">Figure 1</ref> provides an overview of CMAda2. Below, we summarize the complete operations of CMAda2 to further help understand the method. With the chosen sequence of attenuation coefficients (0, 0.005, 0.01), the whole pipeline of CMAda2 is as follows:</p><p>1. generate a synthetic foggy dataset with multiple versions of varying fog density; 2. train a model for fog density estimation on the dataset of step 1; 3. rank the images in the real foggy dataset with the model of step 2 according to fog density; 4. generate a dataset with light synthetic fog (? = 0.005),</p><p>and train a segmentation model on it; 5. apply the segmentation model from step 4 to the lightfog images of the real dataset (ranked lower in step 2) to obtain noisy semantic labels; 6. generate a dataset with dense synthetic fog (? = 0.01); 7. adapt the segmentation model from step 4 to the union of the dense synthetic foggy dataset from step 6 and the light real foggy one from step 5 according to <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Discussion</head><p>CMAda adapts segmentation models from clear weather to dense fog and is inspired by curriculum learning <ref type="bibr" target="#b4">[5]</ref>, in the sense that we first solve easier tasks with our synthetic data, i.e. fog density estimation and semantic scene understanding under light fog, and then acquire new knowledge from the already "solved" tasks in order to better tackle the harder task, i.e. semantic scene understanding under dense real fog. CMAda also exploits the direct control of fog density for synthetic foggy images. This learning approach also bears resemblance to model distillation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b30">31]</ref> or imitation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. The underpinnings of our proposed approach are the following: 1) in light fog objects are easier to recognize than in dense fog, hence models trained on synthetic data are more generalizable to real data in case both data sources contain light rather than dense fog; and 2) models trained on the source domain can be successfully applied to the target domain when the domain gap is small, hence incremental (curriculum) domain adaptation can better propagate semantic knowledge from the source domain to the ultimate target domain than single-step domain adaptation approaches.</p><p>The goal of CMAda is to train a semantic segmentation model for the ultimate target domain z. The standard recipe is to record foggy images x ?z 's and then to manually create semantic labels y ?z 's for those foggy images so that the standard supervised learning can be applied. As discussed in Section 1, there is difficulty to apply this recipe to all adverse weather conditions because manual creation of y ?z 's is very time-consuming and expensive. To address this problem, this work develops methods to automatically create two proxy datasets for (x ?z , y ?z ). The two proxies are defined in <ref type="bibr" target="#b5">(6)</ref> and in <ref type="bibr" target="#b6">(7)</ref>. These two proxies reflect different and complementary characteristics of (x ?z , y ?z ). On the one hand, dense synthetic fog features a similar overall visibility obstruction to dense real fog, but includes artifacts. On the other hand, light real fog captures the true nonuniform and spatially varying structure of fog, but at a different density than dense fog. Learning jointly from both proxy datasets in CMAda reduces the influence of their individual drawbacks.</p><p>The CMAda pipeline presented in Section 4.2.2 is an extension of the original method proposed in the conference version <ref type="bibr" target="#b57">[58]</ref> of this paper from a two-stage approach to a general multiple-stage approach. CMAda is a standalone approach and already outperforms competing methods for SFSU, as discussed in Section 6. In the next section, we present an extension of CMAda, CMAda+, that further boosts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CMAda+ with Synthetic and Densified Real Fog</head><p>As defined in <ref type="bibr" target="#b5">(6)</ref>, images in the synthetic training set D z syn have exactly the same fog density ? z as images in the target domain z. Images in the real dataset D z real , however, have lower fog density than the target fog density ? z , as defined in <ref type="bibr" target="#b6">(7)</ref>. While the lower fog density of the real training images facilitates the self-learning stream of CMAda with real foggy images, the remaining domain gap due to the disparity in fog density hampers finding a better solution. In Section 4.3.1, we present a method to densify fog in real foggy images so that it matches the desired fog density. The fog densification method is general and can be applied beyond CMAda. In Section 4.3.2, we use our fog densification method to upgrade the dataset defined in <ref type="bibr" target="#b6">(7)</ref> to a densified foggy dataset, which is used in CMAda+ along with the synthetic dataset to train the model ? z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Fog Densification of a Real Foggy Scene</head><p>We aim at synthesizing images with increased fog density compared to already foggy real input images for which no depth information is available. In this way, we can generate multiple synthetic versions of each split of our real Foggy Zurich dataset, where each synthetic version is characterized by a different, controlled range of fog densities, so that these densified foggy images can be leveraged in our curriculum adaptation. To this end, we utilize our fog density estimator and propose a simple yet effective approach for increasing fog density when no depth information is available for the input foggy image, by using the assumption of constant transmittance in the scene.</p><p>More formally, we denote the input real foggy image with I l and assume that it can be expressed through the optical model <ref type="bibr" target="#b2">(3)</ref>. Contrary to our fog simulation on clearweather scenes in Section 3, the clear scene radiance R is unknown and the input foggy image I l cannot be directly used as its substitute for synthesizing a foggy image I d with increased fog density, as I l does not correspond to clear weather. Since the scene distance which determines the transmittance through (4) is also unknown, we make the simplifying assumption that the transmittance map for I l is globally constant, i.e.</p><formula xml:id="formula_8">t(x) = t l ,<label>(9)</label></formula><p>and use the statistics for scene distance computed on Cityscapes, which features depth maps, to estimate t l . By using the distance statistics from Cityscapes, we implicitly assume that the distribution of distances of Cityscapes roughly matches that of our Foggy Zurich dataset, which is supported by the fact that both datasets contain similar, road scenes. In particular, we apply our fog density estimator on I l to get an estimate ? l of the input attenuation coefficient. The values for scene distance of all pixels in Cityscapes are collected into a histogram H = {( i , p i ) : i = 1, . . . , N } with N distance bins, where i are the bin centers and p i are the relative frequencies of the bins. We use each bin center as representative of all samples in the bin and compute t l as a weighted average of the transmittance values that correspond to the different bins through (4):</p><formula xml:id="formula_9">t l = N i=1 p i exp (?? l i ) .<label>(10)</label></formula><p>The calculation of t l via (10) enables the estimation of the clear scene radiance R by re-expressing (3) for I l when (9) holds as</p><formula xml:id="formula_10">R(x) = I l (x) ? L t l + L.<label>(11)</label></formula><p>The globally constant atmospheric light L which is involved in <ref type="bibr" target="#b10">(11)</ref> is estimated in the same way as in Section 3.3.</p><p>For the output densified foggy image I d , we select a target attenuation coefficient ? d &gt; ? l and again estimate the corresponding global transmittance value t d similarly to <ref type="bibr" target="#b9">(10)</ref>, this time plugging ? d into the formula. The output image I d is finally computed via (3) as</p><formula xml:id="formula_11">I d (x) = R(x)t d + L (1 ? t d ) .<label>(12)</label></formula><p>If we substitute R in (12) using <ref type="bibr" target="#b10">(11)</ref>, the output image is expressed only through t l , t d , the input image I l and atmospheric light L as  Equation <ref type="formula" target="#formula_0">(13)</ref> implies that our fog densification method can bypass the explicit calculation of the clear scene radiance R in <ref type="formula" target="#formula_0">(11)</ref>, as the output image does not depend on R. In this way, we completely avoid dehazing our input foggy image as an intermediate step, which would pose challenges as it constitutes an inverse problem, and reduce the inference problem just to the estimation of the attenuation coefficient by assuming a globally constant transmittance. Moreover, <ref type="bibr" target="#b12">(13)</ref> implies that the change in the value of a pixel I d (x) with respect to I l (x) is linear in the difference I l (x) ? L. This means that distant parts of the scene, where I l (x) ? L, are not modified significantly in the output, i.e. I d (x) ? I l (x). On the contrary, our fog densification modifies the appearance of those parts of the scene which are closer to the camera and shifts their color closer to that of the estimated atmospheric light irrespective of their exact distance from the camera. This can be observed in the example of <ref type="figure" target="#fig_2">Figure 4</ref>, where the closer parts of the input scene such as the red car on the left and the vegetation on the right have brighter colors in the synthesized output. The overall shift to brighter colors is verified by the accompanying RGB histograms of the input and output images in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><formula xml:id="formula_12">I d (x)= I l (x) + t d ? t l t l (I l (x) ? L) = t d t l I l (x) + 1 ? t d t l L.<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Fog Densification of a Real Foggy Dataset</head><p>When applying our fog densification to an entire dataset in the context of CMAda+, a simple choice is to specify the same target fog density ? z for all images in the dataset. This may completely close the domain gap due to different fog density, but it ignores the variability of the true fog density across different images in the dataset and introduces other domain discrepancies, as our fog densification makes sim-plifying assumptions. Thus, we propose to define the target fog density independently for each input image. Given the dataset D z real defined in <ref type="formula" target="#formula_6">(7)</ref>, instead of mapping all ? l ? [0, ? z?1 ] to ? d = ? z , we choose to perform a linear mapping from [0, ? z?1 ] to [? z?1 , ? z ]. In particular, given a real foggy image with its estimated attenuation coefficient ? l ? [0, ? z?1 ], the target attenuation coefficient is determined as</p><formula xml:id="formula_13">? d = ? z?1 + ? l (? z ? ? z?1 ) ? z?1 .<label>(14)</label></formula><p>Using ? ? l ?? d (x n ) to indicate the densified image for x n , the densified real foggy dataset for CMAda+ at step z is</p><formula xml:id="formula_14">D z real = {(? ? l ?? d (x n ),? z?1 n ) | f (x n ) ? ? z?1 } N n=1 . (15)</formula><p>This densified dataset is then used in CMAda+ for training, along with the synthetic dataset defined in <ref type="formula" target="#formula_5">(6)</ref>, based on the same formulation (8) as CMAda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Semantic Scene Understanding in Multiple Weather Conditions</head><p>In Section 4.2.2 and Section 4.3, specialized approaches have been developed for semantic scene understanding under fog. However, in real world applications weather conditions change constantly, e.g. the weather can change from foggy to sunny or vice versa at any time. We argue that semantic scene understanding methods need to be robust and adaptive to these changes. With this aim, we propose Model Selection, a method for selecting the appropriate model depending on the encountered weather condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Model Selection</head><p>Our method uses two expert models, one specialized for clear weather and the other for fog. In particular, a two-class classifier is trained to distinguish clear weather from fog, with images from the Cityscapes dataset used as samples of the former class and images from three versions of our Foggy Cityscapes-DBF dataset with attenuation coefficients 0.005, 0.01, and 0.02 as samples of the latter class. We select AlexNet <ref type="bibr" target="#b36">[37]</ref> as the architecture of this classifier.</p><p>Denoting the semantic segmentation model specialized for fog by ? Z , the respective model for clear weather by ? 1 , and the aforementioned classifier by g, the semantic labels of a test image x are obtained throug?</p><formula xml:id="formula_15">y = ? 1 (x), if g(x) = 1, ? Z (x) otherwise,<label>(16)</label></formula><p>where label 1 indicates the clear weather class and label 0 indicates fog. The method is not limited to these two conditions and can be directly generalized to handle multiple adverse conditions, such as rain or snow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Foggy Zurich Dataset</head><p>We present the Foggy Zurich dataset, which comprises 3808 images depicting foggy road scenes in the city of Zurich and its suburbs. We provide annotations for semantic segmentation for 40 of these scenes that contain dense fog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Collection</head><p>Foggy Zurich was collected during multiple rides with a car inside the city of Zurich and its suburbs using a Go-Pro Hero 5 camera. We recorded four large video sequences, and extracted video frames corresponding to those parts of the sequences where fog is (almost) ubiquitous in the scene at a rate of one frame per second. The extracted images are manually cleaned by removing the duplicates (if any), resulting in 3808 foggy images in total. The resolution of the frames is 1920?1080 pixels. We mounted the camera inside the front windshield, since we found that mounting it outside the vehicle resulted in significant deterioration in image quality due to blurring artifacts caused by dew.</p><p>In particular, the small water droplets that compose fog condense and form dew on the surface of the lens very shortly after the vehicle starts moving, which causes severe blurring artifacts and contrast degradation in the image, as shown in <ref type="figure" target="#fig_4">Figure 5(b)</ref>. On the contrary, mounting the camera inside the windshield, as we did when collecting Foggy Zurich, prevents these blurring artifacts and affords much  sharper images, to which the windshield surface incurs minimal artifacts, as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Annotation of Images with Dense Fog</head><p>We use our fog density estimator presented in Section 4.2.1 to order all images in Foggy Zurich according to fog density. Based on this ordering, we manually select 40 images with dense fog and diverse visual scenes, and construct the test set of Foggy Zurich therefrom, which we term Foggy Zurichtest. The aforementioned selection is performed manually in order to guarantee that the test set has high diversity, which compensates for its relatively small size in terms of statistical significance of evaluation results. We annotate these images with fine pixel-level semantic annotations using the 19 evaluation classes of the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>: road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky, person, rider, car, truck, bus, train, motorcycle and bicycle. In addition, we assign the void label to pixels which do not belong to any of the above 19 classes, or the class of which is uncertain due to the presence of fog. Every such pixel is ignored for semantic segmentation evaluation. Comprehensive statistics for the semantic annotations of Foggy Zurich-test are presented in <ref type="figure" target="#fig_5">Figure 6</ref>. Furthermore, we note that individual instances of person, rider, car, truck, bus, train, motorcycle and bicycle are annotated separately, which additionally induces bounding box annotations for object detection for these 8 classes, although we focus solely on semantic segmentation in this paper.</p><p>We also distinguish the semantic classes that occur frequently in Foggy Zurich-test. These "frequent" classes are: road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, sky, and car. When performing evaluation on Foggy Zurich-test, we occasionally report the average score over this set of frequent classes, which feature plenty of examples, as a second metric to support the corresponding results.</p><p>Despite the fact that there exists a number of prominent large-scale datasets for semantic road scene understanding, such as KITTI <ref type="bibr" target="#b23">[24]</ref>, Cityscapes <ref type="bibr" target="#b11">[12]</ref> and Mapillary Vistas <ref type="bibr" target="#b46">[47]</ref>, most of these datasets contain few or even no foggy scenes, which can be attributed partly to the rarity of the condition of fog and the difficulty of annotating foggy images. Through manual inspection, we found that even Mapillary Vistas, which was specifically designed to also include scenes with adverse conditions such as snow, rain or nighttime, in fact contains very few images with fog, i.e. in the order of 10 images out of 25000, with relatively more images depicting misty scenes, which have MOR ? 1km, i.e. significantly better visibility than foggy scenes <ref type="bibr" target="#b0">[1]</ref>.</p><p>To the best of our knowledge, the only previous dataset for semantic foggy scene understanding whose scale exceeds that of Foggy Zurich-test is Foggy Driving <ref type="bibr" target="#b58">[59]</ref>, with 101 annotated images. However, most images in Foggy Driving contain relatively light fog and most images with dense fog are annotated coarsely. Compared to Foggy Driving, Foggy Zurich comprises a much greater number of highresolution foggy images. Its larger, unlabeled part is highly relevant for unsupervised or semi-supervised approaches such as the one we have presented in Section 4.2.2, while the smaller, labeled Foggy Zurich-test set features fine semantic annotations for the particularly challenging setting of dense fog, making a significant step towards evaluation of semantic segmentation models in this setting. In <ref type="table" target="#tab_2">Table 1</ref>, we compare the overall annotation statistics of Foggy Zurichtest to some of the aforementioned existing datasets; we note that the comparison involves a test set (Foggy Zurichtest) and unions of training plus validation sets (KITTI and Cityscapes), which are much larger than the respective test sets. The comparatively lower number of humans and vehicles per image in Foggy Zurich-test is not a surprise, as the condition of dense fog that characterizes the dataset discourages road transportation and reduces traffic.</p><p>In order to ensure a sound training and evaluation, we manually filter the unlabeled part of Foggy Zurich and exclude from the resulting training sets that are used in CMAda those images which bear resemblance to any image in Foggy Zurich-test with respect to the depicted scene. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Our model of choice for experiments on semantic segmentation with our CMAda pipeline is the state-of-the-art RefineNet <ref type="bibr" target="#b39">[40]</ref>. We use the publicly available RefineNet-res101-Cityscapes model, which has been trained on the clear-weather training set of Cityscapes. In all experiments of this section, we use a constant learning rate of 5 ? 10 ?5 and mini-batches of size 1. Moreover, we compile all versions of Foggy Cityscapes-DBF by applying our fog simulation (which is denoted by "SDBF" in the following for short) on the same refined set of Cityscapes images that was used in <ref type="bibr" target="#b58">[59]</ref> to compile Foggy Cityscapes-refined. This set comprises 498 training and 52 validation images; we use the former for training. In our experiments, we use the values 0.005 and 0.01 for attenuation coefficient ? both in SDBF and the fog simulation of <ref type="bibr" target="#b58">[59]</ref> (denoted by "SGF") to generate different versions of Foggy Cityscapes-DBF and Foggy Cityscapes respectively with varying fog density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance on Foggy Scenes</head><p>For evaluation, we use 1) Foggy Zurich-test, 2) a subset of Foggy Driving <ref type="bibr" target="#b58">[59]</ref> containing 21 images with dense fog, which we term Foggy Driving-dense, and 3) the entire Foggy Driving <ref type="bibr" target="#b58">[59]</ref>. We summarize our main experimental results in <ref type="table" target="#tab_4">Table 2</ref>. Overall, our method significantly improves the performance of semantic segmentation under dense fog compared to the original RefineNet model which has been trained on clear-weather images of Cityscapes. More specifically, we improve the performance (mIoU) from 34.6% to 46.8% on Foggy Zurich-test and from 35.8% to 43.0% on Foggy Driving-dense. With the new extensions, our fully-fledged CMAda3+ method significantly outperforms CMAda2, which was originally presented in the conference version of this paper <ref type="bibr" target="#b57">[58]</ref>.</p><p>It is worthwhile to mention that these improvements are achieved without using any extra human annotations on top of the original Cityscapes. Also, images in Foggy Driving were taken by different cameras than the GoPro Hero 5 cam-  era used for Foggy Zurich, showing that CMAda also generalizes well to different sensors from that corresponding to the real training set of the method.</p><p>In the rest of Section 6.1, we analyze the effect of the individual components of our approach. This analysis demonstrates the benefit for semantic segmentation of real foggy scenes of: 1) our fog simulation for generating synthetic training data, 2) our fog density estimator against a state-ofthe-art competing method, 3) combining our synthetic foggy data from Foggy Cityscapes-DBF with unlabeled real data from Foggy Zurich through our CMAda pipeline to adapt gradually to dense real fog in multiple steps, and 4) using our fog densification method to further close the gap between light real fog and dense real fog. Finally, we provide some qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Benefit of Adaptation with Our Synthetic Fog</head><p>Our first segmentation experiment shows that our semanticaware fog simulation (SDBF) performs competitively compared to the fog simulation of <ref type="bibr" target="#b58">[59]</ref> (SGF) for generating synthetic data to adapt RefineNet to real dense fog. RefineNet-res101-Cityscapes is fine-tuned on Foggy Cityscapes-DBF and alternatively Foggy Cityscapes, both with attenuation coefficient ? = 0.01, for 8 epochs. The corresponding results in <ref type="table" target="#tab_4">Table 2</ref> are presented in the top two rows under the group "CMAda1". Training on synthetic fog with either type of fog simulation helps to beat the baseline clearweather RefineNet model on all three test sets, the improvement being more significant on Foggy Zurich-test and Foggy Driving. In addition, SDBF slightly outperforms SGF consistently.</p><p>Moreover, in all cases that both synthetic and real foggy data are used in the two-stage CMAda pipeline, corresponding to the rows of <ref type="table" target="#tab_4">Table 2</ref> grouped under "CMAda2", SDBF yields significantly higher segmentation performance on Foggy Zurich-test compared to SGF, while the two methods are on a par on the other two sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Benefit of Our Fog Density Estimator on Real Data</head><p>The second component of the CMAda pipeline that we ablate is the fog density estimator. In particular, <ref type="table" target="#tab_4">Table 2</ref> includes results for the single-stage pipeline with adaptation on real images from the unlabeled part of Foggy Zurich and the two-stage pipeline with adaptation on synthetic and real images from Foggy Cityscapes and Foggy Zurich respectively, where the ranking of real images according to fog density is performed either with the method of <ref type="bibr" target="#b10">[11]</ref> or with our AlexNet-based fog density estimator described in Section 4.2.1. In all experimental settings, our fog density estimator outperforms <ref type="bibr" target="#b10">[11]</ref> significantly in terms of mIoU on all datasets. This fully lifts the need of manually designing features and labeling images for fog density estimation, as was done in <ref type="bibr" target="#b10">[11]</ref>.</p><p>For further verification of our fog density estimator, we conduct a user study on Amazon Mechanical Turk (AMT). In order to guarantee high quality, we only employ AMT Masters in our study and verify the answers via a Known Answer Review Policy. Each human intelligence task (HIT) comprises five image pairs to be compared: three pairs are the true query pairs with images from the real Foggy Zurich dataset, and the rest two pairs contain synthetic fog of different densities and are used for validation. The participants are shown two images at a time, side by side, and are simply asked to choose the one which is more foggy. The query pairs are sampled based on the ranking results of our estimator. In order to avoid confusing cases, i.e. two images of similar fog densities, the two images of each pair need to be ranked at least 20 percentiles apart from each other by our estimator.</p><p>We have collected answers for 12000 pairs in 4000 HITs. The HITs are considered for evaluation only when both validation questions are correctly answered. 87% of all HITs are valid for evaluation. On these 10400 pairs, the agreement between our fog density estimator and human judgment is 89.3%. This high agreement confirms that fog density estimation is a relatively easier task which can be solved  <ref type="figure">. 7</ref> Foggy images from Foggy Zurich, sorted from left to right in ascending order with respect to estimated fog density using our estimator by using synthetic data, and the acquired knowledge can be further exploited for solving high-level tasks on foggy scenes. <ref type="figure">Figure 7</ref> shows foggy images in ascending order of estimated fog density using our estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Benefit of Adaptation with Synthetic and Real Fog</head><p>The main segmentation experiment showcases the effectiveness of our CMAda pipeline. Foggy Cityscapes-DBF and Foggy Cityscapes <ref type="bibr" target="#b58">[59]</ref> are the two alternatives for the synthetic foggy training sets in steps 4 and 6 of the pipeline, corresponding to the two alternatives for fog simulation (SDBF and SGF respectively). Foggy Zurich serves as the real foggy training set. We use the results of our fog density estimation to select 1556 images from Foggy Zurich with light fog and name this set Foggy Zurich-light. We implement CMAda2 by first fine-tuning RefineNet on Foggy Cityscapes-DBF (alternatively Foggy Cityscapes) with ? = 0.005 for 6k iterations and then further fine-tuning it on the union of Foggy Cityscapes-DBF (alternatively Foggy Cityscapes) with ? = 0.01 and Foggy Zurich-light, where the latter set is labeled by the aforementioned initially adapted model. Two-stage curriculum adaptation to dense fog with synthetic and real data, which corresponds to the results in the rows that are grouped under "CMAda2" in <ref type="table" target="#tab_4">Table 2</ref>, consistently outperforms single-stage adaptation with either only synthetic or only real training data ("CMAda1"), irrespective of the selected fog simulation and fog density estimation methods. The combination of our fog simulation SDBF and our fog density estimator delivers the best result on all three test sets among all variants of CMAda2, improving upon the baseline RefineNet model on Foggy Zurich-test by 8.3%. The same combination also provides a clear generalization benefit of 4.2% against the baseline on Foggy Driving, even though this dataset involves different camera sensors and scenes than Foggy Zurich, which is the sole real-world dataset used in our training.</p><p>We note that the significant performance benefit delivered by CMAda both on Foggy Zurich-test and Foggy Driving is not matched by the state-of-the-art domain-adversarial approach of <ref type="bibr" target="#b67">[68]</ref> for adaptation of semantic segmentation models, which we also trained both on our synthetic Foggy Cityscapes-DBF set and our unlabeled real Foggy Zurichlight set. This can be attributed to the fact that images captured under adverse conditions such as fog have large intradomain variance as a result of poor visibility, effects of artificial lighting sources and motion blur. However, we believe that domain-adversarial approaches have the potential to be used for transferring knowledge to adverse weather domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Benefit of Adaptation at Finer Scales</head><p>We also experiment with the three-stage instantiation of CMAda, CMAda3, using the optimal configuration of all components of the pipeline based on the previous comparisons. Compared to CMAda2, CMAda3 adapts the semantic segmentation model at a finer scale, i.e. 1) from clearweather to mist with synthetic misty data; 2) then to light fog with synthetic light foggy data and real misty data; and 3) finally to dense fog with synthetic dense foggy data and real light foggy data. The exact fog densities at each stage are defined in Section 4.2.2. In particular, the extra stage compared to CMAda2 consists in labeling a split of Foggy Zurich with very light estimated fog, which we term Foggy Zurich-light+, via the clear-weather RefineNet model and using it in conjunction with Foggy Cityscapes-DBF with ? = 0.005 to form the training set for the first stage of CMAda.</p><p>Including this extra stage affords higher segmentation performance on all three test sets as reported in row "CMAda3" of <ref type="table" target="#tab_4">Table 2</ref>, outperforming the respective best CMAda2 instance by 3.3% on Foggy Driving-dense. The improvement of CMAda3 over CMAda2 shows that our approach benefits from adaptation at finer scales, which is in line with the rationale of curriculum learning <ref type="bibr" target="#b4">[5]</ref>. However, training for a large number of stages increases the computational cost significantly. Thus, selecting the "optimal" number of stages and the exact fog densities that correspond to the intermediate target domains needs further investigation and could be solved to some extent by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">Benefit of Fog Densification</head><p>The final component of our proposed pipeline that we evaluate is our fog densification method, introduced in Section 4.3. <ref type="table" target="#tab_4">Table 2</ref> shows the results of CMAda2+ and CMAda3+ on the three test datasets, along with the results of their counterparts CMAda2 and CMAda3. CMAda2+ and CMAda2 use the same training parameters. The same holds for CMAda3+ and CMAda3. Applying our fog densification to the real foggy training sets used in CMAda significantly improves performance for both numbers of adaptation stages that are examined. For instance, CMAda3+ outperforms CMAda3 by 3.1%, 2.4% and 0.9% on Foggy Zurich-test, Foggy Driving-dense and Foggy Driving respectively. This is because without fog densification, the images in the synthetic dataset D z syn of each adaptation stage (defined in <ref type="formula" target="#formula_5">(6)</ref>) have the exact same fog density ? z as images in the target domain of that stage, whereas the images in the real dataset D z real have lower fog density than ? z (cf. <ref type="bibr" target="#b6">(7)</ref>). This lower fog density of the real training images facilitates the self-learning, bootstrapping strategy. However, it also creates a domain gap between training and test images due to the difference in their fog density. On the contrary, the dataset with densified fog defined in <ref type="bibr" target="#b14">(15)</ref> matches the target fog density of the test images, which helps close this domain gap and significantly boosts the performance of CMAda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.6">Qualitative Results and Discussion</head><p>In <ref type="figure" target="#fig_6">Figure 8</ref>, we show segmentation results on Foggy Zurichtest generated with our best-performing method CMAda3+, our conference paper method CMAda2 and the single-stage version CMAda1 using only synthetic training data from Foggy Cityscapes-DBF, compared to the method of <ref type="bibr" target="#b58">[59]</ref> that only uses synthetic data from Foggy Cityscapes <ref type="bibr" target="#b58">[59]</ref> and the clear-weather RefineNet model <ref type="bibr" target="#b39">[40]</ref>. This visual comparison demonstrates that our multiple-stage methods CMAda3+ and CMAda2 yield significantly better results and generally capture the road layout more accurately than the two competing approaches and our single-stage method CMAda1. Moreover, the more stages CMAda involves, the more accurate the segmentation result is in general. For instance, on the leftmost image of <ref type="figure" target="#fig_6">Figure 8</ref>, CMAda3+ segments the wall and the vegetation on the right side much better than the other methods and only misclassifies some parts of them as building, which is a much less detrimental error from a driving perspective than confusing these classes with road, as is the case for the other methods. Similarly, the buildings and the tree trunk in the third image are better segmented by CMAda3+.</p><p>To further demonstrate the behavior of CMAda, we also show semantic segmentation results of the clear-weather Re-fineNet model <ref type="bibr" target="#b39">[40]</ref> and the three aforementioned variants of our method for variable fog density in <ref type="figure">Figure 9</ref>. In particular, we have applied our fog density estimator to Foggy Driving and use four images therefrom for which the estimated fog density ranges from very low to very high. First, we observe that the clear-weather baseline performs comparably well for very light fog due to the small domain shift from clear weather, but for higher fog densities CMAda variants outperform this baseline. The advantage gets more pronounced as fog density increases. Second, comparing the different CMAda variants, we conclude that having more adaptation stages leads to increasing returns as fog density increases. For instance, the bus in the highly foggy rightmost image  is correctly recognized only after all three adaptation stages have been applied.</p><p>While we observe a significant improvement with CMAda, semantic segmentation performance on foggy scenes is still much worse than the reported performance by existing papers on clear-weather scenes. Foggy scenes are indeed more challenging than clear-weather scenes with respect to understanding their semantics. There are more underlying causal factors of variation that generated foggy data, which requires either more training data or more intelligent learning approaches to disentangle the increased degrees of freedom. While our method shows considerable improvement by transferring semantic knowledge from clearweather to fog, the models are adapted in an "unsupervised" manner, i.e. without using human annotations of real foggy data. Incorporating a moderate amount of human annota-tions of real foggy scenes into our learning approach is a promising research direction, if significantly better results are desired.</p><p>Our method involves two data streams: partially synthetic data with annotations and real data without annotations. Learning from the real data stream is based on a "selflearning" mechanism, which creates a risk of entering a negative reinforcement loop by adapting to mistakes made at previous stages. In practice, we find that our training process is stable. In order to further investigate this, we follow the literature <ref type="bibr" target="#b52">[53]</ref> to identify and exclude the erroneous predictions from training. In particular, the confidence scores of the predictions are used as a proxy for prediction quality and we generate pseudo-labels only for pixels where this confidence is higher than a defined threshold. This prediction selection step, however, does not provide clear benefit and thus is not included in our approach.</p><p>We believe that the low risk of entering the negative reinforcement loop and the steady improvement of our method can be ascribed to two factors: 1) the accurate human annotations of the partially synthetic data stream restrict the space of adapted models, ruling out solutions that would create severe errors in the inferred labels of the real data; and 2) each adaptation stage is initialized with the solution of the previous stage, which helps smoothly traverse the model space from the initial clear-weather model to the target foggy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance in Multiple Weather Conditions</head><p>We first note that the results which have been presented in <ref type="table" target="#tab_4">Table 2</ref> on the Foggy Driving dataset <ref type="bibr" target="#b58">[59]</ref>, which contains images of varying fog densities from very low to high, show that adaptation with CMAda to dense fog also brings a significant benefit for lower fog densities.</p><p>In the following, we turn to evaluation of our Model Selection method presented in Section 4.4.1 for the task of semantic scene understanding in multiple weather conditions. We consider two conditions: foggy weather and clear weather. This means that the test set comprises a mixture of images captured either in clear weather or under fog. In particular, we report the performance of three domainspecific methods and two variants of our Model Selection on three datasets. The three domain-specific methods are: 1) RefineNet, which is trained on Cityscapes dataset <ref type="bibr" target="#b39">[40]</ref> for clear weather, 2) CMAda2, which is trained for foggy weather, and 3) CMAda3+, which is also trained for foggy weather. The first variant of Model Selection uses RefineNet and CMAda2 as its two expert models and the second one uses RefineNet and CMAda3+ respectively. The three test datasets are Cityscapes-lindau-40, Foggy Zurich-test, and Clear-Foggy-80, which is the union of the two previous sets. Cityscapes-lindau-40 contains the first 40 images (in alphabetical order) from the city of Lindau in the validation set of Cityscapes.</p><p>The performance of all five methods on the three datasets is reported in <ref type="table" target="#tab_6">Table 3</ref>. We share a few observations. First, as discussed in previous sections, our adapted models significantly improve the recognition performance on foggy scenes. Second, it seems that some knowledge initially learned for recognition in clear-weather scenes is forgotten by our models during the adaptation process. This is also evidenced by the visual comparison in <ref type="figure" target="#fig_7">Figure 10</ref>, where the sky in the first image is misclassified after the adaptation. This is because during the adaptation stages, we aim for the best expert model for (dense) foggy scenes and have not included any clear weather images. Adding some clear-weather images into the training data will alleviate this problem, but at a cost of lower performance on foggy scenes. Last but not least, both variants of our Model Selection method demonstrate higher performance than their constituent expert models. The second variant of Model Selection with RefineNet and CMAda3+ yields the best performance. It works especially well on the Clear-Foggy-80 dataset which contains 40 foggy images and 40 clear weather images, due to the good performance of the two expert models in their own domains. The improved performance with Model Selection implies that training multiple expert models-each for a different condition-and adaptively selecting the best one at testing time based on the input is a promising direction for semantic scene understanding in adverse conditions. We also demonstrate the improvement with Model Selection in <ref type="figure" target="#fig_7">Figure 10</ref> when both clear weather and fog are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Investigating the Utility of Dehazing Preprocessing</head><p>For completeness, we conduct an experimental comparison of the baseline RefineNet model of <ref type="table" target="#tab_4">Table 2</ref> and our singlestage CMAda pipeline using only synthetic training data against a dehazing preprocessing baseline, and report the re-  with four options with respect to this dehazing preprocessing: no dehazing at all (already examined in Section 6.1.1), multi-scale convolutional neural networks (MSCNN) <ref type="bibr" target="#b54">[55]</ref>, dark channel prior (DCP) <ref type="bibr" target="#b27">[28]</ref>, and non-local dehazing <ref type="bibr" target="#b5">[6]</ref>. Apart from directly applying the original clear-weather Re-fineNet model on the dehazed test images, the results of <ref type="table">Table 4</ref> Performance comparison on Foggy Zurich-test of RefineNet ("w/o FT") versus fine-tuned versions of it ("FT") trained on Foggy Cityscapes-DBF with attenuation coefficient ? = 0.005, for four options regarding dehazing: no dehazing, MSCNN <ref type="bibr" target="#b54">[55]</ref>, DCP <ref type="bibr" target="#b27">[28]</ref>, and Non-local <ref type="bibr" target="#b5">[6]</ref> Mean IoU over all classes (%) which are included in the "w/o FT" rows of <ref type="table">Tables 4 and  5</ref>, we also fine-tune this model on the dehazed versions of our synthetic Foggy Cityscapes-DBF dataset, and compare against fine-tuning directly on the synthetic foggy images (already examined in Section 6.1.1). Our experimental protocol is consistent: the same dehazing option is used both before fine-tuning and at testing time. The attenuation coefficient for Foggy Cityscapes-DBF is ? = 0.005. The rest details are the same as in Section 6.1.1. Not applying dehazing generally leads to the best results irrespective of using the original model or fine-tuned versions of it. Finetuning without dehazing performs best in all cases but one (Foggy Driving-dense and evaluation on all classes), which confirms the merit of our approach. This lack of significant improvement with dehazing preprocessing is in congruence with the findings of <ref type="bibr" target="#b58">[59]</ref>, which has dissuaded us from including dehazing preprocessing in our default CMAda pipeline. <ref type="figure">Figure 11</ref> illustrates the results of the examined dehazing methods on sample images from Foggy Zurich-test and reveals the issues these methods face on real-world outdoor images with dense fog. Only MSCNN are able to slightly enhance the image contrast while introducing only minor artifacts. This correlates with the superior performance of the segmentation model that uses MSCNN for dehazing preprocessing compared to the models that use the other two methods, as reported in <ref type="table">Table 4</ref>. Still, directly using the original foggy images generally outperforms all dehazing preprocessing alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this article, we have shown the benefit of using partially synthetic as well as unlabeled real foggy data in a curriculum adaptation framework to progressively improve performance of state-of-the-art semantic segmentation models in dense real fog. To this end, we have proposed a novel (a) (b) (c) (d) <ref type="figure">Fig. 11</ref> Representative images from Foggy Zurich-test and dehazed versions of them obtained with the three dehazing methods that we consider in our experiments on utility of dehazing preprocessing. (a) Foggy Zurich-test image. (b) MSCNN <ref type="bibr" target="#b54">[55]</ref>. (c) DCP <ref type="bibr" target="#b27">[28]</ref>. (d) Non-local <ref type="bibr" target="#b5">[6]</ref>. This figure is better seen on an screen and zoomed in <ref type="table">Table 5</ref> Performance comparison on Foggy Driving-dense of Re-fineNet ("w/o FT") versus fine-tuned versions of it ("FT") trained on Foggy Cityscapes-DBF with attenuation coefficient ? = 0.005, for four options regarding dehazing: no dehazing, MSCNN <ref type="bibr" target="#b54">[55]</ref>, DCP <ref type="bibr" target="#b27">[28]</ref>, and Non-local <ref type="bibr" target="#b5">[6]</ref> Mean IoU over all classes (%) fog simulation approach on real scenes, which leverages the semantic annotation of the scene as additional input to a novel dual-reference cross-bilateral filter, and applied it to the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref> to obtain Foggy Cityscapes-DBF. In addition, we have introduced a simple CNN-based fog density estimator which can benefit from large synthetic datasets such as Foggy Cityscapes-DBF that provide straightforward ground truth for this task. On the real data side, we have presented Foggy Zurich, a large-scale realworld dataset of foggy scenes, including pixel-level semantic annotations for 40 scenes with dense fog. Through extensive evaluation, we have showcased that: 1) our curriculum model adaptation exploits both our synthetic and our real data in a synergistic manner and significantly boosts performance on real fog without using any labeled real foggy image, and 2) our fog simulation and fog density estimation methods outperform their state-of-the-art counterparts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ure 3 for ? = 0.02, which corresponds to visibility of ca.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Our foggy image -Foggy Cityscapes-DBF Comparison of our synthetic foggy images against Foggy Cityscapes<ref type="bibr" target="#b58">[59]</ref>. This figure is better seen on a screen and zoomed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Top row, left to right: example input image from Foggy Zurich and synthesized output image with our fog densification. Bottom row, left to right: R, G, and B histogram of the input image, R, G, and B histogram of the output image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Comparison of images taken in fog with the camera mounted (a) inside and (b) outside the front windshield of the vehicle. We opt for the former configuration for collecting Foggy Zurich</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Number of annotated pixels per class for Foggy Zurich-test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8</head><label>8</label><figDesc>Qualitative results for semantic segmentation on Foggy Zurich-test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10</head><label>10</label><figDesc>Qualitative semantic segmentation results under two weather conditions: clear weather (left) and foggy weather (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Absolute and average number of annotated pixels, humans and vehicles for Foggy Zurich-test, Foggy Driving, KITTI and Cityscapes. Only the training and validation sets of KITTI and Cityscapes are considered. "h/im" stands for humans per image, "v/im" for vehicles per image and "Foggy Zurich" for Foggy Zurich-test</figDesc><table><row><cell></cell><cell>Pixels</cell><cell>Humans</cell><cell>Vehicles</cell><cell>h/im</cell><cell>v/im</cell></row><row><cell cols="2">Foggy Zurich 66.1M</cell><cell>27</cell><cell>135</cell><cell>0.7</cell><cell>3.4</cell></row><row><cell cols="2">Foggy Driving 72.8M</cell><cell>290</cell><cell>509</cell><cell>2.9</cell><cell>5.0</cell></row><row><cell>KITTI</cell><cell>0.23G</cell><cell>6.1k</cell><cell>30.3k</cell><cell>0.8</cell><cell>4.1</cell></row><row><cell>Cityscapes</cell><cell>9.43G</cell><cell>24.0k</cell><cell>41.0k</cell><cell>7.0</cell><cell>11.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Performance comparison on Foggy Zurich-test (FZ), Foggy Driving-dense (FDD) and Foggy Driving (FD) of different variants of our CMAda pipeline as well as competing approaches, using with the mean intersection-over-union (mIoU) metric over all classes</figDesc><table><row><cell>Clear-weather</cell><cell>Synthetic fog</cell><cell cols="2">Real fog Density Estimator</cell><cell>FZ</cell><cell>FDD</cell><cell>FD</cell></row><row><cell cols="2">Cityscapes [12] SGF [59] SDBF (ours)</cell><cell>GoPro</cell><cell cols="4">FADE [11] Ours mIoU (%) mIoU (%) mIoU (%)</cell></row><row><cell>Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RefineNet [40]</cell><cell></cell><cell></cell><cell></cell><cell>34.6</cell><cell>35.8</cell><cell>44.3</cell></row><row><cell>SFSU [59]</cell><cell></cell><cell></cell><cell></cell><cell>35.7</cell><cell>35.9</cell><cell>46.3</cell></row><row><cell>AdSegNet [68]</cell><cell></cell><cell></cell><cell></cell><cell>25.0</cell><cell>15.8</cell><cell>29.7</cell></row><row><cell>CMAda2 [58]</cell><cell></cell><cell></cell><cell></cell><cell>42.9</cell><cell>37.3</cell><cell>48.5</cell></row><row><cell>CMAda3+</cell><cell></cell><cell></cell><cell></cell><cell>46.8</cell><cell>43.0</cell><cell>49.8</cell></row><row><cell>Ablation Study</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline [40]</cell><cell></cell><cell></cell><cell></cell><cell>34.6</cell><cell>35.8</cell><cell>44.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.7</cell><cell>35.9</cell><cell>46.3</cell></row><row><cell>CMAda1</cell><cell></cell><cell></cell><cell></cell><cell>36.3 37.5</cell><cell>36.1 36.4</cell><cell>46.3 45.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>38.9</cell><cell>36.6</cell><cell>46.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>39.8</cell><cell>35.7</cell><cell>47.5</cell></row><row><cell>CMAda2</cell><cell></cell><cell></cell><cell></cell><cell>41.5 40.6</cell><cell>37.0 35.5</cell><cell>48.5 47.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.9</cell><cell>37.3</cell><cell>48.5</cell></row><row><cell>CMAda3</cell><cell></cell><cell></cell><cell></cell><cell>43.7</cell><cell>40.6</cell><cell>48.9</cell></row><row><cell>CMAda2+</cell><cell></cell><cell></cell><cell></cell><cell>43.4</cell><cell>40.1</cell><cell>49.9</cell></row><row><cell>CMAda3+</cell><cell></cell><cell></cell><cell></cell><cell>46.8</cell><cell>43.0</cell><cell>49.8</cell></row><row><cell>Fig</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Performance comparison of RefineNet (trained for clear weather), CMAda2 (trained for foggy weather), CMAda3+ (trained for foggy weather), and our Model Selection method on three datasets: Cityscapes-lindau-40 (clear weather), Foggy Zurich-test (foggy weather) and the union of the two Clear-Foggy-80 (clear + foggy weather). "MS R2" stands for Model Selection with RefineNet and CMAda2 as the two expert models and "MS R3+" for Model Selection with RefineNet and CMAda3+ as the two expert models</figDesc><table><row><cell></cell><cell cols="3">Mean IoU over all classes (%)</cell><cell></cell><cell></cell></row><row><cell>Weather</cell><cell cols="5">RefineNet CMAda2 CMAda3+ MS R2 MS R3+</cell></row><row><cell>Clear</cell><cell>67.2</cell><cell>65.1</cell><cell>59.6</cell><cell>67.2</cell><cell>67.2</cell></row><row><cell>Foggy</cell><cell>34.6</cell><cell>42.9</cell><cell>46.8</cell><cell>42.9</cell><cell>46.8</cell></row><row><cell cols="2">Clear + Foggy 54.3</cell><cell>59.1</cell><cell>58.1</cell><cell>59.3</cell><cell>62.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Qualitative semantic segmentation results on images from Foggy Driving with varying fog density. Foggy images in the top row are sorted from left to right in ascending order of estimated fog density using our estimator sults on Foggy Zurich-test and Foggy Driving-dense in Tables 4 and 5 respectively. In particular, we consider dehazing as an optional preprocessing step before feeding the input foggy images to the segmentation model, and experiment</figDesc><table><row><cell>Foggy Images Baseline [40] CMAda1 (ours) CMAda2 (ours) CMAda3+ (ours) Ground Truth Void Baseline [40] Terrain Void CMAda1 CMAda2 CMAda3+ Model Selection Ground Truth Road Fig. 9 Images Terrain Sky</cell><cell>Sidewalk Person</cell><cell>Road Sky Building Rider</cell><cell>Wall Car</cell><cell>Sidewalk Person Fence Truck</cell><cell>Pole Bus</cell><cell>Building Rider Traffic Light Train</cell><cell>Traffic Sign Motorcycle</cell><cell>Wall Car Vegetation Bicycle</cell><cell>Fence Truck</cell><cell>Pole Bus</cell><cell>Traffic Light Train</cell><cell>Traffic Sign Motorcycle</cell><cell>Vegetation Bicycle</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Creating fine pixel-level annotations for dense foggy scenes is very difficult.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work is funded by Toyota Motor Europe via the research project TRACE-Z?rich.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Surface Weather Observations and Reports. U.S. Department of Commerce / National Oceanic and Atmospheric Administration</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Road scene segmentation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent progress in road and lane detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vision Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fog detection system based on computer vision techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bronte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International IEEE Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Referenceless prediction of perceptual fog density and perceptual image defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3888" to="3901" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Metric imitation by manifold transfer for efficient vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ensemble projection for semi-supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time 3d traffic cone detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flash photography enhancement via intrinsic relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards night fog detection through use of in-vehicle multipurpose cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nighttime visibility analysis and estimation method in the presence of dense fog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vision and rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="27" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic fog detection and estimation of visibility distance through use of an onboard camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lavenant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models with surround-view cameras and route planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vision for looking at traffic lights: Issues, survey, and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Philipsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?gelmose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1800" to="1815" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koschmieder</surname></persName>
		</author>
		<title level="m">Theorie der horizontalen Sichtweite. Beitrage zur Physik der freien Atmosph?re</title>
		<imprint>
			<date type="published" when="1924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequential bayesian model update under structured scene prior for semantic road scenes labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno>abs/1607.06235</idno>
		<title level="m">Haze visibility enhancement: A survey and quantitative benchmarking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep transmission network for single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visibility detection in foggy environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Miclea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Silea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Control Systems and Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Watch and learn: Semisupervised learning for object detectors from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exponential contrast restoration in fog conditions for driving assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2257" to="2268" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Mapillary Vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rota Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bayesian defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A fast approximation of the bilateral filter using a signal processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image based fog detection in vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Belzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ili?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Classification of images in fog and fog-free scenes for use in vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ili?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Digital photography with flash and no-flash image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Petschnigg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ACM SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Single image dehazing via multi-scale convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mutual-structure for joint filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast fog detection for camera based advanced driver assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spinneker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International IEEE Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Investigating haze-relevant features in a learning framework for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Vision enhancement in homogeneous and heterogeneous fog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caraffa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Halmaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Transportation Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Improved visibility of road scene images under heterogeneous fog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hauti?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gruyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Halmaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Single image defogging by multiscale depth fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4826" to="4837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Review of video and image defogging algorithms and related studies on image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="165" to="188" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Joint transmission map estimation and dehazing using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno>abs/1708.00581</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
