<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters Compositional Coding Capsule Network with K-Means Routing for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>220 Handan Rd</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Yangpu District</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>220 Handan Rd</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Yangpu District</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Recognition Letters Compositional Coding Capsule Network with K-Means Routing for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text classification is a challenging problem which aims to identify the category of texts. In the process of training, word embeddings occupy a large part of parameters. Under the limitation of limited computing resources, it indirectly limits the ability of subsequent network designs. In order to reduce the number of parameters, the compositional coding mechanism has been proposed recently. Based on this, this paper further explores compositional coding and proposes a compositional weighted coding method. And we apply capsule network to model the relationship between word embeddings, a new routing algorithm, which is based on k-means clustering theory, is proposed to fully mine the relationship between word embeddings. Combined with our compositional weighted coding method and the routing algorithm, we design a neural network for text classification. Experiments conducted on eight challenging text classification datasets show that the proposed method achieves competitive accuracy compared to the state-of-the-art approach with significantly fewer parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recurrent Neural Networks (RNNs, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>), including Long Short Term Memory networks (LSTMs, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>) and Gated Recurrent Units (GRUs, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>) have been increasingly applied to many problems in Natural Language Processing (NLP). Text classification is one of the most basic and important tasks in this field.</p><p>However, NLP models often require a massive number of parameters for word embeddings, resulting in a large storage or memory footprint. To reduce the number of parameters used in word embeddings without hurting the model performance, Shu et al. <ref type="bibr" target="#b21">[22]</ref> proposed to construct the embeddings with few basis vectors. For each word, the composition of basis vectors is determined by a hash code.</p><p>On the other hand, Hinton et al. <ref type="bibr" target="#b5">[6]</ref> presented capsule, which is a small group of neurons. The activities of neurons are used to represent the various properties of an entity. Sabour et al. <ref type="bibr" target="#b19">[20]</ref> applied this concept to neural network firstly, a novel routing algorithm called dynamic routing was adopted to select active capsules. The experiments of Capsule Networks (Cap- * * Corresponding author: Tel.: 86-21-51355528; fax: 86-21-51355558; e-mail: hren17@fudan.edu.cn (Hao Ren), honglu@fudan.edu.cn (Hong Lu) sNets) showed capsules could learn a more robust representation than Convolutional Neural Networks (CNNs) in image classification task.</p><p>In this paper, we aim to reduce the number of parameters used in word embeddings, while maintaining a competitive accuracy compared to the state-of-the-art approach. To do so, we propose a Compositional Weighted Coding (CWC) embedding to reduce the number of parameters. Then we apply CapsNet to the classification of texts, and propose a novel and robust routing algorithm named k-means routing to determine the connection strength between lower-level and upper-level capsules.</p><p>Our CWC embedding significantly differs from the method proposed by Shu et al. <ref type="bibr" target="#b21">[22]</ref>. The work by Shu et al. <ref type="bibr" target="#b21">[22]</ref> selects exclusive codeword vector in each codebook, while our method uses all the codeword vectors in each codebook and then weights them to form the word embedding. To distinguish with the Compositional Coding Embedding (CC Embedding) method proposed by Shu et al. <ref type="bibr" target="#b21">[22]</ref>, we call our compositional coding method as Compositional Weighted Coding Embedding (CWC Embedding). And our k-means routing uses cosine similarity to obtain the coupling coefficient between lower-level and upper-level capsules, while the dynamic routing proposed by Sabour et al. <ref type="bibr" target="#b19">[20]</ref> uses dot product value to determine the coupling coefficient. Furthermore, the coefficient update strategies are also different.</p><formula xml:id="formula_0">(a) ( ! ) (b) + ? ( ! ) ? ? Fig. 1.</formula><p>Comparison of embedding computations between the conventional approach (a) and compositional coding approach (b) for constructing embedding vectors.</p><p>The main contributions of this work are three-folds. First, we propose a new compositional coding approach for constructing the word embeddings with significantly fewer parameters than conventional approach. Second, we propose a novel routing method named k-means routing to decide the connection degree between lower-level and upper-level capsules, it is more stable and robust than dynamic routing. Third, we construct an end-to-end CapsNet with Bidirectional Gated Recurrent Units (BiGRUs, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1]</ref>) for text classification and achieve comparable results to the state-of-the-art method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Compositional Coding Embedding</head><p>Word embeddings play an important role in NLP models. Neural word embeddings encapsulate the linguistic information of words in continuous vectors. However, as each word is assigned an independent embedding vector, the number of parameters in the embedding layer can be huge. For example, when each embedding has 500 dimensions, the network has to hold 100M embedding parameters to represent 200K words.</p><p>Shu et al. <ref type="bibr" target="#b21">[22]</ref> hypothesized that learning independent embeddings causes more redundancy in the embedding vectors, as the inter-similarity among words is ignored. Some words are very similar regarding the semantics. For example, "dog" and"dogs" have almost the same meaning, except one is plural. To efficiently represent these two words, it is desirable to share information between the two embeddings.</p><p>Following the intuition of creating partially shared embeddings, instead of assigning each word a unique ID, Shu et al. <ref type="bibr" target="#b21">[22]</ref> represent each word w with a M dimensional code</p><formula xml:id="formula_1">C w = (C 1 w , C 2 w , . . . , C M w )</formula><p>. Each component C i w is an integer number in <ref type="bibr">[1, K]</ref>. Ideally, similar words should have similar codes. For example, we may desire C dog = (3, 2, 4, 1) and C dogs = (3, 2, 4, 2). Once we have obtained such compact codes for all words in the vocabulary, we use embedding vectors to represent the codes rather than the unique words. More specifically, we create M codebooks E 1 , E 2 , . . . , E M , each containing K codeword vectors. The embedding of a word is computed by summing up the codewords corresponding to all the components in the code as</p><formula xml:id="formula_2">E(C w ) = M i=1 E i (C i w )<label>(1)</label></formula><p>where E i (C i w ) is the C i w -th codeword in the codebook E i . In this way, the number of vectors in the embedding layer will be M ? K, which is usually much smaller than the vocabulary size (|V|). <ref type="figure" target="#fig_1">Fig. 1</ref> gives an intuitive comparison between the compositional approach and the conventional approach (assigning unique IDs).</p><p>Shu et al. <ref type="bibr" target="#b21">[22]</ref> used this method to compress NLP models, and proposed to directly learn the discrete codes in an end-toend neural network by applying the Gumbel-softmax <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11]</ref> trick. Experiments showed the compression rate achieves 98% in a sentiment analysis task and 94% ? 99% in machine translation tasks without performance loss.</p><p>Inspired by this, we introduce the compositional coding mechanism in our model to generate the word embeddings. And to make sure the model could be trained end-to-end without applying the Gumbel-softmax trick, we propose a variant of Compositional Coding Embedding (CC Embedding) method called Compositional Weighted Coding Embedding (CWC Embedding). The CWC Embedding method requires fewer parameters than conventional approach, and has no restrictions on code. It is more flexible than CC Embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Capsule Network</head><p>The concept of capsules is invented by Hinton et al. <ref type="bibr" target="#b5">[6]</ref> and used recently <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7]</ref>. CapsNet is designed for image feature extraction, it is developed based on CNN. However, unlike traditional CNN, in which the presence of feature is represented with scalar value in feature maps, the features in CapsNet are represented with capsules (aka vectors). In the work of Sabour et al. <ref type="bibr" target="#b19">[20]</ref>, the direction of capsules reflects the properties of the features and the length (L 2 norm) of capsules reflects the probability of the presence of different features. The transmission of information between layers follows dynamic routing mechanism. The specific procedure of dynamic routing can be found in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Yang et al. <ref type="bibr" target="#b26">[27]</ref> explored capsule networks with dynamic routing for text classification, and proposed three strategies to stabilize the dynamic routing process. Experiments on six small text classification benchmarks showed the effectiveness of capsule networks for text classification. However, the work only conducted experiments on small datasets, and compared with basic CNN-based or LSTM-based methods, without considering the number of parameters. To make up for the deficiency, we conducted experiments on eight challenging text classification benchmarks with our model, and combined with the proposed CWC embedding to save parameters.</p><p>Inspired by CapsNet, the capsule mechanism is adopted in our model to generate class capsules on the basis of feature capsules. The feature capsules are extracted from BiGRUs. A variant of dynamic routing called k-means routing is proposed to update weights between capsules from one layer to the next layer so that the properties captured by feature capsules can be propagated to suitable class capsules. Thus, each text is modeled as multiple feature capsules, and then modeled as multiple class capsules. Different feature capsules reflect the properties of the feature from different aspects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>In this section, we describe the compositional weighted coding embedding approach, k-means routing algorithm and the end-to-end model in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Compositional Weighted Coding Embedding</head><p>Unlike the CC Embedding method, which selects exclusive codeword vector in each codebook, CWC Embedding method uses all codeword vectors in each codebook to form the word embedding. And CC Embedding method restricts that the code must be integer number, our CWC Embedding method eliminates this limitation.</p><p>Suppose the vocabulary size is |V|, we create M codebooks E 1 , E 2 , . . . , E M , each containing K codeword vectors. For CWC Embedding, the embedding of word w is computed by summing up the weighted codewords corresponding to all the components in the code as</p><formula xml:id="formula_3">E(C w ) = M i=1 K j=1 so f tmax j (C i j w )E i ( j)<label>(2)</label></formula><p>where E i ( j) is the j-th codeword in the codebook E i , C i j w is the j-th code for the codebook E i . From the Formula (2), we can see the code does not need to be integer number. <ref type="figure" target="#fig_0">Fig. 2</ref> gives an intuitive comparison between the CC Embedding and the CWC Embedding.</p><p>Moreover, M and K are hyper-parameters, they are designated by user in the CC Embedding. However, in the CWC Embedding, only M need to be designated, K is determined as follows</p><formula xml:id="formula_4">K = M |V|<label>(3)</label></formula><p>because K M is the total number of all the combination of codeword vectors, it makes sure K M ? |V|, which means each word can be assigned with a unique combination of codeword vectors.</p><p>From the process of word embedding computation, we can see our CWC Embedding method requires fewer parameters than conventional method, just as the CC Embedding method. The more detailed results about parameters are presented in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">K-means Routing</head><p>The capsule layer receives lower-level capsules, which represent low-level features, then the routing algorithm clusters the low-level features to high-level features. We know that k-means clustering is an efficient method to cluster features, and produce a cluster centroid by using all the clustered features. Based on this, we propose k-means routing. We regard the k-means routing algorithm between l th layer's capsules and (l + 1) th layer's capsules as a k-means clustering process. The (l + 1) th layer's capsules are regarded as the cluster centers of the l th layer's capsules.</p><p>Then we briefly review k-means clustering and its optimization procedure. Given n capsules u 1 , . . . , u n and the metric d, k-means clustering is to find k cluster centers v 1 , . . . , v k to minimize the following loss function:</p><formula xml:id="formula_5">L = n i=1 k min j=1 d(u i , v j )<label>(4)</label></formula><p>so the objective function is</p><formula xml:id="formula_6">v 1 , . . . , v k = arg min v 1 ,...,v k L = arg min v 1 ,...,v k n i=1 k min j=1 d(u i , v j )<label>(5)</label></formula><p>Because the objective function contains min operation, it is not smooth and does not always have a gradient. Then we use the approximation <ref type="bibr" target="#b2">[3]</ref> max(? 1 , . . . , ? n ) = lim</p><formula xml:id="formula_7">K?+? 1 K ln ? ? ? ? ? ? ? n i=1 e ? i K ? ? ? ? ? ? ? ? 1 K ln ? ? ? ? ? ? ? n i=1 e ? i K ? ? ? ? ? ? ?<label>(6)</label></formula><p>and min(? 1 , . . . , ? n ) = ? max(?? 1 , . . . , ?? n )</p><p>to smooth the loss function</p><formula xml:id="formula_9">L ? ? 1 K n i=1 ln ? ? ? ? ? ? ? ? k j=1 e ?K?d(u i ,v j ) ? ? ? ? ? ? ? ? = ? 1 K n i=1 ln Z i (8)</formula><p>It is a differentiable function, so we can get its gradient</p><formula xml:id="formula_10">?L ?v j ? n i=1 e ?K?d(u i ,v j ) Z i ?d(u i , v j ) ?v j = n i=1 c i j ?d(u i , v j ) ?v j<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">c i j = so f tmax j ? K ? d(u i , v j )<label>(10)</label></formula><p>For obtaining v j , we need to solve the equations ?L/?v j = 0, which is non-linear mostly and can not be solved analytically. So we need introduce an iterative process, suppose v (r) j is the result of v j after r iterations, then we could let</p><formula xml:id="formula_12">n i=1 c (r) i j ?d u i , v (r+1) j ?v (r+1) j = 0<label>(11)</label></formula><p>for the routing between capsules, we use the following metric:</p><formula xml:id="formula_13">d(u i , v j ) = ? u i u i , v j v j<label>(12)</label></formula><p>here ?, ? is the scalar product operation. We can simply take</p><formula xml:id="formula_14">v (r+1) j = n i=1 c (r) i j u i (13) here c (r) i j = so f tmax j u i u i , v (r) j v (r) j , it means v (r+1) j</formula><p>is the sum of those nearest us to v (r) j . Finally, to achieve a complete routing algorithm, we need to solve these three problems: how to initialize the cluster centers, how to identify capsules at different position, how to guarantee the cluster centers keep the main information of input features. They all can be solved by inserting transformation matrix W i j :</p><formula xml:id="formula_15">v (r+1) j = n i=1 c (r) i j W i j u i<label>(14)</label></formula><p>here</p><formula xml:id="formula_16">c (r) i j = so f tmax j W i j u i W i j u i , v (r) j v (r) j .</formula><p>For the simplicity of this iterative process, we assign the sum of u i averagely to each cluster center as v (0) j . Because we want to use the length of capsule to represent the probability that a category's entity exists, a squash function has been introduced:</p><formula xml:id="formula_17">squash(v j ) = v j 1 + v j 2 v j<label>(15)</label></formula><p>The whole procedure is summarized on Algorithm 1. Inserting W i j is a beautiful trick, which induces different cluster centers by one same initialization method. In addition, W i j can keep the position information and increase or decrease dimension of capsule, which means the cluster centers have enough representation ability. Initialize</p><formula xml:id="formula_18">v j ? 1 k n i=1 W i j u i 3:</formula><p>for r iterations do</p><formula xml:id="formula_19">4: b i j ? W i j u i W i j u i , v j v j 5: c i j ? so f tmax j b i j 6: v j ? n i=1 c i j W i j u i 7:</formula><p>return squash(v j ) K-means routing is similar to dynamic routing in general, but it has three differences. First of all, we do not apply the squash function to capsule v j in the period of iteration, we just squash it after iteration. Secondly, b i j is replaced by new b i j , however, in dynamic routing, b i j is replaced by new b i j plus old b i j . This is the biggest difference between our routing algorithm and dynamic routing. Finally, the cosine similarity is computed between v j and W i j u i instead of dot product.</p><p>According to the b i j update step as described in dynamic routing, after r iterations: where</p><formula xml:id="formula_20">v j (r) ? squash ? ? ? ? ? ? ? i e r? i ?v j Z i? i ? ? ? ? ? ? ?<label>(16)</label></formula><formula xml:id="formula_21">Z i = j e? i ?v j ,? i = W i j u i<label>(17)</label></formula><p>if r ? +?, we find the result of so f tmax will be either 0 or 1. In other words, each lower capsule is linked to sole upper capsule. This is unreasonable, we know there are common characteristics among different categories, so we hope the common characteristics can be linked to all those categories. This is why we do not plus old b i j when we update b i j . Section 4.4 shows the comparison results between k-means routing and dynamic routing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Architecture</head><p>Our architecture consists of a CWC Embedding layer, a Bi-GRU layer and a capsule layer. The aim of CWC Embedding layer is to obtain the word embedding. BiGRU layer extracts lower features and feeds them into capsule layer, which takes advantage of them to form the upper features and correctly classify the texts.</p><p>We design a simple model to validate the effectiveness and test the performance of our approach. The structure is illustrated in <ref type="figure">Fig. 3</ref>. The number of codebook is 8, and the embedding dimension is 64. The hidden size of BiGRU is 128, and the number of recurrent layers is 2. We introduce Dropout <ref type="bibr" target="#b7">[8]</ref> on the outputs of each RNN layer except the last layer, with dropout probability equal to 0.5. The dimension of lower capsules is 8, and the dimension of upper capsules is 16. As the number of categories is related to specific dataset, so the number of upper capsules is specified with particular dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on the 8 benchmark text classification datasets. We first introduce the details of the 8 datasets, and present the experimental settings. Then we conduct experiments with our model and compare with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use publicly available datasets from Zhang et al. <ref type="bibr" target="#b25">[26]</ref> to evaluate our model. These datasets are AG's News (AG), DB-Pedia (DBP), Yahoo! Answers (Yah.A), Sogou News (Sogou), Yelp Review Polarity (Yelp.P), Yelp Review Full (Yelp.F), Amazon Review Polarity (Amz.P) and Amazon Review Full (Amz.F). <ref type="table">Table 3</ref> shows summary of these datasets' main features.</p><p>The datasets used in this paper not only contain English words, but also contain digital numbers, punctuations, Chinese words, etc. So we need to preprocess them to make sure the preprocessed samples can be tokenized by space character easily. To achieve this goal, we turn the sentences to be lower case firstly, then add one space character before and after the words or characters which are not belonging to English words. Additionally, we just take the first 5,000 words into our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Settings</head><p>We implement our model with PyTorch library <ref type="bibr" target="#b16">[17]</ref>, all the experiments are performed on a single NVIDIA Tesla V100 GPU. The number of routing iterations is fixed to 3, and the batch size is 32. We train the model with 10 epochs. A lot of loss functions have been experimented, finally we decided using a compositional loss function of margin loss <ref type="bibr" target="#b19">[20]</ref> and focal loss <ref type="bibr" target="#b13">[14]</ref> to compute our model's loss:</p><formula xml:id="formula_22">L = L m + L f<label>(18)</label></formula><p>where margin loss is</p><formula xml:id="formula_23">L m = 1 k k j=1 T j max(0, 0.9 ? v j ) 2 + 0.5(1 ? T j ) max(0, v j ? 0.1) 2<label>(19)</label></formula><p>and focal loss is</p><formula xml:id="formula_24">L f = ?0.25(1 ? v j ) 2 log v j<label>(20)</label></formula><p>where T j = 1 iff a text of class j is present, the hyper-parameters follow the original paper. It is optimized through ADAM scheme <ref type="bibr" target="#b12">[13]</ref> with learning rate 0.001. The ablation study of loss function can be found in Section 4.4. The code of our work is available on https://github.com/leftthomas/ CCCapsNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with SOTA</head><p>We compare the proposed method against existing state-ofthe-art methods on the eight text classification datasets. The quantitative results of different methods are reported in <ref type="table">Table 1</ref>. Please note our model is trained from scratch. From this table we can see that our method achieves competitive accuracy compared with Region Emb. <ref type="bibr" target="#b17">[18]</ref>, which is also trained from scratch. And compared with the methods which are pre-trained on other text datasets, and then fine-tuned on the target dataset, such as BERT-FiT <ref type="bibr" target="#b22">[23]</ref>, the performance gap of our method is not large.</p><p>We also provide the number of model parameters of theses methods in <ref type="table">Table 2</ref>. Combined with <ref type="table">Table 1</ref>, we can easily conclude that our method greatly reduces the overall parameters of the model without much loss of performance. For example, our model only uses 26.80M parameters, but ULMFiT [10] <ref type="table">Table 4</ref>. Ablation studies for the three loss functions: cross entropy loss (L c ), focal loss (L f ) and margin loss (L m ) on AG's News dataset. The best test accuracy (%) are bold. holds 51.65M parameters on DBPedia dataset, which is around two times as ours. And the performance gap between this two methods is only 0.48%. For XLNet <ref type="bibr" target="#b24">[25]</ref>, the number of parameters of our model is only 1.27% of its on AG's News dataset, and the compression ratio is almost 100 times. Looking at all datasets and comparing with other methods, it is not difficult to see that our method has made great compression in the amount of model parameters, but the performance impact is minimal.</p><formula xml:id="formula_25">L c L f L m L c + L f L c + L m L f + L m L c + L f + L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Selection of loss function. In order to make our method comparable to the state-of-the-art methods and determine which loss function is suitable for our method, we also explore three mainstream loss functions and their combinations, and conduct relevant comparative experiments on AG's News dataset. The experimental results are shown in <ref type="table">Table 4</ref>, the loss functions adopted are cross entropy loss, focal loss and margin loss.</p><p>From the results, we can see that focal loss and margin loss are better than cross entropy loss, and combining these two loss functions can achieve the best accuracy. We can also find that in some cases, the combination of cross entropy loss function will even bring negative effects, making the accuracy lower, e.g., L m vs L c + L m . Based on this experiment, we finally determine to use margin loss + focal loss as our model's loss function. Effectiveness of k-means routing. Section 3.2 shows that kmeans routing is more stable and robust than dynamic routing in theory, here we give experiments to verify this. The quantitative results are shown in <ref type="table" target="#tab_2">Table 5</ref>.</p><p>In order to prove the generality of the conclusion, we have conducted experiments on eight text datasets. From the results, we can see that our method has achieved the best accuracy on seven of them, except Yahoo! Answers dataset. From this, we can conclude that our routing algorithm is significantly better than the dynamic routing algorithm. This conclusion derived from the experiments further complements our theoretical analysis in Section 3.2. Effectiveness of CWC Embedding. To find out how much each component of our proposed method contributes to the overall performance, we conduct a series of experiments, and the relevant quantitative results are shown in <ref type="table">Table 6</ref>. At the same time, we also provide the corresponding model parameters, as shown in <ref type="table" target="#tab_3">Table 7</ref>. To make a fair comparison, we design a simple model as our baseline, which has a conventional embedding layer, a BiGRU layer and a fully connected layer. On this basis, we replace the conventional embedding layer and the fully connected layer to explore the role of each component. At the same time, we also give the results of various combinations to help us better understand the role of each component. Here, we focus on the role of CWC Embedding, the role of capsule will be explained in the next part.</p><p>Look at the first three rows of <ref type="table">Table 6</ref>, the first row is the result of baseline model, the second row is the result of the model which replaces the conventional embedding layer with CC Embedding, and the third row is the result of the model which replaces the conventional embedding layer with our CWC Embedding. From the results we can see the performance of our CWC Embedding is very close to the baseline model. And look at the first three rows of <ref type="table" target="#tab_3">Table 7</ref>, we can observe that our CWC Embedding requires fewer parameters than the baseline model on all the eight datasets. For example, the number of model parameters on AG's News dataset of our CWC Embedding is 2.45M, but the baseline model needs 4.45M parameters, which is around 1.8 times than ours. For the other datasets, our CWC Embedding can save about 24.68% ? 36.29% parameters.</p><p>As for CC Embedding, although it saves parameters like our CWC Embedding, its performance drops sharply (i.e., 74.13% ? 57.87% on Yahoo! Answers dataset). Then we can conclude that our CWC Embedding is more suitable compared with CC Embedding to replace the conventional embedding layer. This also verifies our proposition that our CWC Embedding can maintain competitive accuracy with significantly fewer parameters. Effectiveness of capsule layer. Look at the first row and fourth row of <ref type="table">Table 6</ref> and <ref type="table" target="#tab_3">Table 7</ref>, we can find that the model with capsule layer is slightly better than the model with fully connected layer, the number of parameters added is very small and can be ignored.</p><p>In the case of combination, specifically, the last two rows in <ref type="table">Table 6</ref>, introducing capsule has brought further improvement to the performance of CC Embedding (second row vs fifth row) and CWC Embedding (third row vs sixth row). Especially for CC Embedding on Yelp Review Full dataset, the performance improvement reaches 2.82% (56.66% ? 59.48%). These experimental results also show that introducing capsule layer to replace the fully connected layer is beneficial to the final performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed CWC Embedding, which can maintain competitive accuracy compared with conventional embedding layer with significantly fewer parameters. Then we developed a more robust and stable k-means routing algorithm than dynamic routing, analyzed the limitation of dynamic routing. Finally, introducing the capsule layer further improves the performance of our method. Extensive experiments on text classification task demonstrated the effectiveness of our method, which achieved competitive performance compared to the state-of-the-art methods with significantly fewer parameters. <ref type="table">Table 6</ref>. Ablation studies for the components of our proposal on eight text classification datasets. The model contains a conventional embedding layer, a BiGRU layer and a fully connected layer is used as our baseline. The best test accuracy (%) are bold. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Comparison of embedding computations between the CC Embedding (a) and CWC Embedding (b) for constructing embedding vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 K</head><label>1</label><figDesc>-means Routing 1: procedure Routing(u i , r) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>BiGRU BiGRU ...... ...... Capsule LayerFig. 3. Our model structure. We use L 2 norm of capsule to represent the probability that a category's entity exists.</figDesc><table><row><cell>!</cell><cell></cell><cell>$!</cell><cell></cell></row><row><cell></cell><cell>Compositional</cell><cell></cell><cell></cell></row><row><cell># ??</cell><cell>Layer Weighted Coding Embedding</cell><cell>$"</cell><cell>......</cell></row><row><cell>"</cell><cell></cell><cell>$#</cell><cell></cell></row></table><note>BiGRU ?? ......? #</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .Table 3 .</head><label>123</label><figDesc>Test accuracy (%) on eight text classification datasets. The methods with * indicate using fine-tuning, -means the results are not reported on their papers. The best results are bold. The number of model parameters (M) on eight text classification datasets. The minimum number of model parameters are bold. Statistics of the benchmark text classification datasets.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell>AG</cell><cell cols="6">DBP Yah.A Sogou Yelp.P Yelp.F Amz.P Amz.F</cell></row><row><cell cols="3">DPCNN* [12]</cell><cell cols="4">93.13 99.12 76.10 96.52</cell><cell>97.36</cell><cell>69.42</cell><cell>96.68</cell><cell>65.19</cell></row><row><cell cols="3">ULMFiT* [10]</cell><cell cols="2">94.99 99.20</cell><cell>-</cell><cell>-</cell><cell>97.84</cell><cell>70.02</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Region Emb. [18]</cell><cell cols="4">92.80 98.90 73.70 97.60</cell><cell>96.40</cell><cell>64.90</cell><cell>95.10</cell><cell>60.90</cell></row><row><cell cols="3">BERT-FiT* [23]</cell><cell cols="4">94.75 99.29 77.58 97.57</cell><cell>97.72</cell><cell>69.94</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">BERT-Large* [24]</cell><cell>-</cell><cell>99.36</cell><cell>-</cell><cell>-</cell><cell>98.11</cell><cell>70.68</cell><cell>97.37</cell><cell>65.83</cell></row><row><cell cols="3">XLNet* [25]</cell><cell cols="2">95.55 99.40</cell><cell>-</cell><cell>-</cell><cell>98.63</cell><cell>72.95</cell><cell>97.89</cell><cell>68.33</cell></row><row><cell cols="2">Ours</cell><cell></cell><cell cols="4">92.39 98.72 73.85 97.25</cell><cell>96.48</cell><cell>65.85</cell><cell>94.96</cell><cell>60.95</cell></row><row><cell cols="2">Method</cell><cell></cell><cell>AG</cell><cell>DBP</cell><cell cols="4">Yah.A Sogou Yelp.P Yelp.F</cell><cell>Amz.P</cell><cell>Amz.F</cell></row><row><cell cols="2">DPCNN [12]</cell><cell></cell><cell>4.24</cell><cell>35.34</cell><cell>49.64</cell><cell>7.05</cell><cell>13.09</cell><cell>14.13</cell><cell>59.84</cell><cell>53.73</cell></row><row><cell cols="2">ULMFiT [10]</cell><cell></cell><cell>20.56</cell><cell>51.65</cell><cell>65.96</cell><cell>23.37</cell><cell>29.41</cell><cell>30.45</cell><cell>76.16</cell><cell>70.05</cell></row><row><cell cols="9">Region Emb. [18] 43.81 233.33 370.61 101.78 118.07 127.26</cell><cell>403.85</cell><cell>364.86</cell></row><row><cell cols="3">BERT-FiT [23]</cell><cell cols="6">78.59 451.69 623.32 112.27 184.77 197.21</cell><cell>745.78</cell><cell>672.47</cell></row><row><cell cols="10">BERT-Large [24] 169.65 667.11 895.95 214.55 311.22 327.80 1,059.23 961.49</cell></row><row><cell cols="2">XLNet [25]</cell><cell cols="8">193.26 690.72 919.57 238.16 334.83 351.42 1,082.84 985.10</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>2.46</cell><cell>26.80</cell><cell>37.52</cell><cell>4.71</cell><cell>8.48</cell><cell>9.14</cell><cell>45.15</cell><cell>40.58</cell></row><row><cell cols="4">Dataset # Class # Train # Test</cell><cell>|V|</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AG</cell><cell>4</cell><cell>120k</cell><cell>7.6k</cell><cell>62,535</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DBP</cell><cell>14</cell><cell>560k</cell><cell cols="2">70k 548,338</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yah.A</cell><cell cols="2">10 1,400k</cell><cell cols="2">60k 771,820</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sogou</cell><cell>5</cell><cell>450k</cell><cell cols="2">60k 106,385</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yelp.P</cell><cell>2</cell><cell>560k</cell><cell cols="2">38k 200,790</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yelp.F</cell><cell>5</cell><cell>650k</cell><cell cols="2">50k 216,985</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Amz.P</cell><cell cols="2">2 3,600k</cell><cell cols="2">400k 931,271</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Amz.F</cell><cell cols="2">5 3,000k</cell><cell cols="2">650k 835,818</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies for routing algorithms on eight text classification datasets. The best test accuracy (%) are bold.</figDesc><table><row><cell>m</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Exp CC CWC Capsule AG DBP Yah.A Sogou Yelp.P Yelp.F Amz.P Amz.F 1 ---92.64 98.84 74.13 97.37 Ablation studies for the components of our proposal on eight text classification datasets. The model contains a conventional embedding layer, a BiGRU layer and a fully connected layer is used as our baseline. The minimum number of model parameters (M) are bold.</figDesc><table><row><cell>96.69</cell><cell>66.23</cell><cell>95.09</cell><cell>60.78</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Basic properties of the soft maximum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to forget: continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transforming autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The concrete distribution: a continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafi?t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new method of region embedding for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressing word embeddings via deep compositional code learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How to fine-tune BERT for text classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">XLNet: generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Investigating capsule networks with dynamic routing for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3110" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimal gated unit for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="226" to="234" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
