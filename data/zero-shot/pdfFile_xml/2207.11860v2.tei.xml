<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rei?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunyu</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxiang</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Panoramic Images</term>
					<term>Domain Adaptation</term>
					<term>Vision Transformers</term>
					<term>Scene Understanding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address panoramic semantic segmentation, which provides a full-view and dense-pixel understanding of surroundings in a holistic way. Panoramic segmentation is under-explored due to two critical challenges: (1) image distortions and object deformations on panoramas; (2) lack of annotations for training panoramic segmenters. To tackle these problems, we propose a Transformer for Panoramic Semantic Segmentation (Trans4PASS) architecture. First, to enhance distortion awareness, Trans4PASS, equipped with Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) modules, is capable of handling object deformations and image distortions whenever (before or after adaptation) and wherever (shallow or deep levels) by design. We further introduce the upgraded Trans4PASS+ model, featuring DMLPv2 with parallel token mixing to improve the flexibility and generalizability in modeling discriminative cues. Second, we propose a Mutual Prototypical Adaptation (MPA) strategy for unsupervised domain adaptation. Third, aside from Pinhole-to-Panoramic (PIN2PAN) adaptation, we create a new dataset (SynPASS) with 9,080 panoramic images to explore a Synthetic-to-Real (SYN2REAL) adaptation scheme in 360 ? imagery. Extensive experiments are conducted, which cover indoor and outdoor scenarios, and each of them is investigated with PIN2PAN and SYN2REAL regimens. Trans4PASS+ achieves state-of-the-art performances on four domain adaptive panoramic semantic segmentation benchmarks. Code is available at https://github.com/jamycheung/Trans4PASS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ANORAMIC semantic segmentation offers an omnidirectional and dense visual understanding regimen that integrates 360 ? perception of surrounding scenes and pixel-wise predictions of input images <ref type="bibr" target="#b0">[1]</ref>. The attracted attention of 360 ? cameras are manifesting, with an increasing number of learning systems and practical applications, such as holistic sensing in autonomous vehicles <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> and immersive viewing in augmented-and virtual reality (AR/VR) devices <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In contrast to images captured via pinhole cameras ( <ref type="figure" target="#fig_3">Fig. 3a</ref>-(1)(4)) with a narrow, limited Field of View (FoV), panoramic images with an ultra-wide FoV of 360 ? , deliver complete scene perception in outdoor driving environments ( <ref type="figure" target="#fig_2">Fig. 3a-(2)</ref>) and indoor scenarios ( <ref type="figure" target="#fig_3">Fig. 3a-(5)</ref>).</p><p>However, panoramic images often have large image distortions and object deformations due to the intrinsic equirectangular projection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This renders a vast number of Convolutional Neural Networks (CNNs) and learning methods a sub-optimal solution for panoramic segmentation <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, as they are mostly designed for pinhole images and cannot handle severe deformations. Besides, they fail in establishing long-range contextual dependencies in the ultra-wide 360 ? images which prove essential for accurate semantic segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. To solve this problem, we propose a novel distortion-aware model, i.e., Transformer for PAnoramic Semantic Segmentation (Trans4PASS). Specifically,</p><p>? J. <ref type="bibr">Zhang</ref>    compared with standard Patch Embedding (PE) in <ref type="figure" target="#fig_2">Fig. 2a</ref>, our newly designed Deformable Patch Embedding (DPE) in <ref type="figure" target="#fig_2">Fig. 2b</ref> helps to learn the prior knowledge of panorama characteristics during patchifying the image. In addition, the proposed Deformable MLP (DMLP) enables the model to better adapt to panoramas during feature parsing. We further introduce the up- graded Trans4PASS+, augmented by DMLPv2 with parallel token mixing mechanisms, which enhances the flexibility in modeling discriminative cues and reinforces the capacity in reasoning global contexts, thus, greatly improving the generalizability in wide-FoV scenarios. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, while the performance of Pyramid Vision Transformer (PVT) <ref type="bibr" target="#b12">[13]</ref> degrades as the FoV gradually increases, our transformer benefits from its distortion-aware design and shows inherent robustness, especially larger FoVs tend to widen the gap between Trans4PASS+ and PVT. Apart from the deformation of panoramic images, the scarcity of annotated data is another key difficulty that hinders the progress of panoramic semantic segmentation. Notoriously, it is extremely time-consuming and expensive to produce dense annotations in order to train segmentation transformers in a fully supervised manner <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and this difficulty is further exacerbated for panoramas with ultra-wide FoV and many small and distorted scene elements concurrently appearing in complex environments. Unsupervised Domain Adaptation (UDA) is a scheme that adapts a model from a source domain to a target domain. To address the lack of annotated data, we propose a Mutual Prototypical Adaptation (MPA) strategy for domain adaptive panoramic segmentation. Compared with other adversarial-learning <ref type="bibr" target="#b15">[16]</ref> and pseudo-label self-learning <ref type="bibr" target="#b16">[17]</ref> methods, the advantage of MPA is that mutual prototypes are generated from both source and target domains. In this manner, large-scale labeled data from the source domain and unlabeled data from the target domain are taken into account at the same time. The MPA strategy further unleashes the potential of our adapted model when combining other adaptation methods, enabling our unsupervised models to perform accurately, comparable or superior to previous fully-supervised models.</p><p>Based on the MPA strategy, we first revisit the Pinhole-to-Panoramic (PIN2PAN) paradigm as previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, by considering the label-rich pinhole images as the source domain and the label-scare panoramic images as the target domain. Furthermore, a new dataset (SynPASS) with 9,080 synthetic panoramic images is created by using the CARLA simulator <ref type="bibr" target="#b17">[18]</ref>. Our SynPASS dataset brings two benefits: <ref type="bibr" target="#b0">(1)</ref> The large-scale annotations enable training data-hungry models for panoramic semantic segmentation; <ref type="bibr" target="#b1">(2)</ref> A new Synthetic-to-Real (SYN2REAL) domain adaptive panoramic segmentation scenario is established. As shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>, to realize holistic scene understanding via panoramic semantic segmentation, we thoroughly investigate the two adaptation paradigms. The feature distributions of the sidewalk class from two source (S1, S2) and one target (T) domains are presented in <ref type="figure" target="#fig_3">Fig. 3b</ref>, and the floor class from indoor domains are in <ref type="figure" target="#fig_3">Fig. 3c</ref>. Upon close inspection, two insights become clear:</p><p>(1) The marginal distributions of the synthetic and real domains are close in one dimension (e.g., the shape), whereas the marginal distributions in another dimension (e.g., the appearance) are far apart. <ref type="bibr" target="#b1">(2)</ref> The patterns are reversed between the pinhole and panoramic domains. The insights are intuitive and consistent with common observations, as objects (e.g., sidewalks or floors) in synthetic-and real images are shape-deformed, while real pinholeand panoramic images are similar in appearance. We unfold a comprehensive discussion and results in Sec. 5.6.</p><p>An extensive set of experiments, covering indoor and outdoor scenarios, each investigated under PIN2PAN and SYN2REAL paradigms, demonstrates the superiority of the proposed distortion-aware architecture. Trans4PASS+ attains stateof-the-art performances on four domain adaptive panoramic semantic segmentation benchmarks. On the indoor Stanford2D3D dataset <ref type="bibr" target="#b18">[19]</ref>, our unsupervised model even outperforms the fullysupervised state-of-the-arts. On the indoor synthetic Structured3D dataset <ref type="bibr" target="#b19">[20]</ref>, our SYN2REAL-adapted model surpasses the model trained using extra 1,400 target data. On the outdoor DensePASS dataset <ref type="bibr" target="#b2">[3]</ref>, our Trans4PASS+ model obtains 50.23% in mIoU with a +11.21% gain over the baseline source-only model without adaptation, and our Pin2Pan-adapted model obtains 57.23% in mIoU with a +15.24% boost over the previous best method <ref type="bibr" target="#b20">[21]</ref>.</p><p>This work is extended and built upon our previous conference version <ref type="bibr" target="#b10">[11]</ref>  ? We conduct more comprehensive comparative experiments. Our proposed method outperforms recent state-of-the-art token mixing <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, deformable patchbased learning <ref type="bibr" target="#b25">[26]</ref>, transformer domain adaptation <ref type="bibr" target="#b26">[27]</ref>, and panoramic segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b20">[21]</ref> methods. ? On four panoramic datasets, our framework yields superior results, spanning indoor and outdoor scenarios, before and after PIN2PAN and SYN2REAL domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation</head><p>Dense image semantic segmentation has experienced a steep increase in attention and great progress since Fully Convolutional Networks (FCN) <ref type="bibr" target="#b27">[28]</ref> addressed it as an end-to-end per-pixel classification task. Following FCN, subsequent efforts enhance the segmentation performance by using encoder-decoder architectures <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, aggregating high-resolution representations <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, widening receptive fields <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> and collecting contextual priors <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Inspired by the non-local blocks <ref type="bibr" target="#b38">[39]</ref>, self-attention <ref type="bibr" target="#b39">[40]</ref> is leveraged to establish longrange dependencies <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> within FCNs. Then, contemporary architectures appear to substitute convolutional backbones by transformer ones <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Thus, image understanding can be viewed via a perspective of sequence-tosequence learning with dense prediction transformers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> and semantic segmentation transformers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. More recently, MLP-like architectures <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> that alternate spatial-and channel mixing have sparked enormous interest for tackling visual recognition tasks. However, most of these methods are designed for narrow-FoV pinhole images and often have large accuracy downgrades when applied in the 360 ? domain for holistic panorama-based perception. In this work, we address panoramic semantic segmentation, with a novel distortion-aware transformer architecture which considers a broad FoV already in its design and handles the panorama-specific semantic distribution via parallel MLP-based, channel-wise mixing, and pooling mixing mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Panoramic Segmentation</head><p>Capturing wide-FoV scenes, panoramic images <ref type="bibr" target="#b3">[4]</ref> act as a starting point for a more complete scene understanding. Mainstream outdoor omnidirectional semantic segmentation systems rely on fisheye cameras <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> or panoramic images <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Panoramic panoptic segmentation is also addressed in recent surrounding parsing systems <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, where the video segmentation pipeline with the Waymo open dataset <ref type="bibr" target="#b63">[64]</ref> has a coverage of 220 ? . Indoor methods, on the other hand, focus on either distortion-mitigated representations <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref> or multi-tasks schemes <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>. Yet, most of these works are developed based on the assumption that densely labeled images are implicitly or partially available in the target domain of panoramic images for training a segmentation model.</p><p>However, the acquisition of dense pixel-wise labels is extremely labor-intensive and time-consuming, in particular for panoramas with higher complexities and more small objects implicated in wide-FoV observations. We cut the requirement for labeled target data and circumvent the prohibitively expensive annotation process of determining pixel-level semantics in unstructured real-world surroundings. Different from previous works, we look into panoramic semantic segmentation via the lens of unsupervised transfer learning, and investigate both Syntheticto-Real (SYN2REAL) and Pinhole-to-Panoramic (PIN2PAN) adaptation strategies to profit from rich, readily available datasets like synthetic panoramic or annotated pinhole datasets. In experiments, our panoramic segmentation transformer architecture generalizes to both indoor and outdoor 360 ? scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic and Deformable Vision Transformers</head><p>With the prosperity of vision transformers in the field, some research works develop architectures with dynamic properties. In earlier works, the anchor-based DPT <ref type="bibr" target="#b25">[26]</ref> and the non-overlapping DAT <ref type="bibr" target="#b72">[73]</ref> use deformable designs only in later stages of the encoder and borrow Feature Pyramid Network (FPN) decoders from CNN counterparts. PS-ViT <ref type="bibr" target="#b73">[74]</ref> utilizes a progressive sampling module to locate discriminative regions, whereas Deformable DETR <ref type="bibr" target="#b74">[75]</ref> leverages deformable attention to enhance feature maps. Further, some methods aim to improve the efficiency of vision transformers by adaptively optimizing the number of informative tokens <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref> or dynamically modeling relevant dependencies via query grouping <ref type="bibr" target="#b79">[80]</ref>. Unlike these previous work limited to narrow-FoV images, our distortion-aware segmentation transformer is designed for pixel-dense prediction tasks on wide-FoV images, and can better adapt to panoramas by learning to counteract severe deformations in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Unsupervised Domain Adaptation</head><p>Domain adaptation has been thoroughly studied to improve model generalization to unseen domains, e.g., adapting to the real world from synthetic data collections <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>. Two predominant categories of unsupervised domain adaptation fall either in self-training <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b83">[84]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref> or adversarial learning <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>. Self-training methods usually generate pseudo-labels to gradually adapt through iterative improvement <ref type="bibr" target="#b91">[92]</ref>, whereas adversarial solutions build on the idea of GANs <ref type="bibr" target="#b92">[93]</ref> to conduct image translation <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b93">[94]</ref>, or enforce alignment in layout matching <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref> and feature agreement <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Further adaptation flavors, consider uncertainty reduction <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>, model ensembling <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, category-level alignment <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>, adversarial entropy minimization <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b103">[104]</ref>, and vision transformers <ref type="bibr" target="#b26">[27]</ref>.</p><p>Relevant to our task, PIT <ref type="bibr" target="#b104">[105]</ref> handles the gap of camera intrinsic parameters with FoV-varying adaptation, whereas P2PDA <ref type="bibr" target="#b2">[3]</ref> first tackles PIN2PAN transfer by learning attention correspondences. Aside from distortion-adaptive architecture design, we revisit panoramic semantic segmentation from a prototype adaptation-based perspective where panoramic knowledge is distilled via class-wise prototypes. Differing from recent methods utilizing individual prototypes for source-and target domain <ref type="bibr" target="#b105">[106]</ref>, <ref type="bibr" target="#b106">[107]</ref>, we present mutual prototypical adaptation, which jointly exploits source and target feature embeddings to boost transfer beyond the FoV. Moreover, we study both PIN2PAN and SYN2REAL for learning robust panoramic semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Here, we detail the proposed panoramic semantic segmentation framework in the following structure: the Trans4PASS and  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trans4PASS Architecture</head><p>As newly emerged learning architectures, transformer models are evolving and have attained outstanding performance in vision tasks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b44">[45]</ref>. In this work, we put forward a novel distortionaware Trans4PASS architecture in order to explore the panoramic semantic segmentation task. Considering the trade-off between efficiency and accuracy, there are two different model sizes: the tiny (T) model and the small (S) model. Following traditional CNN/Transformer models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b107">[108]</ref>, both versions of Trans4PASS keep the multi-scale pyramid feature structure in the form of four stages. The layer numbers of four stages in the tiny model are {2, 2, 2, 2}, while in the small model they are {3, 4, 6, 3}. In one segmentation process, given an input image in shape of H?W ?3, the Trans4PASS model first performs image patchifying. The encoder gradually down-samples feature maps f l ?{f 1 ,f 2 ,f 3 ,f 4 } in the l th stage with strides s l ?{4, 8, 16, 32} and channel dimensions C l ?{64,128,320,512}. Then, the decoder parses multi-scale feature maps f l into a unified shape of H 4 ? W 4 ?C emb , where the number of resulting embedding channels is set as C emb =128. Finally, a prediction layer outputs the final semantic segmentation result according to the number of semantic classes of the respective task, and with the same size as the input image.</p><p>However, the raw 360 ? data is generally formulated in the spherical coordinate system (the latitude ??[0, 2?) and longitude ??[? 1 2 ?, 1 2 ?]). To convert it to the Cartesian coordinate system (the x-and y-axes), the equirectangular projection in Eq. 1 is commonly used to transfer 360 ? data as a 2D flat panorama.</p><formula xml:id="formula_0">x = (? ? ? 0 ) cos ? 1 , y = (? ? ? 1 ),<label>(1)</label></formula><p>where (? 0 , ? 1 )=(0, 0) is the central latitude and central longitude.</p><p>Considering the simple equirectangular projection from Eq. 1 as x=? and y=?, the Area Distortion (AD) is approximated by the Jacobian determinant <ref type="bibr" target="#b108">[109]</ref> in Eq. 2 and Eq. 3.</p><formula xml:id="formula_1">J (?, ?) = ?(x) ?(?) ?(x) ?(?) ?(y) ?(?) ?(y) ?(?) .</formula><p>(2)</p><p>AD(x, y) = cos(?)|d?d?| |dxdy| = cos(?) J (?, ?) .</p><p>The AD is associated with cos(?). Thus, the areas (?? =0) located in any panoramic image all include object distortions and deformations. The above observations motivate us to design a distortionaware vision transformer model for panoramic scene parsing.</p><p>Compared with previous state-of-the-art segmentation transformers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b51">[52]</ref> shown in <ref type="figure" target="#fig_5">Fig. 4a</ref> and <ref type="figure" target="#fig_5">Fig. 4b</ref>, our Trans4PASS model ( <ref type="figure" target="#fig_5">Fig. 4c</ref>) is able to address the severe distortions in panoramas via two vital designs: (1) a Deformable Patch Embedding (DPE) module is proposed and applied in the encoder and decoder, enabling the model to extract and parse the feature hierarchy uniformly; (2) a Deformable MLP (DMLP) module is proposed to better collaborate with DPE in the decoder, by adaptively mixing and interpreting the feature token extracted via DPE. Furthermore, a new DMLPv2 module is constructed with a parallel token mixing mechanism. Based on DMLPv2, our architecture is upgraded to Trans4PASS+, being more lightweight yet more effective for panoramic semantic segmentation. The DPE and DMLPs are detailed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deformable Patch Embedding</head><p>Preliminaries on Patch Embedding. Given a 2D C in -channel input image or feature map f ?R H?W ?Cin , the standard Patch Embedding (PE) module reshapes it into a sequence of flattened patches z?R ( HW s 2 )?(s 2 ?Cin) , where (H, W ) is the resolution of the input, (s, s) is the resolution of each patch, and HW s 2 is the number of patches (i.e. the length of the patch sequence). Each element in this sequence is passed through a trainable linear projection layer transforming it into C out dimensional embeddings. The number of input channels C in is equal to the one of output channels C out in a typical patchifying process.</p><p>Consider one patch in z representing a rectangle area s?s with s 2 positions. The position offset relative to the patch-center (s/2, s/2) at a location (i,j)|i, j? <ref type="bibr">[1,s]</ref> in the patch is defined as ? (i,j) ?N 2 . In standard PE, these offsets of a single patch grid are fixed and fall into:</p><formula xml:id="formula_3">? f ixed (i,j) ?[?? s 2 ?, ?+ s 2 ?] 2 .<label>(4)</label></formula><p>Take e.g. a 3?3 patch, offsets ? f ixed (i,j) relative to the patch center (1, 1) will lie in [?1, 1]?[?1, 1]. They are fixed as:</p><formula xml:id="formula_4">? f ixed = {(?1, ?1),(?1, 0), ...,(1, 0),(1, 1)}.<label>(5)</label></formula><p>However, the aforementioned equirectangular projection process leads to severe shape distortions in the projected panoramic image, as seen in <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure" target="#fig_3">Fig. 3</ref>. A standard PE module with fixed patchifying positions makes the Transformer model neglect these shape distortions of objects and the panoramas. Inspired by deformable convolution <ref type="bibr" target="#b109">[110]</ref> and overlapping PE <ref type="bibr" target="#b51">[52]</ref>, we propose Deformable Patch Embeddings (DPE) to perform the patchifying process respectively for the input image in the encoder and the feature maps in the decoder. The DPE module enables the model to learn a data-dependent and distortion-aware offset ? DP E ?N H?W ?2 , thus, the spatial connections of objects presenting in distorted patches can be featured by the model. DPE is learnable and able to predict the adaptive offsets regarding the original input f . Compared to the fixed offset in Eq. (4), the learned offset ? DP E (i,j) is calculated as depicted in Eq. <ref type="formula" target="#formula_5">(6)</ref>.</p><formula xml:id="formula_5">? DP E (i,j) = min(max(? H r , g(f ) (i,j) ), H r ) min(max(? W r , g(f ) (i,j) ), W r ) ,<label>(6)</label></formula><p>where g(?) is the offset prediction function, which we implement via the deformable convolution operation <ref type="bibr" target="#b109">[110]</ref>. The hyperparameter r in Eq. (6) puts a constraint onto the leaned offsets and is better set as 4 based on our experiments. The learned offsets make DPE adaptive and as a result distortion-aware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deformable MLP</head><p>Token mixers play a major role in the competitive modeling ability of attention-based Transformer models. The recent MLPbased models <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b54">[55]</ref> heuristically relax attention-based feature constraints by spatially mixing tokens via MLP projections. Inspired by the success of MLP-based mixers, we design a Deformable MLP (DMLP) token mixer to conduct the adaptive feature parsing for panoramic semantic segmentation. Vanilla-MLP <ref type="bibr" target="#b54">[55]</ref> based modules lack adaptivity which weakens the token mixing of panoramic data. In contrast, linked with the aforementioned DPE module, our DMLP-based decoder performs adaptive token mixing during the overall feature parsing, being aware of the deformation-properties in 360 ? images. DMLPv1 token mixer. Concerning the comparison between MLP-based modules depicted in <ref type="figure" target="#fig_6">Fig. 5</ref>, the vanilla MLP ( <ref type="figure" target="#fig_6">Fig. 5a</ref>) lacks the spatial context modeling, CycleMLP <ref type="figure" target="#fig_6">( Fig. 5b</ref>) has the narrow projected receptive field due to fixed offsets, and our DMLP module ( <ref type="figure" target="#fig_6">Fig. 5c</ref>) generates learned adaptive spatial offsets during mixing tokens and leads to a wider projected grid (i.e., the green panel). Specifically, given a DPE-processed C in -dimensional feature map f ?R H?W ?Cin , the spatial offset ? DM LP (i,j,c) is first predicted channel-wise by using Eq. (6). Then, the offset is flattened as a sequence in shape of ? DM LP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(k,c)</head><p>, where k?HW and c?C in . While the given feature map is projected into a sequence z with the equal shape, the offsets are used to select tokens during mixing the flattened token/patch features z?R HW ?Cin . The mixed token is calculated as: where w?R Cin?Cout is the weight matrix of a fully-connected layer. As shown in <ref type="figure" target="#fig_7">Fig 6d,</ref> the DMLPv1 token mixer has a residual structure, consisting of DPE, two DMLP, and one MLP modules. Formally, the entire four-stage decoder is constructed by DMLPv1 token mixers and is denoted as:</p><formula xml:id="formula_6">z (k,c) = HW k=1 Cin c=1 w T (k,c) ? z (k+? DM LP (k,c) ,c) ,<label>(7)</label></formula><formula xml:id="formula_7">z l := DPE(C l , C emb )(z l ), ? l ?{1,2,3,4} z l := DMLP(C emb , C emb )(? l ) +? l , ? l z l := MLP(C emb , C emb )(? l ) +? l , ? l z l := Up(H/4, W/4)(? l ), ? l p := LN(C emb , C K )( l=1? l ),<label>(8)</label></formula><p>where Up(?) and LN(?) refer to the Upsample-and LayerNorm operations, and p is the prediction of K classes. DMLPv2 token mixer. Achieving the distortion-aware property and maintaining manageable computational complexity, we put forward a simple yet effective DMLPv2 toking mixer structure, which is demonstrated in <ref type="figure" target="#fig_7">Fig. 6e</ref>. Compared to recent token mixers, such as PoolFormer <ref type="bibr" target="#b21">[22]</ref> ( <ref type="figure" target="#fig_7">Fig. 6c</ref>) and FAN <ref type="bibr" target="#b24">[25]</ref>  <ref type="figure" target="#fig_7">(Fig. 6b</ref>), the advanced DMLPv2 is upgraded to a novel parallel token mixing mechanism by using a Squeeze&amp;Excite (SE) <ref type="bibr" target="#b110">[111]</ref> based Channel Mixer (CX) and a non-parametric Pooling Mixer (PX). Such a parallel token mixing mechanism brings two vital perspectives:</p><p>(1) the CX considers space-consistent but channel-wise feature reweighting, enhancing the feature by spotlighting informative channels; (2) the PX and DMLP focus on spatial-wise sampling via fixed or adaptive offsets, yielding mixed tokens highlighted in relevant positions. Thus, it improves the flexibility in modeling discriminative information and thereby reinforces the generalization capacity against domain shifts. Furthermore, compared with the DMLPv1 structure, DMLPv2 reduces the model complexity by using only one DMLP, thus making the model more lightweight. Based on Eq. (8), the DMLPv2-based decoder is upgraded to:</p><formula xml:id="formula_8">z l := DPE(C l , C emb )(z l ), ? l ?{1,2,3,4} z l := PX(C emb , C emb )(? l ) + CX(? l ), ? l z l := DMLP(C emb , C emb )(? l ) + CX(? l ), ? l z l := Up(H/4, W/4)(? l ), ? l p := LN(C emb , C K )( l=1? l ),<label>(9)</label></formula><p>where PX(?) and CX(?) denote the average pooling operator and the channel-wise attention operator. In general, the DMLP-based decoder delivers a spatial-and channel-wise token mixing in an efficient manner, but with a larger receptive field, which improves the expressivity of features in the panoramic imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mutual Prototypical Adaptation</head><p>To unfold the potential of panoramic segmentation models, a largescale dataset is crucial for success. However, labeling panoramic images are extremely time-consuming and expensive, due to the ultra-wide FoV and small elements of panoramas. Thus, we look into Domain Adaptation (DA) to exploit the sub-optimal but label-rich resources for training panoramic models, i.e., exploring Pinhole-to-Panoramic (PIN2PAN) adaptation and the Synthetic-to-Real (SYN2REAL) adaptation in this work.</p><p>Preliminaries on domain adaptation. Given the source (i.e., the pinhole or the synthetic) dataset with a set of labeled images D s ={(x s , y s )|x s ?R H?W ?3 , y s ?{0,1} H?W ?K } and the target (i.e., the panoramic) dataset D t ={(x t )|x t ?R H?W ?3 } without annotations, the objective of DA is to adapt models from the source to the target domain with K shared classes. The model is trained in the source domain D s via the segmentation loss:</p><formula xml:id="formula_9">L s SEG = ? H,W,K i,j,k=1 y s (i,j,k) log(p s (i,j,k) ),<label>(10)</label></formula><p>where p s (i,j,k) is the probability of the source pixel x s (i,j) predicted as the k-th class. To transfer models to the target data, the target pseudo label? t (i,j,k) of pixels x t (i,j) in Eq. <ref type="formula" target="#formula_0">(11)</ref> is calculated based on the most probable class given by the source pre-trained model.</p><formula xml:id="formula_10">y t (i,j,k) = 1 k . =arg max p t (i,j,:)</formula><p>.</p><p>(11)</p><p>The Self-Supervised Learning (SSL) in Eq. <ref type="formula" target="#formula_0">(12)</ref> is used to optimize the model based on the target pseudo labels? t (i,j,k) .</p><formula xml:id="formula_11">L t SSL = ? H,W,K i,j,k=1? t (i,j,k) log(p t (i,j,k) ).<label>(12)</label></formula><p>Proposed Mutual Prototypical Adaptation. As shown in <ref type="figure" target="#fig_8">Fig. 7</ref>, a novel Mutual Prototypical Adaptation (MPA) method is proposed and applied to distill mutual knowledge via the dual-domain prototypes. Using hard pseudo-labels in the output space results in a limited adaptation of SSL methods. To address this, our prototype-based method has two characteristics: (1) it softens the hard pseudo-labels by using them in feature space instead of as direct targets; (2) it performs complementary alignment of semantic similarities in feature space. Thus, it makes the self-supervised learning effect by using prototypes more robust. Further, the nontrivial design of prototype construction includes: (1) Prototypes are generated by using the source ground truth labels and the target pseudo labels, making full usage of labeled data and maintaining the similar properties between domains, such as appearance cues of PIN2PAN and shape priors of SYN2REAL; (2) Prototypes are constructed by using multi-scale feature embeddings, becoming more robust and more expressive; (3) Prototypes are stored in memory and updated along with the model optimization process, keeping the mechanism adaptable between iterations. Specifically, a set of n s source-and n t target feature maps is constructed as</p><formula xml:id="formula_12">F ={f s 1 , . . . ,f s ns } {f t 1 , . . . ,f t nt }, where f is fused from four-stage multi-scale features f = 4</formula><p>l=1 f l and is associated either with its respective source ground-truth label or a target pseudo-label. Each prototype P k is calculated by the mean of all feature vectors (pixel-embeddings) from F that share the class label k. We initialize the mutual prototype memory M={P 1 ,...,P K } by computing the class-wise mean embeddings through the whole dataset. During the training process, the prototype P k is updated at timestep t by P t+1</p><formula xml:id="formula_13">k ?mP t?1 k +(1?m)P t k</formula><p>with a momentum m= 0.999, where P t k is the mean feature vector among embeddings that share the class-label k in the current minibatch. Based on the dynamic memory, the prototypical feature map f is reconstructed by stacking the prototypes P k ?M according to the pixel-wise class distribution in either the source label or the pseudo-label. Inspired by the knowledge distillation loss <ref type="bibr" target="#b111">[112]</ref>, the MPA loss is applied to drive the feature alignment between the feature embedding f and the reconstructed feature mapf . The MPA loss only in the source domain is depicted in Eq. <ref type="formula" target="#formula_0">(13)</ref>:</p><formula xml:id="formula_14">L s M P A = ? ?T 2 KL(?(f s /T )||?(f s /T )) ? (1 ? ?)CE(y s ,?(f s )),<label>(13)</label></formula><p>where KL(?), CE(?), and ?(?) are Kullback-Leibler divergence, Cross-Entropy, and Softmax function, respectively. The temperature T and hyper-parameter ? are 20 and 0.9 in our experiments. Similarly, the target MPA loss is constructed as in Eq. <ref type="bibr" target="#b13">(14)</ref>.</p><formula xml:id="formula_15">L t M P A = ? ?T 2 KL(?(f t /T )||?(f t /T )) ? (1 ? ?)CE(? t ,?(f t )),<label>(14)</label></formula><p>where the pseudo label? t is generated by Eq. <ref type="bibr" target="#b10">(11)</ref>. The final loss is combined by Eq. <ref type="formula" target="#formula_0">(10) (12) (13) (14)</ref> with a weight of ?=0.001 as:</p><formula xml:id="formula_16">L=L s SEG +L t SSL +?(L s M P A +L t M P A ).<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYNPASS: PROPOSED SYNTHETIC DATASET</head><p>Recently, the continuous emergence of panoramic semantic segmentation datasets <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b112">[113]</ref> has facilitated the development of surrounding perception, and simulators have been used to generate multi-modal data <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>. However, there is currently not a readily available large-scale semantic segmentation dataset for outdoor synthetic panoramas, considering that the OmniScape dataset <ref type="bibr" target="#b115">[116]</ref> is still not released as of writing this paper. To explore the domain adaptation problem of SYN2REAL under urban street scenes, we create the SynPASS dataset using the CARLA simulator <ref type="bibr" target="#b17">[18]</ref>. Our virtual sensor suite consists of 6 pinhole cameras located at the same viewpoint to obtain a cubemap panorama image <ref type="bibr" target="#b67">[68]</ref>. The FoV of each pinhole camera was set to 91 ? ?91 ? to ensure the overlapping area between adjacent images. We then re-project the acquired cubemap panorama into a common equirectangular format using the cubemap-to-equirectangular projection algorithm. Given a equirectangular image grid (?, ?), we need to find the corresponding coordinates ( </p><formula xml:id="formula_17">x = W 2 ? tan(? ? i ? 2 ), y = ? H?tan? 2cos(??i ? 2 ) .<label>(16)</label></formula><p>For {I U , I D } indexed by j={0,1}, we have:</p><formula xml:id="formula_18">x = W 2 ? cot?sin?, y = H 2 ? cot?cos(? + j?).<label>(17)</label></formula><p>RGB images and semantic labels are captured simultaneously. In order to ensure the diversity of semantics, we benefit from 8 open-source city maps and set 100?120 initial collection points in every map. Our virtual collection vehicle drives according to the simulator traffic rules. We sample every 50 frames, and keep the first 10 key-frames of images at each initial collection point. To further improve the diversity of data, we modulate the collected weather and time conditions. The weather conditions consist of sunny (25%), cloudy (25%), foggy (25%), and rainy (25%) conditions. The time changes include daytime (85%) and nighttime (15%). In summary, SynPASS contains 9,080 panoramic RGB images and semantic labels with a resolution of 1,024?2,048. Some examples are shown in <ref type="figure">Fig. 8</ref>, and more statistic information are reported in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>The distributions of SynPASS, the panoramic DensePASS <ref type="bibr" target="#b2">[3]</ref>, and the pinhole Cityscapes <ref type="bibr" target="#b13">[14]</ref> datasets are depicted in <ref type="figure">Fig. 9</ref>. The class-wise pixel numbers are accumulated over all images in the respective datasets. Apart from the overlapping 13 classes, the SynPASS dataset has 22 classes in total, including 9 additional classes: other, roadline, ground, bridge, railtrack, groundrail, static, dynamic, and water. It provides more semantic categories to enrich the understanding of the surroundings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Settings</head><p>We experiment with six datasets, including two source domains and one target domain in respective indoor and outdoor scenes.  We address four domain adaptation settings:</p><p>(1) Indoor PIN2PAN: SPin SPan.</p><p>(2) Indoor SYN2REAL: S3D SPan. (2) Indoor SYN2REAL: S3D8 SPan8.</p><p>(3) Outdoor PIN2PAN: CS13 DP13.</p><p>(4) Outdoor SYN2REAL: SP13 DP13. Implementation settings. We train our models with 4 A100 GPUs with an initial learning rate of 5e ?5 , which is scheduled by the poly strategy with power 0.9 over 200 epochs. The optimizer is AdamW <ref type="bibr" target="#b116">[117]</ref> with epsilon 1e ?8 , weight decay 1e ?4 , and batch size is 4 on each GPU. The images are augmented by the random resize with ratio 0.5-2.0, random horizontal flipping, and random cropping to 512?512. For outdoor datasets, the resolution is 1,080?1,080 and batch size is 1. When adapting the models from PIN2PAN, the resolution of indoor pinhole and panoramic images are 1,080?1,080 and 1,024?512 for training. In SYN2REAL, the resolution of synthetic panoramic images are 1,024?512. The outdoor pinhole-and synthetic images are set to 1,024?512 and the panoramic images are with a resolution of 2,048?400. The image size of indoor and outdoor validation sets are 2,048?1024 and 2,048?400, respectively. Adaptation models are trained within 10K iterations on one GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SynPASS Benchmark</head><p>In order to study the performance of panoramic semantic segmentation of current existing approaches and our approach Trans4PASS+ on the proposed synthetic dataset, the SynPASS benchmark with the full 22 classes is established. As shown in <ref type="table" target="#tab_4">Table 2</ref>, we conduct experiments for panoramic semantic segmentation on the SynPASS dataset using either CNN-based approaches (e.g., Fast-SCNN <ref type="bibr" target="#b117">[118]</ref>, DeepLabv3+ <ref type="bibr" target="#b29">[30]</ref>, and HRNet <ref type="bibr" target="#b31">[32]</ref>) or transformer-based approaches (e.g., PVT <ref type="bibr" target="#b12">[13]</ref>, SegFormer <ref type="bibr" target="#b51">[52]</ref>, and the proposed Trans4PASS and Trans4PASS+). All the investigations among transformer-based approaches are conducted considering the trade-off between efficiency and model size within a fair comparison. The models are trained on the overall training set and their performances are reported in different weather and day/night conditions. Compared with existing approaches, Trans4PASS+ (Tiny) surpasses HRNet with the best performance among all the listed CNN-based methods by +5.41% in mIoU on the validation set. The largest improvement lies in the rainy condition with a +7.33% gain. Trans4PASS and Trans4PASS+ consistently outperform PVT and SegFormer in all conditions, which showcases that our models have strong capability to capture panoramic segmentation cues on the synthetic dataset even considering different weather and day/night scenarios.</p><p>Comparing the small versions of Trans4PASS+ and Trans4PASS, Trans4PASS+ performs more accurately in all conditions and clearly elevates the overall mIoU scores on both validation-and testing set. According to <ref type="table" target="#tab_2">Table 1</ref>, the samples are equally distributed among different scenarios and Trans4PASS+ also yields balanced segmentation performance across different kinds of weather and day/night conditions, which demonstrates the robustness of our model in different scenarios. The experimental results of all the investigated models illustrate that there is still remarkable improvement space on the newly established benchmark, since the best performance is 39.16% on the SynPASS test set, indicating that the proposed benchmark is challenging for panoramic semantic segmentation of synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PIN2PAN and SYN2REAL Gaps</head><p>PIN2PAN gaps. We first quantify the PIN2PAN domain gap in outdoor scenarios by assessing &gt;15 off-the-shelf convolutional-and vision-transformer-based semantic segmentation models learned from Cityscapes. <ref type="bibr" target="#b0">1</ref>  <ref type="table" target="#tab_6">Table 3</ref> presents the results evaluated on Cityscapes <ref type="bibr" target="#b13">[14]</ref> and DensePASS <ref type="bibr" target="#b2">[3]</ref> validation sets. It can be observed that traditional CNN-based methods such as PSPNet <ref type="bibr" target="#b33">[34]</ref> and DANet <ref type="bibr" target="#b11">[12]</ref> suffer huge performance downgrades and have a mIoU degradation of ?50% when directly transferred to work on panoramic data. Previous state-of-the-art transformer architectures SETR <ref type="bibr" target="#b14">[15]</ref> and SegFormer <ref type="bibr" target="#b51">[52]</ref> can reduce the mIoU gap to ?40%, which still remains large. The proposed Trans4PASS architectures have high performances on pinhole image semantic segmentation, e.g., Trans4PASS (S) reaches 81.1% in mIoU on Cityscapes, but more importantly, it attains a clearly higher  performance of 44.8% on DensePASS than existing CNN-and transformer-based models, and the gap is also reduced to 36.3%. Trans4PASS+ further enhances the performance on the target panoramic dataset with a mIoU of 44.9% for Trans4PASS+ (S) and its gap decreases to 36.1%. These results reveal that both distortion-aware features and omni-range dependencies learned in both shallow-and high levels of vision transformers, as opposed to the context modeled only in higher-levels of CNNs, are critical for wide-FoV omnidirectional semantic segmentation. Then, we look into the PIN2PAN domain gap in indoor scenes, as analyzed in <ref type="table" target="#tab_7">Table 4</ref> based on the Stanford2D3D dataset <ref type="bibr" target="#b18">[19]</ref>. The pinhole-and panoramic images from Stan-ford2D3D are collected under the same setting, the PIN2PAN gap is smaller compared to the outdoor scenario. Still, in light of other convolutional-and attentional transformer-based architectures, the small Trans4PASS+ variant leads to top mIoU scores of 53.46% and 50.35% for pinhole-and panoramic image semantic segmentation, while its accuracy drop is also largely reduced compared to former state-of-the-art models like Trans4Trans <ref type="bibr" target="#b125">[126]</ref>. SYN2REAL gaps. To measure the SYN2REAL domain gap, for outdoor road-driving scenes, we leverage our SynPASS (SP13) and DensePASS (DP13) datasets by using their overlapping 13 classes for training and testing. Compared to the versatile dense prediction transformer <ref type="bibr" target="#b12">[13]</ref>, Trans4PASS consistently improves the performance in <ref type="table" target="#tab_21">Table.</ref> 5-2 , surpassing the corresponding PVT variant by ?4.4% on the target domain, and resulting more robust omni-segmentation as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Similarly, for the indoor situation, we experiment on Structured3D (S3D8) panoramic <ref type="bibr" target="#b19">[20]</ref> and Stanford2D3D panoramic (SPan8) sets by using their sharing 8 categories. The results are presented in <ref type="table" target="#tab_21">Table.</ref> 5-4 . The benefit of using Trans4PASS+ with DMLPv2 is pronounced, as it brings a mIoU gain of +8.56% compared to the tiny PVT baseline using an FPN decoder. We will further conduct in-depth architecture analysis on the novel designed Trans4PASS+ in Sec. 5.4. PIN2PAN vs. SYN2REAL. In <ref type="table" target="#tab_8">Table 5</ref>, we also study the domain gap comparison between PIN2PAN and SYN2REAL paradigms, to inspect the shift behind each imagery and to answer the key question: which adaptation scheme is more promising for panoramic semantic segmentation. Here, a short answer is provided. For the outdoor scenario, the model overall benefits more from real pinhole scenes than from synthetic panoramic scenes, when transferred to the target domain without any adaptation. The PIN2PANlearned small Trans4PASS+ ( 1 ) reaches 51.40% in mIoU, while the SYN2REAL-transferred variant ( 2 ) only achieves 43.17% falling behind by a clear margin. We conjecture that without any domain adaptation, the rich detailed texture cues available in the real pinhole outdoor dataset play an important role for attaining generalizable segmentation. We will further delve deep into this comparison quantitatively in Sec. 5.6 and qualitatively in Sec. 5.7.</p><p>For the indoor scenario, as shown in <ref type="table" target="#tab_8">Table 5</ref>-3 , the PIN2PAN model also achieves higher performance, as both pinhole-and panoramic images are captured from the Stanford2D3D dataset under the same setting. Further, Trans4PASS+ succeeds to reduce the mIoU gap to only 2.21%. In contrast, transferring from the synthetic S3D8 dataset to real SPan8 (Table 5-4 ) causes a mIoU gap of ?25%. Yet, we find that Trans4PASS+, with parallel token mixing, attains smaller SYN2REAL mIoU gaps than Trans4PASS. In Sec. 5.6, we will further assess how our proposed adaptation strategy mitigates the SYN2REAL performance drop.</p><p>Moreover, as shown in <ref type="table" target="#tab_9">Table 6</ref>, we note that our proposed Trans4PASS+ shows strong zero-shot generalization capacity, when only learning from synthetic images and testing on real panoramic data. It achieves 51.93% in mIoU, which outperforms PSPNet <ref type="bibr" target="#b33">[34]</ref> and UPerNet <ref type="bibr" target="#b126">[127]</ref> variants and is on par with the previous state-of-the-art HRNet [32] (52.00%) trained on both synthetic-and real datasets as suggested in <ref type="bibr" target="#b19">[20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Trans4PASS Structural Analysis</head><p>Effect of DPE. We first study the effectiveness of DPE by comparing it against DePatch from Deformable Patch-based Transformer <ref type="bibr" target="#b25">[26]</ref>. While the object-aware offsets and scales in DPT allow patches shifting around the object, our proposed DPE is flexible to split image patches and is decoupled from object proposals. As depicted in <ref type="table" target="#tab_10">Table 7</ref>-(1), compared with DPT, our DPE-driven Trans4PASS brings +3.01% and +9.39% gains in mIoU on Cityscapes and DensePASS, respectively. Effect of DMLP. To ablate the impacts of different MLP-like modules integrated in the decoder of Trans4PASS, we substitute DMLP (v1) by CycleMLP <ref type="bibr" target="#b22">[23]</ref> and ASMLP <ref type="bibr" target="#b23">[24]</ref> modules. As shown in <ref type="table" target="#tab_10">Table 7</ref>-(1), DMLP is more computationally-efficient with less GFLOPs and a lower number of parameters. Yet, it is more adaptive as opposed to the fixed offsets in CycleMLP, as depicted in <ref type="figure" target="#fig_6">Fig. 5</ref>. The results also confirm the benefit as DMLP outstrips both modules with a clear margin of 3?5% in mIoU. Effect of encoders and decoders. As shown in  to achieve a clearly improved performance (47.74%) as compared to DMLPv1-based Trans4PASS (45.89%), while being more computationally-efficient in GFLOPs and with nearly the same amount of parameters. Training with OHEM <ref type="bibr" target="#b127">[128]</ref> can further enlarge the accuracy gain of DMLPv2 to +11.21%.</p><p>Our introduced parallel token mixing mechanisms improve the flexibility in modeling discriminative information and thereby enhance the inherent generalizability when facing domain shift problems like Pinhole-to-Panoramic. We also compare DMLPv2 with other token mixing modules including the FAN block <ref type="bibr" target="#b24">[25]</ref> and a combination of FAN and average-pooling-based mixer from PoolFormer <ref type="bibr" target="#b21">[22]</ref>. They are combined in a similar parallel way but without adding any deformable designs. Still, both of them are less effective than our DMLPv2, which illustrates that DMLPv2 offers a sweet spot and an optimal path to follow for attaining robust and efficient panoramic segmentation against domain shift problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation of Unsupervised Domain Adaptation</head><p>Ablation on DensePASS. To confirm the generalization capacity of applying Trans4PASS in adaptation methods, FANet <ref type="bibr" target="#b120">[121]</ref> and DANet <ref type="bibr" target="#b11">[12]</ref> used in P2PDA <ref type="bibr" target="#b2">[3]</ref> are replaced by Trans4PASS and Trans4PASS+, as displayed in  (a) Per-class results on DensePASS. Comparison with state-of-the-art panoramic segmentation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, domain adaptation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b105">[106]</ref>, and multi-supervision methods <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>. * denotes performing Multi-Scale (MS) evaluation. ing mixing enhance global information reasoning and reinforce domain adaptation for better seamless panoramic segmentation.</p><p>Comparison with outdoor state-of-the-art methods. In <ref type="table" target="#tab_13">Table 8a</ref>, we contrast Trans4PASS+ against previous state-ofthe-art segmentation frameworks specifically developed for panoramic images, including Panoramic Annular Semantic Segmentation (PASS) <ref type="bibr" target="#b9">[10]</ref> and Efficient Concurrent Attention Network (ECANet) <ref type="bibr" target="#b8">[9]</ref>. PASS effectively improves the performance of ERFNet <ref type="bibr" target="#b119">[120]</ref> by partitioning the panorama into multiple pinhole-like segments (4 segments as suggested by <ref type="bibr" target="#b9">[10]</ref>) and fusing the high-level semantically-meaningful features for final complete segmentation. ECANet emphasizes horizontal omnirange contextual dependencies and strengthens the performance by using an omni-supervised learning regimen using heavy training sources via data distillation. However, both of them are suboptimal for robust omnidirectional surrounding parsing on the dense 19-class segmentation benchmark of DensePASS <ref type="bibr" target="#b2">[3]</ref>.</p><p>Then, we compare MPA-Trans4PASS+ against representative unsupervised domain adaptation pipelines including some built on adversarial learning such as CLAN <ref type="bibr" target="#b15">[16]</ref> and P2PDA <ref type="bibr" target="#b20">[21]</ref>, and self-training schemes like CRST <ref type="bibr" target="#b16">[17]</ref>, SIM <ref type="bibr" target="#b100">[101]</ref>, PCS <ref type="bibr" target="#b105">[106]</ref>, and DAFormer <ref type="bibr" target="#b26">[27]</ref>, both adapted from Cityscapes to DensePASS. Among these methods, P2PDA is the previous best solution for domain adaptive panoramic segmentation on DensePASS, whereas DAFormer serves as a recent well-known state-of-theart transformer-based domain adaptation method. Yet, MPA-Trans4PASS arrives at 56.38% and Trans4PASS+ scores 57.23% in mIoU. Trans4PASS+ outstrips P2PDA-SSL by an absolute value of 15.24%, and at the same time, it exceeds the prototypical approach PCS and the transformer-driven DAFormer.</p><p>We further broaden the comparison by adding multisupervision methods <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref> which require much more data. USSS <ref type="bibr" target="#b128">[129]</ref> relies on multi-source semi-supervised learning, while Seamless-Scene-Segmentation <ref type="bibr" target="#b129">[130]</ref> uses instance-specific labels for auxiliary supervision. ISSAFE <ref type="bibr" target="#b130">[131]</ref> merges training data from Cityscapes, KITTI-360 <ref type="bibr" target="#b112">[113]</ref>, and BDD <ref type="bibr" target="#b136">[137]</ref> for robustifying segmentation. The semantic outputs of these models are projected to the 19 classes in DensePASS to be comparable with others. However, these multi-supervision approaches are less effective for panoramic semantic segmentation. As seen in <ref type="table" target="#tab_13">Table 8a</ref>, our Trans4PASS models harvest top segmentation IoU scores on 12 out of all 19 categories. Evidently, large improvements (more than +3.0% over others) have been obtained on sidewalk, traffic sign, terrain, rider, truck, bus, and train, which are challenging yet critical semantics for autonomous driving. <ref type="table" target="#tab_13">Table 8c</ref> are performed according to the fold-1 data splitting <ref type="bibr" target="#b18">[19]</ref> on the Stanford2D3D panoramic (SPan) dataset. Our MPA-Trans4PASS (Tiny) exceeds the previous state-of-the-art P2PDAdriven DANet and it is even better than the one adapted with a PVT-Small backbone. Overall, Trans4PASS+ (Small) achieves the highest mIoU score (52.53%), even reaching the level of the fullysupervised Trans4PASS+ (53.62%) which does have full access to panoramic image annotations of 1,400 target samples. Comparison with indoor state-of-the-art methods. As shown in <ref type="table" target="#tab_13">Table 8d</ref>, our Trans4PASS+ (?14M parameters) gains a high mIoU score, outperforming existing fully-supervised and transferlearning methods which are based on ResNet-101 backbones (?44M parameters). For example, the versatile HoHoNet <ref type="bibr" target="#b7">[8]</ref> leverages a latent horizontal feature design and obtains 52.0%, whereas some methods <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b133">[134]</ref> use RGB-D input to exploit cross-modal complementary information. Still, our lighter TransPASS+ achieves 52.3% while being unsupervised, and our supervised counterpart can reach 54.0%. These results further verify the distortion adaptability of the proposed Trans4PASS+ architecture for panoramic semantic understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation on Stanford2D3D. The experiments organized in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">PIN2PAN and SYN2REAL Adaptation</head><p>Comparison in the outdoor scenario. In Sec. 5.3, we have briefly assessed the comparison between PIN2PAN and SYN2REAL performance. Here, we inspect this in greater detail by using the two 13-class benchmarks. In <ref type="table" target="#tab_15">Table 9a</ref>, it can be seen that before adaptation, PIN2PAN models generally perform better than their corresponding SYN2REAL ones. This is due to that the rich and detailed texture information available in the pinhole datasets, provide important cues for semantic segmentation. Yet, when stepping further to look into per-class accuracy, we find that SYN2REAL often achieves higher performance on sidewalk. Sidewalks can get stretched and appear at multiple positions across the 360 ? , which is uncommon in pinhole data, and thereby they are difficult for source-only PIN2PAN models. In SYN2REAL, the spatial distribution-and position priors available in the panoramic synthetic dataset, can help context-aware vision transformer models to better detect sidewalks. However, SYN2REAL models struggle on traffic light and traffic sign, whose segmentation may strongly rely on texture cues, while the simulated traffic elements do not present diverse textures for learning a robust segmentation transformer. After adaptation, our PIN2PAN model largely improves the accuracy on sidewalk, revealing that MPA-Trans4PASS+ successfully bridges the domain gap in positional priors. We also find that compared to PVT, Trans4PASS+ is particularly effective for classes pole, person, and car in the SYN2REAL setting, which is due to our DPE that can learn distortion-adaptive offsets for improving the segmentation of these traffic objects. Comparison in the indoor scenario. As shown in <ref type="table" target="#tab_15">Table 9b</ref>, PIN2PAN also generally yields better performances in both adaptation-free and MPA settings. The SYN2REAL indoor models come with unsatisfactory performance on the segmentation of sofa, whereas in the PIN2PAN paradigm, Trans4PASS+ achieves more than +10.0% compared to PVT on sofa, which can get severely deformed in panoramic data. Besides, MPA consistently helps to mitigate the SYN2REAL accuracy downgrades.</p><p>In summary, based on the comparative analyses in indoor and outdoor scenarios, a clear recommendation for attaining robust panoramic semantic segmentation is to adapt from realistic pinhole domains, but for categories with large positional prior differences across domains such as sidewalk, transferring from synthetic data does provide generalization benefits in the zero-shot sourceonly setting. Facing domain shift behind each imagery, MPA-Trans4PASS+ brings largely enhanced adaptation performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Qualitative Analysis</head><p>Panoramic semantic segmentation visualizations. In <ref type="figure" target="#fig_0">Fig. 11a</ref> and <ref type="figure" target="#fig_0">Fig. 11b</ref>, Trans4PASS and Trans4PASS+ models can obtain better panoramic segmentation results than the indoor <ref type="bibr" target="#b12">[13]</ref> and outdoor <ref type="bibr" target="#b51">[52]</ref> baseline models. In outdoor cases <ref type="figure" target="#fig_0">(Fig. 11a</ref>), the Trans4PASS+ model has more accurate classifications and boundary distinctions in e.g., trucks, sidewalks, and pedestrians. The baseline model has difficulty distinguishing the distorted objects, since they lack long-range contexts and distortion-aware features. In indoor cases <ref type="figure" target="#fig_0">(Fig. 11b)</ref>, the challenging objects, like doors and tables, are scarcely identified by the baseline model, but our Trans4PASS+ can segment both objects with precise masks. PIN2PAN vs. SYN2REAL. With the visualizations in <ref type="figure" target="#fig_0">Fig. 13</ref>, we discuss the two domain adaptation paradigms. Before leveraging the MPA approach, the source-trained PIN2PAN model ( 1 ) fails  <ref type="bibr" target="#b135">[136]</ref> has no deformable designs. Zoom in for better view. to fully detect the sidewalk, as the shapes and positional priors of sidewalks in pinhole imagery significantly differ from those in the panoramic domain. In contrast, the source-trained SYN2REAL model ( 3 ) handles the sidewalk parsing well. This is consistent with the feature embedding observation in <ref type="figure" target="#fig_3">Fig. 3</ref> tation maps corroborate with the numerical results in <ref type="table" target="#tab_15">Table 9</ref>. DPE and DMLP visualizations. To investigate the effectiveness of two distortion-aware designs, the visualizations of DPE and DMLP (v1) are shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. The RGB images and DPE from four stages of Trans4PASS are visualized in the top five rows in <ref type="figure" target="#fig_0">Fig. 12</ref>, where the red dots are the centers of the s?s patch sequence and the s 2 yellow dots are the learned offsets from DPE. The offsets result that each pixel is adaptive to distorted objects and space, such as the deformed building and sidewalk in Stage-4 DPE in the outdoor case ( <ref type="figure" target="#fig_0">Fig. 12-(a)</ref>) and the chairs in the indoor case ( <ref type="figure" target="#fig_0">Fig. 12-(b)</ref>). Furthermore, two feature map pairs from the 75 th channel before and after DMLP are displayed in the bottom two rows in <ref type="figure" target="#fig_0">Fig. 12</ref>. Compared to the feature maps before DMLP, the feature maps are enhanced by the DMLP-based token mixer and present semantically recognizable responses, e.g. on regions of distorted sidewalks or objects of deformed cars. Feature embedding comparison. In order to intuitively illustrate the effect of the proposed MPA method on the feature space, the t-SNE visualization of feature embeddings before and after outdoor PIN2PAN domain adaptation is shown in <ref type="figure" target="#fig_0">Fig. 14</ref> represents the center of all pixels that share the same class in its image, and these images are from the training set of the respective domain. The blue triangle ( ) in the source-and target domain and the black triangle ( ) in the mutual domain are the respective domain prototype of a certain class. Before domain adaptation, the feature embeddings of the source domain, the target domain, and their mutual domain are shown in <ref type="figure" target="#fig_0">Fig. 14a, Fig. 14b, and Fig. 14c</ref>, respectively, while after adaptation they are shown in <ref type="figure" target="#fig_0">Fig. 14d</ref>, <ref type="figure" target="#fig_0">Fig. 14e, and Fig. 14f</ref>, respectively. As our proposed MPA method acts on the feature space and provides complementary feature alignment to both domains, their features are supposed to be more closely tied to their mutual prototypes, i.e., both domains go closer to each other bidirectionally. Comparing <ref type="figure" target="#fig_0">Fig. 14c and Fig. 14f</ref>, the proposed MPA method bridges the domain gap in the feature space and ties the feature distribution closer, such as mutual prototypes of sidewalk, person, rider, and truck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-1 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-2 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-3 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-4 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-1 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage-2 DPE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a universal framework with two variants of the In the future, we will explore the combination of cubemap and equiretangular projections, along with the fusion of LiDAR data and panoramic images. Furthermore, it would be interesting to combine two sources, such as pinhole and synthetic datasets, and investigate multi-source domain adaptive panoramic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A MORE QUANTITATIVE RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Analysis of hyper-parameters</head><p>As the spatial correspondence problem indicated in <ref type="bibr" target="#b109">[110]</ref>, if the deformable convolution is added to the shallow or middle layers, the spatial structures are susceptible to fluctuation <ref type="bibr" target="#b56">[57]</ref>. To solve this issue, the regional restriction of learned offsets is used to stabilize the training of our early-stage and four-stage Deformable Patch Embedding (DPE) module.  </p><formula xml:id="formula_21">(a) mIoU(%) -? (b) mIoU(%) -T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. A.1:</head><p>Analysis of hyper-parameters on DensePASS. We analyze the weight ? and the temperature T as shown in <ref type="figure" target="#fig_0">Fig. A.1a and Fig. A.1b</ref>. The Mutual Prototypical Adaptation (MPA) loss and the source-and target segmentation losses are combined by the weight ?. As ? decreases from 0.1 to 0, we set the temperature T =35 in the MPA loss and evaluate the mIoU(%) results on the DensePASS dataset <ref type="bibr" target="#b2">[3]</ref>. If ?=0, the final loss is equivalent to that of the SSL-based method, i.e., the MPA loss is excluded. When ?=0.001 for combining both, MPA and SSL, Trans4PASS obtains a better result. We further investigate the effect of the temperature T in the MPA loss. As shown in <ref type="figure" target="#fig_0">Fig. A.1b</ref>, the performance is not sensitive to the distillation temperature, which illustrates the robustness of our MPA method. Nevertheless, we found that MPA performs better when the temperature is lower, so T =20 is set as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Computational complexity</head><p>We report the complexity of Deformable Patch Embedding (DPE) and two Deformable MLP (DMLP) modules on DensePASS in <ref type="table" target="#tab_21">Table A.</ref>2. The comparison indicates that our methods have better results with the same order of complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B MORE QUALITATIVE RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Panoramic semantic segmentation</head><p>To verify the proposed model, more qualitative comparisons based on DensePASS dataset are displayed in <ref type="figure" target="#fig_0">Fig. B.1</ref>. Specifically, Trans4PASS models can better segment deformed foreground objects, such as trucks in <ref type="figure" target="#fig_0">Fig. B.1a</ref>. Apart from the foreground object, Trans4PASS models yield high-quality segmentation results in the distorted background categories, e.g., fence and sidewalk. For indoor scenarios, more qualitative comparisons are shown in <ref type="figure">Fig. B</ref>.1b, which are from the fold-1 Stanford2D3D-Panoramic dataset <ref type="bibr" target="#b18">[19]</ref>. Our models produce better segmentation results in those categories, such as columns and tables, while the baseline model can hardly identify these deformed objects. <ref type="table" target="#tab_21">Table.</ref> B.1 presents per-class results on the SynPASS benchmark. The small Trans4PASS+ model obtains 39.16% mIoU and sufficient improvements, as compared to the CNN-based HRNet model (+5.07%) and the SegFormer model (+1.92%). Besides, our Trans4PASS models achieve top scores on 17 of 22 classes. However, there is still a lot to be excavated on the SynPASS benchmark, such as the wall, ground, bridge, and dynamic categories, which are challenging cases in the synthetic panoramic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Segmentation on the SynPASS benchmark</head><p>A montage of panoramic semantic segmentation results generated from the validation set of the SynPASS dataset is presented in <ref type="figure" target="#fig_2">Fig. B.2</ref>. Compared with the baseline PVTv2 model <ref type="bibr" target="#b135">[136]</ref>, our Trans4PASS+ model is more robust against adverse situations and obtains more accurate segmentation results, such as the pedestrian in cloudy and sunny scenes, the sidewalk in the foggy and rainy scenes, and the vehicles in the night scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PIN2PAN vs. SYN2REAL visualization</head><p>For a more comprehensive analysis of the two different adaptation paradigms, additional visualization samples are shown in <ref type="figure" target="#fig_3">Fig. B.3</ref>. In the first case, before MPA, there is not a significant deformation in the target-domain sidewalk highlighted by the blue box, thus, the pinhole-trained model obtains more accurate segmentation results than the synthetic-trained model. That means if no distortion appears, the pinhole-trained model benefits more from the same realistic scene appearance as the target domain, and can perform better than the synthetic-trained model. However, in the second case before MPA, the situation is reversed due to the existence of distortion in the highlighted sidewalk from the target domain, which appears in an uncommon position compared that in the pinhole domain. At this point, the synthetic-source trained model benefits more from the similar shape and position prior as in the target domain sidewalk. Nonetheless, after our MPA, both paradigms obtain more complete and accurate segmentation results. This verifies the effectiveness of our proposed mutual prototypical adaptation strategy, which jointly uses ground-truth labels from the source and pseudo-labels from the target, and drives the domain alignment on the feature and output spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Failure Case Analysis</head><p>Some failure cases of panoramic semantic segmentation are presented in <ref type="figure" target="#fig_5">Fig. B.4</ref>. Some erroneous segmentation samples from the three models are presented in <ref type="figure" target="#fig_5">Fig. B.4</ref>. In the outdoor scene, while the PVTv2 baseline <ref type="bibr" target="#b135">[136]</ref> recognizes the truck as a car, the Trans4PASS model can only segment a part of the truck. All three models have difficulty segmenting the building that looks similar to a truck. In the indoor scene, the baseline and the Trans4PASS+ model fail to differentiate between the distorted door and wall, as both are similar in appearance and shape in this case. This issue can potentially be addressed by using complementary panoramic depth information to obtain discriminative features.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Robustness (mIoU) analysis across Fields of View (FoV).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Compared to (a) standard Patch Embeddings, our (b) Deformable Patch Embedding in Trans4PASS partitions 360 ? images while directly considering distortions, e.g. in sidewalks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Domain adaptations for panoramic semantic segmentation include Pinhole-to-Panoramic (PIN2PAN) and Synthetic-to-Real (SYN2REAL) paradigms in both indoor and outdoor scenarios. The feature distributions between the target domain and two source domains are compared in the tSNE-reduced manifold space, including sidewalks and floors. The marginal distributions are plotted along respective axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Trans4PASS+ architectures in Sec. 3.1; the deformable patch embedding module in Sec. 3.2; two deformable MLP variants in Sec. 3.3; and the mutual prototypical adaptation in Sec. 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Transformer with FPN-like decoder (b) Transformer with vanilla-MLP (c) Trans4PASS with DPE and DMLP Comparison of segmentation transformers. Transformers (a) borrow a FPN-like decoder [15] from CNN counterparts or (b) adopt a vanilla-MLP decoder [52] for feature fusion. (c) Trans4PASS integrates Deformable Patch Embeddings (DPE) and the Deformable MLP (DMLP) module for capabilities to handle distortions (see warped terrain) and mix patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of MLP modules. The spatial offsets of DMLP are learned adaptively from the input feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Comparison of token mixing structures. PE: Patch Embedding, DPE: Deformable PE, Self-Attn: Self-Attention, CX: Channel Mixer, PX: Pooling Mixer, and DMLP: Deformable MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Diagram of mutual prototypical adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Examples of images and semantic labels in different conditions from the established SynPASS dataset. Distributions of SynPASS, DensePASS, and Cityscapes in terms of class-wise pixel counts per image. We use the logarithmic scaling of the vertical axis and insert the pixel count above the bar. There are 13 classes overlapping across three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>x, y) of each grid position on the cubemap C={I F , I R , I B , I L , I U , I D } to look up the value, where ??(??, ?), ??(? 1 2 ?, 1 2 ?), {I F , I R , I B , I L , I U , I D }?R H?W are the front, right, back, left, top, and bottom view in the cubemap format, respectively. For {I F , I R , I B , I L } indexed by i={1,2,3,4}, we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :</head><label>10</label><figDesc>Omnidirectional segmentation before (blue lines) and after (green lines) mutual prototypical adaptation. The mIoU (%) scores in eight directions are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Panoramic semantic segmentation visualizations. The baseline model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 :</head><label>12</label><figDesc>DPE and DMLP visualizations. The ? dots in four stages are sampling points shifted by learned offsets w.r.t. the ? patch center of DPE (from decoder). The bottom two rows show the #75 channel maps of stage-3 before and after DMLP. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. B. 1 :</head><label>1</label><figDesc>More panoramic semantic segmentation visualizations. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. B. 2 :Fig. B. 3 :</head><label>23</label><figDesc>SynPASS segmentation visualizations. Zoom in for better view. More PIN2PAN vs. SYN2REAL visualizations before and after MPA, respectively. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. B. 4 :</head><label>4</label><figDesc>Failure case visualizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, K. Yang, S. Rei?, K. Peng, and R. Stiefelhagen are with Karlsruhe Institute of Technology, Germany.</figDesc><table /><note>? H. Shi and K. Wang are with Zhejiang University, China.? C. Ma is with ByteDance Inc., China.? H. Fu is with Beihang University, China.? * corresponding author (kailun.yang@kit.edu).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 .</head><label>1</label><figDesc>SynPASS statistic information.</figDesc><table><row><cell></cell><cell>Cloudy</cell><cell>Foggy</cell><cell>Rainy</cell><cell>Sunny</cell><cell>ALL</cell></row><row><cell>Split</cell><cell cols="4">train / val / test train / val / test train / val / test train / val / test</cell><cell>train / val / test</cell></row><row><cell cols="6">#Frames 1420 / 420 / 430 1420 / 430 / 420 1420 / 430 / 420 1440 / 410 / 420 5700 / 1690 / 1690</cell></row><row><cell>Split</cell><cell>day / night</cell><cell>day / night</cell><cell>day / night</cell><cell>day / night</cell><cell>day / night</cell></row><row><cell>#Frames</cell><cell>1980 / 290</cell><cell>1710 / 560</cell><cell>2040 / 230</cell><cell>1970 / 300</cell><cell>7700 / 1380</cell></row><row><cell>Total</cell><cell>2270</cell><cell>2270</cell><cell>2270</cell><cell>2270</cell><cell>9080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Indoor Panoramic and Real dataset as the target domain: Stanford2D3D [19] Panoramic (SPan) has 1,413 panoramas and 13 classes. Results are averaged by the official 3 folds, following [19], unless otherwise stated. (2) Indoor Pinhole and Real dataset as the first source domain: Stanford2D3D [19] Pinhole (SPin) has 70,496 pinhole images and the same 13 classes as its panoramic dataset. (3) Indoor Panoramic and Synthetic dataset as the second source domain: Structured3D [20] (S3D) has 21,835 synthetic panoramic images and 29 classes. (4) Outdoor Panoramic and Real dataset as the target domain: DensePASS [3] (DP) collected from cities around the world has 2,000 images for transfer optimization and 100 labeled images for testing, annotated with 19 classes.</figDesc><table /><note>(5) Outdoor Pinhole and Real dataset as the first source domain: Cityscapes [14] (CS) has 2,979 and 500 images in the train and val set, and has the same 19 classes as DensePASS. (6) Outdoor Panoramic and Synthetic dataset as the second source domain: SynPASS (SP) contains 9,080 panoramic images and 22 categories. More details in Sec. 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 .</head><label>2</label><figDesc>SynPASS benchmark is evaluated on full 22 classes and is divided into four weather conditions, day-and night-time. Fast-SCNN (Fast-SCNN) 30.84 22.68 26.16 27.19 29.68 24.75 26.31 21.30 DeepLabv3+ (MobileNetv2) 38.94 35.19 35.43 37.73 36.01 30.55 36.72 29.66 HRNet (W18Small) 42.92 37.94 37.37 41.45 39.19 32.22 39.80 34.09 PVT (Tiny) 39.92 34.99 34.01 39.84 36.71 27.36 36.83 32.37 PVT (Small) 40.75 36.14 34.29 40.14 37.92 28.80 37.47 32.68 SegFormer (B1) 45.34 41.43 40.33 44.36 42.97 33.15 42.68 37.36 SegFormer (B2) 46.07 40.99 40.10 44.35 44.08 33.99 42.49 37.24 Trans4PASS (Tiny) 46.90 41.97 41.61 45.52 44.48 34.73 43.68 38.53 Trans4PASS (Small) 46.74 43.49 43.39 45.94 45.52 37.03 44.80 38.57 Trans4PASS+ (Tiny) 48.33 43.41 43.11 46.99 46.52 35.27 45.21 38.85 Trans4PASS+ (Small) 48.87 44.80 45.24 47.62 47.17 37.96 46.47 39.16</figDesc><table><row><cell>Method</cell><cell cols="5">Cloudy Foggy Rainy Sunny Day Night</cell><cell>ALL</cell></row><row><cell></cell><cell>val</cell><cell>val</cell><cell>val</cell><cell>val</cell><cell>val val val test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 .</head><label>3</label><figDesc>Performance gaps of CNN-and transformer-based models from Cityscapes (CS) @ 1024?512 to DensePASS (DP).</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell>CS</cell><cell cols="2">DP mIoU Gaps</cell></row><row><cell>SwiftNet [119]</cell><cell>ResNet-18</cell><cell cols="2">75.4 25.7</cell><cell>-49.7</cell></row><row><cell>Fast-SCNN [118]</cell><cell>Fast-SCNN</cell><cell cols="2">69.1 24.6</cell><cell>-44.5</cell></row><row><cell>ERFNet [120]</cell><cell>ERFNet</cell><cell cols="2">72.1 16.7</cell><cell>-55.4</cell></row><row><cell>FANet [121]</cell><cell>ResNet-34</cell><cell cols="2">71.3 26.9</cell><cell>-44.4</cell></row><row><cell>PSPNet [34]</cell><cell>ResNet-50</cell><cell cols="2">78.6 29.5</cell><cell>-49.1</cell></row><row><cell>OCRNet [122]</cell><cell>HRNetV2p-W18</cell><cell cols="2">78.6 30.8</cell><cell>-47.8</cell></row><row><cell>DeepLabV3+ [30]</cell><cell>ResNet-101</cell><cell cols="2">80.9 32.5</cell><cell>-48.4</cell></row><row><cell>DANet [12]</cell><cell>ResNet-101</cell><cell cols="2">80.4 28.5</cell><cell>-51.9</cell></row><row><cell>DNL [123]</cell><cell>ResNet-101</cell><cell cols="2">80.4 32.1</cell><cell>-48.3</cell></row><row><cell cols="2">Semantic-FPN [124] ResNet-101</cell><cell cols="2">75.8 28.8</cell><cell>-47.0</cell></row><row><cell>ResNeSt [125]</cell><cell>ResNeSt-101</cell><cell cols="2">79.6 28.8</cell><cell>-50.8</cell></row><row><cell>OCRNet [122]</cell><cell>HRNetV2p-W48</cell><cell cols="2">80.7 32.8</cell><cell>-47.9</cell></row><row><cell>SETR-Naive [15]</cell><cell>Transformer-L</cell><cell cols="2">77.9 36.1</cell><cell>-41.8</cell></row><row><cell>SETR-MLA [15]</cell><cell>Transformer-L</cell><cell cols="2">77.2 35.6</cell><cell>-41.6</cell></row><row><cell>SETR-PUP [15]</cell><cell>Transformer-L</cell><cell cols="2">79.3 35.7</cell><cell>-43.6</cell></row><row><cell>SegFormer-B1 [52]</cell><cell>SegFormer-B1</cell><cell cols="2">78.5 38.5</cell><cell>-40.0</cell></row><row><cell>SegFormer-B2 [52]</cell><cell>SegFormer-B2</cell><cell cols="2">81.0 42.4</cell><cell>-38.6</cell></row><row><cell>Trans4PASS (T)</cell><cell>Trans4PASS (T)</cell><cell cols="2">79.1 41.5</cell><cell>-37.6</cell></row><row><cell>Trans4PASS (S)</cell><cell>Trans4PASS (S)</cell><cell cols="2">81.1 44.8</cell><cell>-36.3</cell></row><row><cell>Trans4PASS+ (T)</cell><cell>Trans4PASS+ (T)</cell><cell cols="2">78.6 41.6</cell><cell>-37.0</cell></row><row><cell>Trans4PASS+ (S)</cell><cell>Trans4PASS+ (S)</cell><cell cols="2">81.0 44.9</cell><cell>-36.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 .</head><label>4</label><figDesc>Performance gaps from Stanford2D3D-Pinhole (SPin) to Stanford2D3D-Panoramic (SPan) dataset on fold-1.</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell>SPin</cell><cell cols="2">SPan mIoU Gaps</cell></row><row><cell>Fast-SCNN [118]</cell><cell>Fast-SCNN</cell><cell cols="2">41.71 26.86</cell><cell>-14.85</cell></row><row><cell>SwiftNet [119]</cell><cell>ResNet-18</cell><cell cols="2">42.28 34.95</cell><cell>-7.87</cell></row><row><cell>DANet [12]</cell><cell>ResNet-50</cell><cell cols="2">43.33 37.76</cell><cell>-5.57</cell></row><row><cell>DANet [12]</cell><cell>ResNet-101</cell><cell cols="2">40.09 31.81</cell><cell>-8.28</cell></row><row><cell cols="2">Trans4Trans-T [126] PVT-T</cell><cell cols="2">41.28 24.45</cell><cell>-16.83</cell></row><row><cell>Trans4Trans-S [126]</cell><cell>PVT-S</cell><cell cols="2">44.47 23.11</cell><cell>-21.36</cell></row><row><cell>Trans4PASS (T)</cell><cell>Trans4PASS (T)</cell><cell cols="2">49.05 46.08</cell><cell>-2.97</cell></row><row><cell>Trans4PASS (S)</cell><cell>Trans4PASS (S)</cell><cell cols="2">50.20 48.34</cell><cell>-1.86</cell></row><row><cell>Trans4PASS+ (T)</cell><cell>Trans4PASS+ (T)</cell><cell cols="2">48.99 46.75</cell><cell>-2.24</cell></row><row><cell>Trans4PASS+ (S)</cell><cell>Trans4PASS+ (S)</cell><cell cols="2">53.46 50.35</cell><cell>-3.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 .</head><label>5</label><figDesc>SYN2REAL vs. PIN2PAN domain gaps.</figDesc><table><row><cell>1 Outdoor PIN2PAN:</cell><cell>CS13</cell><cell>DP13</cell><cell>mIoU Gaps</cell></row><row><cell>PVT (Tiny)</cell><cell>63.70</cell><cell>44.04</cell><cell>-19.66</cell></row><row><cell>PVT (Small)</cell><cell>65.88</cell><cell>46.19</cell><cell>-19.69</cell></row><row><cell>Trans4PASS (Tiny)</cell><cell>71.63</cell><cell>49.21 (+5.17)</cell><cell>-22.42</cell></row><row><cell>Trans4PASS (Small)</cell><cell>75.21</cell><cell>50.96 (+4.77)</cell><cell>-24.25</cell></row><row><cell>Trans4PASS+ (Tiny)</cell><cell>72.92</cell><cell>49.16 (+5.12)</cell><cell>-23.76</cell></row><row><cell>Trans4PASS+ (Small)</cell><cell>74.52</cell><cell>51.40 (+5.21)</cell><cell>-23.12</cell></row><row><cell>2 Outdoor SYN2REAL:</cell><cell>SP13</cell><cell>DP13</cell><cell>mIoU Gaps</cell></row><row><cell>PVT (Tiny)</cell><cell>51.05</cell><cell>35.26</cell><cell>-15.79</cell></row><row><cell>PVT (Small)</cell><cell>52.94</cell><cell>38.74</cell><cell>-14.20</cell></row><row><cell>Trans4PASS (Tiny)</cell><cell>61.08</cell><cell>39.68 (+4.42)</cell><cell>-21.40</cell></row><row><cell>Trans4PASS (Small)</cell><cell>62.76</cell><cell>43.18 (+4.44)</cell><cell>-19.58</cell></row><row><cell>Trans4PASS+ (Tiny)</cell><cell>60.37</cell><cell>39.62 (+4.36)</cell><cell>-20.75</cell></row><row><cell>Trans4PASS+ (Small)</cell><cell>61.59</cell><cell>43.17 (+4.43)</cell><cell>-18.42</cell></row><row><cell>3 Indoor PIN2PAN:</cell><cell>SPin8</cell><cell>SPan8</cell><cell>mIoU Gaps</cell></row><row><cell>PVT (Tiny)</cell><cell>59.70</cell><cell>53.98</cell><cell>-5.72</cell></row><row><cell>PVT (Small)</cell><cell>60.46</cell><cell>57.71</cell><cell>-2.75</cell></row><row><cell>Trans4PASS (Tiny)</cell><cell>64.25</cell><cell>58.93 (+4.95)</cell><cell>-5.32</cell></row><row><cell>Trans4PASS (Small)</cell><cell>66.51</cell><cell>62.39 (+4.68)</cell><cell>-4.12</cell></row><row><cell>Trans4PASS+ (Tiny)</cell><cell>65.09</cell><cell>59.55 (+5.57)</cell><cell>-5.54</cell></row><row><cell>Trans4PASS+ (Small)</cell><cell>65.29</cell><cell>63.08 (+5.37)</cell><cell>-2.21</cell></row><row><cell>4 Indoor SYN2REAL:</cell><cell>S3D8</cell><cell>SPan8</cell><cell>mIoU Gaps</cell></row><row><cell>PVT (Tiny)</cell><cell>68.90</cell><cell>42.04</cell><cell>-26.86</cell></row><row><cell>PVT (Small)</cell><cell>66.46</cell><cell>45.82</cell><cell>-20.64</cell></row><row><cell>Trans4PASS (Tiny)</cell><cell>76.84</cell><cell>48.63 (+6.59)</cell><cell>-28.21</cell></row><row><cell>Trans4PASS (Small)</cell><cell>77.29</cell><cell>51.70 (+5.88)</cell><cell>-25.59</cell></row><row><cell>Trans4PASS+ (Tiny)</cell><cell>76.91</cell><cell>50.60 (+8.56)</cell><cell>-26.31</cell></row><row><cell>Trans4PASS+ (Small)</cell><cell>76.88</cell><cell>51.93 (+6.11)</cell><cell>-24.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 .</head><label>6</label><figDesc>Comparison of SYN2REAL transfer learning between methods followed Structured3D<ref type="bibr" target="#b19">[20]</ref>. Synthetic: S3D8, Real: SPan8.</figDesc><table><row><cell>Indoor SYN2REAL:</cell><cell>Data</cell><cell>S3D8</cell><cell>SPan8</cell></row><row><cell>PSPNet (ResNet-50)</cell><cell>Synthetic</cell><cell>-</cell><cell>26.13</cell></row><row><cell>PSPNet (ResNet-50)</cell><cell>Synthetic+Real</cell><cell>-</cell><cell>49.71</cell></row><row><cell>UPerNet (ResNet-50)</cell><cell>Synthetic</cell><cell>-</cell><cell>28.75</cell></row><row><cell>UPerNet (ResNet-50)</cell><cell>Synthetic+Real</cell><cell>-</cell><cell>49.60</cell></row><row><cell>HRNet (W18)</cell><cell>Synthetic</cell><cell>-</cell><cell>37.92</cell></row><row><cell>HRNet (W18)</cell><cell>Synthetic+Real</cell><cell>-</cell><cell>52.00</cell></row><row><cell>PVT (Tiny)</cell><cell>Synthetic</cell><cell>68.90</cell><cell>42.04</cell></row><row><cell>PVT (Small)</cell><cell>Synthetic</cell><cell>66.46</cell><cell>45.82</cell></row><row><cell>Trans4PASS (Tiny)</cell><cell>Synthetic</cell><cell>76.84</cell><cell>48.63</cell></row><row><cell>Trans4PASS (Small)</cell><cell>Synthetic</cell><cell>77.29</cell><cell>51.70</cell></row><row><cell>Trans4PASS+ (Tiny)</cell><cell>Synthetic</cell><cell>76.91</cell><cell>50.60</cell></row><row><cell>Trans4PASS+ (Small)</cell><cell>Synthetic</cell><cell>76.88</cell><cell>51.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 .</head><label>7</label><figDesc>Trans4PASS structural analysis. * and ? denote DPT<ref type="bibr" target="#b25">[26]</ref> and our DPE, while ? is trained with OHEM. "#P" is short for #Parameters in millions. CX: Channel Mixer, PX: Pooling Mixer with 3?3). Models are trained on Cityscapes (CS) @ 512?512 and tested on DensePASS (DP) @ 2048?400.</figDesc><table><row><cell>Network</cell><cell>Encoder Decoder</cell><cell>GFLOPs #P</cell><cell>CS DP</cell></row><row><cell cols="2">(1) Compare PEs and MLPs:</cell><cell></cell><cell></cell></row><row><cell cols="2">Trans4PASS MiT-B1* DMLPv1+DPT [26]</cell><cell cols="2">13.11 13.10 69.48 36.50</cell></row><row><cell cols="2">Trans4PASS MiT-B1 ? CycleMLP [23]+DPE</cell><cell cols="2">9.83 13.60 73.49 40.16</cell></row><row><cell cols="2">Trans4PASS MiT-B1 ? ASMLP [24]+DPE</cell><cell cols="2">13.40 14.19 73.65 42.05</cell></row><row><cell cols="2">Trans4PASS MiT-B1 ? DMLPv1+DPE</cell><cell cols="2">12.02 13.93 72.49 45.89 (+9.39)</cell></row><row><cell cols="2">(2) Compare encoders and decoders:</cell><cell></cell><cell></cell></row><row><cell>PVT [13]</cell><cell>PVT-T FPN</cell><cell cols="2">11.17 12.76 71.46 31.20</cell></row><row><cell>PVT [13]</cell><cell>PVT-T Vanilla MLP</cell><cell cols="2">14.56 12.84 70.60 32.85</cell></row><row><cell>PVT [13]</cell><cell>PVT-T DMLPv1</cell><cell cols="2">13.11 13.10 71.75 35.18 (+3.98)</cell></row><row><cell cols="2">Trans4PASS PVT-T ? DMLPv1+DPE</cell><cell cols="2">13.18 13.10 69.62 36.50 (+5.30)</cell></row><row><cell cols="2">SegFormer [52]MiT-B1 Vanilla MLP</cell><cell cols="2">13.27 13.66 74.93 39.02</cell></row><row><cell cols="2">SegFormer [52]MiT-B1 FPN</cell><cell cols="2">9.88 13.58 73.96 41.14</cell></row><row><cell cols="2">SegFormer [52]MiT-B1 DMLPv1</cell><cell cols="2">11.82 13.92 73.10 45.14 (+6.12)</cell></row><row><cell cols="2">Trans4PASS MiT-B1 ? DMLPv1</cell><cell cols="2">12.02 13.93 72.49 45.89 (+6.87)</cell></row><row><cell cols="2">(3) Compare token mixing structures:</cell><cell></cell><cell></cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? PoolFormer [22]</cell><cell cols="2">9.47 13.47 70.52 43.18</cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? FAN [25]</cell><cell cols="2">10.96 13.81 71.15 42.54</cell></row><row><cell cols="4">Trans4PASS+ MiT-B1 ? PoolFormer [22]+FAN [25] 10.96 13.81 72.21 45.97</cell></row><row><cell cols="4">Trans4PASS+ MiT-B1 ? PoolFormer [22]+FAN [25] 10.96 13.81 67.24 47.69</cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? DMLPv2 (PX)</cell><cell cols="2">10.22 13.60 72.62 46.66 (+7.64)</cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? DMLPv2 (CX)</cell><cell cols="2">11.70 13.95 72.51 47.06 (+8.04)</cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? DMLPv2 (PX+CX)</cell><cell cols="2">11.70 13.95 73.44 47.74 (+8.72)</cell></row><row><cell cols="2">Trans4PASS+ MiT-B1 ? DMLPv2 (PX+CX)</cell><cell cols="2">11.70 13.95 72.67 50.23 (+11.21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 -</head><label>7</label><figDesc>(2), based on the same encoder as PVT, a DMLP-based decoder brings a +3.98% increase compared to the FPN-and MLPbased decoders. When DPE is applied in the early stage of the PVT encoder, a larger increase of +5.30% can be obtained. Similarly, substantial improvements (+6.12% and +6.87%) can be reached with a SegFormer encoder. These results show that DPE and DMLP can be easily integrated into different backbones, significantly boosting their distortion-adaptability for enhancing omnidirectional scene segmentation. Effect of DMLPv2. We step further to assess the effects of</figDesc><table /><note>the new DMLPv2 module for parallel token mixing, which is embedded on our upgraded Trans4PASS+. As shown in Table 7- (3), the final Trans4PASS+ using DMLPv2, coupled with Pooling Mixer (PX) and Channel Mixer (CX) (see Fig. 6e), strikingly boosts the mIoU on DensePASS by +8.72% in mIoU over the baseline. It can be seen that PX and CX both contribute signif- icantly. DMLPv2 empowers the distortion-aware Trans4PASS+</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8b .</head><label>8b</label><figDesc>Trans4PASS comes with &gt;10% performance gains due to the collected long-range dependencies and distortion-aware features, and Trans4PASS+ further enlarges the improvement. Without the advantage of a superior network architecture, the proposed Mutual Prototypical Adaptation (MPA) solution achieves 51.93% and 54.77% with the tiny and the small version of Trans4PASS, clearly surpassing those attained with the previous state-of-the-art P2PDA (51.05% and 52.91%). The second and third ablation groups inTable 8bshow how Trans4PASS-T and -S match up against each other. Individually, MPA is on par with the Self-Supervised Learning (SSL) solution. When combining both, MPA and SSL, Trans4PASS-S sets new state-of-the-art performance on DensePASS, arriving at 55.25% in mIoU and 56.38% with Multi-Scale (MS) testing. The small variant of Trans4PASS+ further elevates mIoU to 57.23%. These results certify that MPA works collaboratively with pseudo labels and offers a complementary feature alignment incentive. Omnidiretional segmentation. To showcase the effectiveness of MPA on omnidirectional segmentation, the panorama is divided into 8 directions and mIoU scores are calculated in each direction</figDesc><table /><note>separately. The polar diagram in Fig. 10 demonstrates that MPA reliably improves the adaptation performance of PVTv2 [136], TransPASS, and Trans4PASS+. Due to the panorama boundary at 180? , Trans4PASS performs less satisfactorily in that direction. Still, consistent and large accuracy gains with MPA in all direc- tions are observed, and Trans4PASS+ successfully enhances the performance around 180 ? , as the embedded channel-and pool-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 8 .</head><label>8</label><figDesc>Comparisons and ablation studies of PIN2PAN domain adaptation in indoor and outdoor scenarios.<ref type="bibr" target="#b62">63</ref>.59 18.22 47.01 9.45 12.79 17.00 8.12 6.41 34.24 10.15 18.43 4.96 2.31 46.03 3.19 0.59 0.00 8.30 5.55 PASS (ERFNet) [10] 23.66 67.84 28.75 59.69 19.96 29.41 8.26 4.54 8.07 64.96 13.75 33.50 12.87 3.17 48.26 2.17 0.82 0.29 23.76 19.46 ECANet (Omni-supervised) [9] 43.02 81.60 19.46 81.00 32.02 39.47 25.54 3.85 17.38 79.01 39.75 94.60 46.39 12.98 81.96 49.25 28.29 0.00 55.36 29.47 CLAN (Adversarial) [16] 31.46 65.39 21.14 69.10 17.29 25.49 11.17 3.14 7.61 71.03 28.19 55.55 18.86 2.76 71.60 26.42 17.99 59.53 9.44 15.91 CRST (Self-training) [17] 31.67 68.18 15.72 76.78 14.06 26.11 9.90 0.82 2.66 69.36 21.95 80.06 9.71 1.25 65.12 38.76 27.22 48.85 7.10 18.08 P2PDA (Adversarial) [21] 41.99 70.21 30.24 78.44 26.72 28.44 14.02 11.67 5.79 68.54 38.20 85.97 28.14 0.00 70.36 60.49 38.90 77.80 39.85 24.02 SIM (Self-training) [101] 44.58 68.16 32.59 80.58 25.68 31.38 23.60 19.39 14.09 72.65 26.41 87.88 41.74 16.09 73.56 47.08 42.81 56.35 47.72 39.30 PCS (Self-training) [106] 53.83 78.10 46.24 86.24 30.33 45.78 34.04 22.74 13.00 79.98 33.07 93.44 47.69 22.53 79.20 61.59 67.09 83.26 58.68 39.80 DAFormer (Self-training) [27] 54.67 73.75 27.34 86.35 35.88 45.56 36.28 25.53 10.65 79.87 41.64 94.74 49.69 25.15 77.70 63.06 65.61 86.68 65.12 48.13 USSS (IDD) [129] 26.98 68.85 5.41 67.39 15.10 21.79 13.18 0.12 7.73 70.27 8.84 85.53 22.05 1.71 58.69 16.41 12.01 0.00 23.58 13.90 USSS (Mapillary) [129] 30.87 71.01 31.85 76.79 12.13 23.61 11.93 3.23 10.15 73.11 31.24 89.59 16.05 3.86 65.27 24.46 18.72 0.00 9.08 14.48 Seamless (Mapillary) [130] 34.14 59.26 24.48 77.35 12.82 30.91 12.63 15.89 17.73 75.61 33.30 87.30 19.69 4.59 63.94 25.81 57.16 0.00 11.59 19.04 SwiftNet (Cityscapes) [119] 25.67 50.73 32.76 70.24 12.63 24.02 18.79 7.18 4.01 64.93 23.70 84.29 14.91 0.97 43.46 8.92 0.04 4.45 12.77 8.77 SwiftNet (Merge3) [131] 32.04 68.31 38.59 81.48 15.65 23.91 20.74 5.95 0.00 70.64 25.09 90.93 32.66 0.00 66.91 42.30 5.97 0.07 6.85 12.66 Trans4PASS (S) (ours) 55.25 78.39 41.62 86.47 31.56 45.47 34.02 22.98 18.33 79.63 41.35 93.80 49.02 22.99 81.05 67.43 69.64 86.04 60.85 39.20 Trans4PASS (S) (ours)* 56.38 79.91 42.68 86.26 30.68 42.32 36.61 24.81 19.64 78.80 44.73 93.84 50.71 24.39 81.72 68.86 66.18 88.62 63.87 46.62 Trans4PASS+ (S) (ours) 56.45 79.05 49.82 86.67 37.32 38.99 30.24 23.17 21.47 78.34 44.85 93.79 49.63 28.30 80.65 69.63 70.72 90.27 59.09 40.62 Trans4PASS+ (S) (ours)* 57.23 79.25 50.07 86.77 37.97 39.75 32.51 22.49 23.15 77.76 44.02 93.84 50.63 29.57 80.64 70.91 75.27 90.61 60.58 41.60</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>Road</cell><cell>S.walk</cell><cell>Build.</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Tr. light</cell><cell>Tr. sign</cell><cell>Veget.</cell><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>M.cycle</cell><cell>Bicycle</cell></row><row><cell>ERFNet [120]</cell><cell>16.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Adaptation results on SPan @ fold-1.</figDesc><table><row><cell>Network FANet DANet Trans4PASS (T) Trans4PASS (S) Trans4PASS (T) Trans4PASS (T) Trans4PASS (T) Trans4PASS (T) Trans4PASS (T) Trans4PASS (T) Trans4PASS+ (T) MPA + SSL + MS Method P2PDA P2PDA P2PDA P2PDA -Warm-up SSL MPA MPA + SSL MPA + SSL + MS Trans4PASS (S) -Trans4PASS (S) Warm-up Trans4PASS (S) SSL Trans4PASS (S) MPA Trans4PASS (S) MPA + SSL Trans4PASS (S) MPA + SSL + MS Trans4PASS+ (S) MPA + SSL + MS (b) Adaptation results on DensePASS. mIoU(%) 35.67 41.99 51.05 52.91 45.89 50.56 51.86 51.93 53.26 54.72 55.13 48.73 52.59 54.67 54.77 55.25 56.38 57.23</cell><cell>Network DANet DANet PVT-Tiny PVT-Tiny PVT-Small PVT-Small Trans4PASS (T) Trans4PASS (T) Trans4PASS+ (T) Trans4PASS (S) Trans4PASS (S) Trans4PASS+ (S) DANet Trans4PASS (S) Trans4PASS+ (S) (c) Method Method mIoU(%) -40.28 P2PDA 42.26 -24.45 P2PDA 39.66 -23.11 P2PDA 43.10 -46.08 MPA 47.48 MPA 49.23 -48.34 MPA 52.15 MPA 52.53 Supervised 44.15 Supervised 53.31 Supervised 53.62 Supervised StdConv [66] CubeMap [66] DistConv [66] UNet [132] GaugeNet [133] UGSCNN [67] HexRUNet [134] Tangent [135] (ResNet-101) RGB Input mIoU(%) RGB 32.6 RGB 33.8 RGB 34.6 RGB-D 35.9 RGB-D 39.4 RGB-D 38.3 RGB-D 43.3 45.6 HoHoNet [8] (ResNet-101) RGB 52.0 Trans4PASS (Small) RGB 52.1 Trans4PASS (Small+MS) RGB 53.0 Trans4PASS+ (Small+MS) RGB 54.0 UDA Trans4PASS (Source only) RGB 48.1 Trans4PASS (MPA) RGB 50.8 Trans4PASS (MPA+MS) RGB 51.2 Trans4PASS+ (MPA+MS) RGB 52.3</cell></row></table><note>(d) Comparison on SPan avg. of 3 folds.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 9 .</head><label>9</label><figDesc>Domain adaptation of PIN2PAN and SYN2REAL. Method mIoU Road S.walk Build. Wall Fence Pole Tr. light Tr. sign Veget. Terrain Sky Person Car (1) Outdoor PIN2PAN: CS13 ? DP13 PVT (S) Source-only 46.19 70.24 37.48 84.21 25.43 35.57 14.33 09.15 10.89 76.85 37.98 93.22 39.53 65.52 PVT (S) MPA 48.14 74.06 42.25 85.24 27.51 38.05 14.83 09.02 13.78 78.36 37.83 93.65 41.54 69.66 Trans4PASS+ (S) Source-only 51.40 76.24 42.20 85.57 30.51 40.23 28.13 15.53 17.64 78.04 32.62 93.44 50.88 77.12 Trans4PASS+ (S) MPA 54.33 78.97 50.71 86.72 32.55 45.87 30.86 14.59 18.66 79.81 44.80 93.86 49.53 79.37</figDesc><table><row><cell>Network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(2) Outdoor SYN2REAL: SP13 ? DP13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVT (S)</cell><cell cols="7">Source-only 38.74 55.39 36.87 80.84 19.72 15.18 08.04 05.39</cell><cell>02.17</cell><cell cols="3">72.91 32.01 90.81 26.76 57.40</cell></row><row><cell>PVT (S)</cell><cell>MPA</cell><cell cols="6">40.90 70.78 42.47 82.13 22.79 10.74 13.54 01.27</cell><cell>00.30</cell><cell cols="3">71.15 33.03 89.69 29.07 64.73</cell></row><row><cell cols="8">Trans4PASS+ (S) Source-only 43.17 73.72 43.31 79.88 19.29 16.07 20.02 08.83</cell><cell>01.72</cell><cell cols="3">67.84 31.06 86.05 44.77 68.58</cell></row><row><cell>Trans4PASS+ (S)</cell><cell>MPA</cell><cell cols="6">45.29 67.28 43.48 83.18 22.02 21.98 22.72 07.86</cell><cell>01.52</cell><cell cols="3">73.12 40.65 91.36 42.69 70.87</cell></row><row><cell></cell><cell></cell><cell cols="8">(a) Per-class results on DensePASS13 dataset before and after MPA.</cell><cell></cell></row><row><cell>Network</cell><cell cols="2">Method</cell><cell>mIoU</cell><cell>Ceiling</cell><cell>Chair</cell><cell>Door</cell><cell>Floor</cell><cell>Sofa</cell><cell>Table</cell><cell>Wall</cell><cell>Window</cell></row><row><cell cols="3">(3) Indoor PIN2PAN: SPin8 ? SPan8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVT (S)</cell><cell cols="2">Source-only</cell><cell>57.71</cell><cell>85.69</cell><cell>51.71</cell><cell>18.54</cell><cell>90.78</cell><cell>34.76</cell><cell>65.34</cell><cell>74.87</cell><cell>39.98</cell></row><row><cell>PVT (S)</cell><cell></cell><cell>MPA</cell><cell>57.95</cell><cell>85.85</cell><cell>51.76</cell><cell>18.39</cell><cell>90.78</cell><cell>35.93</cell><cell>65.43</cell><cell>75.00</cell><cell>40.43</cell></row><row><cell>Trans4PASS+ (S)</cell><cell cols="2">Source-only</cell><cell>63.08</cell><cell>87.49</cell><cell>59.49</cell><cell>23.40</cell><cell>90.96</cell><cell>46.21</cell><cell>70.98</cell><cell>76.24</cell><cell>49.80</cell></row><row><cell>Trans4PASS+ (S)</cell><cell></cell><cell>MPA</cell><cell>64.52</cell><cell>85.08</cell><cell>58.72</cell><cell>34.97</cell><cell>91.12</cell><cell>46.25</cell><cell>71.72</cell><cell>77.58</cell><cell>50.75</cell></row><row><cell cols="3">(4) Indoor SYN2REAL: S3D8 ? SPan8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PVT (S)</cell><cell cols="2">Source-only</cell><cell>45.82</cell><cell>80.39</cell><cell>27.25</cell><cell>05.99</cell><cell>84.54</cell><cell>05.79</cell><cell>40.86</cell><cell>68.74</cell><cell>52.98</cell></row><row><cell>PVT (S)</cell><cell></cell><cell>MPA</cell><cell>46.07</cell><cell>80.95</cell><cell>28.08</cell><cell>06.11</cell><cell>84.33</cell><cell>06.24</cell><cell>40.60</cell><cell>69.22</cell><cell>53.00</cell></row><row><cell>Trans4PASS+ (S)</cell><cell cols="2">Source-only</cell><cell>51.93</cell><cell>82.58</cell><cell>50.49</cell><cell>19.26</cell><cell>89.17</cell><cell>05.84</cell><cell>39.46</cell><cell>71.21</cell><cell>57.40</cell></row><row><cell>Trans4PASS+ (S)</cell><cell></cell><cell>MPA</cell><cell>52.47</cell><cell>85.95</cell><cell>49.62</cell><cell>19.62</cell><cell>90.11</cell><cell>03.64</cell><cell>40.69</cell><cell>71.07</cell><cell>59.06</cell></row><row><cell></cell><cell></cell><cell cols="7">(b) Per-class results on SPan8 dataset before and after MPA.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>, where the marginal distributions of the synthetic and real domains are close in one dimension encoding information like deformed shapes and positional priors. Yet, the SYN2REAL model cannot identify the traffic signs, which are successfully recognized via the PIN2PAN model that exploits the rich textures learned from pinhole realistic scenes, as the pinhole-source and panoramic-target domains are close in another dimension encoding appearance cues. Yet, after MPA-based domain adaptation, the PIN2PAN model (Fig. 13: PIN2PAN vs. SYN2REAL visualizations before and after MPA, respectively. The black areas indicate misprediction.</figDesc><table><row><cell>RGB</cell><cell>GT</cell></row><row><cell>Pin2Pan before</cell><cell>Pin2Pan after</cell></row><row><cell>1</cell><cell>2</cell></row><row><cell>Syn2Real before</cell><cell>Syn2Real after</cell></row><row><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>Pin2Pan MPA</cell></row><row><cell>1</cell><cell>2</cell></row><row><cell>Pin vs. Syn before MPA</cell><cell>Pin vs. Syn after MPA</cell></row><row><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>Syn2Real MPA</cell></row></table><note>2 ) can also seamlessly detect the sidewalk, which indicates that our distortion- aware MPA-adapted Trans4PASS+ successfully fixes the large gap in shape-deformations and position-priors. However, the adapted SYN2REAL model ( 4 ) has difficulty to discover the traffic signs, which lack diverse textures in the simulated data. These segmen-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>. Each dot SNE visualizations before and after domain adaptation in outdoor scenes. are the prototype of source or target domain and represents the mutual prototype. Zoom in for better view.</figDesc><table><row><cell>Void</cell><cell>Road</cell><cell>Sidewalk</cell><cell>Buidling</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell cols="3">Traffic Light Traffic Sign Vegetation</cell></row><row><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>Motorcycle</cell><cell>Bicycle</cell></row><row><cell cols="3">(a) Source before</cell><cell cols="3">(b) Target before</cell><cell></cell><cell cols="3">(c) Mutual before</cell></row><row><cell cols="3">(d) Source after</cell><cell></cell><cell cols="2">(e) Target after</cell><cell></cell><cell cols="3">(f) Mutual after</cell></row><row><cell cols="2">Fig. 14: t-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Transformer for PAnoramic Semantic Segmentation (Trans4PASS) architecture to revitalize 360 ? scene understanding. The Deformable Patch Embedding (DPE) and the Deformable MLP (DMLP) modules empower Trans4PASS with distortion awareness. A Mutual Prototypical Adaptation (MPA) strategy is introduced for transferring semantic information from the labelrich source domain to the label-scarce target domain, by combining source labels and target pseudo-label for feature alignment in feature and output space. A new dataset, termed SynPASS, is created. It enables the supervised training of panoramic segmentation models, and it further provides an alternative Synthetic-to-Real (SYN2REAL) domain adaptation paradigm, which is compared to the Pinhole-to-Panoramic (PIN2PAN) adaptation scenario. The framework obtains state-of-the-art accuracy on four competitive domain adaptive panoramic semantic segmentation benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Table A.1 shows that r=4 has a better result, and we set it as default in our experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE A . 1 .</head><label>A1</label><figDesc>Effect of regional restriction (r) on DensePASS.</figDesc><table><row><cell>Alpha (?)</cell><cell></cell><cell></cell><cell></cell><cell>Temperature (?)</cell><cell></cell></row><row><cell>0.1</cell><cell>54.25</cell><cell></cell><cell>100</cell><cell>54.9</cell><cell></cell></row><row><cell>0.01</cell><cell>54.33</cell><cell></cell><cell>35</cell><cell>54.89</cell><cell></cell></row><row><cell>0.001</cell><cell>54.9</cell><cell></cell><cell>20</cell><cell>55.25</cell><cell></cell></row><row><cell>0.0001</cell><cell>54.7</cell><cell></cell><cell>10</cell><cell>54.96</cell><cell></cell></row><row><cell>0</cell><cell>54.67</cell><cell></cell><cell>1</cell><cell>54.95</cell><cell></cell></row><row><cell></cell><cell>None</cell><cell>r=1</cell><cell>r=2</cell><cell>r=4</cell><cell>r=8</cell></row><row><cell>mIoU(%)</cell><cell>45.74</cell><cell>44.51</cell><cell>45.59</cell><cell>45.89</cell><cell>45.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE A .</head><label>A</label><figDesc>2. Computational complexity. GFLOPs @512?512. PE [52] DPE DPT [26] CycleMLP [23] ASMLP [24] DMLP DMLPv2</figDesc><table><row><cell>GFLOPs</cell><cell>0.16 0.36</cell><cell>7.65</cell><cell>1.25</cell><cell>4.83</cell><cell>3.45</cell><cell>3.13</cell></row><row><cell cols="2">#Params(M) 0.01 0.02</cell><cell>2.90</cell><cell>0.45</cell><cell>1.04</cell><cell>0.79</cell><cell>0.80</cell></row><row><cell cols="3">mIoU(%) 45.14 50.23 36.50</cell><cell>40.16</cell><cell>42.05</cell><cell cols="2">45.89 50.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE B .</head><label>B</label><figDesc>1. Per-class results on the test set of the SynPASS benchmark. 64.14 10.22 00.00 00.08 07.77 26.68 80.63 33.87 60.93 32.91 00.14 00.00 89.61 01.37 00.01 03.80 20.81 00.00 01.02 00.00 00.01 34.68 DeepLabv3+ (MobileNetv2) 29.66 75.14 11.50 00.00 25.04 11.05 39.90 89.19 43.73 62.44 62.41 01.59 00.00 91.86 00.54 00.01 26.13 43.72 08.30 12.67 01.06 05.03 41.21 HRNet (W18Small) 34.09 75.94 28.76 00.00 29.59 23.74 59.21 91.68 52.63 63.94 64.63 00.70 00.00 93.02 00.81 00.01 27.42 65.54 08.08 16.32 01.10 03.80 42.99 PVT (Tiny) 32.37 74.83 19.94 00.24 21.82 13.15 62.59 93.14 49.09 67.27 46.44 01.69 09.63 96.09 00.18 02.64 08.81 61.11 14.09 12.04 00.99 05.05 51.33 PVT (Small) 32.68 78.02 27.12 00.27 23.48 16.51 59.81 92.87 50.21 66.22 43.50 01.12 08.67 96.34 00.44 00.15 02.82 63.88 13.78 15.15 01.58 08.78 48.29 SegFormer (B1) 37.36 78.24 20.59 00.00 38.28 21.09 68.72 94.50 59.72 68.43 67.51 00.83 09.86 96.08 00.56 01.38 20.79 69.59 23.38 19.91 01.38 08.97 52.07 SegFormer (B2) 37.24 79.25 23.58 00.00 40.01 20.14 65.28 92.80 46.92 68.64 77.45 01.42 15.00 96.33 00.57 00.58 02.68 67.60 25.86 20.80 01.99 20.92 51.53 Trans4PASS (Tiny) 38.53 79.17 28.18 00.13 36.04 23.69 69.16 95.51 61.71 69.77 71.12 01.53 16.98 96.50 00.56 01.60 15.22 70.48 26.03 23.11 02.08 09.24 49.77 Trans4PASS (Small) 38.57 80.02 24.56 00.07 41.49 25.23 72.00 95.89 59.88 69.07 77.08 01.04 13.72 96.69 00.67 00.73 05.60 72.56 25.93 22.45 02.78 08.34 52.65 Trans4PASS+ (Tiny) 38.85 80.38 24.20 00.12 43.63 26.08 71.85 94.85 55.85 68.05 76.27 01.51 16.24 96.45 01.07 01.86 16.44 72.39 21.51 23.21 03.13 08.70 50.81 Trans4PASS+ (Small) 39.16 80.33 20.66 00.10 44.83 26.47 73.62 96.52 59.26 72.23 80.05 00.98 19.54 97.23 00.56 00.28 11.77 70.73 25.78 21.91 03.14 04.25 51.23</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>Building</cell><cell>Fence</cell><cell>Other</cell><cell>Pedestrian</cell><cell>Pole</cell><cell>RoadLine</cell><cell>Road</cell><cell>SideWalk</cell><cell>Vegetation</cell><cell>Vehicles</cell><cell>Wall</cell><cell>TrafficSign</cell><cell>Sky</cell><cell>Ground</cell><cell>Bridge</cell><cell>RailTrack</cell><cell>GroundRail</cell><cell>TrafficLight</cell><cell>Static</cell><cell>Dynamic</cell><cell>Water</cell><cell>Terrain</cell></row><row><cell>Fast-SCNN (Fast-SCNN)</cell><cell>21.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. MMSegmentation: https://github.com/open-mmlab/mmsegmentation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the Federal Ministry of Labor and Social Affairs (BMAS) through the AccessibleMaps project under Grant 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, in part by the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition, and in part by Hangzhou SurImage Technology Company Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Is context-aware CNN ready for the surroundings? Panoramic semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1866" to="1881" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eliminating the blind spot: Adapting 3D object detection and monocular depth estimation to 360 ? panoramic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DenseP-ASS: Dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<editor>ITSC</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Review on panoramic imaging and its applications in scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05570</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting head movement in panoramic video: A deep reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2693" to="2708" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spherical DNNs and their applications in 360 ? images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep learning for omnidirectional vision: A survey and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10468</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HoHoNet: 360 indoor holistic understanding with latent horizontal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PASS: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4171" to="4185" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bending reality: Distortion-aware transformers for adapting to panoramic semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Joint 2D-3D-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured3D: A large photo-realistic dataset for structured 3D modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roitberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9478" to="9491" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MetaFormer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">CycleMLP: A MLP-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">AS-MLP: An axial shifted MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the robustness in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DPT: Deformable patch-based transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DAFormer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Strip pooling: Rethinking spatial pooling for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context prior for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining contextual information beyond image for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2375" to="2398" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Covariance attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lasang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1805" to="1818" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">CTNet: Context-based tandem network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CSWin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Contextual transformer networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">P2T: Pyramid pooling transformer for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12011</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-scale high-resolution vision transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">MLP-mixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Vision permutator: A permutable MLP-like architecture for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Restricted deformable convolution-based road scene semantic segmentation using surround view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4350" to="4362" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wood-Scape: A multi-task, multi-camera fisheye dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Perrotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;dea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sistu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uric?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semantic cameras for 360-degree environment perception in automated urban driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petrovai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Semantic segmentation of panoramic images using a synthetic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Semantic segmentation of outdoor panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Orhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bastanlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIVP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="650" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Distortion convolution module for semantic segmentation of panoramic images based on the imageforming principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIM</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>IV</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Waymo open dataset: Panoramic video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Panoramic panoptic segmentation: Insights into surrounding parsing for mobile agents via unsupervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10711</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Spherical CNNs on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">SpherePHD: Applying CNNs on a spherical PolyHeDron representation of 360?i mages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Equivariant networks for pixelized spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Complementary bi-directional feature compression for indoor 360?semantic segmentation with self-distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02437</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pano-SfMLearner: Self-Supervised multi-task learning of depth and semantics in panoramic videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="832" to="836" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DeepPanoContext: Panoramic 3D scene understanding with holistic scene context graph and relation-based optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Vision transformer with deformable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Vision transformer with progressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Not all images are worth 16x16 words: Dynamic vision transformers with adaptive sequence length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">DynamicViT: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A-ViT: Adaptive tokens for efficient vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Evo-ViT: Slow-fast token evolution for dynamic vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dynamic group transformer: A general vision transformer backbone with dynamic group attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Class-balanced pixel-level self-labeling for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Domain-agnostic prior for transfer semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Improving semantic segmentation via efficient self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">DecoupleNet: Decoupled network for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">SePiCo: Semantic-guided pixel contrast for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08808</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A nonadversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Contextual-relation consistent domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Content-consistent matching for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Uncertainty reduction for model adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Prototypical contrast adaptation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Unsupervised intra-domain adaptation for semantic segmentation through selfsupervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">PIT: Position-invariant transform for cross-FoV domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Prototypical cross-domain self-supervised learning for few-shot unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Semantic-driven generation of hyperlapse from 360 degree video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2610" to="2621" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Deformable convolutional networks,&quot; in ICCV</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">KITTI-360: A novel dataset and benchmarks for urban scene understanding in 2D and 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">SELMA: Semantic large-scale multimodal acquisitions in variable weather, daytime and viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Testolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Giordani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zorzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09788</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">SynWoodScape: Synthetic surround-view fisheye camera dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Sekkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>RA-L, 2022</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">The OmniScape dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Sekkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Fast-SCNN: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P K</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">In defense of pretrained ImageNet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RA-L</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">ResNeSt: Splitattention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Universal semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Seamless scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">ISSAFE: Improving semantic segmentation in accidents by fusing event-based data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Tangent images for mitigating spherical distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">PVT v2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="424" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">BDD100K: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
