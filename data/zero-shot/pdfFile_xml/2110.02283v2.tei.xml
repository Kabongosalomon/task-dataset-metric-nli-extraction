<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-training an Unsupervised Constituency Parser with Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nickil</forename><surname>Maveli</surname></persName>
							<email>n.maveli@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Co-training an Unsupervised Constituency Parser with Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two classifiers, we show that the interplay between them helps improve the accuracy of both, and as a result, effectively parse. A seed bootstrapping technique prepares the data to train these classifiers. Our analyses further validate that such an approach in conjunction with weak supervision using prior branching knowledge of a known language (left/right-branching) and minimal heuristics injects strong inductive bias into the parser, achieving 63.1 F 1 on the English (PTB) test set. In addition, we show the effectiveness of our architecture by evaluating on treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art results. 1 PCFGs (L-PCFGs;  the notion of inside trees versus out-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (PLMs) have become a standard tool in the Natural Language Processing (NLP) toolkit, offering the benefits of learning from large amounts of unlabeled data while providing modular function in many NLP tasks that require supervision. Recent work has shown that PLMs capture different types of linguistic regularities and information, for instance, the lower layers capture phrase-level information which becomes less prominent in the upper layers <ref type="bibr" target="#b19">(Jawahar et al., 2019)</ref>, span representations constructed from these models can encode rich syntactic phenomena, like the ability to track subject-verb agreement <ref type="bibr" target="#b15">(Goldberg, 2019)</ref>, dependency trees can be embedded <ref type="figure">Figure 1</ref>: A depiction of a syntax tree, with the inside string as depicted by the sequence x i ? ? ? x j and the outside string as depicted by the sequence (x 1 ? ? ? x i?1 , x j+1 ? ? ? x n ) that provides external context for the inside representations. within the geometry of BERT's hidden states <ref type="bibr" target="#b17">(Hewitt and Manning, 2019)</ref>, and most relevantly to this paper, syntactic information via self-attention mechanisms <ref type="bibr" target="#b41">(Wang et al., 2019;</ref><ref type="bibr" target="#b21">Kim et al., 2020)</ref>.</p><p>We offer another perspective on the way PLMs represent syntactic information. We demonstrate the usability of PLMs to capture syntactic information by developing an unsupervised parsing model that makes heavy use of PLMs. The learning algorithm is light in the injection of hard bias to parse text, emphasizing the role of PLMs in capturing syntactic information.</p><p>Our approach to unsupervised parsing is inspired by recent work in the area of spectral learning for parsing <ref type="bibr" target="#b10">(Cohen et al., , 2013</ref> and unsupervised estimation of probabilistic context-free grammars (PCFGs; <ref type="bibr" target="#b7">Clark and Fijalkow, 2020)</ref>. At its core, our learning algorithm views the presence or absence of a node dominating a substring in the final parse tree as a latent variable, where patterns of co-occurrence of the string that the node dominates (the "inside" string) and the rest of the sentence (the "outside" string) dictate whether the node is present or not. With spectral learning for latent-variable side trees is important, but in our case, given that the trees are not present during learning, we have to further specialize it to extract information only from the strings.</p><p>Consider the diagram of a syntax tree in <ref type="figure">Figure 1</ref>, decomposed into two parts. Following the main notion in spectral learning, each of these parts (the orange part and the blue part) is a "view" of the whole tree that provides information on the identity of the node that spans the words x i ? ? ? x j . In the case of the tree being unobserved during training, we have to rely only on the substrings that are spanned by the blue part or the orange part, to hypothesize whether indeed a node exists there.</p><p>To represent the inside and outside views, we make use of PLMs. We encode these substrings, and then bootstrap a classifier that determines whether a given span is a constituent or not. The bootstrapping process alternates between the two views, and at each point adds predictions on the training set that it is confident about to train a new classifier. This can be thought of as a form of co-training <ref type="bibr" target="#b43">(Yarowsky, 1995;</ref><ref type="bibr" target="#b2">Blum and Mitchell, 1998)</ref>, a training technique that relies on multiple views of training instances. We formulate the task of identifying constituents and distituents (referring to spans that are not constituents) in a sentence as a binary classification task by devising a strategy to convert the unlabeled data into a classification task. Firstly, we build a sequence classification model by fine-tuning a Transformer-based PLM on the unlabeled training sentences to distinguish between the true and false inside strings of constituents. Secondly, we use the highly-confident inside strings to produce the outside strings. Additionally, through the use of semi-supervised learning techniques, we jointly use both the inside and outside passes to enrich the model's ability to determine the breakpoints in a sentence. Our final model achieves 63.1 sentence F 1 averaged over multiple runs with random seed on the Penn Treebank test set. We also report strong results for the Japanese and Chinese treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation and Inference</head><p>We give a treatment to the problem of unsupervised constituency parsing. In that setup, the training algorithm is given an unlabeled corpus (set of sentences) and its goal is to learn a function mapping a sentence x to an unlabeled phrase-structure tree y that indicates the constituents in x. In previous work with models such as the Constituent-Context Model (CCM; <ref type="bibr" target="#b24">Klein and Manning 2002)</ref>, the Dependency Model with Valence (DMV; <ref type="bibr" target="#b25">Klein and Manning 2005)</ref>, and Unsupervised Maximum Likelihood estimator for Data-Oriented Parsing <ref type="bibr">(UML-DOP;</ref><ref type="bibr" target="#b3">Bod 2006)</ref>, the parts of speech (POS) of the words in x are also given as input both during inference and during training, but we do not make use of such POS tags.</p><p>Inference While our learning algorithm is grammarless, for inference we make use of a dynamic programming algorithm, akin to CYK, to predict the parse tree. Inference assumes that each possible span in the tree was scored with a score function s(i, j) where i and j are endpoints in the sentence. The score function is learned through our algorithm. We then proceed by finding the tree t * such that:</p><formula xml:id="formula_0">t * = arg max t?T (i,j)?t s(i, j),</formula><p>where T is the set of possible binary trees over the sentence and (i, j) ? t, with a slight abuse of notation, denotes that the span (i, j) appears in t. When s(i, j) is the probability of a span (i, j) being in the correct tree, this formulation gives the tree with the highest expected number of correct constituents <ref type="bibr" target="#b16">(Goodman, 1996)</ref>. This formulation has been used recently by several unsupervised constituency parsing algorithms <ref type="bibr">(Kim et al., 2019b,a;</ref><ref type="bibr" target="#b5">Cao et al., 2020;</ref><ref type="bibr" target="#b26">Li et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Algorithm</head><p>At the core of our approach lies the notion of inside and outside strings. For a given sentence x = x 1 ? ? ? x n and a span (i, j), the inside string of span (i, j) is the sequence x i ? ? ? x j while the outside string is the pair (x 1 ? ? ? x i?1 , x j+1 ? ? ? x n ). We denote by h in (i, j) representations for inside strings and h out (i, j) representations for outside strings. Both are vectors derived from a PLM (RoBERTa; <ref type="bibr" target="#b28">Liu et al. 2019</ref>, as we see later).</p><p>These two types of strings provide two views of a given possible splitting point in the syntax tree. We offer three ways, with increasing complexity, to bootstrap a score function that helps identify whether a node should dominate a given span. The main idea behind this bootstrapping is to start with a small seed set of training examples <ref type="bibr">(x, i, j, b)</ref> where (i, j) is a span in a sentence x and b ? {0, 1}, depending on whether the span (i, j)</p><p>is dominated by a node in the syntactic tree or not. Bootstrapping the seed set is dependent only on either the inside string or the outside string, and the corresponding classifier built from this bootstrapped seed set returns a probability p(b | x, i, j). Once a classifier is learned using the bootstrapping seed set, the classifier is applied on the training set, and the seed set is added to more examples where the classifier is confident of the label b. This is also known as self-training <ref type="bibr" target="#b30">(McClosky et al., 2006</ref><ref type="bibr" target="#b31">(McClosky et al., , 2008</ref>.</p><p>In the next three sections, we present three learning algorithms of increasing complexity in their use of inside and outside strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Using Inside Strings</head><p>The inside model m in which is modeled at a sentence level, computes an inside score s in (i, j) from the inside vector representation h in (i, j) of each span in the unlabeled input training sentence U. To compute h in (i, j), we fine-tune the sequence classification model that encodes a fixed-vector representation for each token in the dataset. This captures the phrase information of the inner content in the span. In order to prepare the features for the inside model, we make use of a seed bootstrapping technique (Section 4.2.1). Once we build the inside model m in , we get the most confidently-classified inside strings from U based on a set threshold ? = (? min , ? max ). Here, ? min and ? max , form the confidence bounds to select distituents and constituents respectively. We select a random sample of c constituents and d distituents with appropriate labels from these most confident inside strings comprising the labeled inside set I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Using Inside and Outside Strings</head><p>To perform the iterative self-training procedure, we follow the steps as detailed in <ref type="figure" target="#fig_0">Figure 2</ref>. While building the outside model, we extract the tokens at the span boundaries of the pair of outside strings, which is of the form consisting of the triple (x i?1 , [MASK], x j+1 ). The outside model computes an outside score s out (i, j) from the outside vector representation h out (i, j) of each span, which models the contextual information of the span. To compute h out (i, j), we extract the triple for every span (i, j) in the dataset and fine-tune another sequence classification model that encodes a fixed-vector representation for each triple.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">An Iterative Co-training Algorithm</head><p>Co-training <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>) is a classic multi-view training method, which trains a classifier by exploiting two (or more) views of the training instances. Our final learning algorithm is indeed inspired by it, where we consider the inside and the outside strings to be the two views. Once we have the inside m in and the outside classifiers m out that are trained on their respective conditionally independent inside h in (i, j) and outside h out (i, j) feature sets, we can make use of an iterative approach. At each iteration, only the inside strings? that are confident to be likely the insides of constituents and distituents according to the outside model are moved to the labeled training set of the inside model I. Thus, the outside model (teacher) provides the labels to the inside strings on which the inside model (student) is uncertain. Similarly, only the outside strings? that are confident to be the likely outsides of constituents and distituents according to the inside model are moved to the labeled training set of the outside model O.</p><p>Thus, the inside model provides the labels to the outside strings on which the outside model is uncertain. We describe the steps in <ref type="figure" target="#fig_1">Figure 3</ref>. Finally, we combine the scores obtained by the inside and the outside model to get the score s(i, j) for each span:</p><formula xml:id="formula_1">s(i, j) = s in (i, j) ? s out (i, j).</formula><p>Co-training requires the two views to be independent of each other conditioned on the label of the Inputs: I is the set of labeled inside sentences; O is the set of labeled outside sentences; U is a set of unlabeled sentences.</p><p>Algorithm: Loop for K iterations:</p><p>? Choose c pseudo-constituents and d pseudo-distituents from the most confidently predicted outside strings?out from U based on ? ? Extract the inside strings? corresponding to the c pseudo-constituents and d pseudo-distituents of outside ? I = I ?? ? Train the inside model min based on hin(i, j) derived from I ? Choose c pseudo-constituents and d pseudo-distituents from the most confidently predicted inside strings?in from U based on ? ? Extract the outside strings? corresponding to the c pseudo-constituents and d pseudo-distituents of inside</p><formula xml:id="formula_2">? O = O ?? ? Train the outside model mout based on hout(i, j) derived from O</formula><p>Output: Two models min, mout, that predict the inside and outside scores for unlabeled sentences. We combine these predictions by multiplying together and optionally re-normalizing their class probability scores. training instance. This is the type of assumption that, for example, PCFGs satisfy, when breaking a tree into an outside and inside tree: the two trees are conditionally independent given the nonterminal that connects them. In our case, we satisfy this assumption by creating inside and outside string representations separately, as we see later in Section 4. <ref type="figure">Figure 4</ref> illustrates the underlying pipeline of our weakly supervised parsing framework in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section, we describe our experimental setup: the data we use, the exact details of the experimental use of our approach to unsupervised parsing, and our evaluation methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We evaluate our methodology on the Penn Treebank (PTB; <ref type="bibr" target="#b29">Marcus et al. 1993</ref>) with the standard splits (2-21 for training, 22 for validation, 23 for test). For preprocessing, we keep all punctuation and remove any trailing punctuation. To maintain the unsupervised nature of our experiments, we avoid the common practice of using gold parses of the validation set for either early stopping <ref type="bibr" target="#b35">(Shen et al., 2018</ref><ref type="bibr" target="#b36">(Shen et al., , 2019</ref><ref type="bibr" target="#b13">Drozdov et al., 2019)</ref> or hyperparameter tuning <ref type="bibr" target="#b22">(Kim et al., 2019a)</ref>. Addition-ally, we experiment on Chinese with version 5.1 of the Chinese Penn Treebank (CTB; <ref type="bibr" target="#b42">Xue et al. 2005)</ref> with the same splits as in <ref type="bibr" target="#b6">Chen and Manning (2014)</ref>, and the Japanese Keyaki Treebank (KTB; <ref type="bibr" target="#b4">Butler et al. 2012)</ref>. For KTB, we shuffle the corpus and use 80% of the sentences for training, 10% for validation, and 10% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-view Learning</head><p>In this section, we devise the task of identifying constituents in a sentence by training two models with different views of the data. Ideally, these views complement each other and help each model improve the performance of the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Seed Bootstrapping</head><p>We treat identifying constituents from unlabeled sentences as a sequence classification task. To generate the constituent class, we take the complete sentence (start:end), as a sentence in itself is a constituent, and also the largest among all of its other constituents. To generate the distituent class, we take (start:end-1), ? ? ? , (start:end-6) slices, where start and end denote the 0 th and N th position (sentence length) respectively. We select the distituents in this manner because the longer the sentence, there would be a significantly unlikely chance that the span of the constituents extends till the very end of the sentence. Additionally, we make use of casing-specific information by adding contiguous title-case words while allowing only the apostrophe mark. Since all of the sentences for the constituent class start with capital letters, we identify the most common first word and generate lower-case equivalents of contiguous title-case words, which starts with it to account for bias due to the casing of spans. While we do use a fixed template to perform the seed bootstrapping process, this is part of the inductive bias of the algorithm, and is relatively easy to acquire. In our analysis, we assume the language is already known before and thereby its structure (left/right-branching), a form of weak supervision.</p><p>For CTB, we follow the exact same process as PTB for preparing the input data for the firstlevel sequence classifier, but we do not rely on case-specific information and perform no postprocessing. Meanwhile, since KTB is a treebank of a strongly left-branching language, we design our modeling approach slightly differently compared to before, although along the same style. To prepare the data for the sequence classifier, we choose (S (S (S the) (S boy)) (S (S ate) (S (S the) (S apple)))) <ref type="figure">Figure 4</ref>: Block diagram detailing our approach. We perform the self-training procedure for five iterations which follow multiple steps; (I): Fine-tune a RoBERTa BASE model (teacher) on a downstream task using a cross-entropy loss after seed bootstrapping; (II): Synthetically annotate this data using the teacher model and select top K samples corresponding to each class to form the final synthetic dataset; We fine-tune a RoBERTa BASE model (student) on this dataset using hard labels and retrieve the outside strings from the most confident insides; (III): Train the outside classifier on these outside strings; We perform the co-training procedure for two iterations which follow a two-fold optimizing step; (IV): Retrieve the inside strings from the most confident outsides and train the inside classifier; (V): Retrieve the outside strings from the most confident insides and train the outside classifier. the slice (start:end) in the sentence to label the constituent class, whereas, (start+1:end), ? ? ? , (start+4:end) slices are chosen to label the distituent class. We also split the sentences on " * " mark and treat the resulting fragmented parts as constituents too. Our training does not depend on the development set with the gold-standard annotated trees since we base the necessary string slicing decision on the feedback from the validation split after the bootstrapping procedure in an iterative fashion (increment/decrement the value of slice counter by 1) until we see a degradation in performance (measured using F 1 score) on the synthetic set of seed constituents and distituents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Inside Model</head><p>We fine-tune the RoBERTa model with a sequence classification layer on top using a cross-entropy loss (see Section A.1 in Appendix for training and hyperparameter details). As we supply input data, the entire pre-trained RoBERTa BASE model and the additional untrained classification layer is trained on our specific downstream task. To compute h in (i, j), we run the RoBERTa BASE model and retrieve the [CLS] token representation for the span enclosed between the i th and the j th element. The inside model is evaluated on MCC (Matthews Correlation Coefficient) as well as F 1 because the classes are imbalanced. After fine-tuning, our best inside model achieves 0.62 MCC and 0.91 F 1 on the internal validation set. Finally, we fine-tune the inside model on the unlabeled training sentences that generates an inside score s in (i, j) for every span. Since our major focus was on PTB, we have listed a few heuristics that inject further bias into the algorithm acting as the another form of weak supervision. Moreover, incorporating such rules was not necessary for CTB and KTB as our models showed superior performance without them.</p><p>Once we compute the inside score, s in (i, j), we use the following refinement strategies to prune out false constituents: We delete any constituent if it starts or ends with the most common word succeeding the comma punctuation. Additionally, we take the most common starting word and check if its accompanying word does not belong to the NLTK stop words list. We assign the scores of these corresponding spans in the CYK chart cell to the maximum value. Intuitively, from the linguistic definition of constituents, we refrain from bracketing if we identify a contiguous group of rare titlecase or uppercase words (tokens not in the top 100 most frequent list in the PTB training sentences). These heuristics only contribute to a certain extent in making the parser strong, and should be considered as a standard post-processing step. Overall, we observe 3.8 F 1 improvements in the case of the inside model. We further note that the contribution due to additional heuristics is much less than the combined self-training and co-training gains since their effect becomes insignificant after multiple iterations of the self-training process due to the predictions approximately following the template rules. As described in <ref type="figure" target="#fig_0">Figure 2</ref>, we perform selftraining on the inside model for five iterations. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Outside Model</head><p>We extract the outside strings of spans having the inside score satisfying a pre-determined cutoff value. The Constituent-Context Model <ref type="bibr" target="#b24">(Klein and Manning, 2002)</ref> use a smoothing ratio of 1:5 (constituents to distituents) for the WSJ-10 section to take into account the skewness of random spans more likely to represent distituents. In the same vein, the values of lower and upper bounds of the threshold are chosen to ensure the distribution of class labels is about 1:10 (with the distituent class being the majority) which is a crude estimate considering much larger sentence lengths in the WSJ-Full section. Moreover, from a linguistic standpoint, we can be certain that the distituents must necessarily outnumber the constituents. For the self-training experiments, we set the thresholds, ? min as 0.0005 and ? max as 0.995. We treat the outside strings satisfying the upper and lower bounds of the threshold as gold-standard outside of constituents and distituents respectively. To compute h out (i, j), we run the RoBERTa BASE model on leftoutside, i.e., (i ? 1) th element and right-outside, i.e., (j + 1) th element, along with a [MASK] placeholder token separating the two, and extract the [CLS] token representation. As done previously, we fine-tune the outside model on the unlabeled training sentences that generates an outside score s out (i, j) for every span.  <ref type="bibr" target="#b36">(Shen et al., 2019)</ref> 47.7 49.4 63.9 -Tree Transformer ? <ref type="bibr" target="#b41">(Wang et al., 2019)</ref> 50.5 52.0 66.2 -Neural PCFG ? <ref type="bibr" target="#b22">(Kim et al., 2019a)</ref> 50.8 52.6 64.6 -DIORA <ref type="bibr" target="#b13">(Drozdov et al., 2019)</ref> -58.9 60.5 -Compound PCFG ? <ref type="bibr" target="#b22">(Kim et al., 2019a)</ref> 55.2 60.1 70.5 -S-DIORA ? <ref type="bibr" target="#b12">(Drozdov et al., 2020)</ref> 57.6 64.0 71.8 -Constituency Test <ref type="bibr" target="#b5">(Cao et al., 2020)</ref> 62.8 65.9 68.   <ref type="bibr" target="#b22">Kim et al. (2019a)</ref> and take the baseline numbers of certain models from <ref type="bibr" target="#b22">(Kim et al., 2019a;</ref><ref type="bibr" target="#b5">Cao et al., 2020)</ref>. ? denotes models trained without punctuation and denotes models trained on additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Jointly Learning with Inside and Outside Models</head><p>Once we have the outside model, we run it on the training sentences and choose the outside string that the classifier is highly confident about. We extract their inside strings again using the same bounds of the threshold as done previously and retrain the inside model on the old highly confident inside strings along with the new inside strings obtained from the highly confident outside strings. Similarly, the same technique can be applied to the outside model to augment its input data too. We repeat this process twice <ref type="figure" target="#fig_1">(Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We report the F 1 score with reference to gold trees in the PTB test set (section 23). Following prior work <ref type="bibr" target="#b22">(Kim et al., 2019a;</ref><ref type="bibr" target="#b35">Shen et al., 2018</ref><ref type="bibr" target="#b36">Shen et al., , 2019</ref><ref type="bibr" target="#b5">Cao et al., 2020)</ref>, we remove punctuation and collapse unary chains before evaluation, and calculate F 1 ignoring trivial spans, i.e., single-word spans and whole-sentence spans, and we perform the averaging at sentence-level (macro average) rather than span-level (micro average), which means that we compute F 1 for each sentence and later average over all sentences. We also mention the oracle upper bound, which is the highest possible score with binarized trees since we compare them against  non-binarized gold trees according to the convention, as most unsupervised parsing methods output fully binary trees. We additionally use the standard PARSEVAL metric computed by the evalb program. 3 Although evalb calculates the micro average F 1 score, it differs from our micro average metric in the sense that it counts the whole sentence spans, and calculates duplicated spans instead of removing them. Following the recommendations put forth by previous work that has done a comprehensive empirical evaluation on this topic <ref type="bibr" target="#b27">(Li et al., 2020b)</ref>, we report results on both length ? 10 as well as all-length test data.  <ref type="bibr" target="#b38">Shi et al. (2021)</ref> in our analysis because their boost in the performance is contingent on the nature of the supervision data (especially the QA-SRL dataset) rather than on the actual learning process itself. Furthermore, the authors mention that a vast amount of hyperlinks match syntactic constituents, hence restricting the scope for the actual algorithm to derive meaningful trees.  <ref type="table">Table 3</ref>: Evalb F 1 on the full (F 1 -all) and length ? 10 (F 1 -10) sentences of the KTB test set discarding punctuation corresponding to KTB-40 and KTB-10, respectively. We take the baseline numbers of models from <ref type="bibr" target="#b27">Li et al. (2020b)</ref>. See <ref type="table" target="#tab_17">Table 7</ref> to view the hyperparameters used for evalb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>(right-branching) and Japanese (left-branching). We use Transformer models for the representations of the spans for both Chinese and Japanese. See Section A.1 in the Appendix for training details. Tables 2 and 3 shows the results for CTB and KTB respectively. Moreover, we do not include a few models chosen previously for PTB during our analysis, as extending those models for CTB or KTB is non-trivial due to several reasons: such as lack of domain-related datasets (as DIORA uses SNLI and MultiNLI for training), and lack of linguistic knowledge expertise (not easily cross-lingual transferable notion for designing constituency tests). <ref type="figure" target="#fig_4">Figure 8</ref> in the Appendix shows step-wise qualitative analysis for a sample sentence taken from the PTB training set. See <ref type="figure">Figures 9 and 10</ref> in Appendix to see the visualization for an example tree at every stage of the pipeline for CTB and KTB respectively. As we can observe from all the example tree outputs, the parser using the inside and outside models after the co-training stage produces fewer crossing brackets than the vanilla inside model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Self-training</head><p>PLMs that possess rich contextualized textual representations can assist parsing when we have a large volume of unlabeled data. For this reason, we might expect that self-training in combination with pre-training adds no extra information to the finetuned parser. However, we find that self-training improves the performance of the parser by about 9.8%, demonstrating that self-training provides advantages complementary to the pre-trained contextualized embeddings (see <ref type="table" target="#tab_12">Table 5</ref> in Appendix for <ref type="bibr">PRPN</ref>   a more detailed analysis at different stages).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Co-training</head><p>The question of how to integrate multi-view information is important. One of the options would be to concatenate both the inside and outside vectors while performing training and inference. With this experiment setting, we see negligible improvement as it only scores 13.2 F 1 on the PTB test (without self-training). The whole idea of separating the two models for co-training is to learn constituent boundaries to identify the splitting points in a sentence through independent views of data. This corroborates the effectiveness of co-training compared with concatenation: the simple concatenation strategy cannot fully harvest the information corresponding to each view and indeed render the optimization intractable. After co-training, the parser achieves 63.1 F 1 averaged over four runs, outperforming the previous best-published result (see <ref type="table" target="#tab_13">Table 6</ref> in Appendix to view the improvement at each step). <ref type="figure">Figure 6</ref> in Appendix compares the performance of different models over varying sentence length (see <ref type="figure">Figure 5</ref> in Appendix to understand the extent to which bootstrapping helps compared to the vanilla inside model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Distituent Selection</head><p>To understand the extent to which the type of the disitituent selection impacts the performance, we assess two settings on the PTB -random and leftbranching bias. In the random setting, we select distituents from the slice (start:r), where r is a random number generated between start+1 and end-1, both inclusive. This produces 19.3 F 1 for the inside model. Whereas, in the left-branching bias setting, we prepare the seed bootstrapping process as explained in the Section 4.2.1 similar to KTB (a left-branching treebank). This results in 11.2 F 1 score for the inside model. Hence, the manner in which we perform the initial classification has a strong impact on the final tree structures. In the future, we would like to develop error analysis protocols for both CTB and KTB using human-in-the-loop process (leveraging feedback from the respective language experts) and provide an in-depth statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Linguistic Error Analysis</head><p>Recently, neural network-based approaches have shown promising results on inducing parse trees directly from words. Our weakly-supervised parser is comparable in behavior to a fully unsupervised parser as it does not rely on syntactic annotations. We highlight some themes most relevant to our method.</p><p>Learning from distant supervision. A related work to ours <ref type="bibr" target="#b38">(Shi et al., 2021)</ref> uses answer fragments and webpage hyperlinks to mine syntactic constituents for parsing. Many previous studies depend on punctuation as a strong signal to detect constituent boundaries <ref type="bibr" target="#b39">(Spitkovsky et al., 2013;</ref><ref type="bibr" target="#b33">Parikh et al., 2014)</ref>.</p><p>Incorporating bootstrapping techniques. Cotraining <ref type="bibr" target="#b43">(Yarowsky, 1995;</ref><ref type="bibr" target="#b2">Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b0">Abney, 2007)</ref> and self-training <ref type="bibr" target="#b40">(Steedman et al., 2003;</ref><ref type="bibr" target="#b30">McClosky et al., 2006;</ref><ref type="bibr" target="#b8">Cohen and Smith, 2010)</ref> are bootstrapping methods that attempt to convert a fully unsupervised learning problem to a semi-supervised learning form. More recently, <ref type="bibr" target="#b32">Mohananey et al. (2020)</ref>; <ref type="bibr" target="#b37">Shi et al. (2020)</ref>; <ref type="bibr" target="#b40">Steedman et al. (2003)</ref> have shown the benefits of using self-training as a standard post-hoc processing step for unsupervised parsing models.</p><p>Using inside-outside representations constructed with a latent tree chart parser. Drawing inspiration from the inside-outside algorithm <ref type="bibr" target="#b1">(Baker, 1979)</ref>, DIORA <ref type="bibr" target="#b13">(Drozdov et al., 2019)</ref> optimizes an autoencoder objective and computes a vector representation for each node in a tree by combining child representations recursively. To recover from errors and make DIORA more robust to local errors when computing the best parse in the bottom-up chart parsing, an improved variant of DIORA, S-DIORA <ref type="bibr" target="#b12">(Drozdov et al., 2020)</ref> achieves it.</p><p>Inducing tree structure by introducing an inductive bias to RNNs. PRPN <ref type="bibr" target="#b35">(Shen et al., 2018)</ref> introduces a neural parsing network that has the ability to make differentiable parsing decisions using structured attention mechanism to regulate skip connections in an RNN. ON-LSTM <ref type="bibr" target="#b36">(Shen et al., 2019)</ref> enables hidden neurons to learn information by a combination of gating mechanism as well as activation function. In URNNG, <ref type="bibr" target="#b23">Kim et al. (2019b)</ref> employs parameterized function over latent trees to handle intractable marginalization and inject strong inductive biases for the unsupervised learning of the recurrent neural network grammar (RNNG; <ref type="bibr" target="#b14">Dyer et al. 2016)</ref>. <ref type="bibr" target="#b34">Peng et al. (2019)</ref> introduces PaLM that acts as an attention component on top of RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enhancing PCFGs.</head><p>Compound PCFG <ref type="bibr" target="#b22">(Kim et al., 2019a)</ref> which consists of a Variational Autoencoder (VAE) with a PCFG decoder, found the original PCFG is fully capable of inducing trees if it uses a neural parameterization. <ref type="bibr" target="#b20">Jin et al. (2019)</ref> show that the flow-based PCFG induction model is capable of using morphological and semantic information in context embeddings for grammar induction. <ref type="bibr" target="#b44">Zhu et al. (2020)</ref> proposes neural L-PCFGs to simultaneously induce both constituents and dependencies.</p><p>Concerning PLMs. Tree Transformer <ref type="bibr" target="#b41">(Wang et al., 2019)</ref> adds locality constraints to the Transformer encoder's self-attention such that the attention heads resemble a tree structure. <ref type="bibr" target="#b21">Kim et al. (2020)</ref> extract trees from pre-trained transformers.</p><p>Refining based on constituency tests. With the help of transformations and RoBERTa model to make grammaticality decisions, <ref type="bibr" target="#b5">(Cao et al., 2020)</ref> were able to achieve strong performance for unsupervised parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a simple yet effective method of inducing constituency trees which is the first of its kind in achieving performance comparable to the supervised binary tree RNNG model and setting a new state-of-the-art result for unsupervised parsing using weak supervision. Our model generalizes to multiple languages of known treebanks. We have done comprehensive linguistic error analysis showing a step-by-step breakdown of the F 1 performance for the inside model, inside model with self-training, and the inside-outside model with a co-training-based approach. The effectiveness of our multi-view learning strategy is evident in our experiments. Future work could aim to augment the parser's capabilities to investigate cross-domain generalization and efficient cross-lingual transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training Details</head><p>We use the Adam optimizer and, on the bootstrapped dataset, fine-tune roberta-base consisting of default 125M trainable parameters with a learning rate 3e ? 5, batch size 32, maximum epochs 10, maximum sequence length 256, and gradient checkpointing for all our models. The values were chosen as default based on sequence classification tasks on the GLUE benchmark 5 as mentioned in HuggingFace Transformers. <ref type="bibr">6</ref> We use a train/validation random split of 80/20 on the bootstrapped dataset which contains 100,000 sentences (50,152 for the distituent class and 49,848 for the constituent class) to monitor the validation loss and perform early stopping. The average sentence length is about 22 tokens. Note that the development set of PTB is kept untouched. We set the patience value at 2. Model checkpointing, as well as logging, is carried out after every 100 steps.</p><p>We use a p3.8xlarge AWS instance with a single GPU having 64 GB memory to conduct all our experiments. The estimated training time for the inside model is about 0.2h, inside model with self-training (3 iterations) is about 12h, and insideoutside model with co-training (2 iterations) is about 18h. While the inference time for all the models is roughly 1h.</p><p>For the Chinese monolingual experiment, we use bert-base-chinese which is trained on cased Chinese Simplified and Traditional text, and for Japanese monolingual experiment, we use cl-tohoku/bert-base-japanese which is trained on Japanese Wikipedia available at https://huggingface.co/models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>We tried several strategies to augment the distituent class for our models, but without concrete gains. Some of those include word deletion (randomly selects tokens in the sentence and replace them by a special token), span deletion (Same as word deletion, but puts more focus on deleting consecutive words), reordering (randomly sample several pairs of span and switch them pairwise) and substitution (sample some words and replace them with synonyms). <ref type="bibr">2-6 7-11 12-16 17-21 22-26 27-31 32-36 37-41 42-46 47-51 52-56 57-61</ref> Sentence Length  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Effect of Bootstrapping</head><p>As shown in <ref type="figure">Figure 5</ref>, the final model with cotraining identifies constituents from shorter sentences (WSJ-10) much more precisely compared to the rest of the models. There is a lower performance in F 1 around sentence length of 50-55 zone, but that improves for longer sentences. 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Stages of Self-training</head><p>Self-training boosts the performance of the inside model by 5.5 F 1 points as shown in <ref type="table" target="#tab_12">Table 5</ref>. As can be seen, the effect of the initial set of candidate constituents and distituents on the final performance is 55.9 F 1 which is not insignificant. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Stages of Co-training</head><p>After co-training, the performance of the insideoutside joint model increases by 1.7 F 1 points as shown in  ently possesses contextual knowledge due to being trained on a large corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Unsupervised Labeled Parsing</head><p>We explore unsupervised labeled constituency parsing to identify meaningful constituent spans such as Noun Phrases (NP) and Verb Phrases (VP) to see if the parser can extract such labels. Labeled parsing is usually evaluated on whether a span has the correct label. We can effectively induce span labels using the clustering of the learned phrase vectors from the inside and outside strings. When labeling a gold bracket, our method achieves 61.2 F 1 on the full PTB test set and is comparable with the current best model, DIORA. See <ref type="figure">Figure 7</ref> to view the visualization of induced and linguistic alignment. RoBERTa does not strictly output word-level vectors. Rather, the output are subword vectors which we aggregate with meanpooling to achieve a word-level representation using SentenceTransformers. <ref type="bibr">9</ref> We use 600 codes while doing the clustering initially, such that we are left with about 25 clusters after the most common label assignment process, i.e., the number of distinct phrase types. The phrase clusters are assigned to {'NP': 7, 'PP': 5, 'WHPP': 3, 'ADVP': 3, 'ADJP': 2, 'S': 2, 'WHADVP': 1, 'UCP': 1, 'VP': 1, 'PRN': 1, 'QP': 1, 'SBAR': 1, 'WHNP': 1, 'CONJP': 1}</p><p>according to the majority gold labels in that cluster. These 14 assigned phrase types correspond with the 14 most frequent labels. <ref type="table" target="#tab_19">Table 8</ref> lists the induced non-terminal grouped across different clusters and also their correctness in identifying the gold labels. The further course of action would be to have a joint single model that is capable of achieving both bracketing and labeling. Further, these induced labels can function as features for the inside and outside models to achieve even better predictive ability. It also warrants a multi-lingual exploration in this area. A.6 Non-Terminal Label Alignment <ref type="figure">Figure 7</ref> shows the alignment between gold and induced labels. We observe that some of the induced non-terminals clearly align to linguistic nonterminals. For instance, S-2 non-terminal has a high resemblance with NP. Similarly, S-8 has a high resemblance with ADVP.    <ref type="figure">Figure 7</ref>: Alignment between induced and gold labels of the top-performing clusters. We cluster the constituent inside vectors derived from the ground truth parse (without labels) using the K-Means algorithm and assign each constituent with the most common label within its cluster. Accuracy is the probability of correctly predicting the most common label.  <ref type="figure">Figure 9</ref>: Example tree taken from the CTB training set. After the co-training procedure (b), the parser correctly identifies constituents "???????", "???????????", and "?????????????" compared to the previous step using the inside model (a). It only makes 3 errors due to crossing brackets at "?? ???????", "???????", and "????????".  <ref type="figure">Figure 10</ref>: Example tree taken from the KTB training set. After the co-training procedure (b), the parser correctly identifies constituents "??? ?", "? ?", "*hearer* ??? ? ? ? *?* ?? ?? ?? ???", "*pro* ????", "?? ?", and "?? ? ?", while incorrectly tagging "???????? ??? ?? ?" as a distituent compared to the previous step using the inside model (a).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituent</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our self-training algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our co-training algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 5: F 1 grouped by sentence length on the PTB test set for different strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F1Figure 8 :F1</head><label>8</label><figDesc>Displays the parse tree output for a sample sentence: (a) Using Inside (b) Using Inside and Outside (c) Gold Tree. After the co-training procedure (b), the parser correctly identifies constituents "the new post" and "of world-wide advanced materials operations" which were earlier identified as distituents by the inside model (a). It makes two errors due to crossing brackets -namely "of vice president", "the new post of vice president", and "the new post of vice president of world-wide advanced materials operations".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Unlabeled sentence-level F 1 on the full as well as sentences of length ? 10 of the PTB test set without punctuation or unary chains. We evaluate each model using the evaluation script provided by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Unlabeled sentence-level F 1 on the CTB test set. We evaluate each model using the evaluation script provided by Kim et al. (2019a) and take the baseline numbers also from Kim et al. (2019a).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>shows the unlabeled F 1 scores for our</cell></row><row><cell>model compared to existing unsupervised parsers</cell></row><row><cell>on PTB. The vanilla inside model is in itself com-</cell></row><row><cell>petitive and is already in the range of previous</cell></row><row><cell>best models like DIORA (Drozdov et al., 2019),</cell></row><row><cell>Compound PCFG (Kim et al., 2019a). 4 See Ap-</cell></row><row><cell>pendix A.5 to assess our model's performance on</cell></row><row><cell>unsupervised labeled parsing.</cell></row><row><cell>We further evaluate how our method works for</cell></row><row><cell>languages with different branching types -Chinese</cell></row><row><cell>3 https://nlp.cs.nyu.edu/evalb</cell></row></table><note>4 We do not include the results of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Average recall per constituent category (i.e. label recall) in (%). The results of PRPN, ON, URNNG, and Compound PCFG are taken from<ref type="bibr" target="#b22">Kim et al. (2019a)</ref>, S-DIORA from<ref type="bibr" target="#b12">Drozdov et al. (2020)</ref>, and Constituency Test from<ref type="bibr" target="#b5">Cao et al. (2020)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4</head><label>4</label><figDesc>When a definite article is linked with a singular noun, the inner spans need to be shelved, accommodating the larger span with the definite article. E.g.: the [ stock market ]</figDesc><table><row><cell>shows that our model achieves strong ac-</cell></row><row><cell>curacy while predicting all the phrase types except</cell></row><row><cell>for the Adjective Phrase (ADJP). We list some of</cell></row><row><cell>the most common mistakes our parse makes and</cell></row><row><cell>suggest likely explanations for each:</cell></row><row><cell>Bracketing inner NP of a definite Noun</cell></row><row><cell>Phrase. Grouping NP too early overlooking broader</cell></row><row><cell>context. Due to the way it is trained, the parser ag-</cell></row><row><cell>gressively groups rare words in the corpus. Build-</cell></row><row><cell>ing a better outside model can fix this type of error</cell></row><row><cell>to a considerable extent. E.g.: Shearson [ Lehman</cell></row><row><cell>Hutton ] Inc.</cell></row><row><cell>Omitting conjunction joining two phrases. It</cell></row><row><cell>shows poor signs of understanding co-ordination</cell></row><row><cell>cases in which conjunction is an adjacent sibling</cell></row><row><cell>of the nodes being shifted, or is the leftmost or</cell></row><row><cell>rightmost node being shifted. E.g.: Notable [ &amp;</cell></row><row><cell>Quotable ]</cell></row><row><cell>Confusing contractions with Possessives. Due</cell></row><row><cell>to the presence of a lot of contraction phrases like</cell></row><row><cell>(they're, it's), the parser confuses it with that of</cell></row><row><cell>the Possessive NPs, causing unnecessary splitting.</cell></row><row><cell>Expanding the contractions can be a good way to</cell></row><row><cell>correct these systematic errors. E.g.: the company</cell></row><row><cell>[ 's $ 488 million in 1988 ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Unlabeled sentence-level F 1 on the full PTB</cell></row><row><cell>test set after applying the iterative Self-training algo-</cell></row><row><cell>rithm on the Inside model.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>. Compared to using self-training,</cell></row><row><cell>one of the reasons the benefit is not significant</cell></row><row><cell>may be attributed to the fact that the inside vec-</cell></row><row><cell>tors (built upon Transformer architecture) inher-</cell></row><row><cell>7 For evaluating PTB and CTB, we use Yoon</cell></row><row><cell>Kim's script available at https://github.com/</cell></row><row><cell>harvardnlp/compound-pcfg. Whereas for evalu-</cell></row><row><cell>ating KTB, we use Jun Li's script available at https:</cell></row><row><cell>//github.com/i-lijun/UnsupConstParseEval.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>Unlabeled sentence-level F 1 on the full PTB test set after applying the iterative Co-training algorithm on the joint Inside and Outside model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>The hyperparameters used for evalb</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 :</head><label>8</label><figDesc>Investigation of phrase clusters that shows several syntactic properties. Clearly, there are patterns surrounding identification of people/organization names, time-related signals, quantities etc.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code and pre-trained models are available at https://github.com/Nickil21/ weakly-supervised-parsing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We only use the top 5K inside strings for self-training to cover maximum possible iterations as it is representative of the whole training set in terms of the average sentence length and punctuation marks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://gluebenchmark.com/ 6 https://huggingface.co/transformers/ v2.3.0/examples.html#glue</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For analysis purposes, we use the test set instead of the standard validation set to avoid tuning on the test set based on feedback received from the validation set to keep the nature of our experiments purely unsupervised.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the anonymous reviewers, Alexandra Birch, Frank Keller, Ankur Parikh, Marcio Fonseca, Ronald Cardenas, Zheng Zhao and Yftah Ziser for their feedback on earlier versions of this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Details</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-training and co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semisupervised Learning for Computational Linguistics</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="26" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trainable grammars for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech communication papers presented at th 97th Meeting of the</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979" />
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/279943.279962</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98</title>
		<meeting>the Eleventh Annual Conference on Computational Learning Theory, COLT&apos; 98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An all-subtrees approach to unsupervised parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
		<idno type="DOI">10.3115/1220175.1220284</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keyaki treebank: phrase structure with functional information for japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Hotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruriko</forename><surname>Otomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kei</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text Annotation Workshop</title>
		<meeting>Text Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised parsing via constituency tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.389</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4798" to="4808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consistent unsupervised estimators for anchored PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathana?l</forename><surname>Fijalkow</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00323</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="409" to="422" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1502" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A provably correct learning algorithm for latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1052" to="1061" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable pcfgs: Algorithms and sample complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">69</biblScope>
			<biblScope unit="page" from="2399" to="2449" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subendhu</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">O</forename><surname>&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4832" to="4845" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised labeled parsing with deep inside-outside recursive autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Pei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1507" to="1512" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing BERT&apos;s syntactic abilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1901.05287</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.3115/981863.981887</idno>
	</analytic>
	<monogr>
		<title level="m">34th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Santa Cruz, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1419</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep inside-outside recursive autoencoder with all-span objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyue</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.322</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3610" to="3615" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What does BERT learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of PCFGs with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lane</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Edmiston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compound probabilistic context-free grammars for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1105" to="1117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073106</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Natural language grammar induction with a generative constituent-context model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1407" to="1419" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heads-up! unsupervised constituency parsing via self-attention heads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An empirical comparison of unsupervised constituency parsing methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.300</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3278" to="3283" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">When is self-training effective for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-training for unsupervised parsing with PRPN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhad</forename><surname>Mohananey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.iwpt-1.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</title>
		<meeting>the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral unsupervised parsing with additive tree metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1100</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1062" to="1072" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PaLM: A hybrid parser and language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1376</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3644" to="3651" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural language modeling by jointly learning syntax and lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Wei Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the role of supervision in unsupervised constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.614</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7611" to="7621" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning syntax from naturally-occurring bracketings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Ozanirsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2941" to="2949" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bootstrapping statistical parsers from small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ruhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Crim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tree transformer: Integrating tree structures into self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaushian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1098</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1061" to="1070" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<idno type="DOI">10.1017/S135132490400364X</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<idno type="DOI">10.3115/981658.981684</idno>
	</analytic>
	<monogr>
		<title level="m">33rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The return of lexical dependencies: Neural lexicalized PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00337</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="647" to="661" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
