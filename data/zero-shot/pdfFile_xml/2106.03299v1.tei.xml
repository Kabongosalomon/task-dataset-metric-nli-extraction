<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Instance Segmentation using Inter-Frame Communication Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukjun</forename><surname>Hwang</surname></persName>
							<email>sj.hwang@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miran</forename><surname>Heo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seoung</forename><forename type="middle">Wug</forename><surname>Oh</surname></persName>
							<email>seoh@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon</forename><forename type="middle">Joo</forename><surname>Kim</surname></persName>
							<email>seonjookim@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Video Instance Segmentation using Inter-Frame Communication Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a mean of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved the state-of-theart performance (AP 44.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code will be made available.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the growing interest toward the video domain in computer vision, the task of video instance segmentation (VIS) is emerging <ref type="bibr" target="#b0">[1]</ref>. Most of the current approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> extend image instance segmentation models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and take frame-wise inputs. These per-frame methods extend the concept of temporal tracking by matching frame-wise predictions of high similarities. The models can be easily customized to real-world applications as they run in an online <ref type="bibr" target="#b8">[9]</ref> fashion, but they show limitation in dealing with occlusions and motion blur that are common in videos.</p><p>On the contrary, per-clip models are designed to overcome such challenges by incorporating multiple frames while sacrificing the efficiency. Previous per-clip approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> aggregate information within a clip to generate instance-specific features. As the features are generated per instance, the number of instances in addition to the number of frames has a significant impact on the overall computation. Recently proposed VisTR <ref type="bibr" target="#b10">[11]</ref> adapted DETR <ref type="bibr" target="#b12">[13]</ref> to the VIS task and reduced the inference time by inserting the entire video, not a clip, to its offline end-to-end network. However, its full self-attention transformers <ref type="bibr" target="#b13">[14]</ref> over the space-time inputs involve explosive computations and memories. In this work, we raise the following question: can a per-clip method be efficient while attaining great accuracy?</p><p>To achieve our goal, we introduce Inter-frame Communication Transformers (IFC) to greatly reduce the computations of the full space-time transformers. Similar to recent works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> that alleviate the explosive computational growth inherent in attention-based models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>, IFC takes a decomposition strategy utilizing two transformers. The first transformer (Encode-Receive, E) encodes each frame independently. To exchange the information between frames, the second transformer (Gather-Communicate, G) executes attention between a small number of memory tokens that hold concise information of the clip. The memory tokens are utilized to store the overall context of the clip, for example "a hand over a lizard" in <ref type="figure">Fig. 1</ref>. The concise information assists detecting the lizard that is largely occluded by the hand in the first frame, without employing an expensive pixel-level attention over space and time. The memory tokens are only in charge of the communications between frames, and the features of each frame are enriched and correlated through the memory tokens.</p><p>We further reduce overheads while taking advantages of per-clip pipelines by concisely representing each instance with a unique convolutional weight <ref type="bibr" target="#b6">[7]</ref>. Despite the changes of appearances at different frames, the instances of the same identity share commonalities because the frames are originated from the same source video. Therefore, we can effectively capture instance-specific characteristics in a clip with dynamically generated convolutional weights. In companion with the segmentation, we track instances by uniformly applying the weights to all frames in a clip. Moreover, all executions of our spatial decoder are instance-agnostic except for the final layer which applies instance-specific weights. Accordingly, our model is highly efficient and also suitable for scenes with numerous instances.</p><p>In addition to the efficient modeling, we provide optimizations and an instance tracking algorithm that are designed to be VIS-centric. By the definition of AP VIS , the VIS task <ref type="bibr" target="#b0">[1]</ref> aims to maximize the objective similarity: space-time mask IoU. Inspired by previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, our model is optimized to maximize the similarity between bipartitely matched pairs of ground truth masks and predicted masks. Furthermore, we again adopt the similarity maximization for tracking instances of same identities, which effectively links predicted space-time masks using bipartite matching. As both of our training and inference algorithm are fundamentally designed to address the key challenge of VIS task, our method attains an outstanding accuracy.</p><p>From these improvements, IFC sets the new state-of-the-art by using ResNet-50: 42.8% AP and more surprisingly, in 107.1 fps. Furthermore, our model also shows great speed-accuracy balance under near-online setting, which leads to a huge practicality. We believe that our model can be a powerful baseline for video instance segmentation approaches that follow the per-clip execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video instance segmentation The VIS task <ref type="bibr" target="#b0">[1]</ref> extends the concept of tracking to the image instance segmentation task. The early solutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> follow per-frame pipeline, which utilize additional tracking head to the models that are mainly designed to solve image instance segmentation. More advanced algorithms that are recently proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> take video characteristics into consideration, which result in improved performance.</p><p>Per-clip models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> dedicate computations to extract information from multiple frames for higher accuracy. By exploiting multiple frames, per-clip models can effectively handle typical challenges in video, i.e., motion blurs and occlusions. Our model is designed to be highly efficient while following the per-clip pipeline, which leads to fast and accurate predictions.</p><p>Transformers Recently, transformers <ref type="bibr" target="#b13">[14]</ref> are greatly impacting many tasks in computer vision. After the huge success of DETR <ref type="bibr" target="#b12">[13]</ref>, which has brought a new paradigm to the object detection task, numerous vision tasks are incorporating transformers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> in place of CNNs. For classification tasks in both NLP and computer vision, many adopt an extra classification token to the input of transformers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref>. All the input tokens affect each other as the encoders are mainly composed of the self-attention, thus the classification token can be used to determine the class of the overall input. MaX-DeepLab <ref type="bibr" target="#b18">[19]</ref> adopted the concept of memory and proposed a novel dual-path transformer for the panoptic segmentation task <ref type="bibr" target="#b22">[23]</ref>. By making use of numerous memory tokens similar to the previous classification token, MaX-DeepLab integrates the transformer and the CNN by making both feedback itself and the other.</p><p>We further utilize the concept of the memory tokens to the videos. Using Inter-frame Communication Transformers, each frame runs independently while sharing their information with interim communications. The communications lead to higher accuracy while the execution independence between frames accelerates the inference. Transformer Encoder <ref type="figure">Figure 1</ref>: Overview of IFC framework. Our transformer encoder block has two components: 1) Encode-Receive (E) simultaneously encodes frame tokens and memory tokens. 2) Only memory tokens pass Gather-Communicate (G) to perform communications between frames. The outputs from the stack of N E encoder blocks goes into two modules, spatial decoder and transformer decoder, to generate segmentation masks.</p><formula xml:id="formula_0">? ? ? ? ? ? ! " ? ? # " ? ! " $ " ! ! # ! ! " # ! ! # # ! ! $ " $ " # " # " $ ! # $ ! $ % ! % ! % # % $ % # % % , % Spatial Decoder</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The proposed method follows a per-clip pipeline which takes a video clip as input and outputs clip-level results. We also introduce Inter-frame Communication Transformers, which can effectively share frame-wise information within a clip with a high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model architecture</head><p>Inspired by DETR <ref type="bibr" target="#b12">[13]</ref>, our network consists of a CNN backbone and transformer encoder-decoder layers <ref type="figure">(Fig. 1</ref>). The input clip is first independently embedded into a feature map through the backbone. Then, the embedded clip passes through our inter-frame communication encoder blocks that enrich the feature map by allowing information exchange between frames. Next, a set of transformer decoder layers that take the encoder outputs and object queries as inputs predict unique convolutional weights for each instance in the clip. Finally, the masks for each instance across the clip are computed in one shot by convolving the encoded feature map with the unique convolutional weight.</p><p>Backbone Given an input clip {x i } T i=1 ? R T ?H0?W0?3 , composed of T frames with 3 color channels, the CNN backbone processes the input clip frame-by-frame. As the result, the clip is encoded into a set of low-resolution features,</p><formula xml:id="formula_1">{f 0 i } T i=1 ? R T ?H?W ?C ,</formula><p>where C is the number of channels and H, W = H0 32 , W0 32 .</p><p>Inter-Frame Communication Encoder Given an image, humans can effortlessly summarize the scene with only a few words. Also, frames from a same video share a lot of commonalities, the difference between them is sufficiently summarized and communicated even with a small bandwidth. Based on this hypothesis, we propose an inter-frame communication encoder to make the computation to be mostly frame-wise independent with some communications between frames. Specifically, we adopt memory tokens for both summarizing per-frame scenes and the means of communications.</p><p>Our encoder blocks are composed of two phases of separate transformers: Encode-Receive (E) and Gather-Communicate (G). Both Encode-Receive and Gather-Communicate follow the typical transformer encoder architecture <ref type="bibr" target="#b13">[14]</ref>, which consists of an addition of fixed positional encoding, a multi-head self-attention module, and a feed forward network.</p><p>Encode-Receive operates in a per-frame manner, taking a frame-level feature map and corresponding memory tokens. Passing through Encode-Receive, we expect two functionalities: (1) image features encode per-frame information to the memory tokens, and (2) image features receive information of different frames that are gathered in the memory tokens. Gather-Communicate operates across frames to form a clip-level knowledge. It takes the memory tokens from each frame as inputs and performs communications between frames. Alternating two phases through multiple layers, the encoder can efficiently learn consensus representations across frames.</p><p>In more detail, given the frame embedding {f 0 i } T i=1 , we spatially flatten each feature R H?W ?C ? R HW ?C . The initial memory tokens m 0 of size M are copied per frame and concatenated to each frame feature as follows:</p><formula xml:id="formula_2">[f 0 t , m 0 t ] ? R (HW +M )?C , t ? {1, 2, ? ? ? , T },<label>(1)</label></formula><p>where [?, ?] indicates a concatenation of two feature vectors. Note that the initial memory tokens m 0 are trainable parameters learnt during training.</p><p>The first phase of IFC is Encode-Receive, which processes frames individually as follows:</p><formula xml:id="formula_3">[f l t , m l t ] = E l ([f l?1 t , m l?1 t ]),<label>(2)</label></formula><p>where E l denotes the l-th Encode-Receive layer. With a self-attention computed over the frame pixel locations and the memory tokens, the information of each frame can be passed to the memory tokens and vise-versa.</p><p>The outputs of Encode-Receive are grouped by memory indices and formulate the inputs for Gather-Communicate layer. The grouping can be understood as a decomposition of memory tokens, and becomes computationally beneficial when the total size of gathered memory tokens increases.</p><formula xml:id="formula_4">[m l 1 (i), m l 2 (i), ? ? ? , m l T (i)] = G l ([ m l 1 (i), m l 2 (i), ? ? ? , m l T (i)]), i ? {1, 2, ? ? ? , M },<label>(3)</label></formula><p>where G l denotes the l-th Gather-Communicate layer. The processed outputs are redistributed by the originated frame and get concatenated as m t = [m t (1), m t (2), ? ? ? , m t (M )]. Unlike Encode-Receive, Gather-Communicate utilizes the attention mechanism to convey the information from different frames over the input clip.</p><p>Defining a the l-th inter-frame encoder block (IFC l ) as E l followed by G l , the stack of N E encoder blocks can be inductively formulated as:</p><formula xml:id="formula_5">[f l Complexity comparison.</formula><p>In <ref type="table" target="#tab_0">Table 1</ref>, we analyze the computational complexity of transformer encoder variants applied for video input in terms of the Big-O complexity and FLOPs. The complexity of the original transformer encoder layer <ref type="bibr" target="#b13">[14]</ref> is O(C 2 N + CN 2 ), where N is the number of inputs. Without any communication between frames, No Comm, it shows the smallest amount of computation (O(C 2 T HW + CT (HW ) 2 )). As indicated as Full THW in <ref type="table" target="#tab_0">Table 1</ref>, the complexity of VisTR <ref type="bibr" target="#b10">[11]</ref> that performs a full space-time self-attention is O(C 2 (T HW ) + C(T HW ) 2 ) thus either a higher resolution or an increase of number of input frames leads to a massive increase in computations. VisTR bypasses the problem by highly reducing the input resolution and utilizing GPUs with tremendous memory capacity. However, as such solutions cannot resolve the fundamental issues, it is impractical to real-world videos. Moreover, VisTR remains as a complete offline strategy because it takes the entire video as an input.</p><p>An intriguing improvement for the na?ve full self-attention would be the decomposition of the attention into space and time axis <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>. In Decompose T-HW, we decompose attention computation into spatial and temporal attention. The complexity of the separation of space-time leads to the sum of the two transformer encoder: O(T (C 2 (HW ) + C(HW ) 2 )) and O(HW (C 2 T + CT 2 )). In comparison to the full self-attention, the decomposition lowers the computational growth relative to the number of frames.</p><p>Our encoder, IFC, that communicates between frames using the memory tokens leads to a huge benefit to the total computations adding only a small amount of  <ref type="table" target="#tab_0">Table 1</ref>. Finally, with respect to the number of frames of the input, we can expect approximate linear increase rather than the high increase of computation occurred in VisTR.</p><p>Decoders and output heads As depicted in <ref type="figure">Fig. 1</ref>, the transformer decoder of our model is stacked with N D layers <ref type="bibr" target="#b13">[14]</ref>. Contrary to VisTR, where the number of object queries increases proportionally to the number of frames, our model receives learnt encodings of fixed size N q for object queries. Also, by utilizing these encodings throughout the entire frames, our model can effectively deal with clips of various lengths. A set of projection matrices are applied to {f N E t , m N E t } T t=1 for the generation of keys and values. The object queries turn into output embeddings by the transformer decoder, and the embeddings are eventually used as an input to the output heads.</p><p>There are two output heads on top of the transformer decoder, a class head and a segmentation head, each composed of two fully-connected layers. The output embeddings from the transformer decoder are independently inserted to the heads, resulting in N q predictions per a clip. The class head outputs a class probability distribution of instancesp(c) ? R Nq?|C| . Note that the possible classes C c include no object ? class in addition to the given classes of a dataset.</p><p>The segmentation head generates N q conditional convolutional weights w ? R Nq?C in a manner similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>. For the conditional convolution, the output feature of the encoder {f N E t } T t=1 is reused by undoing the flatten operation. For the upsampling, the encoder feature passes through fpn-style <ref type="bibr" target="#b24">[25]</ref> spatial decoder without temporal connections resulting in T feature maps that are 1/8 of the input resolution. Finally, the resulting feature maps are convolved with each convolutional weights to generate segmentation mask as follows:</p><formula xml:id="formula_6">s i = {f t ? w i } T t=1 ,<label>(5)</label></formula><p>where w i is i-th convolutional weight, ? indicate 1 ? 1 spatial convolution operation, and the result? i is a spatial-temporal object mask in shape of R T ?H ?W where H = H0 8 , W = W0 8 . Note that, for an instance, a common weight is applied throughout the video clip. Our spatial decoder is of instanceagnostic design which gets highly efficient than decoders of instance-specific designs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10]</ref> as the number of detected instances increases. Meanwhile, thanks to our segmentation head which specifies and captures the characteristics of an instance, IFC can conduct both segmentation and tracking at once within a clip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance matching and loss</head><p>To train our network, we first assign the ground truth for each instance estimation and then a set of loss function between each the ground truth and prediction pair. For a given input clip, our model generate a fixed-size set of class-labeled masks</p><formula xml:id="formula_7">{? i } Nq i=1 = {(p i (c),? i )} Nq i=1</formula><p>. The ground truth set of the clip can be represented as y i = (c i , s i ); c i is the target class label including ?, and s i is the target mask which is down-sampled to the size of the prediction masks for efficient similarity calculation. One-to-one bipartite matching between the prediction set {? i } Nq i=1 and the ground truth set {y i } K i=1 is performed to find the best assignment of a prediction to a ground truth. The objective can be formally described as:?</p><formula xml:id="formula_8">= arg max ??S Nq K i=1 sim(y i ,? ?(i) ),<label>(6)</label></formula><p>where sim(y i ,? ?(i) ) refers a pair-wise similarity over a permutation of ? ? S Nq . Following prior work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, the bipartite matching is efficiently computed using Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>. We find that box-based similarity measurement as used in DETR <ref type="bibr" target="#b12">[13]</ref> shows weaknesses in matching instances in video clip due to the case of occlusion and disappear-and-reappear. Therefore, we define sim(y i ,? ?(i) ) to be mask-based term as</p><formula xml:id="formula_9">1 {ci =?} [p ?(i) (c i )+? 0 DICE(s i ,? ?(i) )],</formula><p>where DICE denotes dice coefficients <ref type="bibr" target="#b26">[27]</ref>.</p><p>Given the optimal assignment?, we refer to the K matched predictions and (N q ? K) non-matched predictions as positive and negative pairs respectively. The positive pairs aim to predict the ground truth masks and classes while the negative pairs are optimized to predict the ? class. The final loss is a sum of the losses from positive pairs and negative pairs where each can be computed as follows:</p><formula xml:id="formula_10">L pos = K i=1 [? logp? (i) (c i ) Cross-entropy loss +? 1 (1 ? DICE(s i ,?? (i) )</formula><p>Dice loss <ref type="bibr" target="#b26">[27]</ref> ) + ? 2 FOCAL(s i ,?? (i) )</p><p>Sigmoid-focal loss <ref type="bibr" target="#b27">[28]</ref> ],</p><formula xml:id="formula_11">L neg = Nq i=k+1 [? logp? (i) (?)].<label>(7)</label></formula><p>As (N q ? K) is likely to be much greater than K, we down-weight L neg by a factor of 10 to resolve the imbalance, following prior work <ref type="bibr" target="#b12">[13]</ref>. The goal of video instance segmentation <ref type="bibr" target="#b0">[1]</ref> is to maximize the space-time IoU between a prediction and a ground truth mask. Therefore, our mask-related losses (Dice loss and Sigmoid-focal loss) are spatio-temporally calculated over an entire clip, rather than averaging the losses that are accumulated frame-by-frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Clip-level instance tracking</head><p>To infer a video input that is longer than the clip length, we match instances using the predicted masks of overlapping frames. Let Y I and Y A be the result sets of clip I and A excluding the ? class. The goal is to perform matching of same identities between pre-collected instance set Y I and Y A . We first calculate the matching scores which are space-time soft IoU at intersecting frames between Y I and Y A . Then, we find optimal paired indices? S using Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref> to the gathered matching score S ? [0, 1] |Y I |?|Y A | . We update Y I (i) by concatenating Y A (? S (i)) if S(i,? S (i)) is above a certain threshold, and add non-matched prediction sets to Y I as new instances. Note that a previous per-clip model (MaskProp <ref type="bibr" target="#b9">[10]</ref>) also utilizes soft IoU for tracking instances, but the matching scores are computed per-frame and averaged for intersecting frames. Different from MaskProp, using space-time soft IoU leads to an accurate tracking as it can better represent the definition of mask similarities between clips which brings at most 2% AP increase. The overall tracking pipeline can be effectively implemented in GPU-friendly manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the proposed method using YouTube-VIS 2019 and 2021 <ref type="bibr" target="#b0">[1]</ref>. We demonstrate the effectiveness of our model regarding both accuracy and speed. We further examine how different settings affect the overall performance and efficiency of IFC encoder. Unless specified, all models for measurements used N E = 3, N D = 3, stride of 1, and ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We used detectron2 <ref type="bibr" target="#b31">[32]</ref> for our code basis, and hyper-parameters mostly follow the settings of DETR <ref type="bibr" target="#b12">[13]</ref> unless specified. We used AdamW <ref type="bibr" target="#b32">[33]</ref> optimizer with initial learning rate of 10 ?4 for transformers, and 10 ?5 for backbone. We first pre-train the model for image instance segmentation on COCO <ref type="bibr" target="#b33">[34]</ref> by setting our model to T = 1. The pre-train procedure follows the shortened training schedule of DETR <ref type="bibr" target="#b12">[13]</ref>, which runs 300 epochs with a decay of the learning rate by a factor of 10 at 200 epochs. Using the pre-trained weights, the models are trained on targeted dataset using the batch (a) AP and FPS on YouTube-VIS 2019 val set. For fairness, FPS is measured on a same machine, using a single RTX 2080Ti GPU. We used the official codes and checkpoints provided by the authors for the measurements. We report the clip settings of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. T : window size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>YouTube-VIS 2019 evaluation results. We compare our proposed IFC to the state-of-the-art models in the video instance segmentation task on YouTube-VIS 2019 val in <ref type="table" target="#tab_1">Table 2</ref> (a). We measure the accuracy by AP and our model sets the highest score among all online, near-online, and offline models while presenting the fastest runtime. As mentioned earlier, IFC is highly efficient during the inference thanks to three advantages: (1) memory token-based decomposition for transformer encoder (2) instance-agnostic spatial decoder (3) GPU-friendly instance matching. Moreover, our model does not make use of any heavy modules such as deformable convolutions <ref type="bibr" target="#b34">[35]</ref> or cascading networks <ref type="bibr" target="#b35">[36]</ref>. Thanks to these advantages, IFC achieves an outstanding runtime, which is faster speed than online models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>During the inference, our method is able to freely adjust the length of the clip (T ) as needed. If the input clip length is set to contain entire video frames, our method becomes an offline method (like VisTR <ref type="bibr" target="#b10">[11]</ref>) that processes the entire video in one shot. As the offline inference can skip matching between clips and maximize the GPU utilization, our method represents surprisingly fast runtime (107.1 FPS). On the other hand, if the application requires instant outputs given a video stream, we can reduce the clip length to make our method near-online. In the near-online scenario with T = 5, our system is still able to process a video in real-time (46.5 FPS) with only a small delay.</p><p>YouTube-VIS 2021 evaluation results. The recently introduced dataset YouTube-VIS 2021 is an improved version of YouTube-VIS 2019. The newly added videos in the dataset include higher (a) Various encoders taking clips of different lengths (see <ref type="table" target="#tab_0">Table 1)   T=5  T=10  T=15  T=20  AP  AP75  FPS  AP  AP75  FPS  AP  AP75  FPS  AP  AP75</ref>  number of instances and frames. In <ref type="table" target="#tab_1">Table 2</ref> (b), we refer the results reported in <ref type="bibr" target="#b2">[3]</ref>, which evaluated <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> using official implementations. Again, our model achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we provide ablation studies and discuss how different settings impact the overall performance. The experiments are conducted using YouTube-VIS 2019 val set. For every ablation studies, we report the mean of five runs as the results may vary by each run due to the insufficient number of training and testing set of YouTube-VIS dataset.</p><p>Box-based and mask-based bipartite matching. We observe how the different policies for bipartite matching affect the performance. As our model does is a box-free method, we adjust our model to predict bounding boxes similar to VisTR <ref type="bibr" target="#b10">[11]</ref> and conduct bipartite matching <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref> using the predicted boxes. The change of optimization from mask-based to box-based brings a noticeable performance drop as shown in <ref type="table" target="#tab_1">Table 2</ref> (c). With the VIS-centric design, the mask-based optimization shows more robustness than box-based optimizations under typical video circumstances such as instances with heavy overlaps and partial occlusions.</p><p>Differing window strides. In addition to the clip length (T ), we further optimize our runtime placing a stride S between clips, as shown in <ref type="table" target="#tab_1">Table 2</ref> (d). IFC can be used in a near-online manner, which takes clips that are consecutively extracted from a video. The placement of a larger stride reduces temporal intersections, which lessens computational overheads but also causes difficulty in matching instances. By enlarging the stride from S = 1 to S = 3, IFC accomplishes approximately 150% speed improvement with only 0.1% AP drop. The tendency of high speed gain and low accuracy drop persists under various conditions. Therefore, our model can be applied to conditions where the enlargement of strides is necessary, i.e., using devices that are not powerful enough but has to maintain high inference speed.</p><p>Various decomposition strategies of encoders. In <ref type="table" target="#tab_0">Table 1</ref>, we observed the computational gaps derived from the decomposition of the encoder layers. Extending <ref type="table" target="#tab_0">Table 1</ref>, we now investigate the how the decomposition strategies affect the accuracy in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The models are evaluated with variety of window sizes (T = 5, 10, <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20)</ref> as an increase of window size T has pros and cons. When matching predictions from different clips, greater T is advantageous due to an enlargement of temporal intersections between clips. On the contrary, frames in longer clips are likely to be composed of diverse appearances, which disrupt tracking and segmenting instances within a clip. Therefore, the key to the performance enhancement is to cope with the appearance changes by precisely encoding and correlating space-time inputs.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref> (a), the full self-attention <ref type="bibr" target="#b10">[11]</ref> surpasses the encoder without communications as the length of clips increase. However, the enlargement of the window size highly slows down the inference speed, and the improvements are marginal that the tremendous computation and memory usage cannot be compensated. The decomposition of space-time maintains comparable speed even if the window is large, but fails to achieve high accuracy.</p><p>Our model is shows fast inference as the only additional computations of IFC are from utilizing a small number of memory tokens. Furthermore, by effectively encoding the space-time inputs with the communications between frames, IFC can take advantages of enlarging the window size, and surpasses other encoders.</p><p>Memory tokens. We also study the effects of utilizing memory tokens. As mentioned, the motivation of using the memory tokens is to build communications between frames. Different from the video instance segmentation task, the image segmentation task is consisted of a single frame. Therefore, the use of the memory tokens does not lead to improvements to the image instance segmentation task as mutual communications cannot be solely made (see <ref type="table" target="#tab_3">Table 3</ref> (b)). Meanwhile, the utilization of the memory tokens achieves great improvements by effectively passing the information between frames. Results in <ref type="table" target="#tab_3">Table 3</ref> (a, c) demonstrate that the use of memory tokens achieves higher accuracy than the encoder without any communications (No comm), which emphasizes the importance of the communications. We evaluate how the size of the memory tokens affect the overall accuracy in <ref type="table" target="#tab_3">Table 3</ref> (c) and set the default size of the tokens M to be 8.</p><p>In Section 3.1, we demonstrated the formulation of the inputs for Gather-Communicate layer, which groups the outputs of Encode-Receive by memory indices. As aforementioned, the formulation can be considered as a decomposition of memory tokens: insertion to the Gather-Communicate layer by separate M groups each consisting of T tokens. In <ref type="table" target="#tab_3">Table 3</ref> (d), we investigate the impact of inserting the unified M T tokens as a whole. Compared to the unified insertion, the decomposition brings better accuracy as the memories of same indices have more correspondences, which ease the encoders to build attentions in between.</p><p>We choose a memory index attending foreground instances and visualize the attention map in <ref type="figure" target="#fig_3">Fig. 2</ref>. As shown in the results of the upper clip, we find that the memory token has more interests to instances that are relatively difficult to detect; it more attends the heavily occluded car at the rear. The clip at the bottom is composed of frames with huge motion blurs and appearance changes. With the communications of memory tokens, IFC successfully tracks and segments the rabbit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel video instance segmentation network using Inter-frame Communication Transformers (IFC), which alleviates full space-time attention and successfully builds communications between frames. Finally, our network presents a rapid inference and sets the new state-of-the-art on the YouTube-VIS dataset. For the future work, we plan to integrate temporal information, which indeed would take a step further to the human video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Our framework is designed for the VIS task, which targets to classify and segment foreground instances of predefined classes. Recently, while investigating the capabilities of transformers, many disregard the importance of efficiency and take inputs of tremendous sizes. In comparison, IFC focuses on reducing the overall computation while improving the performance. We believe our network can positively impact many industrial fields that require high accuracy and speed, i.e., alert system, autonomous driving, robotics. We want to note that for the community to move in the right direction, the studies on VIS should be aware of potential misuses which violates personal privacy.</p><p>COCO <ref type="bibr" target="#b33">[34]</ref>, YouTube-VIS <ref type="bibr" target="#b0">[1]</ref>, detectron2 <ref type="bibr" target="#b31">[32]</ref> license: CC-4.0, CC-4.0, Apache-2.0</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>computation over No Comm while providing sufficient channels for communication. The complexity of each phase in our proposed encoder is: O(C 2 T (HW + M ) + CT (HW + M ) 2 ) for Encode-Receive and O(C 2 T M + CT 2 M ) for Gather-Communicate respectively. Assuming that M is kept small (e.g., 8), the computation needed for Gather-Communicate can be neglected, while the complexity of Encode-Receive can be approximated to O(C 2 T HW + CT (HW ) 2 ) as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, each clip composed of T = 5 frames downscaled to either 360p or 480p. The models are trained for 8 epochs, and decays the learning rate by 10 at 6th epoch. For the evaluation, the input videos are downscaled to 360p, which follows MaskTrack R-CNN<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Visualizations of results and attention maps of memory tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Complexity comparison. Various transformer encoders for space-time input. As the overall FLOPs can vary by the number of detected instances, listed values are measured only at the encoders.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">FLOPs (G) 1</cell><cell></cell></row><row><cell>Communication Type</cell><cell>Complexity per Layer</cell><cell cols="2">360 ? 640</cell><cell cols="2">720 ? 1280</cell></row><row><cell></cell><cell></cell><cell>T=5</cell><cell>T=36</cell><cell>T=5</cell><cell>T=36</cell></row><row><cell>No Comm</cell><cell>O(C 2 T HW + CT (HW ) 2 )</cell><cell>5.17</cell><cell>37.23</cell><cell>24.62</cell><cell>177.29</cell></row><row><cell>Full THW</cell><cell>O(C 2 T HW + C(T HW ) 2 )</cell><cell cols="4">6.94 148.70 50.63 1815.38</cell></row><row><cell>Decompose T-HW</cell><cell cols="2">O(C 33</cell><cell>60.24</cell><cell>36.73</cell><cell>265.50</cell></row><row><cell>IFC (M = 8)</cell><cell>O(C 2 T HW + CT (HW ) 2 )</cell><cell>5.52</cell><cell>39.73</cell><cell>25.05</cell><cell>180.39</cell></row></table><note>2 T HW + CT (HW ) 2 + CT 2 HW ) 8.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluations on various settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Encoder variations. We show how different encoders affect the overall performance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , m l t ] = IFC l ([f l?1 t , m l?1 t ]), 1 ? l ? N E ,(4)where[f N E t , m N E t ]is the final result. The stacking of multiple encoders brings communications between frames, thus each frame can have coincidence to the other, specifying the identities of instances in a given clip.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Measured using flop_count function of fvcore==0.1.5.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We follow detectron2<ref type="bibr" target="#b31">[32]</ref> for measuring FPS.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<idno>ECCV. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05970</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sg-net: Spatial granularity network for one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<idno>CVPR. 2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno>ECCV. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<idno>CVPR. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple object tracking: A literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno>CVPR. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>CVPR. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Video instance segmentation with a propose-reduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13746</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno>ECCV. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<idno>ECCV. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<title level="m">Is space-time attention all you need for video understanding? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>CVPR. 2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno>ICLR. 2021</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<title level="m">Vision transformers for dense prediction</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>O?ep</surname></persName>
		</author>
		<idno>ECCV. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<title level="m">Comprehensive feature aggregation for video instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
