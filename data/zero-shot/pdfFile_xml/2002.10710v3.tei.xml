<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Emotion-Cause Pair Extraction via Learning to Link</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolin</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
							<email>qiuchili@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<settlement>Padua</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
							<email>dwsong@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Emotion-Cause Pair Extraction via Learning to Link</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion-cause pair extraction (ECPE), as an emergent natural language processing task, aims at jointly investigating emotions and their underlying causes in documents. It extends the previous emotion cause extraction (ECE) task, yet without requiring a set of pre-given emotion clauses as in ECE. Existing approaches to ECPE generally adopt a two-stage method, i.e., (1) emotion and cause detection, and then (2) pairing the detected emotions and causes. Such pipeline method, while intuitive, suffers from two critical issues, including error propagation across stages that may hinder the effectiveness, and high computational cost that would limit the practical application of the method. To tackle these issues, we propose a multi-task learning model that can extract emotions, causes and emotion-cause pairs simultaneously in an end-toend manner. Specifically, our model regards pair extraction as a link prediction task, and learns to link from emotion clauses to cause clauses, i.e., the links are directional. Emotion extraction and cause extraction are incorporated into the model as auxiliary tasks, which further boost the pair extraction. Experiments are conducted on an ECPE benchmarking dataset. The results show that our proposed model outperforms a range of state-ofthe-art approaches in terms of both effectiveness and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion cause extraction (ECE)  aims to extract possible causes for a given emotion clause. While ECE has attracted an increasing attention due to its theoretical and practical significance, it requires that the emotion signals should be given. In practice emotion annotation is rather labor intensive, limiting the applicability of ECE in practical settings. To address the limitation of ECE, the emotion-cause pair extraction (ECPE) task was recently proposed in <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref>. Unlike <ref type="bibr">ECE [Lee et al., 2010]</ref>, ECPE aims to extract emotions and causes without any given emotion signals, and thus better aligns with real-world applications. An illustrative example is given in <ref type="figure" target="#fig_1">Figure 1(a)</ref>, showing that Document (Clauses) <ref type="bibr">C1</ref>: Yesterday morning, C2: a policeman visited the old man with the lost money, C3: and told him that the thief was caught. C4: The old man was very happy, C5: and deposited the money in the bank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Cause Pair</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document (Clauses)</head><p>C1: Yesterday morning, C2: a policeman visited the old man with the lost money, C3: and told him that the thief was caught. C4: The old man was very happy, C5: and deposited the money in the bank. clause 4 serves as the emotion and clauses 2 and 3 are the corresponding causes. Typically, ECPE is formulated as extracting emotion-cause pairs, e.g., (clause 4, clause 2) and (clause 4, clause 3), directly from provided documents. ECPE is a challenging task as it requires the extraction of emotions, causes and emotion-cause pairs. Existing work in ECPE mainly focuses on how to collaboratively extract emotions and causes and combine them in an appropriate way. Thus, a two-stage method <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref> is typically adopted, which divides pair extraction into two steps: firstly detecting emotions and causes, and then pairing them based on the likelihood of cartesian products between them. Such pipeline method approach is intuitive and straightforward. However, two critical issues arise. One is the error propagation from the first step to the second. The other issue is that, owing to the step-by-step structure, the two-stage models are often computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Cause Pair</head><p>To tackle the afore-mentioned issues, we propose an endto-end multi-task learning model for predicting emotioncause pairs, namely E2EECPE, which connects emotions and causes within one single stage. More specifically, benefitting from end-to-end architectures, E2EECPE resolves the is-sue of error propagation while substantially reduces the computation time compared with the state-of-the-art models. Particularly, our model views emotion-cause pair extraction as a link prediction problem, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b), and predicts whether there exists a directional link from the emotion to cause. Furthermore, we incorporate into the model two auxiliary tasks, namely emotion extraction and cause extraction, which are oriented to further enhance the expressiveness of the intermediate emotion representation and cause representation.</p><p>Extensive experiments are carried out on a benchmarking ECPE dataset. The experimental results demonstrate the effectiveness and efficiency of the proposed E2EECPE model, in comparison with a variety of state-of-the-art baselines. Moreover, further ablation study indicates that the auxiliary tasks are beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First of all, our work is related to extracting causes based on emotions explicitly presented in documents, i.e., <ref type="bibr">ECE [Lee et al., 2010]</ref>. Earlier work views ECE as a word-level sequence tagging problem and tries to solve it with corresponding tagging techniques. Therefore, primary efforts have been made on discovering refined linguistic features <ref type="bibr" target="#b1">[Chen et al., 2010;</ref><ref type="bibr" target="#b5">Lee et al., 2013]</ref>, yielding improved performance. In line with other tagging related tasks such as named entity recognition (NER), support vector machines (SVMs) <ref type="bibr" target="#b3">[Gui et al., 2014]</ref> and conditional random fields (CRFs) <ref type="bibr" target="#b5">[Lafferty et al., 2001]</ref> have been used for ECE. More recently, instead of concentrating on word-level cause detection, clause-level extraction  is put forward in that the impact of individual words in a cause can span over the whole sequence in the clause.</p><p>With the emergence and development of deep representation learning, neural models has also been utilized in ECE. <ref type="bibr" target="#b2">[Cheng et al., 2017]</ref> leverages long short-term memory networks (LSTMs) <ref type="bibr">[Hochreiter and Schmidhuber, 1997</ref>] to promote the context awareness of clause modelling. <ref type="bibr" target="#b4">[Gui et al., 2017]</ref> views the information extraction problem as the retrieval task in question answering (QA) and examines the effect of memory networks <ref type="bibr">[Sukhbaatar et al., 2015]</ref> for extraction. Likewise, taking advantage of attention mechanism <ref type="bibr" target="#b0">[Bahdanau et al., 2014]</ref>, ] employs a coattention based model and achieves the state-of-the-art performance.</p><p>In light of recent advances in multi-task learning, joint extraction of emotions and causes is investigated  to exploit the mutual information between two correlated tasks. However, these works do not explicitly combine two tasks into one. Thereafter, <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref> argues that, while co-extraction of emotions and causes is important, emotion-cause pair extraction (ECPE) is a more challenging problem that is worth putting more emphasis on. Nevertheless, <ref type="bibr" target="#b7">[Xia and Ding, 2019</ref>] adopts a two-stage approach, which performs emotion and cause extraction first and then pairs the extracted emotions and causes. As discussed in the previous section, such two stage approach suffers from error propagation and high computation cost. Our work aims to tackle these challenges in ECPE. Rather than processing emotion-cause pair extraction as a two stage task (as used in the existing work), we consolidate two stages into a unified multi-task learning framework, and further consider it as a link prediction task which could be solved in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To solve ECPE in end-to-end fashion, we take inspiration from the link prediction problem in graph learning, which aims at predicting potential edges between unconnected vertices in a graph. Essentially, if we consider emotion-cause pairs in a document as triplets in a graph, then the extraction of such pairs is a sort of link prediction from a graph that is at first armed with no edges but only vertices. In order to achieve above procedure, we borrow the idea of learning a graph-based dependency parser <ref type="bibr" target="#b3">[Dozat and Manning, 2016]</ref> and adapt it to our target task. Coupling link prediction with auxiliary emotion extraction and cause extraction tasks, our model is capable of jointly, and more effectively, extracting emotions, causes, and emotion-cause pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Generally, for any provided document D, we could split it into a sequence of clauses based on punctuation, i.e., D =</p><formula xml:id="formula_0">{c i } |D| i=1 , where c i could further be decomposed into words, i.e., c i = {w j } |ci| j=1 .</formula><p>Here, |D| is the number of clauses in the document and |c i | is the number of words in the i-th clause. ECPE aims to extract a set of |P | emotion-cause pairs</p><formula xml:id="formula_1">P = {(c e k , c c k )} |P | k=1 from the document D,</formula><p>where c e k , c c k represents the emotion clause and the cause clause in the k-th pair, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>An overview of the proposed E2EECPE approach is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. The bottom layer is a clause encoder (Section 3.3) and a document modelling layer (Section 3.4) which transform the word embeddings into the contextualized clause representations. The middle part consists of auxiliary tasks (Section 3.7), i.e., emotion extraction and cause extraction. The top most part is a biaffine attention layer (Section 3.5) which first encodes interaction between the emotion representation and cause representation, and then outputs a postion-weighted pair matrix for pair extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Embedding &amp; Clause Encoder</head><p>With the purpose of integrating words into clause-level neural models, we embed each word in a clause into lowdimensional vectors <ref type="bibr" target="#b0">[Bengio et al., 2003]</ref>, by which we could represent each word in the clause with its vector representation 1 c i = {w j } |ci| j=1 , where w j ? R de and d e is the dimensionality of the embedding.</p><p>After that, we need to attain contextualized representations of clauses. Owing to the recognized performance and local context awareness of the convolutional neural networks (CNNs) on text classification benchmarks <ref type="bibr" target="#b5">[Kim, 2014]</ref>, we adopt CNNs as the backbone of our clause encoder.</p><p>For an embedded clause c = {w j } |c| j=1 , we apply onedimensional convolution operations with kernels of different sizes over the word sequence:</p><formula xml:id="formula_2">C t = ?(conv t (w 1 , ? ? ? , w |c| ))<label>(1)</label></formula><p>where conv t denotes the t-th convolution operation and C t ? R |c|?dc is the output of the operation. d c is the number of filters employed in one convolution operation and ?(?) used here is actually max(0, ?).</p><p>Then max-pooling is used to distill the features for concatenation. Hence, we finally get context-aware features for the clause:c</p><formula xml:id="formula_3">t = maxpool(C t ) (2) c = [? tct ]<label>(3)</label></formula><p>where c ? R |t|?dc is the convoluted feature and ? means vector concatenation. |t| is the total number of convolution operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Modelling</head><p>Since we have sequential clauses in a document, the influences brought by document-level structures become a crucial part that we should fit into our model. A straightforward idea is to leverage temporal relations among clauses with LSTMs. Specifically, provided with the encoded clause representations {c i } |c| i=1 , we employ a bidirectional LSTM to update clause-level features and get h i ? R 2d h :</p><formula xml:id="formula_4">h i = [LSTM f (c i ) ? LSTM b (c i )]<label>(4)</label></formula><p>where LSTM f (?) and LSTM b (?) denote the forward and backward unidirectional LSTMs, respectively. d h is the dimensionality of hidden states for a unidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Biaffine Attention</head><p>Motivated by advances in link prediction <ref type="bibr" target="#b6">[Schlichtkrull et al., 2018]</ref>, we can directly compute the similarity scores among vertex representations (clause representations in our task), e.g., ?(z p z q ) for any representations of vertex p and q, to make predictions. However, the above predictions are only concerned with undirectional circumstances since z p z q = z q z p , which is not adequate for emotion-cause pair extraction. To solve the problem, we utilize biaffine transform to complete the filling of adjacent matrices, which are called pair matrices in our work. This idea is similar to dependency parsing <ref type="bibr" target="#b3">[Dozat and Manning, 2016]</ref> that is also directional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion &amp; Cause Representation</head><p>According to biaffine attention mechanism, each vertex in the graph should have two independent representations, i.e., one is for pointing out and the other for pointed in. In doing so, the pair matrix output by the transformation is asymmetric and direction-aware.</p><p>The emotion representation and cause representation are separately offered as below:</p><formula xml:id="formula_5">z e i = ?(W e h i + b e ) (5) z c i = ?(W c h i + b c )<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">W e ? R dz?2d h , b e ? R dz and W c ? R dz?2d h , b c ? R<label>dz</label></formula><p>are two sets of trainable weights and biases, respectively for the emotion and cause representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biaffine Transform</head><p>Then, we implement biaffine transform on the collected emotion and cause representations. In other words, with the purpose of merging these two kinds of representations into our aimed pair matrices, we fold emotion-cause dynamics into two components. On the one hand, we need to perform a bilinear like operation on each possible pair of emotion and cause. On the other hand, we believe bilinear transform is not enough to deal with such complicated interactions, and thus we facilitate it by injecting bias. More specifically, we calculate each entry in the expected pair matrix as follows:</p><formula xml:id="formula_7">M p,q = (W m z e p + b m ) z c q<label>(7)</label></formula><p>where W m ? R dz?dz and b m ? R dz are learnable parameters of affine transform, while M p,q indicates an entry of the pair matrix in the p-th row, q-th column. Constrained by the inherent property of an adjacent matrix, we further activate the pair matrix with the sigmoid function</p><formula xml:id="formula_8">g(?):M p,q = g(M i,j )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Asymmetric Position Weight Matrix</head><p>A trivial observation on the co-occurrence patterns of emotions and causes is that the emotion and the cause in a unique pair appear near each other in term of their absolute positions in the document. Thus, position embeddings are introduced to directly encode positions into vectors <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref>. Different from the existing approaches, our work is based on graph learning, and thereby can not be aided by manipulation of embeddings. Instead, we apply proximity weights on features as in <ref type="bibr" target="#b8">[Zhang et al., 2019]</ref>, but extent it to matrices. Moreover, we notice that in reality people are more likely to inform the causes before expressing emotions, which will be verified with statistics of dataset, implying we should assign asymmetric position weight matrices, instead of symmetric ones, to the original matrix representations.</p><p>Specifically, the entries in the lower triangular of position weight matrix for the given document is of more significance:</p><formula xml:id="formula_9">A p,q = |D| ? |p ? q ? 1| + |D| + (9)</formula><p>where is a small number for smoothing. Finally, we obtain the features for indicating pair links as follows:M p,q =M p,q A p,q (10) where denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Multi-task Setting</head><p>As aforementioned, emotion extraction and cause extraction can be viewed as auxiliary tasks to augment the emotion and cause representations for constructing more expressive pair matrix. Hence we develop a multi-task paradigm, which shares the fundamental part of network structure for the main task with auxiliary tasks. To achieve this goal, we first acquire features dedicated for classification with following procedure:z</p><formula xml:id="formula_10">e i = ?(W e h i +b e )<label>(11)</label></formula><formula xml:id="formula_11">z c i = ?(W c h i +b c ) (12) whereW e ? R dz?2d h ,b e ? R dz andW c ? R dz?2d h , b c ? R dz</formula><p>are again two sets of trainable weights and biases, respectively. Subsequently, predictions are produced by two fully connected layers followed by softmax normalization layers:</p><formula xml:id="formula_12">y e = softmax(? eze i +b e )<label>(13)</label></formula><formula xml:id="formula_13">y c = softmax(? czc i +b c )<label>(14)</label></formula><p>where? e ? R 2?dz ,b e ? R 2 and? c ? R 2?dz ,b c ? R 2 are weights and biases for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Training Objective</head><p>Eventually, the whole structure can be trained by standard gradient descent. Accordingly, the objective function is a combination of cross entropy with L 2 -norm regularization, formulated as below:</p><formula xml:id="formula_14">L pair = ? p,q Y p,q log(M p,q ) ? p,q (1 ? Y p,q )log(1 ?M p,q )<label>(15)</label></formula><formula xml:id="formula_15">L aux = ? i [ k y e k log(? e k ) + k y c k log(? c k )]<label>(16)</label></formula><p>where (p, q) and i, k serve as enumerators over all elements. y e , y c , and Y are correspondingly the ground truth. Furthermore, we add two coefficients to balance the influences of above two objective functions. The ultimate training objective then becomes: where the term ? is used to adjust the potential influences brought by multi-task learning, which is refined according to a pilot study. ? stands for all parameters that need to be optimized, while ? is a coefficient for L 2 -norm regularization.</p><formula xml:id="formula_16">L = L pair + ?L aux + ?||?|| 2<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Inference</head><p>With well trained model, we can infer emotion-cause pairs by comparing each entry inM with a predefined threshold ?</p><formula xml:id="formula_17">Y p,q = 1,M p,q &gt; ? 0,M p,q ? ?<label>(18)</label></formula><p>where? is the inference result matrix with binary (1-0) indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We carry out experiments on a publicly available dataset for emotion-cause pair extraction, released by <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref>. Consisting of news crawled from web, the dataset is referred to as NEWS in the rest of the paper. It is randomly split into ten folds. In our experiments, to evaluate our trained model, each fold is further divided into two parts, namely train set and test set which respectively take 90% and 10% of the data. <ref type="table">Table 1</ref> shows some basic statistics of the dataset.</p><p>A key observation is that most documents only contain one emotion-cause pair therein, implying the sparsity of the pair matrix. Therefore the issue of label imbalance will be elaborated in following discussions. Moreover, a large amount of emotion-cause pairs have the emotion and the cause within 1 relative offset, suggesting the necessity of using proximity constraints (exactly what position weight matrix does) in the predicted pair matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Settings</head><p>For all our experiments, pre-trained word vectors on Weibo (a Chinese micro-bloging website) using Word2Vec <ref type="bibr" target="#b6">[Mikolov et al., 2013]</ref> are leveraged to initialize the word embeddings. The dimensionality of embeddings (i.e., d e ) is set to 200. We use 4 convolutional layers (i.e., |t|) whose kernel sizes are {2,3,4,5} for the clause encoder and the number of filters for all the convolutional layers (i.e., d c ) is 50, for capturing gramlevel features. In order to avoid overfitting, we apply dropout to embeddings and outputs of the clause encoder, yielding 0.5 probability of randomized zeroes on features. The dimensionality for hidden states of a unidirectional LSTM (i.e., d h ) is 300. The dimensionalities for all fully connected layers in the main task and auxiliary tasks (i.e., d z ) are 100. Moreover, the batch size and learning rate are determined through grid parameter search, which are 32 and 10 -3 , respectively. The coefficient for L 2 -norm regularization (i.e., ?) is 10 -5 . Based on a pilot study, we find the best value for the threshold (i.e., ?) in the inference stage is 0.3, which will be detailed in next section. The coefficient for the trade-off in objective function (i.e., ?) is 1. In addition, the smoothing term in the calculation of position weight matrix (i.e., ) is 1. Furthermore, Adam is used as the optimizer and all trainable parameters are randomly initialized with uniform distribution <ref type="bibr" target="#b4">[He et al., 2015]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines &amp; Evaluation Metrics</head><p>Our approach 2 is compared with a range of strong baselines, which are the state-of-the-art methods proposed by <ref type="bibr" target="#b7">[Xia and Ding, 2019]</ref> for emotion-cause pair extraction. These baselines are either one-stage or two-stage models. The two-stage models are listed below. They first extract emotions and causes with multi-task architectures independently or interactively, then classify the cartesian products of emotions and causes extracted in the first stage into pairs or non-pairs.</p><p>? Indep firstly considers emotion extraction and cause extraction as independent tasks and extract emotions and causes with multi-task learning, then pairs the extracted emotions and causes with a classifier. ? Inter-CE and Inter-EC typically follow the procedure of Indep, however, assist emotion extraction and cause extraction with directed interaction modelling.</p><p>The one-stage models neglect the second stage in the twostage models, and consider the cartesian products as predictions. They are listed as follows.</p><p>? Indep w/o filter removes the classifier of Indep.</p><p>? Inter-CE w/o filter removes the classifier of Inter-CE.</p><p>? Inter-EC w/o filter removes the classifier of Inter-EC.</p><p>Precision, recall, and macro F1 measures are adopted as effectiveness metrics in our experiments. Meanwhile, runtime is used as a measure of efficiency. The final results are obtained by averaging the ten folds results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result &amp; Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results in Effectiveness</head><p>We perform a comparison of E2EECPE with one-stage and two-stage baseline models to quantitatively understand in what ways E2EECPE is more effective than the baselines. <ref type="table" target="#tab_3">Table 2</ref> gives the results in terms of precision, recall and macro-F1 measures. The comparison results demonstrate that our model E2EECPE consistently outperforms the baselines for the main task (emotion-cause pair extraction) with regard to recall and F1, indicating the representation power and the effectiveness of our model. Nevertheless, we also observe that our model performs less well in precision than the twostage baseline models. With additional observation that the baseline models are performing poorly on recall, we conjecture the existing models suffer from predicting only few testing instances as pairs. Furthermore, E2EECPE is superior on the two auxiliary tasks (emotion extraction and cause extraction). We attribute the improvement to multi-task structure in our model which combines auxiliary tasks and the main task. Apart from that, the one-stage models yield lower results than E2EECPE on cause extraction, suggesting that error propagation is a comparably severe issue in the existing models but is alleviated in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Runtime Analysis</head><p>To confirm that our end-to-end model is more efficient than the two-stage models, we perform runtime analysis among the two-stage baseline models and ours. Concretely, we display the average running time consumed by models on a epoch in different folds.   <ref type="figure" target="#fig_3">Figure 3</ref> suggests that our model is 6-7 times faster than two-stage models, indicating the efficiency of our end-to-end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results listed in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To understand the efficacy of auxiliary tasks and position weight matrix, we conduct an ablation study on E2EECPE. Specifically, we separately ablate auxiliary tasks and position weight matrix from E2EECPE, and call them E2EECPE w/o auxiliary and E2EECPE w/o position, respectively.</p><p>The results in <ref type="table" target="#tab_4">Table 3</ref> show a significant performance drop of E2EECPE w/o auxiliary and a relatively minor drop of E2EECPE w/o position compared with E2EECPE, verifying the remarkable benefit of the multi-task learning schema. Meanwhile, the results that E2EECPE w/o position only differs slightly from E2EECPE based on all metrics, indicate the fact that imposing position information is still of importance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Threshold in Inference Stage</head><p>Inference based on pair matrix is powerful, yet we do not exactly know what threshold (i.e., ?) is the most suitable one for its expressiveness. It is therefore helpful to explore the effect of the threshold by altering it and examining the results. From <ref type="table" target="#tab_5">Table 4</ref>, we conclude that 0.3 is the most appropriate one for our studied task. With increases of ?, drops of F1 are noted, implying potential loss of extracted pairs. In addition, we also speculate that the reason why the best value is not around 0.5 (the expectation of random variables ranging uniformly from 0 to 1) is that the element-wise multiplication of a position weight matrix with the sigmoid-activated pair matrix produces a smaller expectation (as upper bound decreases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Issue of Label Imbalance</head><p>In order to measure the impact brought by label imbalance, typically in the form of pair matrix sparsity, we remove the examples containing more than one pair for test set in each fold to make up a Hard dataset, then record the mean results across ten folds correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset P R F1</head><p>Full 0.6478 0.6105 0.6280 Hard 0.6002 0.6479 0.6226 We can observe in <ref type="table" target="#tab_6">Table 5</ref> that our model encounters a failure on the Hard dataset with decreases on precision and F1 measure, suggesting that further investigation is needed to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>The emotion-cause pair extraction task is a new and more realistic task that seeks to identify emotion-cause pairs in documents. However, previous two-stage models are inherently limited by the idea of solving this task via two stages. To this end, we propose an end-to-end model that regards the oriented problem as predicting directional links between emotions and causes via biaffine attention. Additionally, we also aid the model with auxiliary tasks and position weight matrix. Experimental results prove the superiority of our model over other baselines.</p><p>Based on the work in this paper, we believe there are some promising directions yet to be explored. On the one hand, more fancy models such as graph neural networks are expected to be developed to incorporate with learned position information instead of refined one. On the other hand, the label imbalance issue should be addressed with task-specific tactics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>An example document and extracted emotion-cause pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Extracting emotion-cause pairs of the given document via learning to link.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of E2EECPE. EE, CE are short for emotion extraction, cause extraction respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Runtime analysis (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of emotion extraction, cause extraction, and emotion-cause pair extraction with precision, recall, and F1-measure as metrics. The results in bold are the best performing ones under each column. The results of emotion extraction and cause extraction for one-stage and two-stage models are exactly the same because one-stage models are ablated ones of two-stage models. ? indicates results that are significantly better than best performing baseline Inter-EC with paired t-test (p is smaller than 0.05).</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>E2EECPE</cell><cell cols="3">0.6478 0.6105 0.6280</cell></row><row><cell cols="4">E2EECPE w/o auxiliary 0.5982 0.5340 0.5635</cell></row><row><cell>E2EECPE w/o position</cell><cell cols="3">0.6421 0.6158 0.6275</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results. The results in bold are the best performing ones under each column.</figDesc><table><row><cell>?</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell cols="4">0.2 0.5743 0.6456 0.6071</cell></row><row><cell cols="4">0.3 0.6478 0.6105 0.6280</cell></row><row><cell cols="4">0.4 0.6757 0.5849 0.6265</cell></row><row><cell cols="4">0.5 0.7185 0.5543 0.6255</cell></row><row><cell cols="4">0.6 0.7326 0.5385 0.6201</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Effect of threshold. The results in bold are the best performing ones under each column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The results for verifying the issue of label imbalance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If not specified, we use notations in bold as the vector representations of their original concepts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code is available in https://github.com/shl5133/E2EECPE.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<editor>Bengio, R?jean Ducharme, Pascal Vincent, and Christian Jauvin</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A neural probabilistic language model</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic constructions</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An emotion cause corpus for chinese microblogs with multipleuser structures</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>TALLIP</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Joint learning for emotion classification and emotion cause detection</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic construction in chinese weibo text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Timothy</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D Manning ;</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep biaffine attention for neural dependency parsing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kim ; Yoon Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<editor>Sophia Yat Mei Lee, Ying Chen, Chu-Ren Huang, and Shoushan Li</editor>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="390" to="416" />
		</imprint>
	</monogr>
	<note>Detecting emotion causes with a linguistic rule-based approach 1</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A co-attention neural network model for emotion cause analysis with emotional context awareness</title>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Jason Weston, Rob Fergus</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotioncause pair extraction: A new task to emotion analysis in texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding ; Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Syntax-aware aspect-level sentiment classification with proximity-weighted convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
