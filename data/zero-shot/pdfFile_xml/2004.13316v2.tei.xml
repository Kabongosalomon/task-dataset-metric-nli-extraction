<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xiaokang</roleName><forename type="first">Wenlong</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>IEEE</roleName><forename type="first">Yang</forename><surname>Fellow</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
						</author>
						<title level="a" type="main">SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Object Detection</term>
					<term>Feature Denoising</term>
					<term>Rotation Detection</term>
					<term>Boundary Problem</term>
					<term>Aerial Images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Small and cluttered objects are common in real-world which are challenging for detection. The difficulty is further pronounced when the objects are rotated, as traditional detectors often routinely locate the objects in horizontal bounding box such that the region of interest is contaminated with background or nearby interleaved objects. In this paper, we first innovatively introduce the idea of denoising to object detection. Instance-level denoising on the feature map is performed to enhance the detection to small and cluttered objects. To handle the rotation variation, we also add a novel IoU constant factor to the smooth L1 loss to address the long standing boundary problem, which to our analysis, is mainly caused by the periodicity of angular (PoA) and exchangeability of edges (EoE). By combing these two features, our proposed detector is termed as SCRDet++. Extensive experiments are performed on large aerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image dataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD and our released S 2 TLD by this paper. The results show the effectiveness of our approach. The released dataset S 2 TLD is made public available, which contains 5,786 images with 14,130 traffic light instances across five categories.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O BJECT detection is one of the fundamental tasks in computer vision and various general-purpose detectors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> based on convolutional neural networks (CNNs) have been devised. Promising results have been achieved on public benchmarks including MS COCO <ref type="bibr" target="#b7">[8]</ref> and VOC2007 <ref type="bibr" target="#b8">[9]</ref> etc. However, most existing detectors do not pay particular attention to some common aspects for robust object detection in the wild: small size, cluttered arrangement and arbitrary orientations. These challenges are especially pronounced for aerial image <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> which has become an important area for detection in practice, for its various civil applications, e.g. resource detection, environmental monitoring, and urban planning.</p><p>In the context of remote sensing, we also present some specific discussion to motivate this paper, as shown in <ref type="figure" target="#fig_1">Fig.  1</ref>. It shall be noted that these three aspects also prevail in other scenarios e.g. natural images and scene texts. 1) Small objects. Aerial images often contain small objects overwhelmed by complex surrounding scenes.</p><p>2) Cluttered arrangement. Objects e.g. vehicles and ships in aerial images are often densely arranged, leading to interclass feature coupling and intra-class feature boundary blur.  3) Arbitrary orientations. Objects in aerial images can appear in various orientations. Rotation detection is necessary especially considering the high aspect ratio issue: the horizontal bounding box for a rotated object is more loose than an aligned rotated one, such that the box contains a large portion of background or nearby cluttered objects as disturbance. Moreover, it will be greatly affected by nonmaximum suppression, see <ref type="figure" target="#fig_1">Fig. 1</ref>(a).</p><p>As described above, the small/cluttered objects problem can be interleaved with the rotation variance. In this paper, we aim to address the first challenge by seeking a new way of dismissing the noisy interference from both background and other foreground objects. While for rotation alignment, a new rotation loss is devised accordingly. Our both techniques can serve as plug in for existing detectors <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, in an out of box manner. We give further description as follows.</p><p>For small and cluttered object detection, we devise a denoising module and in fact denoising has not been studied for objection detection. We observe two common types of noises that are orthogonal to each other: i) image level noise, which is object-agnostic, and ii) instance level noise, specifically often in the form of mutual interference between objects, as well as background interference. Such noises are ubiquitous and pronounced in aerial images which are remotely sensed. In fact, denoising has been a long standing task <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> in image processing while they are rarely designated for object detection, and the denoising is finally performed on raw image for the purpose of image enhancement rather than downstream semantic tasks, especially in an end-to-end manner.</p><p>In this paper, we explore the way of performing instance level denoising (InLD) and particularly in the feature map (i.e. latent layers' outputs by CNNs), for robust detection. The hope is to reduce the inter-class feature coupling and intra-class interference, meanwhile blocking background interference. To this end, a novel InLD component is designated to decouple the features of different object categories into their respective channels approximately. Meanwhile, in the spatial domain, the features of the object and background are enhanced and weakened, respectively. It is worth noting that the above idea is conceptually similar to but inherently different from the recent efforts <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> for image level feature map denoising (ImLD), which is used as a way of enhancing the image recognition model's robustness against attack, rather than location sensitive object detection. Readers are referred to Tab. 5 for a quick verification that our InLD can more effectively improve detection than ImLD for both horizontal and rotation cases.</p><p>On the other hand, as discussed above, as a closely interleaved problem to small/cluttered object detection, accurate rotation estimation is addressed by devising a novel IoU-Smooth L1 loss. It is motivated by the fact that the existing state-of-the-art regression-based rotation detection methods e.g. five-parameter regression <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> suffer from the issue of discontinuous boundaries, which is inherently caused by the periodicity of angular (PoA) and exchangeability of edges (EoE) <ref type="bibr" target="#b25">[26]</ref> (see details in Sec. 3.3.2).</p><p>We conduct extensive ablation study and experiments on multiple datasets including both aerial images from DOTA <ref type="bibr" target="#b9">[10]</ref>, DIOR <ref type="bibr" target="#b10">[11]</ref>, UCAS-AOD <ref type="bibr" target="#b26">[27]</ref>, as well as natural image dataset COCO <ref type="bibr" target="#b7">[8]</ref>, scene text dataset ICDAR2015 <ref type="bibr" target="#b27">[28]</ref>, small traffic light dataset BSTLD <ref type="bibr" target="#b28">[29]</ref> and our newly released S 2 TLD to illustrate the promising effects of our techniques.</p><p>The preliminary content of this paper has partially appeared in the conference version [30] 1 , with the detector 1. Compared with the conference version, this journal version has made the following extensions: i) we take a novel feature map denoising perspective to the small and cluttered object detection problem, and specifically devise a new instance-level feature denoising technique for detecting small and cluttered objects with little additional computation and parameter overhead; ii) comprehensive ablation study of our instance-level feature denoising component across datasets, which can be easily plugged into existing detectors. Our new method significantly outperforms our previous detector in the conference version (e.g. overall detection accuracy 72.61% versus 76.81%, and 75.35% versus 79.35% on the OBB and HBB task of DOTA-v1.0 dataset, respectively); iii) We collect, annotate and release a new small traffic light dataset (5,786 images with 1,4130 traffic light instances across five categories) to further verify the versatility and generalization performance of the instance-level denoising module; iv) last but not least, the paper has been largely rephrased and expanded to cover the discussion of upto-date works including those on image denoising and small object detection. The source code is also released.</p><p>named SCRDet (Small, Cluttered, and Rotated Object Detector). In this journal version, we extend our improved detector called SCRDet++. The overall contributions are: 1) To our best knowledge, we are the first to develop the concept of instance level noise (at least in the context of object detection), and design a novel Instance-Level Denoising (InLD) module in feature map. This is realized by supervised segmentation whose ground truth is approximately obtained by the bounding box in object detection. The proposed module effectively addresses the challenges in detecting small size, arbitrary direction, and dense distribution objects with little computation and parameter increase.</p><p>2) Towards more robust handling of arbitrarily-rotated objects, an improved smooth L1 loss is devised by adding the IoU constant factor, which is tailored to solve the boundary problem of the rotating bounding box regression.</p><p>3) We create and release a real-world traffic light dataset: S 2 TLD. It consists of 5,786 images with 14,130 traffic light instances across five categories: red, green, yellow, off and wait on. It further verifies the effectiveness of InLD, and it is available at https://github.com/Thinklab-SJTU/S2TLD. 4) Our method achieves state-of-the-art performance on public datasets for rotation detection in complex scenes like the aerial images. Experiments also show that our InLD module, which can be easily plugged into existing architectures, can notably improve detection on different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We first discuss existing detectors for both horizontal bounding box based detection and rotation detection. Then some representative works on image denoising and small object detection are also introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Horizontal Region Object Detection</head><p>There is an emerging line of deep network based object detectors. R-CNN <ref type="bibr" target="#b0">[1]</ref> pioneers the CNN-based detection pipeline. Subsequently, region-based models such as Fast R-CNN <ref type="bibr" target="#b2">[3]</ref>, Faster R-CNN <ref type="bibr" target="#b6">[7]</ref>, and R-FCN <ref type="bibr" target="#b5">[6]</ref> are proposed, which achieves more cost-effective detection. SSD <ref type="bibr" target="#b3">[4]</ref>, YOLO <ref type="bibr" target="#b4">[5]</ref> and RetinaNet <ref type="bibr" target="#b14">[15]</ref> are representative single-stage methods, and their single-stage structure further improves detection speed. In addition to anchor-based methods, many anchor-free also have become popular in recent years. FCOS <ref type="bibr" target="#b30">[31]</ref>, CornerNet <ref type="bibr" target="#b31">[32]</ref>, CenterNet <ref type="bibr" target="#b32">[33]</ref> and ExtremeNet <ref type="bibr" target="#b33">[34]</ref> attempt to predict some keypoints of objects such as corners or extreme points, which are then grouped into bounding boxes, and these detectors have also been applied to the field of remote sensing <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. R-P-Faster R-CNN <ref type="bibr" target="#b36">[37]</ref> achieves satisfactory performance in small datasets. The method <ref type="bibr" target="#b37">[38]</ref> combines both deformable convolution layers <ref type="bibr" target="#b38">[39]</ref> and region-based fully convolutional networks (R-FCN) to improve detection accuracy further. The work <ref type="bibr" target="#b39">[40]</ref> adopts top-down and skipped connections to produce a single high-level feature map of a fine resolution, improving the performance of the deformable Faster R-CNN model. IoU-Adaptive R-CNN <ref type="bibr" target="#b40">[41]</ref> reduces the loss of small object information by a new IoU-guided detection network. FMSSD <ref type="bibr" target="#b41">[42]</ref> aggregates the context information both in multiple scales and the same scale feature maps. However, objects in aerial  <ref type="bibr" target="#b14">[15]</ref> as an embodiment). Our SCRDet++ mainly consists of four modules: basic embodiment for feature extraction, Image-level denoising for removing common image noise, instance-level denoising module for suppressing instance noise (i.e., inter-class feature coupling and distraction between intra-class and background) and the 'class+box' branch for predicting classification score and bounding box position. 'C' and 'A' represent the number of object categories and the number of anchor at each feature point, respectively.</p><p>images with small size, cluttered distribution and arbitrary rotation are still challenging, especially for horizontal region detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Arbitrary-Oriented Object Detection</head><p>The demand for rotation detection has been increasing recently like for aerial images and scene texts. Recent advances are mainly driven by the adoption of rotated bounding boxes or quadrangles to represent multi-oriented objects. For scene text detection, RRPN <ref type="bibr" target="#b15">[16]</ref> employs rotated RPN to generate rotated proposals and further perform rotated bounding box regression. TextBoxes++ <ref type="bibr" target="#b42">[43]</ref> adopts vertex regression on SSD. RRD <ref type="bibr" target="#b43">[44]</ref> further improves TextBoxes++ by decoupling classification and bounding box regression on rotation-invariant and rotation sensitive features, respectively. EAST <ref type="bibr" target="#b44">[45]</ref> directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps with a single neural network. Recent text spotting methods like FOTS <ref type="bibr" target="#b45">[46]</ref> show that training text detection and recognition simultaneously can greatly boost detection performance. In contrast, aerial images object detection is more challenging: first, multi-category object detection requires the generalization of the detector. Second, small objects in aerial images are usually densely arranged on a large scale. Third, aerial image detection requires a more robust algorithm due to the variety of noises. Many aerial images rotation detection algorithms are designed for different problems. ICN <ref type="bibr" target="#b22">[23]</ref>, ROI Transformer <ref type="bibr" target="#b23">[24]</ref>, and SCRDet <ref type="bibr" target="#b29">[30]</ref> are representative of two-stage aerial images rotation detectors, which are mainly designed from the perspective of feature extraction. From the results, they have achieved good performance in small or dense object detection. Compared to the previous methods, R <ref type="bibr" target="#b2">3</ref> Det <ref type="bibr" target="#b17">[18]</ref> and RSDet <ref type="bibr" target="#b46">[47]</ref> are based on a single-stage detection method which pay more attention to the trade-off of accuracy and speed. Gliding Vertex <ref type="bibr" target="#b47">[48]</ref> and RSDet <ref type="bibr" target="#b46">[47]</ref> achieve more accurate object detection via quadrilateral regression prediction. Axis Learning <ref type="bibr" target="#b35">[36]</ref> and O 2 -DNet <ref type="bibr" target="#b34">[35]</ref> are combined with the latest popular anchor-free ideas, to overcome the problem of too many anchors in anchor-based detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Denoising</head><p>Deep learning has obtained much attention in image denoising. The survey <ref type="bibr" target="#b18">[19]</ref> divides image denoising using CNNs into four types (see the references therein): 1) additive white noisy images; 2) real noisy images; 3) blind denoising and 4) hybrid noisy images, as the combination of noisy, blurred and low-resolution images. In addition, image denoising also helps to improve the performance of other computer vision tasks, such as image classification <ref type="bibr" target="#b19">[20]</ref>, object detection <ref type="bibr" target="#b20">[21]</ref>, semantic segmentation <ref type="bibr" target="#b21">[22]</ref>, etc. In addition to image noise, we find that there is also instance noise in the field of object detection. Instance noise describes object-aware noise, which is more widespread in object detection than object-agnostic image noise. In this paper, we will explore the application of image-level denoising and instance-level denoising techniques to object detection in complex scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Small Object Detection</head><p>Small object detection remains an unsolved challenge. Common small object solutions include data augmentation <ref type="bibr" target="#b48">[49]</ref>, multi-scale feature fusion <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b49">[50]</ref>, tailored sampling strategies <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, generative adversarial networks <ref type="bibr" target="#b52">[53]</ref>, and multi-scale training <ref type="bibr" target="#b53">[54]</ref> etc. In this paper, we show that denoising is also an effective means to improve the detection performance of small objects. In complex scenes, the feature information of small objects is often overwhelmed by the background area, which often contains a large number of similar objects. Unlike ordinary image-level denoising, we will use instance-level denoising to improve the detection capabilities of small objects, which is a new perspective. This paper mainly considers designing a generalpurpose instance level feature denoising module, to boost the performance of horizontal detection and rotation detection in challenging aerial imagery, as well as natural images and scene texts. Besides, we also design an IoU-Smooth L1 loss to solve the boundary problem of the arbitrary-oriented object detection for more accurate rotation estimation. It mainly consists of four modules: i) feature extraction via CNNs which can take different forms of CNNs from existing detectors e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, ii) image-level denoising (ImLD) module for removing common image noise, which is optional as its effect can be well offset by the subsequent InLD as devised in this paper; iii) instance-level denoising (InLD) module for suppressing instance noise (i.e., inter-class feature coupling and distraction between intra-class and background) and iv) the class and box branch for predicting score and (rotated) bounding box. Specifically, we first describe our main technique i.e. instance-level denoising module (InLD) in Sec. 3.2, which further contains a comparison with the image level denoising module (ImLD). Finally, we detail the network learning which involves a specially designed smooth loss for rotation estimation in Sec. 3.3. Note that in experiments we show that InLD can replace and strike a more effective role for detection than ImLD, making ImLD a dispensable component in our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Instance-level Feature Map Denoising</head><p>In this subsection, we present our devised instance-level feature map denoising approach. To emphasis the importance of our instance-level operation, we further compare it with image-level denoising in feature map, which is also adopted for robust image recognition model learning in <ref type="bibr" target="#b19">[20]</ref>. To our best knowledge, our approach is the first for using (instance level) feature map denoising for object detection. The denoising module can be learned in an end-to-end manner together with other modules, which is optimized for the object detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Instance-Level Noise</head><p>Instance-level noise generally refers to the mutual interference among objects, and also that from background. We discuss its properties in the following aspects. In particular, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, the adversary effect to object detection is especially pronounced in the feature map that calls for feature space denoising rather than on the raw input image. 1) The non-object with object-like shape has a higher response in the feature map, especially for small objects (see the top row of <ref type="figure" target="#fig_4">Fig. 3</ref>).</p><p>2) Clutter objects that are densely arranged tend to suffer the issue for inter-class feature coupling and intra-class feature boundary blurring (see the middle row of <ref type="figure" target="#fig_4">Fig. 3</ref>).</p><p>3) The response of object is not prominent enough surrounded by the background (see the bottom row of <ref type="figure" target="#fig_4">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Mathematical Modeling of Instance-Level Denoising</head><p>To dismiss instance level noise, one can generally refer to the idea of attention mechanism, as a common way of reweighting the convolutional response maps to highlight the important parts and suppress the uninformative ones, such as spatial attention <ref type="bibr" target="#b54">[55]</ref> and channel-wise attention <ref type="bibr" target="#b55">[56]</ref>. We show that existing aerial image rotation detectors, including FADet <ref type="bibr" target="#b26">[27]</ref>, SCRDet <ref type="bibr" target="#b29">[30]</ref> and CAD-Det <ref type="bibr" target="#b24">[25]</ref>, often use the simple attention mechanism to re-weight the output, which can be reduced into the following general form:</p><formula xml:id="formula_0">Y = A(X) X = Ws X Wc = Ws C i=1 xi ? w i c (1)</formula><p>where X, Y ? R C?H?W represents two feature maps of input image. The attention function A(X) refers to the proposal output by a certain attention module e.g. <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Note is the element-wise product. W s ? R H?W and W c ? R C denote the spatial weight and channel weight. w i c indicates the weight of the i-th channel, respectively. Throughout the paper, means the concatenation operation for connecting tensor along the feature map's channels.</p><p>However, Eq. 1 simply distinguishes feature response between objects and background in spatial domain, and w i c is only used to measure the importance of each channel. In other words, the interaction between intra-class objects The noise is randomly generated by a Gaussian function with a mean value of 0 and a variance of 0.005. The first and third columns: images; the rest columns: feature maps. The contrast between foreground and background in the feature map of the clean image is more obvious (second column), and the boundaries between dense objects are clearer (fourth column). and inter-class objects is not considered which is important for detection in complex scene. We are aimed to devise a new network that can not only distinguish object from background, but also weaken the mutual interference among objects. Specifically, we propose adding instancelevel denoising (InLD) module at intermediate layers of convolutional networks. The key is to decouple the feature of different object categories into their respective channels, and meanwhile the features of objects and background are enhanced and weakened in the spatial domain, respectively.</p><p>As a result, our new formulation is as follows, which considers the total I number of object categories with one additional category for background:</p><formula xml:id="formula_1">Y = DInLD(X) X = WInLD X = I+1 i=1 W i InLD X i = I+1 i=1 C i j=1 w i j x i j (2) where W InLD ? R C?H?W is a hierarchical weight. W i</formula><p>InLD ? R Ci?H?W , X i ? R Ci?H?W denotes the weight and feature response corresponding to the i-th category, and its channel number is denoted by</p><formula xml:id="formula_2">C i , for C = I i=1 C i +C bg . w i</formula><p>j and x i j denotes the weight and feature of the i-th category along the j-th channel, respectively.</p><p>As can be seen from Eq. 1 and Eq. 2, D InLD (X) can be approximated as a combination of multiple A i (X i ), which denotes the attention function of category i. Thus we have:</p><formula xml:id="formula_3">Y = DInLD(X) X = I+1 i=1 A i (X i ) X i<label>(3)</label></formula><p>Without loss of generality, consider an image containing objects belonging to the first I 0 (I 0 ? I) categories. In this paper, we aim to decouple the above formula into three parts as concatenated to each other (see <ref type="figure" target="#fig_6">Fig. 5</ref>):</p><formula xml:id="formula_4">Y = I 0 i=1 C i p=1 w i p x i p categories in image ? I j=I 0 +1 C j q=1 w j q x j q categories not in image ? C bg k=1 w bg k x bg k background<label>(4)</label></formula><p>For background and unseen categories not in image, ideally the response is filtered by our devised denoising module to be as small as possible. From this perspective, Eq. 4 can be further interpreted by:</p><formula xml:id="formula_5">Y = I 0 i=1 C i p=1 w i p x i p categories in image ? I j=I 0 +1 Oj categories not in image ? O bg background (5)</formula><p>where O denotes tensor with small feature response one aims to achieve, for each category O j and background O bg .</p><p>In the following subsection, we show how to achieve the above decoupled feature learning among categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Implementation of Instance-Level Denoising</head><p>Based on the above derivations, we devise a practical neural network based implementation. Our analysis starts with the simplest case with a single channel for each category's weight W i InLD in Eq. 2, or namely C i = 1. In this setting, the learned weight W InLD can be regarded as the result of semantic segmentation of the image for specific categories (a three-dimensional one-hot vector). Then more channels of weight W InLD in D InLD can be guided by semantic segmentation, as illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref> and <ref type="figure" target="#fig_6">Fig. 5</ref>. In semantic segmentation task, the feature responses of each category on the previous layers of the output layer tend to be separated in the channel dimension, and the feature responses of the foreground and background in the spatial dimension are also polarized. Hence one can adopt a semantic segmentation network for the operations in Eq. 5. Another advantage for holding this semantic segmentation view is that it can be conducted in an end-to-end supervised fashion, whose learned denoising weights can be more reliable and effective than the self-attention based alternatives <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. In <ref type="figure" target="#fig_2">Fig. 2</ref>, we give a specific implementation as follows. The input feature map expands the receptive field by N dilated convolutions <ref type="bibr" target="#b56">[57]</ref> and a 1 ? 1 convolutional layer at first. For instance, the values of N take the numbers of {1, 1, 1, 1, 1} on pyramid levels P3 to P7, respectively as set in our experiments. The feature map is then processed by two parallel 1 ? 1 convolutional layers to obtain the two important outputs. One output (a three-dimensional one-hot feature map) is used to perform coarse multi-class segmentation, and the annotated bounding box in detection tasks can be used as the approximate ground truth. The hope is that this output will guide the other output into a denoising feature map.</p><p>As shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, this denoising feature map and the original feature map are combined (by dot operation) to obtain the final decoupled feature map. The purpose is in two-folds: along the channel dimension, inter-class feature responses of different object categories (excluding the background) are basically decoupled into their respective channels; In the spatial dimension, intra-class feature boundaries are sharpened due to the feature response of the object area is enhanced and background is weakened. As such, the three issues as raised in the beginning of this subsection are alleviated.</p><p>As shown in the upper right corner of <ref type="figure" target="#fig_2">Fig. 2</ref>, the classification model is decomposed into two terms: objectness and category classification, as written by:</p><formula xml:id="formula_6">P (class i , object) = P (class i |object) category classification * P (object) objectness (6)</formula><p>This probability map P (object) relates to whether the anchor for each feature point is an object. While the above decoupled features are directly used for object classification P (class i |object) (as well as rotation regression which will be discussed in Sec. 3.3).</p><p>During training, the probability map P (object) will be used as a weight for the regression loss (see Eq. 9), making those ambiguous positive samples get smaller weights and giving higher quality positive samples more attention. We find in the experiment that the introduction of the probability map can speed up the convergence of the model and improve the detection results, as shown in Tab. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Comparison with Image-Level Denoising</head><p>Image denoising is a fundamental task in image processing, which may impose notable impact to image recognition, as has been recently studied and verified in <ref type="bibr" target="#b19">[20]</ref>. Specifically, the work <ref type="bibr" target="#b19">[20]</ref> shows that the transformations performed by the network layers exacerbate the perturbation, and the hallucinated activations can overwhelm the activations due to true signal, which leads to worse prediction.</p><p>Here we also study this issue in the context of aerial images through directly borrow the image level denoising model <ref type="bibr" target="#b19">[20]</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, we add Gaussian noise on the raw aerial images and compare with the clean ones. The same feature map on clean and noisy images, extracted from the same channel of a res3 block in the same detection network trained on clean images are visualized. Though the noise has little effect and it is difficult to distinguish by naked eyes. However, it becomes more obvious in the feature map such that the objects are gradually submerged in the background or the boundary between the objects tends to be blurred.</p><p>Since the convolution operation and the traditional denoising filters are highly correlated, we resort to a potential solution <ref type="bibr" target="#b19">[20]</ref> which employs convolutional layers to simulate different types of differential filters, such as non-local means, bilateral filtering, mean filtering, and median filtering. Inspired by the success of these operation in adversarial attacks <ref type="bibr" target="#b19">[20]</ref>, in this paper we migrate and extend these differential operations for object detection. We show the generic form of ImLD in <ref type="figure" target="#fig_2">Fig. 2</ref>. It processes the input features by a denoising operation, such as non-local means or other variants. The denoised representation is first processed by a 1 ? 1 convolutional layer, and then added to the module's input via a residual connection. The simulation of ImLD is expressed as follows:</p><formula xml:id="formula_7">Y = F(X) + X<label>(7)</label></formula><p>where F(X) is the output by a certain filter. X, Y ? R C?H?W represent the whole feature map of input image. The effect of the imposed denosing module is shown in Tab. 1. In the following, we further show that the more notable detection improvement comes from the InLD module and its effect can well cover the image level one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function Design and Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Horizontal Object Detection</head><p>For horizontal detection, regressing the bounding box by:</p><formula xml:id="formula_8">tx = (x ? xa)/wa, ty = (y ? ya)/ha tw = log(w/wa), t h = log(h/ha), t x = (x ? xa)/wa, t y = (y ? ya)/ha t w = log(w /wa), t h = log(h /ha)<label>(8)</label></formula><p>where x, y, w, h denote the box's center coordinates, width, and height, respectively. Variables x, x a , x are for the ground-truth box, anchor box, and predicted box, respectively (likewise for y, w, h).</p><p>The multi-task loss of horizontal detection is defined as: </p><formula xml:id="formula_9">L h = ?reg N</formula><p>where N indicates the number of anchors, t n is a binary value (t n = 1 for foreground and t n = 0 for background, no    regression for background). p(object n ) indicates the probability that the current anchor is the object. v nj denotes the predicted offset vectors of the n-th anchor, v nj is the targets vector between n-th anchor and ground-truth it matches. t n represents the label of object, p n is the probability distribution of various classes calculated by sigmoid function. u ij , u ij denote the label and predict of mask's pixel respectively. The hyper-parameter ? reg , ? cls , ? InLD control the trade-off and are set to 1 by default. The classification loss L cls is focal loss <ref type="bibr" target="#b14">[15]</ref>. The regression loss L reg is smooth L1 loss as defined in <ref type="bibr" target="#b2">[3]</ref>, and the InLD loss L InLD is pixel-wise softmax cross-entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Rotation Object Detection</head><p>In contrast, we need to redefine the representation of the bounding box. <ref type="figure" target="#fig_8">Fig. 6(a)</ref> shows the rectangular definition of the 90 degree angle representation range <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. ? denotes the acute angle to the x-axis, and for the other side we refer it as w. Note this definition is also officially adopted by OpenCV 2 .</p><p>Rotation detection needs to carefully address the boundary problem. In particular, there exists the boundary problem for the angle regression, as shown in <ref type="figure" target="#fig_9">Fig. 7(a)</ref>. It shows that an ideal form of regression (the blue box rotates counterclockwise to the red box), but the loss of this situation is very large due to the periodicity of angular (PoA) and exchangeability of edges (EoE). Therefore, the model has to be regressed in other complex forms like in <ref type="figure" target="#fig_9">Fig. 7(b)</ref> (such as the blue box rotating clockwise while scaling w and h), increasing the difficulty of regression, as shown in <ref type="figure" target="#fig_10">Fig. 8(a)</ref>.</p><p>As for the regression equation of ?, we use two forms as the baseline to be compared:</p><p>? Direct regression (default in this paper), namely Reg. (??). The model directly predicts the angle offset t * ? :</p><formula xml:id="formula_11">t ? =(? ? ? a ) ? ?/180 t ? =(? ? ? a ) ? ?/180<label>(10)</label></formula><p>? Indirect regression, marked as Reg. * (sin ?, cos ?). It predicts two vectors (t * sin ? and t * cos ? ) to match the two targets from ground truth (t sin ? and t cos ? ):</p><p>t sin ? = sin (? ? ?/180), t cos ? = cos (? ? ?/180) t sin ? = sin (? ? ?/180), t cos ? = cos (? ? ?/180)</p><p>To ensure that t 2 sin ? + t 2 cos ? = 1 is satisfied, we perform the following normalization processing:</p><formula xml:id="formula_13">t sin ? = t sin ? t 2 sin ? + t 2 cos ? t cos ? = t cos ? t 2 sin ? + t 2 cos ?<label>(12)</label></formula><p>It should be noted that indirect regression is a simpler way to avoid boundary problems.</p><p>In order to better solve the boundary problem, we introduce the IoU constant factor |?log(IoU )| |Lreg(v j ,vj )| in the traditional smooth L1 loss, as shown in Eq. 13. This new loss function is named IoU-smooth L1 loss. It can be seen that in the boundary case, the loss function is approximately equal to | ? log(IoU )| ? 0, eliminating the sudden increase in loss caused by |L reg (v j , v j )|, as shown in <ref type="figure" target="#fig_10">Fig. 8(b)</ref>. The new regression loss can be divided into two parts:</p><formula xml:id="formula_14">Lreg(v j ,vj )</formula><p>|Lreg(v j ,vj )| determines the direction of gradient propagation, and | ? log(IoU )| for the magnitude of gradient. In addition, using IoU to optimize location accuracy is consistent 2. https://opencv.org <ref type="table">/  TABLE 3</ref> Ablative study by accuracy (%) of the number of dilated convolution on pyramid levels and the InLD loss L InLD in InLD on OBB task of DOTA. It can be found that supervised learning is the main contribution of InLD rather than more convolution layers.   </p><formula xml:id="formula_15">Lreg(v nj , vnj) |Lreg(v nj , vnj)| |? log(IoU )| + ? cls N N n=1 L cls (pn, tn) + ?InLD h ? w h i w j LInLD(u ij , uij)<label>(13)</label></formula><p>where IoU is the overlap of prediction and ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Experiments are performed on a server with GeForce RTX 2080 Ti and 11G memory. We first give the description of the dataset, and then use these datasets to verify the advantage of the proposed method. Source code is available at https: //github.com/SJTU-Thinklab-Det/DOTA-DOAI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Protocols</head><p>We choose a wide variety of public datasets from both aerial images as well as natural images and scene texts for evaluation. The details are as follows. DOTA <ref type="bibr" target="#b9">[10]</ref>: DOTA-v1.0 is a complex aerial image dataset for object detection, which contains objects exhibiting a wide variety of scales, orientations, and shapes. DOTA-v1.0 contains 2,806 aerial images and 15 common object categories from different sensors and platforms. The fully annotated DOTA-v1.0 benchmark contains 188,282 instances, each of which is labeled by an arbitrary quadrilateral. There are two detection tasks for DOTA: horizontal bounding boxes (HBB) and oriented bounding boxes (OBB). The training set, validation set, and test set account for 1/2, 1/6, 1/3 of the entire data set, respectively. In contrast, DOTA-v1.5 uses the same images as DOTA-v1.0, but extremely small instances (less than 10 pixels) are also annotated. Moreover, a new category, containing 402,089 instances in total is added in this version. While DOTA-v2.0 contains 18 common categories, 11,268 images and 1,793,658 instances. Compared to DOTA-v1.5, it includes the new categories. The 11,268 images in DOTA-v2.0 are split into training, validation, test-dev, and testchallenge sets. We divide the images into 600 ? 600 subimages with an overlap of 150 pixels and scale it to 800 ? 800. DIOR <ref type="bibr" target="#b10">[11]</ref>: DIOR is another large aerial images dataset labeled by a horizontal bounding box. It consists of 23,463 images and 190,288 instances, covering 20 object classes. DIOR has a large variation of object size, not only in spatial resolutions, but also in the aspect of inter-class and intra-class size variability across objects. The complexity of DIOR is also reflected in different imaging conditions, weathers, seasons, and image quality, and it has high inter-class similarity and intra-class diversity. The training protocol of DIOR is basically consistent with DOTA-v1.0. The short names c1-c20 for categories in our experiment are defined as: Airplane, Airport, Baseball field, Basketball court, Bridge, Chimney, Dam, Expressway service area, Expressway toll station, Golf field, Ground track field, Harbor, Overpass, Ship, Stadium, Storage tank, Tennis court, Train station, Vehicle, and Wind mill. <ref type="bibr" target="#b78">[79]</ref>: UCAS-AOD contains 1,510 aerial images of approximately 659 ? 1,280 pixels, it contains two categories of 14,596 instances. In line with <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we randomly select 1,110 for training and 400 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCAS-AOD</head><p>BSTLD <ref type="bibr" target="#b28">[29]</ref>: BSTLD contains 13,427 camera images at a resolution of 720 ? 1,280 pixels and contains about 24,000 annotated small traffic lights. Specifically, 5,093 training images are annotated by 15 labels every 2 seconds, but only 3,153 images contain the instance, about 10,756. There are very few instances of many categories, so we reclassify them into 4 categories (red, yellow, green, off). In contrast, 8,334 consecutive test images are annotated by 4 labels at about 15 fps. In this paper, we only use the training set of BSTLD, whose median traffic light width is 8.6 pixels. In the experiment, we divide BSTLD training set into a training set and a test set according to the ratio of 6 : 4. Note that we use the RetinaNet with P2 feature level and FPN to verify InLD, and scale the size of the input image to 720 ? 1,280.    S 2 TLD: S 2 TLD 3 is our collected and annotated traffic light dataset as released in this paper. It contains 5,786 images of approximately 1,080 ? 1,920 pixels (1,222 images) and 720 ? 1,280 pixels (4,564 images). It also contains 5 categories (namely red, yellow, green, off and wait on) of 14,130 instances. The scenes cover a variety of lighting, weather and traffic conditions, including busy street scenes innercity, dense stop-and-go traffic, strong changes in illumination/exposure, flickering/fluctuating traffic lights, multiple visible traffic lights, image parts that can be confused with traffic lights (e.g. large round tail lights), as shown in <ref type="figure" target="#fig_12">Fig. 9</ref>. The training strategy is consistent with BSTLD.</p><p>In addition to the above datasets, we also use natural image dataset COCO <ref type="bibr" target="#b7">[8]</ref> and scene text dataset ICDAR2015 <ref type="bibr" target="#b27">[28]</ref> for further evaluation. <ref type="bibr" target="#b2">3</ref>. S 2 TLD is available at https://github.com/Thinklab-SJTU/S2TLD</p><p>The experiments are initialized by ResNet50 <ref type="bibr" target="#b59">[60]</ref> by default unless otherwise specified. The weight decay and momentum for all experiments are set 0.0001 and 0.9, respectively. We employ MomentumOptimizer over 8 GPUs with a total of 8 images per minibatch. We follow the standard evaluation protocol of COCO, while for other datasets, the anchors of RetinaNet-based method have areas of 32 2 to 512 2 on pyramid levels from P3 to P7, respectively. At each pyramid level we use anchors at seven aspect ratios {1, 1/2, 2, 1/3, 3, 5, 1/5} and three scales {2 0 , 2 1/3 , 2 2/3 }. For rotating anchor-based method (RetinaNet-R), the angle is set by an arithmetic progression from ?90 ? to ?15 ? with an interval of 15 degrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>The ablation study covers the detailed evaluation of the effect of image level denoising (ImLD) and instance level denoising (InLD), as well as their combination.</p><p>Effect of Image-Level Denoising. We experiment with five denoising modules introduced in [20] on DOTA-v1.0. We use our previous work R 3 Det <ref type="bibr" target="#b17">[18]</ref>, one of the most stateof-the-art methods on the DOTA-v1.0, as the baseline. From Tab. 1, one can observe that most methods work effectively <ref type="bibr">TABLE 9</ref> AP and mAP (%) across categories of OBB and HBB task on DOTA. MS indicates multi-scale training and testing. <ref type="table">Backbone  PL  BD  BR  GTF  SV  LV  SH  TC  BC  ST  SBF  RA  HA  SP  HC  mAP  Two-stage</ref>   except the mean filtering. Among them, the non-local with Gaussian is the most effective (1.95% higher).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBB Task</head><p>Effect of Instance-Level Denoising. The purpose of designing InLD is to make the feature of different categories decoupled in the channel dimension, while the features of the object and non-object are enhanced and weakened in the spatial dimension, respectively. We have designed some verification tests and obtained positive results as shown in Tab. 2. We first explore the utility of weakening the nonobject noise by binary semantic segmentation, and the detection mAP has increased from 65.73% to 68.12%. The result on multi-category semantic segmentation further proves that there is indeed interference between objects, which is reflected by the 1.31% increase of detection mAP (reaching 69.43%). From the above two experiments, we can preliminarily speculate that the interference in the non-object area is the main reason that affects the performance of the detector. It is surprising to find that coproducting the prediction score for objectness (see P (object) in Eq. 6) can further improve performance and speed up training with a final accuracy of 69.81%. Experiments in Tab. <ref type="bibr" target="#b5">6</ref> show that InLD has greatly improved the R 3 Det's performance of small objects, such as BR, SV, LV, SH, SP, HC, which increased by 3.94%, 0.84%, 4.32%, 8.48%, 10.15%, and 9.41%, respectively. While the  accuracy is greatly improved, the detection speed of the model is only reduced by 1fps (at 13fps). In addition to the DOTA-v1.0 dataset, we have used more datasets to verify the general applicability, such as DIOR, ICDAR, COCO and S 2 TLD. InLD obtains 1.44%, 1.55%, 1.4% and 0.86% improvements in each of the four datasets according to Tab. 5 and <ref type="figure" target="#fig_1">Fig. 10</ref> shows the visualization results before and after using InLD. In order to investigate whether the performance improvement brought by InLD is due to the extra computation (dilated convolutions) or supervised learning (L InLD ), we perform ablation experiments by controlling the number of dilated convolutions and supervision signal. Tab. 3 shows that supervised learning is the main contribution of InLD rather than more convolution layers.</p><p>In particular, we conduct a detailed study on the SJTU Small Traffic Light Dataset (S 2 TLD) which is our newly released traffic detection dataset. Compared with BSTLD, S 2 TLD has more available categories. In addition, S 2 TLD contains two different resolution images taken from two different cameras, which can be used for more challenging detection tasks. Tab. 4 shows the effectiveness of InLD on these two traffic light datasets.</p><p>Effect of combining ImLD and InLD. A natural idea is whether we can combine these two denoising structures, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. For more comprehensive study, we perform detailed ablation experiments on different datasets and different detection tasks. The experimental results are listed in Tab. 5, and we tend to get the following remarks:</p><p>1) Most of the datasets are relatively clean, so ImLD does not obtain a significant increase in all datasets. 2) The performance improvement of detectors with InLD is very significant and stable, and is superior to ImLD.</p><p>3) The gain by the combination of ImLD and InLD is not large, mainly because their effects are somewhat overlapping: InLD weakens the feature response of the non-object region while weakening the image noise interference.</p><p>Therefore, ImLD is an optional module depending on the dataset and computing environment. We will not use ImLD in subsequent experiments unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of IoU-Smooth L1 Loss on detectors and datasets.</head><p>The IoU-Smooth L1 loss 4 eliminates the boundary effects of the angle, making it easier for the model to regress to the objects coordinates. Tab. 7 shows that new loss improves three detectors' accuracy to 69.83%, 68.65% and 76.20%. Angle direct regression (Reg.) always suffer from boundary discontinuity. In contrast, angle indirect regression (Reg * .) is a simpler way to avoid it and has an advantage in DOTA-v1.0, DOTA-v1.5 and DOTA-v2.0 according to Tab. 8. IoU-Smooth L1 Loss further improves the performance to 66.99%, 59.16% and 46.31% on three datasets.</p><p>Effect of data augmentation and backbone. Using ResNet101 as backbone and data augmentation (random horizontal, vertical flipping, graying, and rotation), we observe a reasonable improvement in Tab. 6 (69.81% ? 72.98%). We improve the final performance of the model from 72.98% to 74.41% by using ResNet152 as backbone. <ref type="bibr" target="#b3">4</ref>. Source code of IoU-Smooth L1 Loss is separately available at: https: //github.com/yangxue0827/RotationDetection (a) Small vehicle and large vehicle (HBB task).</p><p>(b) Plane (OBB task). Due to the extreme imbalance of categories in the dataset, this provides a notable advantage to data augmentation, but we have found that this does not affect the functioning of InLD under these heave settings, from 72.81% to 74.41%. All experiments are performed on the OBB task on DOTA-v1.0, and the final model based on R 3 Det is also named R 3 Det++ 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art Methods</head><p>We compare our proposed InLD with the state-of-the-art algorithms on two datasets DOTA-v1.0 <ref type="bibr" target="#b9">[10]</ref> and DIOR <ref type="bibr" target="#b10">[11]</ref>. Our model outperforms all other models. <ref type="bibr" target="#b4">5</ref>. Code of R 3 Det and R 3 Det++ are all available at https://github. com/Thinklab-SJTU/R3Det Tensorflow.</p><p>Results on DOTA-v1.0. We compare our results with the state-of-the-arts results in DOTA-v1.0 as depicted in Tab. 9. The results of DOTA-v1.0 reported here are obtained by submitting our predictions to the official DOTA-v1.0 evaluation server 6 . In the OBB task, we add the proposed InLD module to a single-stage detection method (R 3 Det++) and a twostage detection method (FPN-InLD). Our methods achieve the best performance, 76.56% and 76.81% respectively. To make fair comparison, we do not use overlays of various tricks, oversized backbones, and model ensemble, which are often used on DOTA's leaderboard methods. In the HBB 6. https://captain-whu.github.io/DOTA/evaluation.html  <ref type="bibr" target="#b26">[27]</ref>, SCRDet <ref type="bibr" target="#b29">[30]</ref> and CAD-Det <ref type="bibr" target="#b24">[25]</ref> use the simple attention mechanism by Eq. 1, but our performance is far better than all. <ref type="figure" target="#fig_1">Fig. 11</ref> shows some aerial sub-images, and <ref type="figure" target="#fig_1">Fig. 12</ref> shows the aerial images of large scenes. In general, our method has the following two advantages over other methods: i) we have solved the boundary problem in rotation detection, which is not considered by many methods; ii) an instance level denoising method is used, which is very helpful for complex remote sensing images.</p><p>Results on DIOR and UCAS-AOD. DIOR is a new large-scale aerial images dataset, and has more categories than DOTA. In addition to the official baselines, we also give our final detection results in Tab. 10. It should be noted that the baseline we reproduce is higher than the official one. In the end, we obtain 77.80% and 75.11% mAP on FPN and RetinaNet based methods. Tab. 11 illustrates the comparison of performance on UCAS-AOD dataset. As we can see, our method achieves 96.95% for OBB task and is the best out of all the existing published methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented an instance level denoising technique in the feature map for improving detection especially for small and densely arranged objects e.g. in aerial images. The core idea of InLD is to make the feature of different categories decoupled over different channels, while the features of the object and non-object are enhanced and weakened in the space, respectively. Meanwhile, the IoU constant factor is added to the smooth L1 loss to address the boundary problem in rotation detection for more accurate rotation estimation. We perform extensive ablation studies and comparative experiments on multiple aerial image datasets such as DOTA, DIOR, UCAS-AOD, small traffic light dataset BSTLD and our released S 2 TLD, and demonstrate that our method achieves the state-of-the-art detection accuracy. We also use natural image dataset COCO and scene text dataset ICDAR2015 to verify the effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>X</head><label></label><figDesc>. Yang, J. Yan, W. Liao, X. Yang are with Department of Computer Science and Engineering, Shanghai Jiao Tong University, and also with the MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University. Jin Tang is with Anhui Province Key Laboratory of Multimodal Cognitive Computation, and School of Computer Science and Technology, Anhui University. Tao He is with COWAROBOT Co., Ltd., and Anhui Province Key Laboratory of Multimodal Cognitive Computation. Correspondence author: Junchi Yan E-mail: {yangxue-2019-sjtu,yanjunchi,igoliao,xkyang}@sjtu.edu.cn, tj@ahu.edu.cn, tommie.he@cowarobot.com (a) Horizontal detection. (b) Rotation detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Small, cluttered and rotated objects in complex scene whereby rotation detection plays an important role. Red boxes indicate missing detection which are suppressed by non-maximum suppression (NMS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The pipeline of our method (using RetinaNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>illustrates the pipeline of the proposed SCRDet++.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Images (left) and their feature maps before (middle) and after (right) the instance-level denoising operation. First row: non-object with object-like shape. Second row: inter-class feature coupling and intraclass feature boundary blurring. Third row: weak feature response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Feature maps corresponding to clean images (top) and to their noisy versions (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Feature map with decoupled category-specific feature signals along channels. The abbreviation 'HA', 'SP', 'SH', and 'SV' indicate 'Harbor', 'Swimming pool', 'Ship', and 'Small vehicle', respectively. 'Others' include background and unseen categories that do not appear in the image. Features of different categories are decoupled into their respective channels (top and middle), while the features of object and background are enhanced and suppressed in spatial domain, respectively (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>t</head><label></label><figDesc>n ? p(objectn) j?{x,y,w,h} Lreg(v nj , vnj) + ? cls N N n=1 L cls (pn, tn) + ?InLD h ? w h i w j LInLD(u ij , uij)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Rotation box definitions (OpenCV definition). ? denotes the acute angle to the x-axis, and for the other side we refer it as w. The range of angle representation is [?90, 0). (a) Ideal case. (b) Actual case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Boundary discontinuity of angle regression. Blue, green, red bounding box denotes anchor/proposal, ground-truth, prediction box.(a) Smooth L1 loss. (b) IoU-smooth L1 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Detection results by two losses. For this dense arrangement case, the angle estimation error will also make the classification even harder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>The short names for categories are defined as (abbreviationfull name): PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large vehicle, SH-Ship, TC-Tennis court, BC-Basketball court, ST-Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA-Harbor, SP-Swimming pool, HC-Helicopter, CC-container crane, AP-airport and HP-helipad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Illustrations of the five categories and different lighting and weather conditions in our collected S 2 TLD dataset as released in the paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(a) COCO: the red boxes represent missed objects and the orange boxes represent false alarms.(b) ICDAR2015: red arrows denote missed objects.(c) S 2 TLD: the red box represent missed object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .</head><label>10</label><figDesc>Visual illustration of detection results on the datasets of COCO, ICDAR2015, S 2 TLD before (right) and after (left) using InLD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 .</head><label>11</label><figDesc>Visual illustration of detection results on OBB task on DOTA-v1.0 of different objects by the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 12 .</head><label>12</label><figDesc>Detection examples of our proposed method in large scenarios on DOTA-v1.0 dataset. Our method can both effectively handle the dense (top plot with white bounding box) and rotating (bottom plot with red bounding box) cases. Zoom in for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Ablative study of five image level denoising settings as used in<ref type="bibr" target="#b19">[20]</ref> on the OBB task of DOTA-v1.0 dataset.</figDesc><table><row><cell>Base Model</cell><cell>Image-Level Denoising</cell><cell>mAP (%)</cell></row><row><cell></cell><cell>none</cell><cell>65.73</cell></row><row><cell></cell><cell>bilateral, dot prod</cell><cell>66.94</cell></row><row><cell>R 3 Det [18]</cell><cell>bilateral, gaussian nonlocal, dot prod</cell><cell>67.03 66.82</cell></row><row><cell></cell><cell>nonlocal, gaussian</cell><cell>67.68</cell></row><row><cell></cell><cell>nonlocal, gaussian, 3x3 mean</cell><cell>66.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Ablative study for speed and accuracy of InLD on OBB task of DOTA. Binary-Mask and Multi-Mask refer to binary and multi-class semantic segmentation, respectively. Coproduct denotes multiplying the objectness term P (object) or not in Eq. 6.</figDesc><table><row><cell>Base Model</cell><cell>Mask Type</cell><cell>Coproduct</cell><cell>FPS</cell><cell>mAP (%)</cell></row><row><cell></cell><cell>null</cell><cell>?</cell><cell>14</cell><cell>65.73</cell></row><row><cell>R 3 Det [18]</cell><cell>Binary-Mask Multi-Mask Multi-Mask</cell><cell>? ? ?</cell><cell>13.5 13 13</cell><cell>68.12 69.43 69.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Detailed ablative study by accuracy (%) of the effect of InLD on two traffic light datasets. Note the category 'wait on' is only available in our collected S 2 TLD dataset as released by this paper.</figDesc><table><row><cell>Dataset</cell><cell>Base Model</cell><cell>InLD</cell><cell>red</cell><cell>yellow</cell><cell>green</cell><cell>off</cell><cell>wait on</cell><cell>mAP</cell></row><row><cell>S 2 TLD</cell><cell>RetinaNet [15] FPN [14]</cell><cell>? ? ? ?</cell><cell>97.94 98.15 97.98 98.04</cell><cell>88.63 87.66 87.55 92.84</cell><cell>97.17 97.12 97.42 97.69</cell><cell>90.13 93.88 93.42 92.06</cell><cell>92.40 93.75 98.31 99.08</cell><cell>93.25 94.11 94.93 95.94</cell></row><row><cell>BSTLD [29]</cell><cell>RetinaNet [15] FPN [14]</cell><cell>? ? ? ?</cell><cell>69.91 70.50 89.27 89.88</cell><cell>19.71 24.05 47.82 49.93</cell><cell>77.11 77.16 92.01 92.42</cell><cell>22.33 22.51 40.73 42.45</cell><cell>----</cell><cell>47.26 48.56 67.46 68.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Ablative study by accuracy (%) of ImLD, InLD and their combination (numbers in bracket denote relative improvement against using InLD alone) on different datasets and different detection tasks.</figDesc><table><row><cell cols="3">Dataset and task</cell><cell></cell><cell>Base Model</cell><cell>Baseline</cell><cell>ImLD</cell><cell>InLD</cell><cell>ImLD + InLD</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet-H [18]</cell><cell>62.21</cell><cell>62.39</cell><cell>65.40</cell><cell>65.62 (+0.22)</cell></row><row><cell cols="4">DOTA-v1.0 OBB [10]</cell><cell>RetinaNet-R [18]</cell><cell>61.94</cell><cell>63.96</cell><cell>64.52</cell><cell>64.60 (+0.08)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>R 3 Det [18]</cell><cell>65.73</cell><cell>67.68</cell><cell>69.81</cell><cell>69.95 (+0.14)</cell></row><row><cell cols="4">DOTA-v1.0 HBB [10]</cell><cell>RetinaNet [15]</cell><cell>67.76</cell><cell>68.05</cell><cell>68.33</cell><cell>68.50 (+0.17)</cell></row><row><cell cols="2">DIOR [11]</cell><cell></cell><cell></cell><cell>RetinaNet [15] FPN [14]</cell><cell>68.05 71.74</cell><cell>68.42 71.83</cell><cell>69.36 73.21</cell><cell>69.35 (-0.01) 73.25 (+0.04)</cell></row><row><cell cols="3">ICDAR2015 [28]</cell><cell></cell><cell>RetinaNet-H [18]</cell><cell>77.13</cell><cell>-</cell><cell>78.68</cell><cell>-</cell></row><row><cell cols="2">COCO [8]</cell><cell></cell><cell></cell><cell>FPN [14] RetinaNet [15]</cell><cell>36.1 34.4</cell><cell>--</cell><cell>37.2 35.8</cell><cell>--</cell></row><row><cell>S 2 TLD</cell><cell></cell><cell></cell><cell></cell><cell>RetinaNet [15]</cell><cell>93.25</cell><cell>-</cell><cell>94.11</cell><cell>-</cell></row><row><cell cols="7">with IoU-dominated metric, which is more straightforward</cell></row><row><cell cols="7">and effective than coordinate regression.</cell></row><row><cell>Lr =</cell><cell>?reg N</cell><cell>N n=1</cell><cell cols="2">t n ? p(objectn)</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">j?{x,y,w,h,?}</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Ablative study by accuracy (%) of each component in our method on the OBB task of DOTA-v1.0 dataset. For RetinaNet, 'H' and 'R' represents the horizontal and rotation anchors, respectively.</figDesc><table><row><cell>Base Method</cell><cell>Backbone</cell><cell>InLD</cell><cell>Data Aug.</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>RetinaNet-H [18]</cell><cell>ResNet50 ResNet50</cell><cell>? ?</cell><cell>? ?</cell><cell>88.87 88.83</cell><cell>74.46 74.70</cell><cell>40.11 40.80</cell><cell>58.03 65.85</cell><cell>63.10 59.76</cell><cell>50.61 53.51</cell><cell>63.63 67.38</cell><cell>90.89 90.82</cell><cell>77.91 78.49</cell><cell>76.38 80.52</cell><cell>48.26 52.02</cell><cell>55.85 59.77</cell><cell>50.67 53.56</cell><cell>60.23 66.80</cell><cell>34.23 48.24</cell><cell>62.22 65.40</cell></row><row><cell>RetinaNet-R [18]</cell><cell>ResNet50 ResNet50</cell><cell>? ?</cell><cell>? ?</cell><cell>88.92 88.96</cell><cell>67.67 70.77</cell><cell>33.55 33.30</cell><cell>56.83 62.02</cell><cell>66.11 66.35</cell><cell>73.28 75.69</cell><cell>75.24 73.49</cell><cell>90.87 90.84</cell><cell>73.95 78.73</cell><cell>75.07 77.21</cell><cell>43.77 47.54</cell><cell>56.72 55.59</cell><cell>51.05 51.52</cell><cell>55.86 58.06</cell><cell>21.46 37.65</cell><cell>62.02 64.52</cell></row><row><cell>R 3 Det [18]</cell><cell>ResNet50 ResNet152 ResNet50 ResNet101 ResNet152</cell><cell>? ? ? ? ?</cell><cell>? ? ? ? ?</cell><cell>88.78 89.24 88.63 89.25 89.20</cell><cell>74.69 80.81 75.98 83.30 83.36</cell><cell>41.94 51.11 45.88 49.94 50.92</cell><cell>59.88 65.62 65.45 66.20 68.17</cell><cell>68.90 70.67 69.74 71.82 71.61</cell><cell>69.77 76.03 74.09 77.12 80.23</cell><cell>69.82 78.32 78.30 79.53 78.53</cell><cell>90.81 90.83 90.78 90.65 90.83</cell><cell>77.71 84.89 78.96 82.14 86.09</cell><cell>80.40 84.42 81.28 84.57 84.04</cell><cell>50.98 65.10 56.28 65.33 65.93</cell><cell>58.34 57.18 63.01 63.89 60.80</cell><cell>52.10 68.10 57.40 67.56 68.83</cell><cell>58.30 68.98 68.45 68.48 71.31</cell><cell>43.52 60.88 52.93 54.89 66.24</cell><cell>65.73 72.81 69.81 72.98 74.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Ablative study by accuracy (%) of IoU-Smooth L1 loss by using it or not in the three methods on the OBB task of DOTA-v1.0 dataset. Numbers in bracket denote the relative improvement by using the proposed IoU-Smooth L1 loss.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>IoU-Smooth L1</cell><cell>InLD</cell><cell>PL</cell><cell>BD</cell><cell>BR</cell><cell>GTF</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>TC</cell><cell>BC</cell><cell>ST</cell><cell>SBF</cell><cell>RA</cell><cell>HA</cell><cell>SP</cell><cell>HC</cell><cell>mAP</cell></row><row><cell>RetinaNet-R [18]</cell><cell>ResNet50 ResNet50</cell><cell>? ?</cell><cell>? ?</cell><cell>88.92 89.27</cell><cell>67.67 74.93</cell><cell>33.55 37.01</cell><cell>56.83 64.49</cell><cell>66.11 66.00</cell><cell>73.28 75.87</cell><cell>75.24 77.75</cell><cell>90.87 90.76</cell><cell>73.95 80.35</cell><cell>75.07 80.31</cell><cell>43.77 54.75</cell><cell>56.72 61.17</cell><cell>51.05 61.07</cell><cell>55.86 64.78</cell><cell>21.46 51.24</cell><cell>62.02 68.65 (+6.63)</cell></row><row><cell>SCRDet [30] FPN [14]</cell><cell>ResNet101 ResNet101 ResNet101 ResNet101</cell><cell>? ? ? ?</cell><cell>? ? ? ?</cell><cell>89.65 89.41 90.25 89.77</cell><cell>79.51 78.83 85.24 83.90</cell><cell>43.86 50.02 55.18 56.30</cell><cell>67.69 65.59 73.24 73.98</cell><cell>67.41 69.96 70.38 72.60</cell><cell>55.93 57.63 73.77 75.63</cell><cell>64.86 72.26 77.00 82.82</cell><cell>90.71 90.73 90.77 90.76</cell><cell>77.77 81.41 87.74 87.89</cell><cell>84.42 84.39 86.63 86.14</cell><cell>57.67 52.76 68.89 65.24</cell><cell>61.38 63.62 63.45 63.17</cell><cell>64.29 62.01 72.73 76.05</cell><cell>66.12 67.62 67.96 68.06</cell><cell>62.04 61.16 60.23 70.24</cell><cell>68.89 69.83 (+0.94) 74.90 76.20 (+1.30)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Ablative study by accuracy (%) of IoU-Smooth L1 loss on the OBB task of DOTA-v1.0, DOTA-v1.5 and DOTA-v2.0.</figDesc><table><row><cell>Method</cell><cell>Loss</cell><cell>DOTA-v1.0</cell><cell>DOTA-v1.5</cell><cell>DOTA-v2.0</cell></row><row><cell>RetinaNet-H</cell><cell>Smooth L1 (Reg.) Smooth L1 (Reg.  *  )</cell><cell>64.17 65.78</cell><cell>56.10 57.17</cell><cell>43.06 43.92</cell></row><row><cell></cell><cell>IoU-Smooth L1</cell><cell>66.99</cell><cell>59.16</cell><cell>46.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Accuracy (%) on DIOR. * indicates our own implementation, higher than the official baseline. ? indicates data augmentation is used.</figDesc><table><row><cell></cell><cell>Backbone</cell><cell>c1</cell><cell>c2</cell><cell>c3</cell><cell>c4</cell><cell>c5</cell><cell>c6</cell><cell>c7</cell><cell>c8</cell><cell>c9</cell><cell>c10</cell><cell>c11</cell><cell>c12</cell><cell>c13</cell><cell>c14</cell><cell>c15</cell><cell>c16</cell><cell>c17</cell><cell>c18</cell><cell>c19</cell><cell>c20</cell><cell>mAP</cell></row><row><cell>Two-stage methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Faster-RCNN [7]</cell><cell>VGG16</cell><cell>53.6</cell><cell>49.3</cell><cell>78.8</cell><cell>66.2</cell><cell>28.0</cell><cell>70.9</cell><cell>62.3</cell><cell>69.0</cell><cell>55.2</cell><cell>68.0</cell><cell>56.9</cell><cell>50.2</cell><cell>50.1</cell><cell>27.7</cell><cell>73.0</cell><cell>39.8</cell><cell>75.2</cell><cell>38.6</cell><cell>23.6</cell><cell>45.4</cell><cell>54.1</cell></row><row><cell>Mask-RCNN [75]</cell><cell>ResNet-50 ResNet-101</cell><cell>53.8 53.9</cell><cell>72.3 76.6</cell><cell>63.2 63.2</cell><cell>81.0 80.9</cell><cell>38.7 40.2</cell><cell>72.6 72.5</cell><cell>55.9 60.4</cell><cell>71.6 76.3</cell><cell>67.0 62.5</cell><cell>73.0 76.0</cell><cell>75.8 75.9</cell><cell>44.2 46.5</cell><cell>56.5 57.4</cell><cell>71.9 71.8</cell><cell>58.6 68.3</cell><cell>53.6 53.7</cell><cell>81.1 81.0</cell><cell>54.0 62.3</cell><cell>43.1 43.0</cell><cell>81.1 81.0</cell><cell>63.5 65.2</cell></row><row><cell>PANet [76]</cell><cell>ResNet-50 ResNet-101</cell><cell>61.9 60.2</cell><cell>70.4 72.0</cell><cell>71.0 70.6</cell><cell>80.4 80.5</cell><cell>38.9 43.6</cell><cell>72.5 72.3</cell><cell>56.6 61.4</cell><cell>68.4 72.1</cell><cell>60.0 66.7</cell><cell>69.0 72.0</cell><cell>74.6 73.4</cell><cell>41.6 45.3</cell><cell>55.8 56.9</cell><cell>71.7 71.7</cell><cell>72.9 70.4</cell><cell>62.3 62.0</cell><cell>81.2 80.9</cell><cell>54.6 57.0</cell><cell>48.2 47.2</cell><cell>86.7 84.5</cell><cell>63.8 66.1</cell></row><row><cell>CornerNet [32]</cell><cell>Hourglass104</cell><cell>58.8</cell><cell>84.2</cell><cell>72.0</cell><cell>80.8</cell><cell>46.4</cell><cell>75.3</cell><cell>64.3</cell><cell>81.6</cell><cell>76.3</cell><cell>79.5</cell><cell>79.5</cell><cell>26.1</cell><cell>60.6</cell><cell>37.6</cell><cell>70.7</cell><cell>45.2</cell><cell>84.0</cell><cell>57.1</cell><cell>43.0</cell><cell>75.9</cell><cell>64.9</cell></row><row><cell>FPN [14]</cell><cell>ResNet-50 ResNet-101</cell><cell>54.1 54.0</cell><cell>71.4 74.5</cell><cell>63.3 63.3</cell><cell>81.0 80.7</cell><cell>42.6 44.8</cell><cell>72.5 72.5</cell><cell>57.5 60.0</cell><cell>68.7 75.6</cell><cell>62.1 62.3</cell><cell>73.1 76.0</cell><cell>76.5 76.8</cell><cell>42.8 46.4</cell><cell>56.0 57.2</cell><cell>71.8 71.8</cell><cell>57.0 68.3</cell><cell>53.5 53.8</cell><cell>81.2 81.1</cell><cell>53.0 59.5</cell><cell>43.1 43.1</cell><cell>80.9 81.2</cell><cell>63.1 65.1</cell></row><row><cell>CSFF [77]</cell><cell>ResNet-101</cell><cell>57.2</cell><cell>79.6</cell><cell>70.1</cell><cell>87.4</cell><cell>46.1</cell><cell>76.6</cell><cell>62.7</cell><cell>82.6</cell><cell>73.2</cell><cell>78.2</cell><cell>81.6</cell><cell>50.7</cell><cell>59.5</cell><cell>73.3</cell><cell>63.4</cell><cell>58.5</cell><cell>85.9</cell><cell>61.9</cell><cell>42.9</cell><cell>86.9</cell><cell>68.0</cell></row><row><cell>FPN  *  SCRDet++ (FPN  *  )</cell><cell>ResNet-50 ResNet-50</cell><cell>66.57 66.35</cell><cell>83.00 83.36</cell><cell>71.89 74.34</cell><cell>83.02 87.33</cell><cell>50.41 52.45</cell><cell>75.74 77.98</cell><cell>70.23 70.06</cell><cell>81.08 84.22</cell><cell>74.83 77.95</cell><cell>79.03 80.73</cell><cell>77.74 81.26</cell><cell>55.29 56.77</cell><cell>62.06 63.70</cell><cell>72.26 73.29</cell><cell>72.10 71.94</cell><cell>68.64 71.24</cell><cell>81.20 83.40</cell><cell>66.07 62.28</cell><cell>54.56 55.63</cell><cell>89.09 90.00</cell><cell>71.74 73.21</cell></row><row><cell>SCRDet++ (FPN  *  )  ?</cell><cell>ResNet-101</cell><cell>80.79</cell><cell>87.67</cell><cell>80.46</cell><cell>89.76</cell><cell>57.83</cell><cell>80.90</cell><cell>75.23</cell><cell>90.01</cell><cell>82.93</cell><cell>84.51</cell><cell>83.55</cell><cell>63.19</cell><cell>67.25</cell><cell>72.59</cell><cell>79.20</cell><cell>70.44</cell><cell>89.97</cell><cell>70.71</cell><cell>58.82</cell><cell>90.25</cell><cell>77.80</cell></row><row><cell>Single-stage methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSD [4]</cell><cell>VGG16</cell><cell>59.5</cell><cell>72.7</cell><cell>72.4</cell><cell>75.7</cell><cell>29.7</cell><cell>65.8</cell><cell>56.6</cell><cell>63.5</cell><cell>53.1</cell><cell>65.3</cell><cell>68.6</cell><cell>49.4</cell><cell>48.1</cell><cell>59.2</cell><cell>61.0</cell><cell>46.6</cell><cell>76.3</cell><cell>55.1</cell><cell>27.4</cell><cell>65.7</cell><cell>58.6</cell></row><row><cell>YOLOv3 [78]</cell><cell>Darknet-53</cell><cell>72.2</cell><cell>29.2</cell><cell>74.0</cell><cell>78.6</cell><cell>31.2</cell><cell>69.7</cell><cell>26.9</cell><cell>48.6</cell><cell>54.4</cell><cell>31.1</cell><cell>61.1</cell><cell>44.9</cell><cell>49.7</cell><cell>87.4</cell><cell>70.6</cell><cell>68.7</cell><cell>87.3</cell><cell>29.4</cell><cell>48. 3</cell><cell>78.7</cell><cell>57.1</cell></row><row><cell>RetinaNet [15]</cell><cell>ResNet-50 ResNet-101</cell><cell>53.7 53.3</cell><cell>77.3 77.0</cell><cell>69.0 69.3</cell><cell>81.3 85.0</cell><cell>44.1 44.1</cell><cell>72.3 73.2</cell><cell>62.5 62.4</cell><cell>76.2 78.6</cell><cell>66.0 62.8</cell><cell>77.7 78.6</cell><cell>74.2 76.6</cell><cell>50.7 49.9</cell><cell>59.6 59.6</cell><cell>71.2 71.1</cell><cell>69.3 68.4</cell><cell>44.8 45.8</cell><cell>81.3 81.3</cell><cell>54.2 55.2</cell><cell>45.1 44.4</cell><cell>83.4 85.5</cell><cell>65.7 66.1</cell></row><row><cell>RetinaNet  *  SCRDet++ (RetinaNet  *  )</cell><cell>ResNet-50 ResNet-50</cell><cell>59.98 64.33</cell><cell>79.02 78.99</cell><cell>70.85 73.24</cell><cell>83.37 85.72</cell><cell>45.25 45.83</cell><cell>75.93 75.99</cell><cell>64.53 68.41</cell><cell>76.87 79.28</cell><cell>66.63 68.93</cell><cell>80.25 77.68</cell><cell>76.75 77.87</cell><cell>55.94 56.70</cell><cell>60.70 62.15</cell><cell>70.38 70.38</cell><cell>61.45 67.66</cell><cell>60.15 60.42</cell><cell>81.13 80.93</cell><cell>62.76 63.74</cell><cell>44.52 44.44</cell><cell>84.46 84.56</cell><cell>68.05 69.36</cell></row><row><cell>SCRDet++ (RetinaNet  *  )  ?</cell><cell>ResNet-101</cell><cell>71.94</cell><cell>84.99</cell><cell>79.48</cell><cell>88.86</cell><cell>52.27</cell><cell>79.12</cell><cell>77.63</cell><cell>89.52</cell><cell>77.79</cell><cell>84.24</cell><cell>83.07</cell><cell>64.22</cell><cell>65.57</cell><cell>71.25</cell><cell>76.51</cell><cell>64.54</cell><cell>88.02</cell><cell>70.91</cell><cell>47.12</cell><cell>85.10</cell><cell>75.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 11</head><label>11</label><figDesc>Performance by accuracy (%) on UCAS-AOD dataset. task, we also conduct the same experiments and obtain competitive detection mAP, about 74.37% and 76.24%. Model performance can be further improved to 79.35% if multiscale training and testing are used. It is worth noting that FADet</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>Plane</cell><cell>Car</cell></row><row><cell>YOLOv2 [80]</cell><cell>87.90</cell><cell>96.60</cell><cell>79.20</cell></row><row><cell>R-DFPN [58]</cell><cell>89.20</cell><cell>95.90</cell><cell>82.50</cell></row><row><cell>DRBox [81]</cell><cell>89.95</cell><cell>94.90</cell><cell>85.00</cell></row><row><cell>S 2 ARN [82]</cell><cell>94.90</cell><cell>97.60</cell><cell>92.20</cell></row><row><cell>RetinaNet-H [18]</cell><cell>95.47</cell><cell>97.34</cell><cell>93.60</cell></row><row><cell>ICN [23]</cell><cell>95.67</cell><cell>-</cell><cell>-</cell></row><row><cell>FADet [27]</cell><cell>95.71</cell><cell>98.69</cell><cell>92.72</cell></row><row><cell>R 3 Det [18]</cell><cell>96.17</cell><cell>98.20</cell><cell>94.14</cell></row><row><cell>SCRDet++ (R 3 Det)</cell><cell>96.95</cell><cell>98.93</cell><cell>94.97</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ICML, CVPR, ECCV, AAAI, ACM MM, IJCV, IEEE TIP etc. His Github projects on object detection have received over 5,000 stars, and ported to the projects MMRotate and AlphaRotate. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection in optical remote sensing images: A survey and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="296" to="307" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in vhr optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition Applications and Methods</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">R3det: Refined single-stage detector with feature refinement for rotating object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3163" to="3171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning on image denoising: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="501" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive denoising filtering for object detection applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rinaldo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dapas: Denoising autoencoder to prevent adversarial attack in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning roi transformer for oriented object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection with circular smooth label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Featureattentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>Icdar 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A deep learning approach to traffic lights: Detection, tracking, and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Behrendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Botros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1370" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="268" to="279" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Axis learning for orientated objects detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">908</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An efficient and robust integrated geospatial object detection framework for high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">666</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deformable convnet with aspect ratio constrained nms for object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1312</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deformable faster r-cnn with aggregating multi-layer features for partially occluded object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1470</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Iou-adaptive deformable r-cnn: Make full use of iou for multi-class object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">286</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fmssd: Feature-merged single-shot detection for multiscale objects in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3377" to="3390" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">East: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5676" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning modulated loss for rotated object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2458" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1452" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Augmentation for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kisantal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CS &amp; IT Conference Proceedings</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>CS &amp; IT Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extended feature pyramid network for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Seeing small faces from robust anchor&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5127" to="5136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hambox: Delving into mining high-quality anchors on face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Perceptual generative adversarial networks for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Position detection and direction prediction for arbitrary-oriented ships via multitask rotation region convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="50" to="839" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Radet: Refine feature pyramid network and multi-layer axttention network for arbitrary-oriented object detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sard: Towards scale-aware rotated object detection in aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="173" to="855" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-scale feature integrated attention-based rotation network for object detection in vhr aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1686</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attention-based mask oriented bounding box representation for multi-category object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rotationaware and multi-scale convolutional neural network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="294" to="308" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7247" to="7257" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Ienet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented object detection in remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="223" to="373" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A2rmnet: Adaptively aspect ratio multi-scale network for object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">1594</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Salience biased loss for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08103</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Enhanced feature representation in detection for optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">2095</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Cross-scale feature fusion for object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Single shot anchor refinement network for oriented object detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="150" to="87" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
