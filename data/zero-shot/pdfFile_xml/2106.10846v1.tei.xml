<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trainable Class Prototypes for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyi</forename><surname>Li</surname></persName>
							<email>lijianyi1488@stu.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Communications Xi&apos;an Jiaotong University Xi&apos;an</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guizhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communications Xi&apos;an Jiaotong University Xi&apos;an</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trainable Class Prototypes for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Metric learning is a widely used method for few shot learning in which the quality of prototypes plays a key role in the algorithm. In this paper we propose the trainable prototypes for distance measure instead of the artificial ones within the meta-training and task-training framework. Also to avoid the disadvantages that the episodic meta-training brought, we adopt non-episodic meta-training based on self-supervised learning. Overall we solve the few-shot tasks in two phases: meta-training a transferable feature extractor via self-supervised learning and training the prototypes for metric classification. In addition, the simple attention mechanism is used in both meta-training and task-training. Our method achieves state-ofthe-art performance in a variety of established few-shot tasks on the standard few-shot visual classification dataset, with about 20% increase compared to the available unsupervised few-shot learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Metric learning is a widely used method for few shot learning in which the quality of prototypes plays a key role in the algorithm. In this paper we propose the trainable prototypes for distance measure instead of the artificial ones within the meta-training and task-training framework. Also to avoid the disadvantages that the episodic meta-training brought, we adopt non-episodic meta-training based on self-supervised learning. Overall we solve the few-shot tasks in two phases: meta-training a transferable feature extractor via self-supervised learning and training the prototypes for metric classification. In addition, the simple attention mechanism is used in both meta-training and task-training. Our method achieves state-ofthe-art performance in a variety of established few-shot tasks on the standard few-shot visual classification dataset, with about 20% increase compared to the available unsupervised few-shot learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years deep learning has made major advances in computer vision areas such as image recognition, video object detection and tracking. A deep neural network needs a large amount of labeled data to fit its parameters whereas it is laborious to label so many examples by human annotators. Thus the problem of learning with few labeled samples called few-shot learning has been paid more and more attention. Fewshot learning is described as a classification task set in N -way and k -shot, which means to distinguish N categories, each of which has k (quite small) labeled samples. The model predict classes for new examples only depending on k labeled data. The annotated data is called the support set, and the new data belonging to the N categories is called query set.</p><p>People have proposed varieties of few-shot methods, all of which rely on meta-training assisted with base classes. The universal approach is to use the base classes to construct fake few-shot tasks for training the network first, with the purpose of enabling the network an ability to accomplish real fewshot tasks through simulating the process of carrying out the fake tasks. This is called the meta-training stage with tasks as samples. Next, use the trained network to complete real few-shot tasks of novel classes, and calculate the classification accuracy on the query set in the tasks to evaluate the algorithm, which is usually called the meta-testing. The whole procedure is shown in <ref type="figure">Fig.1</ref>.</p><p>Few-shot learning algorithms could be classified into three categories. The first <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> is based on metric learning, which consists of three steps of feature extraction, distance measure, <ref type="figure">Fig. 1</ref>. The universal method used in supervised few-shot learning, which consists of meta-training and meta-testing. In the meta-training, the training sample is actually a mimic few-shot task comprised of some labeled data chosen from base classes. And in the meta-testing the model will solve a real task with few labeled data and an unlabeled query set chosen from novel classes. We show a model trained for solving 3-way 1-shot tasks in this <ref type="figure">figure.</ref> and prediction, relying on effective metric design and reducing the cross entropy loss in meta training to improve classification accuracy. The second are the teacher-student network based methods including <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. The teacher network guides the student network to solve the few-shot tasks in terms of parameter initialization, parameter update and other aspects. The algorithm enables the teacher network to obtain the ability to instruct the student network via meta-training. The third category such as <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> is based on the transduction, which propagates the label of the support data to the queries through a specific graph, thereby obtaining the predicted class of the query set. The algorithm optimizes the accuracy of propagation label through meta-training.</p><p>The meta-training determines the model's performance in the few-shot learning algorithm. However it brings two obvious drawbacks. First, the meta-training requires a large number of labeled auxiliary examples (base classes). Those algorithms can not work without adequate labeled samples. Second, the meta-training phase uses tasks as training samples. Therefore, a task type decided by the values of N and k needs to be certain before meta-training to ensure that the number of images contained by each mimic few shot task (i.e. a metatraining sample) is consistent during training. The meta-trained network can only be used to solve few-shot tasks with the same type as the meta-training samples, and it performs worse in other types of tasks. However, in reality we need to solve various types of few-shot tasks, and it is unreasonable to metatrain the network from scratch in order to solve a few shot task with new type.</p><p>In order to solve these two problems, we solve the few shot classification based on self-supervised learning according to <ref type="bibr" target="#b10">[11]</ref>. Specifically, our method called TrainPro-FSL abandons the episodic meta-training phase, which takes the fake fewshot tasks as samples, and uses instead two new phases: the non-episodic meta-training via self-supervised learning directly using a single image as a training sample, and the task-training to solve a real few shot task. In the first phase, a discriminative self-supervised learning method is used to obtain a feature extractor with good generalization ability using unlabeled images. In the second phase, we use metric learning to solve real few-shot tasks. The meta-trained feature extractor is used to extract features from all the images in the current task, and SGC is carried out, based on a specific graph defined by the current task as the feature aggregation method described in <ref type="bibr" target="#b10">[11]</ref>. We use the aggregated support set features to obtain the class prototypes which are trained via a simple neural network. Finally we use the class prototypes to implement distance measure and further classify the query images.</p><p>Furthermore, we demonstrate that the attention mechanism are helpful for both meta-training and task-training. We add attention structure to the neural network in the meta-training phase. In the task-training phase, an attention mask that does not increase network parameters is introduced to correct the query image features. Both of these methods can improve the classification accuracy of our algorithm.</p><p>Our key contributions can be summarized as follows:</p><p>? We propose TrainPro-FSL, a unsupervised few-shot learning algorithm with trainable prototypes. By adopting the methodology of self-supervised learning, the two problems intrinsic in the existing episodic meta-training paradigm are solved simultaneously. Thus our method does not require a large number of labeled samples for training. In addition, the meta-trained model can carry out different types of real few-shot tasks. ? We propose to use trainable prototypes to implement metric classification, which is better than the manual design in <ref type="bibr" target="#b11">[12]</ref>. ? We propose to use a simple attentive mechanism in the metric learning to further correct the feature of examples in the query set. ? Adequate experiments demonstrate that our method reaches state-of-the-art accuracy on miniImageNet , a standardized benchmark in few-shot learning.</p><p>The paper is organized as follows. In II, we introduce the related works. Our methodology is described in III. In IV, experimental results on the standard vision dataset are shown in comparison with the proposed works. Finally, a conclusion is drawn in V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>In this section we aim to show the metric learning used in few-shot learning algorithms proposed in previous years. In addition, we introduce some unsupervised few-shot learning methods presented recently and popular self-supervised algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Metric Learning</head><p>The core of metric learning is to extract features from the support set and query set, then obtain the class prototypes using the support set, and predict classes of queries via the nearest neighbor algorithm and attention mechanism. Therefore the quality of class prototypes obtained in algorithm are vital for the performance of the metric learning. Through episodic meta-training, a metric based method obtains a feature extractor that facilitates completing the classification task based on distance measurement. Matching Networks <ref type="bibr" target="#b0">[1]</ref> used LSTM to extract full context embeddings from images and applied attention mechanism to classify. Prototypical Networks <ref type="bibr" target="#b1">[2]</ref> proposed to use Euclidean distance to better measure the similarity between features, and use prototypes of each class to classify queries. Relation Network <ref type="bibr" target="#b2">[3]</ref> used a neural network to replace the traditional distance metric, and directly output the queries' categories via an end-to-end network. DC-IMP <ref type="bibr" target="#b12">[13]</ref> introduced dense classification and leverage implanting to bring metric learning the task dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unsupervised Few Shot Learning</head><p>The base classes in unsupervised methods has no labels. Some existing methods use unsupervised learning or data enhancement methods to leverage these unlabeled base classes to artificially construct fake support set and query set for meta training. They are able to combine with the few-shot learning methods as mentioned above (such as MAML <ref type="bibr" target="#b4">[5]</ref> and Prototypical Net <ref type="bibr" target="#b1">[2]</ref>) to fulfil few-shot tasks. UFLST <ref type="bibr" target="#b13">[14]</ref> and CACTU <ref type="bibr" target="#b14">[15]</ref> use clustering to make pseudo-labels for unlabeled examples, then use the pseudo-labeled data as ordinary labeled data to construct fake few-shot tasks to complete meta-training. AAL <ref type="bibr" target="#b15">[16]</ref> and UMTRA <ref type="bibr" target="#b16">[17]</ref> took each instance as one class and randomly sample multiple examples to construct a fake support set, then generate a corresponding query set according to the support set by data augmentation techniques. ULDA <ref type="bibr" target="#b17">[18]</ref> developed a new simple data augmentation method to enhance the difference between the support set distribution and query set distribution when constructing the fake few-shot tasks for meta-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-Supervised Learning</head><p>The popular self-supervised algorithms can be divided into three categories, namely contrastive self-supervised learning, generative self-supervised learning and discriminative selfsupervised learning. The core of contrastive self-supervised learning is contrastive learning. The main idea of contrastive learning is to make the features of positive sample pairs close to each other in the feature space, and keep the features of negative sample pairs away from each other, such as BYOL <ref type="bibr" target="#b18">[19]</ref>, SimCLR <ref type="bibr" target="#b19">[20]</ref>, and their performance can even be comparable to supervised algorithms. Generative self-supervised learning algorithms such as image colorization <ref type="bibr" target="#b20">[21]</ref> and VQ-VAE <ref type="bibr" target="#b21">[22]</ref>, usually take the original image as the groundtruth, train the network and make the network output close to the groundtruth. In generative self-supervised learning, images are generated at the pixel level, so the features extracted by the network are partial to image details, rather than semantic features.</p><p>Discriminant self-supervised learning can train the classification algorithm without the category label. The category here is set by the algorithm itself, and the label of the training sample is automatically generated according to the algorithm setting. In the discriminative self-supervised algorithm, the design of pretext tasks is diverse. Commonly used algorithms include jigsaw puzzle <ref type="bibr" target="#b22">[23]</ref>, rotation prediction <ref type="bibr" target="#b23">[24]</ref>, and deep clustering <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>The notations and problem formulation of self-supervised few-shot learning are introduced in III-A, and our paradigm is presented in III-B. Finally, the loss design for trainable prototypes and attention mechanism are described in III-C and III-D respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Given two datasets, namely D base and D task with disjoint classes. D base consists of a large number of unlabeled examples from the base classes. D task has a small number of labeled examples called the support set D s , along with some unlabeled ones called the query set D q , all from the new classes. They stand for the total data in a few-shot learning task. The number of classes in the novel dataset D task , the number of support samples and the number of query inputs for each of these classes are denoted N , k and q respectively. So there are totally N ? (k + q) examples in a few-shot learning task. Our aim is to predict the classes of the query set of D task . Different from the previous works like <ref type="bibr" target="#b7">[8]</ref>, our D base does not have any labels. So we train the classification network with only a few labeled examples namely D s in a real sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Paradigm of Solution</head><p>We first train a backbone deep neural network able to extract useful and compact features from inputs, which will be used as a generic feature extractor. In this non-episodic meta-training phase, we train the network with <ref type="bibr" target="#b25">[26]</ref>, a kind of discrimitive self-supervised learning method, which promises a transferable feature extractor. Thus we obtain the extractor f ? : R w?h?3 ? R e .</p><formula xml:id="formula_0">D base = x 1 , x 2 , ......, x n where x ? R w?h?3 via ODC in</formula><p>We then use f ? to obtain the features of the total data in D task (both D s and D q ) namely f ? (D task ) = {f ? (x)|x ? D task }. Then we step into the second phase namely the task-training phase. First we build a nearest neighbor graph using the cosine similarity according to <ref type="bibr" target="#b26">[27]</ref> :</p><formula xml:id="formula_1">cos (f ? (x 1 ) , f ? (x 2 )) = f ? (x 1 ) T f ? (x 2 ) f ? x 1 2 f ? x 2 2<label>(1)</label></formula><p>The base graph denoted G task (V, E) uses f ? (D task ) to construct vertices. In details, its vertices matrix V ? R [N ?(k+q)]?e is the stacked representations of support set and query set i.e. each vertex represents an image's feature. We make the values of graph edges represent the similarity between vertices-that is, similar vertices have larger adjacency values. To get the adjacency matrix E ? R [N ?(k+q)]?[N ?(k+q)] , we first define a similarity matrix S with the same dimension computed as follows:</p><formula xml:id="formula_2">S i,j = cos(V i,: , V j,: ) i = j 0 i = j<label>(2)</label></formula><p>where V i,: denotes the i -th row in V . Then we just save the m largest values on each row and on the corresponding column in S to obtain a more sparse matrix helpful to reduce the interference. Finally, we normalize the resulting matrix to get the adjacency matrix:</p><formula xml:id="formula_3">E = D ? 1 2 SD ? 1 2<label>(3)</label></formula><p>where D is the degree diagonal matrix computed by D i,i = j S i,j . We can consider E as the Laplacian matrix in GCN <ref type="bibr" target="#b27">[28]</ref> used to aggregate information among vertices.</p><p>We aggregate features for each vertex via the graph structure to get V new according to <ref type="bibr" target="#b10">[11]</ref>. Then we use the metric learning to classify the query images based on V new . In metric learning, the quality of class prototypes has a great influence on classification accuracy. Therefore, we abandon the traditional method of artificially constructing class prototypes and use neural networks to obtain good prototypes via training to implement the metric classification. In order to improve the performance, we adopt the attention mechanism in training phase in the meanwhile.</p><p>Our paradigm is illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. In general TrainPro-FSL has two phases: (1) Meta-training phase: training a generic feature extractor via ODC algorithm. (2) Task-training phase: training the prototypes using the support set data after the feature aggregation through graph. Once the latter is finished, the performance of this model is evaluated by measuring the distance between features of D q and prototypes. The process of task-training phase is formalized in Algorithm 1.</p><p>The details of the two phases are provided in the following, first the meta-training phase then the task-training phase.</p><p>Meta-training phase: We follow the methodology called ODC, an effective discrimitive self-supervised learning method, proposed in <ref type="bibr" target="#b28">[29]</ref>. The ODC is an online update deep clustering algorithm, which improves the DeepCluster <ref type="bibr" target="#b24">[25]</ref> in the way of pseudo-label update. It discards the clustering after each epoch of training, and chooses to update the image features and pseudo-labels at the same time during the training process. ODC adds sample memory and centroids memory to DeepCluster to achieve the online update of pseudo-labels.</p><p>In a training process of ODC, there are three main steps: update network parameters, update sample memory and update centroids memory. During training we adopt the simple cross Algorithm 1: The process of task-training phase with prototypes training.</p><p>Input: A N -way k -shot task with the dataset D task = {D s , D q }; The meta-trained feature extractor f ? Output: Trained prototypes 1 Obtain features of all inputs including labeled and unlabeled ones, f ? (D task ); 2 Build the graph G task (V, E) based on f ? (D task ); 3 Aggregate vertices features V of the graph to get V new ; 4 randomly initialize ?;</p><p>Classifier training; <ref type="bibr" target="#b4">5</ref> Use manifold augmentation to extend labeled data in semantic level according to <ref type="bibr" target="#b10">[11]</ref>, and get the augmented feature set V new aug ; 6 Train Cls ? using V new aug and cross entropy loss to obtain Cls ? * ; Prototypes training; <ref type="bibr" target="#b6">7</ref> Initialization the prototypes proto 0 randomly; <ref type="bibr" target="#b7">8</ref> Use Cls ? * to calculate the loss of the prototypes classification and the metric classification of the support set according to Eq. (8); 9 Update proto 0 based on the loss function and finally obtain proto * ; 10 Return proto * ; entropy loss as the loss function. Through the self-supervised learning we obtain a feature embedding, which can get features with semantic information.</p><p>Task-training phase: In this phase we aim to obtain the well trained prototypes for the distance measure. Therefore we need to train a classifier firstly as the base neural network to train the prototypes. Specifically we fix the meta-trained parameters in the backbone and train a task-dependent classifier Cls ? on the transferred representations of the fewshot task's dataset namely D task . Before training a linear classifier with D s having few labeled examples, a method similar to simplified graph convolution <ref type="bibr" target="#b11">[12]</ref>, namely the graph aggregation is used according to <ref type="bibr" target="#b10">[11]</ref>. We construct the graph G task (V, E) for the current few-shot task through the steps introduced before.</p><p>Then we propagate feature ( <ref type="bibr" target="#b11">[12]</ref>) to obtain new features for each vertex:</p><formula xml:id="formula_4">V new = (?I + E) ? V<label>(4)</label></formula><p>where I is the identity matrix and ? is a hyperparameter denoting the number of times to aggregate feature. At the same time, ? is also a key value to balance between the neighbors representations and the self-ones. After aggregation, we use the labeled part of the vertices to train the task-dependent classifier Cls ? , a simple fully connected network. Then we use the trained network Cls ? with loss function specially designed for prototypes training to do the backpropagation and further obtain the trained prototypes. The design for loss function is introduced in details in III-C. After finishing the task-training, we have the helpful class prototypes to do distance measure. As done in the method of usual metric learning algorithms, we classify the query image into the class corresponding to the prototype with the closest distance to the feature of this query example. To improve the classification accuracy we design a simple attention mask add to the query examples' features before the calculation of distance which is described in III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Design for Trainable Prototypes</head><p>In the metric learning algorithm, the quality of the prototypes determine the classification accuracy of the distance metric, but the method of directly averaging the support examples features as in the <ref type="bibr" target="#b1">[2]</ref> cannot achieve the best results.</p><p>We proposes trainable prototypes and consider it as a network parameter to participate in the training, so that you can get the prototypes that cannot design artificially. During the prototypes training, the design of loss function is very important. There are two requirements for the good prototypes: one is that it can be correctly classified by a trained classification head, and the other is that it can effectively improve the accuracy of metric classification. According to these two aspects, the corresponding loss function can be designed respectively.</p><p>The first term of the total loss is the cross-entropy loss of prototypes classification. In the calculation loss, the groundtruth of the class prototype is the one-hot form of its corresponding category which is a vector of length N, each category corresponds to a dimension in the vector, the value is 1 in the dimension representing the category to which it belongs, and the value is 0 in other dimensions. The classification loss can be described as follows:</p><formula xml:id="formula_5">L class = 1 N N i=1 ? 1 N N c=1 y i,c log (? i,c )<label>(5)</label></formula><p>where? i,c is the value of the prediction vector of the i-th class prototype in the c-th dimension, y i,c is the value on the c-th dimension of the one-hot label of the i-th class prototype. In order to make the training process converge, it is not enough to use only the cross entropy loss, and an entropy loss of the prototypes classification result must be designed. Because the last layer of the classification head is the softmax layer, so that the classification result is a probability distribution. The value of each dimension represents the probability that the prototypes belongs to the corresponding class. We believe that the higher credibility of the classification result of the prototype is better. Therefore, the value of the classification result in the corresponding dimension of the correct category should be close to 1, and the value of other positions should be close to 0. In order to strengthen this trend, Entropy loss can be designed as follows:</p><formula xml:id="formula_6">L entropy = 1 N N i=1 ? 1 N N c=1? i,c log (? i,c )<label>(6)</label></formula><p>Entropy loss hopes that the smaller the entropy of the prediction result, the better, which ensures that the prediction result tends to the one-hot vector and strengthens the training tendency. In addition to the above two losses, cross-entropy loss needs to be calculated for the metric classification results of the support examples, which is called metric loss. This loss is to ensure that the class prototypes have a good distance metric classification ability. Since the query examples has no label, the support examples are used instead in calculating the metric loss during training. We write the metric loss as:</p><formula xml:id="formula_7">L metric = 1 N ? S N ?S i=1 ? 1 N N c=1 y i,c log sof tmax cos V new i,: , proto c<label>(7)</label></formula><p>where the proto c represents the prototype of the c-th category. The total loss is the weighted summation of these three items expressed as follows:</p><p>L total = ?L entropy + ?L class + L metric (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Attention Mechanism</head><p>We aim to pay more attention to important features while ignoring other content that are not vital for distance measure by adding attention mechanism to metric learning. Therefore, we introduce the attention mask used to correct the query examples features. Because the number of labeled support examples at task-training phase is small, the network parameters involved in this process cannot be increased, otherwise it will cause serious overfitting. The purpose of adding an attention mechanism in the metric classification is to highlight the features channel related to a certain category in both the query example feature and the corresponding prototype, so as to obtain the corrected query feature that is easier to classify. For this purpose, it is found that the class prototype trained in task-training is a natural attention mask.</p><p>The channel corresponding to a larger value in a prototype represents an important feature that distinguishes this class from other classes. Multiplying the prototype as the attention mask with the query example feature can highlight the channel related to this class in the query feature. If the query image belongs to this category, multiply the mask and then do the distance measure to get a larger similarity value, which can be better distinguished from other categories; if the query image does not belong to this category, then the similarity value will be smaller, and a better classification accuracy can also be achieved.</p><p>Therefore the attention mask are proposed as follows:</p><p>atten mask n = Sof tmax (? * abs (proto n ))</p><p>where atten mask n represents the attention mask corresponding to the n-th category and ? is a scale factor. Then we use the attention mask to correct the query example features:</p><p>query n = ? * query * atten mask n + query</p><p>where query n is the query example features after correcting by the attention mask of the n-th category. query is the aggregated query features. ? is a weighting factor set as 10000 in our experiments. In addition to adding the attention mask in the task-training phase, we also add the attention module SENet <ref type="bibr" target="#b29">[30]</ref> to the network in the meta-training phase, and verify its effectiveness through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We conduct experiments on the widely used few-shot image classification benchmark: miniImageNet <ref type="bibr" target="#b0">[1]</ref>, which is a derivative of ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Models and implementation details</head><p>Architecture. In the meta-training phase we use ResNet50 <ref type="bibr" target="#b30">[31]</ref> as the structure of feature extractors f ? . This backbone has 50 convolutional layers grouped into 16 blocks. And in every block we add a SE block. We set the input size as 224? 224 and flatten the outputs as inputs to the graph aggregation, so that e = 2048 in V ? R [N ?(k+q)]?e .</p><p>In consideration of the extreme few labeled examples we take only one fully connected layer and a following softmax layer as the structure of the classifier Cls to avoid overfitting. And in the metric classification we measure the feature distance by calculating the cosine between two vectors.</p><p>Optimization and hyper-parameters setup. For the metatraining phase, we train the backbone in a total of 400 epochs from scratch using the SGD optimizer <ref type="bibr" target="#b31">[32]</ref> and the crossentropy loss. In order to get a better model we also adopt early stopping. We set the total clusters number as 1680 while the unlabled samples used in ODC composed of 168 categories. For the task-training phase, we train the classifier and the prototypes in 11 epochs 1000 epochs respectively, using the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> and the loss function shown in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I</head><p>Performance of TrainProto-FSL in comparison to the previous works on miniImageNet on 5-way 1-shot and 5-way 5-shot tasks. Average accuracies are reported with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-way Accuracy miniImageNet</head><p>1-shot 5-shot CACTUs-MAML <ref type="bibr" target="#b14">[15]</ref> 39.90?0.74% 53.97?0.70% CACTUs-ProtoNets <ref type="bibr" target="#b14">[15]</ref> 39.18?0.71% 53.36?0.70% UFLST <ref type="bibr" target="#b13">[14]</ref> 33.77?0.70% 45.03?0.73% UMTRA <ref type="bibr" target="#b16">[17]</ref> 39.93??% 50.73??% unsupervised AAL-ProtoNets <ref type="bibr" target="#b15">[16]</ref> 37.67?0.39% 40.29?0.68% AAL-MAML++ <ref type="bibr" target="#b15">[16]</ref> 34.57?0.74% 49.18?0.47% ULDA-ProtoNets <ref type="bibr" target="#b17">[18]</ref> 40.63?0.61% 55.41?0.57% ULDA-MetaOptNet <ref type="bibr" target="#b17">[18]</ref> 40.71?0.62% 54.49?0.58% CSSL-FSL Image168 <ref type="bibr" target="#b10">[11]</ref> 54 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on miniImageNet</head><p>The miniImageNet dataset consists of 100 classes randomly sampled from the ImageNet and each class contains 600 images of size 84?84. It is usually divided into three parts <ref type="bibr" target="#b7">[8]</ref>: training set with 64 base classes, validation set with 16 classes, and testing set with 20 novel classes. In the meta-training phase we use 168 classes randomly chosen from ImageNet according to <ref type="bibr" target="#b10">[11]</ref> to get a well feature extractor, having no labels. In the task-training phase we sample novel classes to design few-shot tasks as inputs. We ensure that the novel classes have never been seen in the meta-training phase.</p><p>We evaluate our method on 1000 randomly sampled tasks and report their mean accuracy in TABLE I. We compare our method in both 5-way 1-shot and 5-way 5-shot setting with some classical supervised few-shot learning methods and novel unsupervised methods proposed recently. It can be found that our method is much better than previous unsupervised fewshot learning methods( <ref type="bibr" target="#b13">[14]</ref> etc.), improving them by more than 18%. The CSSL-FSL <ref type="bibr" target="#b10">[11]</ref> also use non-episodic metatraining to train a feature extractor via a contrastive selfsupervised algorithm. Our method improves it by more than 4%. Even compared with supervised methods( <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b1">[2]</ref>), our method still has improvement by 2-11% both on 5-way 1-shot and 5-way 5-shot tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation experiments</head><p>In this section, we conduct ablation experiments to analyze how trainable prototypes and attention mechanism affects the few-shot image classification performance. TABLE II shows the results of the ablation studies on miniImageNet in 5-way 1shot, 5-way 5-shot, 5-way 10-shot and 5-way 30-shot setting. In the table, without TrainProto means that the mean value of support features is directly calculated as the prototype to participate in the distance measure and without AttenMask means don't use attention mask to correct the query features. From the table we can see the original model performs best. Attention mask before the distance measure improves accuracy by 0.1%. When the number of shots is 1 or 5, the improvement in accuracy brought by the AttenMask is more obvious, indicating that the attention mask can achieve greater advantages when there are few label samples.</p><p>The SENet used in feature extractor can provide about 1% extra gain. And without TrainProto, the result will decrease by 2-6%. The reason is that TrainProto is actually equivalent to obtaining the class prototypes through a very complicated function, which cannot be designed manually. Therefore, the prototypes obtained by training must be better than those obtained by directly averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Experiments</head><p>In this section, we compare the effects of two different attention modules on meta-training. We added Non-Local(NL) <ref type="bibr" target="#b33">[34]</ref> and SENet modules(SE) to the ODC backbone network respectively, and compared their effects on the accuracy of few shot classification after training. The results are shown in TABLE III. We also compares the algorithm without attention module(w/o Atten) in meta-training with the two improved algorithms mentioned above.</p><p>Generally speaking, the addition of SENet and Non-local modules can improve the performance of the algorithm, and the improvement of SENet is more than that of Non-local. On the few-sample classification tasks with shot=1 and 5, the two attention modules brought the higher improvement in classification accuracy, reaching a maximum of 1.2%. But when shot=10, the SENet module brings a 0.06% decrease in accuracy, and when shot=30, the Non-local module brings a 0.38% decrease in accuracy. In general, these two attentions have played a positive role in the training of the feature extractor, and the improvement of SEnet is more obvious especially when shot is extreme small. TABLE III Performance of TrainProto-FSL with different attention modules on miniImageNet on different types of tasks. For each type of task, the best-performing method is in bold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-way</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>A novel method of obtaining prototypes for metric classification called TrainProto-FSL is proposed in this paper, which contains classifier training and prototypes training. And the whole framework designed as <ref type="bibr" target="#b10">[11]</ref> helps our algorithm avoid the shortcomings related with the traditional episodic metatraining.</p><p>Experiments show a state-of-the-art performance on a standard vision dataset miniImageNet. It proves that the trained prototypes get from neural network are more efficacious than the hand-designed ones, and the attention mechanism is also useful for the performance improvement. In the following study we will explore the effective self-supervised learning algorithm specially for few shot learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of the proposed paradigm. The left shows the meta-training phase using ODC resulting in a global feature extractor. The right is the task-training phase comprised of graph-aggregation, classifier training with the support set and the prototypes training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc>Results of ablation studies on miniImageNet. The meta-training dataset consists of 168 classes from ImageNet for all the four models. 92?0.91% 73.94?0.63% 77.22?0.55% 83.10?0.43%</figDesc><table><row><cell>SENet AttenMask TrainProto ? ? ? ? ? ? ? ?</cell><cell>1-shot 57.90?0.86% 58.82?0.91% 59.00?0.70% 58.</cell><cell>5-shot 72.55?0.62% 73.83?0.63% 71.77?0.52%</cell><cell>10-shot 77.22?0.52% 77.13?0.55% 74,54?0.47%</cell><cell>30-shot 82.96?0.41% 83.09?0.43% 77.02?0.43%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lgm-net: Learning to generate matching networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06331</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Few-shot image classification via contrastive selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09942</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense classification and implanting for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lifchitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9258" to="9267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via self-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised learning via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised few-shot learning via distribution shift-based augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05805</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR, 2020</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepcluster: A general clustering framework based on deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="809" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploiting unsupervised inputs for accurate few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09849</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
