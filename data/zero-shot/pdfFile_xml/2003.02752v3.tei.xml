<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combating Noisy Labels by Agreement: A Joint Training Method with Co-Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Open FIESTA Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
							<email>boan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combating Noisy Labels by Agreement: A Joint Training Method with Co-Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning with noisy labels is a practically challenging problem in weakly supervised learning. The stateof-the-art approaches "Decoupling" and "Co-teaching+" claim that the "disagreement" strategy is crucial for alleviating the problem of learning with noisy labels. In this paper, we start from a different perspective and propose a robust learning paradigm called JoCoR, which aims to reduce the diversity of two networks during training. Specifically, we first use two networks to make predictions on the same mini-batch data and calculate a joint loss with Co-Regularization for each training example. Then we select small-loss examples to update the parameters of both two networks simultaneously. Trained by the joint loss, these two networks would be more and more similar due to the effect of Co-Regularization. Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is superior to many state-of-the-art approaches for learning with noisy labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks (DNNs) achieve remarkable success on various tasks, and most of them are trained in a supervised manner, which heavily relies on a large number of training instances with accurate labels <ref type="bibr" target="#b13">[14]</ref>. However, collecting large-scale datasets with fully precise annotations is expensive and time-consuming. To alleviate this problem, data annotation companies choose some alternating methods such as crowdsourcing <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43]</ref> and online queries <ref type="bibr" target="#b2">[3]</ref> to improve labelling efficiency. Unfortunately, these methods usually suffer from unavoidable noisy labels, which have been proven to lead to noticeable decrease in performance of DNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>As this problem has severely limited the expansion of neural network applications, a large number of algorithms * Corresponding author. have been developed for learning with noisy labels, which belongs to the family of weakly supervised learning frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. Some of them focus on improving the methods to estimate the latent noisy transition matrix <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. However, it is challenging to estimate the noise transition matrix accurately. An alternative approach is training on selected or weighted samples, e.g., Mentornet <ref type="bibr" target="#b15">[16]</ref>, gradient-based reweight <ref type="bibr" target="#b29">[30]</ref> and Co-teaching <ref type="bibr" target="#b11">[12]</ref>. Furthermore, the state-of-the-art methods including Co-teaching+ <ref type="bibr" target="#b40">[41]</ref> and Decoupling <ref type="bibr" target="#b22">[23]</ref> have shown excellent performance in learning with noisy labels by introducing the "Disagreement" strategy, where "when to update" depends on a disagreement between two different networks. However, there are only a part of training examples that can be selected by the "Disagreement" strategy, and these examples cannot be guaranteed to have ground-truth labels <ref type="bibr" target="#b11">[12]</ref>. Therefore, there arises a question to be answered: Is "Disagreement" necessary for training two networks to deal with noisy labels?</p><p>Motivated by Co-training for multi-view learning and semi-supervised learning that aims to maximize the agreement on multiple distinct views <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>, a straightforward method for handling noisy labels is to apply the regularization from peer networks when training each single network. However, although the regularization may improve the generalization ability of networks by encouraging agreement between them, it still suffers from memorization effects on noisy labels <ref type="bibr" target="#b43">[44]</ref>. To address this problem, we propose a novel approach named JoCoR (Joint Training with Co-Regularization). Specifically, we train two networks with a joint loss, including the conventional supervised loss and the Co-Regularization loss. Furthermore, we use the joint loss to select small-loss examples, thereby ensuring the error flow from the biased selection would not be accumulated in a single network.</p><p>To show that JoCoR significantly improves the robustness of deep learning on noisy labels, we conduct extensive experiments on both simulated and real-world noisy datasets, including MNIST, CIFAR-10, CIFAR-100 and Clothing1M datasets. Empirical results demonstrate that the robustness of deep models trained by our proposed approach is superior to many state-of-the-art approaches. Furthermore, the ablation studies clearly demonstrate the effectiveness of Co-Regularization and Joint Training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In this section, we briefly review existing works on learning with noisy labels.</p><p>Noise rate estimation. The early methods focus on estimating the label transition matrix <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. For example, F-correction <ref type="bibr" target="#b27">[28]</ref> uses a two-step solution to heuristically estimate the noise transition matrix. An additional softmax layer is introduced to model the noise transition matrix <ref type="bibr" target="#b9">[10]</ref>. In these approaches, the quality of noise rate estimation is a critical factor for improving robustness. However, noise rate estimation is challenging, especially on datasets with a large number of classes.</p><p>Small-loss selection. Recently, a promising method of handling noisy labels is to train models on small-loss instances <ref type="bibr" target="#b29">[30]</ref>. Intuitively, the performance of DNNs will be better if the training data become less noisy. Previous work showed that during training, DNNs tend to learn simple patterns first, then gradually memorize all samples <ref type="bibr" target="#b0">[1]</ref>, which justifies the widely used small-loss criterion: treating samples with small training loss as clean ones. In particular, MentorNet <ref type="bibr" target="#b15">[16]</ref> firstly trains a teacher network, then uses it to select clean instances for guiding the training of the student network. As for Co-teaching <ref type="bibr" target="#b11">[12]</ref>, in each mini-batch of data, each network chooses its small-loss instances and exchanges them with its peer network for updating the parameters. The authors argued that these two networks could filter different types of errors brought by noisy labels since they have different learning abilities. When the error from noisy data flows into the peer network, it will attenuate this error due to its robustness.</p><p>Disagreement. The "Disagreement" strategy is also applied to this problem. For instance, Decoupling <ref type="bibr" target="#b22">[23]</ref> updates the model only using instances on which the predictions of two different networks are different. The idea of disagreement-update is similar to hard example mining <ref type="bibr" target="#b32">[33]</ref>, which trains model with examples that are misclassified and expects these examples to help steer the classifier away from its current mistakes. For the "Disagreement" strategy, the decision of "when to update" depends on a disagreement between two networks instead of depending on the label. As a result, it would help decrease the divergence between these networks. However, as noisy labels are spread across the whole space of examples, there may be many noisy labels in the disagreement area, where the Decoupling approach cannot handle noisy labels explicitly. Combining the "Disagreement" strategy with crossupdate in Co-teaching, Co-teaching+ <ref type="bibr" target="#b40">[41]</ref> achieves excel-  <ref type="bibr" target="#b15">[16]</ref>, Decoupling <ref type="bibr" target="#b22">[23]</ref>, Co-teaching+ <ref type="bibr" target="#b40">[41]</ref> and JoCoR. Assume that the error flow comes from the biased selection of training instances, and error flow from network A or B is denoted by red arrows or green arrows, respectively. First panel: M-Net maintains only one network (A). Second panel: Decoupling maintains two networks (A&amp;B). The parameters of two networks are updated, when the predictions of them disagree (!=). Third panel: In Co-teaching+, each network teaches its small-loss instances with prediction disagreement (!=) to its peer network. Fourth panel: JoCoR also maintains two networks (A&amp;B) but trains them as a whole with a joint loss, which makes predictions of each network closer to ground true labels and peer network's. lent performance in improving the robustness of DNNs against noisy labels. In spite of that, Co-teaching+ only selects small-loss instances with different predictions from two models so very few examples are utilized for training in each mini-batch when datasets are with extremely high noise rate. It would prevent the training process from efficient use of training examples. This phenomenon will be explicitly shown in our experiments in the symmetric-80% label noise case.</p><p>Other deep learning methods. In addition to the aforementioned approaches, there are some other deep learning solutions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref> to deal with noisy labels, including pseudo-label based <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> and robust loss based approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b45">46]</ref>. For pseudo-label based approaches, Joint optimization <ref type="bibr" target="#b34">[35]</ref> learns network parameters and infers the ground-true labels simultaneously. PENCIL <ref type="bibr" target="#b39">[40]</ref> adopts label probability distributions to supervise network learning and to update these distributions through back-propagation end-to-end in each epoch. For robust loss based approaches, F-correct <ref type="bibr" target="#b27">[28]</ref> proposes a robust risk minimization method to learn neural networks for multi-class classification by estimating label corruption probabilities. GCE <ref type="bibr" target="#b45">[46]</ref> combines the advantages of the mean absolute loss and the cross entropy loss to obtain a better loss function and presents a theoretical analysis of the proposed loss functions in the context of noisy labels.</p><p>Semi-supervised learning. Semi-supervised learning also belongs to the family of weakly supervised learning frameworks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref>. There are some interesting works from semi-supervised learning that are highly relevant to our approach. In contrast to "Disagreement" strategy, many of them are based on a agree-ment maximization algorithm. Co-RLS <ref type="bibr" target="#b33">[34]</ref> extends standard regularization methods like Support Vector Machines (SVM) and Regularized Least squares (RLS) to multi-view semi-supervised learning by optimizing measures of agreement and smoothness over labelled and unlabelled examples. EA++ <ref type="bibr" target="#b18">[19]</ref> is a co-regularization based approach for semi-supervised domain adaptation, which builds on the notion of augmented space and harnesses unlabeled data in the target domain to further assist the transfer of information from source to target. The intuition is that different models in each view would agree on the labels of most examples, and it is unlikely for compatible classifiers trained on independent views to agree on an incorrect label. This intuition also motivates us to deal with noisy labels based on the agreement maximization principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>As mentioned before, we suggest to apply the agreement maximization principle to tackle the problem of noisy labels. In our approach, we encourage two different classifiers to make predictions closer to each other by explicit regularization method instead of hard sampling employed by the ??IJDisagreement??? strategy. This method could be considered as a meta-algorithm that trains two base classifiers by one loss function, which includes a regularization term to reduce divergence between the two classifiers.</p><p>For multi-class classification with M classes, we suppose the dataset with N samples is given as</p><formula xml:id="formula_0">D = {x i , y i } N i=1</formula><p>, where x i is the i-th instance with its observed label as y i ? {1, . . . , M }. Similar to Decoupling and Co-teaching+, we formulate the proposed JoCoR approach with two deep neural networks denoted by f (x, ? 1 ) and f (x, ? 2 ), while p 1 = [p 1 1 , p 2 1 , . . . , p M 1 ] and p 2 = [p 1 2 , p 2 2 , . . . , p M 2 ] denote their prediction probabilities of instance x i , respectively. In other words, p 1 and p 2 are the outputs of the "softmax" layer in ? 1 and ? 2 . Network. For JoCoR, each network can be used to predict labels alone, but during the training stage these two networks are trained with a pseudo-siamese paradigm, which means their parameters are different but updated simultaneously by a joint loss (see <ref type="figure" target="#fig_1">Figure 2</ref>). In this work, we call this paradigm as "Joint Training".</p><p>Specifically, our proposed loss function on x i is constructed as follows:</p><formula xml:id="formula_1">(x i ) = (1 ? ?) * sup (x i , y i ) + ? * con (x i )<label>(1)</label></formula><p>In the loss function, the first part sup is conventional supervised learning loss of the two networks, the second part con is the contrastive loss between predictions of the two networks for achieving Co-Regularization. Classification loss. For multi-class classification, we use Cross-Entropy Loss as the supervised part to minimize the distance between predictions and labels. </p><formula xml:id="formula_2">sup (x i , y i ) = C1 (x i , y i ) + C2 (x i , y i ) = ? N i=1 M m=1 y i log(p m 1 (x i )) ? N i=1 M m=1 y i log(p m 2 (x i ))<label>(2)</label></formula><p>Intuitively, two networks can filter different types of errors brought by noisy labels since they have different learning abilities. In Co-teaching <ref type="bibr" target="#b11">[12]</ref>, when the two networks exchange the selected small-loss instances in each minibatch data, the error flows can be reduced by peer networks mutually. By virtue of the joint-training paradigm, our JoCoR would consider the classification losses from both two networks during the "small-loss" selection stage. In this way, JoCoR can share the same advantage of the cross-update strategy in Co-teaching. This argument will be clearly supported by the ablation study in the later section. Contrastive loss. From the view of agreement maximization principles <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref>, different models would agree on labels of most examples, and they are unlikely to agree on incorrect labels. Based on this observation, we apply the Co-Regularization method to maximize the agreement between two classifiers. On one hand, the Co-Regularization term could help our algorithm select examples with clean labels since an example with small Co-Regularization loss means that two networks reach an agreement on its predictions. On the other hand, the regularization from peer networks helps the model find a much wider minimum, which is expected to provide better generalization performance <ref type="bibr" target="#b44">[45]</ref>.</p><p>In JoCoR, we utilize the contrastive term as Co-Regularization to make the networks guide each other. To measure the match of the two networks' predictions p 1 and p 2 , we adopt the Jensen-Shannon (JS) Divergence. To simplify implementation, we could use the symmetric Kullback-Leibler(KL) Divergence to surrogate this term.</p><formula xml:id="formula_3">con = D KL (p 1 ||p 2 ) + D KL (p 2 ||p 1 )<label>(3)</label></formula><p>where Shuffle training set D; <ref type="bibr">3:</ref> for n = 1, . . . , I max do 4:</p><formula xml:id="formula_4">D KL (p 1 ||p 2 ) = N i=1 M m=1 p m 1 (x i ) log p m 1 (x i ) p m 2 (x i ) D KL (p 2 ||p 1 ) = N i=1 M m=1 p m 2 (x i ) log p m 2 (x i ) p m 1 (x i ) Algorithm 1 JoCoR Input: Network f with ? = {? 1 , ? 2 },</formula><p>Fetch mini-batch D n from D; 5:</p><formula xml:id="formula_5">p 1 = f (x, ? 1 ), ?x ? D n ; 6: p 2 = f (x, ? 2 ), ?x ? D n ; 7:</formula><p>Calculate the joint loss by (1) using p 1 and p 2 ; 8:</p><p>Obtain small-loss setsD n by (4) from D n ; <ref type="bibr">9:</ref> Obtain L by (5) onD n ; <ref type="bibr">10:</ref> Update ? = ? ? ??L; <ref type="bibr">11:</ref> end for 12:</p><formula xml:id="formula_6">Update R(t) = 1 ? min t T k ?, ? 13: end for Output: ? 1 and ? 2</formula><p>Small-loss Selection Before introducing the details, we first clarify the connection between small losses and clean instances. Intuitively, small-loss examples are likely to be the ones that are correctly labelled <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Thus, if we train our classifier only using small-loss instances in each minibatch data, it would be resistant to noisy labels.</p><p>To handle noisy labels, we apply the "small-loss" criterion to select "clean" instances (step 8 in Algorithm 1). Following the setting of Co-teaching+, we update R(t) (step 12), which controls how many small-loss data should be selected in each training epoch. At the beginning of training, we keep more small-loss data (with a large R(t)) in each mini-natch since deep networks would fit clean data first <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>. With the increase of epochs, we reduce R(t) gradually until reaching 1 ? ? , keeping fewer examples in each mini-batch. Such operation will prevent deep networks from over-fitting noisy data <ref type="bibr" target="#b11">[12]</ref>.</p><p>In our algorithm, we use the joint loss (1) to select smallloss examples. Intuitively, an instance with small joint loss means that both two networks could be easy to reach a consensus and make correct predictions on it. As two networks have different learning abilities based on different initial conditions, the selected small-loss instances are more likely to be with clean labels than those chosen by a single model. Specifically, we conduct small-loss selection as follows:</p><formula xml:id="formula_7">D n = arg min D n :|D n |?R(t)|Dn| (D n )<label>(4)</label></formula><p>After obtaining the small-loss instances, we calculate the average loss on these examples for further backpropagation: <ref type="table">Table 1</ref>. Comparison of state-of-the-art and related techniques with our JoCoR approach. In the first column, "small loss": regarding small-loss samples as "clean" samples, which is based on the memorization effects of deep neural networks; "double classifiers": training two classifiers simultaneously; "cross update": updating parameters in a cross manner instead of a parallel manner; "joint training": training two classifiers with a joint loss; "disagreement": updating two classifiers on disagreed examples during the entire training epochs; "agreement": maximizing the agreement of two classifiers by regularization during the whole training epochs. Relations to other approaches. We compare JoCoR with other related approaches in <ref type="table">Table 1</ref>. Specifically, Decoupling applies the "disagreement" strategy to select instances while Co-teaching use small-loss criterion. Besides, Coteaching updates parameters of networks by the "crossupdate" strategy to reduce the accumulated error flow.</p><formula xml:id="formula_8">L = 1 |D| x?D (x)<label>(5)</label></formula><p>Combining the "disagreement" strategy and the "crossupdate" strategy, Co-teaching+ achieves excellent performance. As for our JoCoR, we also select small-loss examples but update the networks by Joint Training. Furthermore, we use the Co-Regularization to maximize agreement between the two networks. Note that Co-Regularization in our proposed method and "disagreement??? strategy in Decoupling are both essentially to reduce the divergence between the two classifiers. The difference between them lies in that the former uses an explicit regularization methods with all training examples while the latter employs hard sampling that reduces the effective number of training examples. It is especially important in the case of small-loss selection, because the selection would further decrease the effective number of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we first compare JoCoR with some stateof-the-art approaches, then analyze the impact of Joint Training and Co-Regularization by ablation study. We also analyze the effect of ? in (1) by sensitivity analysis and put it in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup</head><p>Datasets. We verify the effectiveness of our proposed algorithm on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and Clothing1M <ref type="bibr" target="#b37">[38]</ref>, and the detailed characteristics of these datasets can be found in supplementary materials. These datasets are popularly used for the evaluation of learning with noisy labels in previous literatures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>. Especially, Clothing1M is a large-scale real-   world dataset with noisy labels, which is widely used in the related works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b37">38]</ref>. Since all datasets are clean except Clothing1M, following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, we need to corrupt these datasets manually by the label transition matrix Q, where Q ij = Pr[? = j|y = i] given that noisy? is flipped from clean y. Assume that the matrix Q has two representative structures: (1) Symmetry flipping <ref type="bibr" target="#b35">[36]</ref>; (2) Asymmetry flipping <ref type="bibr" target="#b27">[28]</ref>: simulation of fine-grained classification with noisy labels, where labellers may make mistakes only within very similar classes.</p><p>Following F-correction <ref type="bibr" target="#b27">[28]</ref>, only half of the classes in the dataset are with noisy labels in the setting of asymmetric noise, so the actual noise rate in the whole dataset ? is half of the noisy rate in the noisy classes. Specifically, when the asymmetric noise rate is 0.4, it means ? = 0.2. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example of noise transition matrix.</p><p>For experiments on Clothing1M, we use the 1M images with noisy labels for training, the 14k and 10k clean data for validation and test, respectively. Note that we do not use the 50k clean training data in all the experiments because only noisy labels are required during the training process <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>. For preprocessing, we resize the image to 256 ? 256, crop the middle 224 ? 224 as input, and perform normalization. Baselines. We compare JoCoR (Algorithm 1) with the following state-of-the-art algorithms, and implement all methods with default parameters by PyTorch, and conduct all the experiments on NVIDIA Tesla V100 GPU.</p><p>(i) Co-teaching+ <ref type="bibr" target="#b40">[41]</ref>, which trains two deep neural networks and consists of disagreement-update step and cross-update step.</p><p>(ii) Co-teaching <ref type="bibr" target="#b11">[12]</ref>, which trains two networks simultaneously and cross-updates parameters of peer networks.</p><p>(iii) Decoupling <ref type="bibr" target="#b22">[23]</ref>, which updates the parameters only using instances which have different predictions from two classifiers.</p><p>(iv) F-correction <ref type="bibr" target="#b27">[28]</ref>, which corrects the prediction by the label transition matrix. As suggested by the authors, we first train a standard network to estimate the transition matrix Q.  Network Structure and Optimizer. We use a 2-layer MLP for MNIST, a 7-layer CNN network architecture for CIFAR-10 and CIFAR-100. The detailed information can be found in supplementary materials. For Clothing1M, we use ResNet with 18 layers. For experiments on MNIST, CIFAR-10 and CIFAR-100, Adam optimizer (momentum=0.9) is used with an initial learning rate of 0.001, and the batch size is set to 128. We run 200 epochs in total and linearly decay learning rate to zero from 80 to 200 epochs.</p><p>For experiments on Clothing1M, we also use Adam optimizer (momentum=0.9) and set batch size to 64. During the training stage, we run 15 epochs in total and set learning rate to 8 ? 10 ?4 , 5 ? 10 ?4 and 5 ? 10 ?5 for 5 epochs each.</p><p>As for ? in our loss function (1), we search it in [0.05, 0.10, 0.15,. . .,0.95] with a clean validation set for best performance. When validation set is also with noisy labels, we use the small-loss selection to choose a clean subset for validation. As deep networks are highly nonconvex, even with the same network and optimization method, different initializations can lead to different local optimum. Thus, following Decoupling <ref type="bibr" target="#b22">[23]</ref>, we also take two networks with the same architecture but different initializations as two classi-fiers. Measurement. To measure the performance, we use the test accuracy, i.e., test accuracy = (# of correct predictions) / (# of test). Besides, we also use the label precision in each mini-batch, i.e., label precision = (# of clean labels) / (# of all selected labels). Specifically, we sample R(t) of small-loss instances in each mini-batch and then calculate the ratio of clean labels in the small-loss instances. Intuitively, higher label precision means less noisy instances in the mini-batch after sample selection, so the algorithm with higher label precision is also more robust to the label noise. All experiments are repeated five times. The error bar for STD in each figure has been highlighted as a shade. Selection setting. Following Co-teaching, we assume that the noise rate ? is known. To conduct a fair comparison in benchmark datasets, we set the ratio of small-loss samples R(t) as identical: R(t) = 1 ? min t T k ?, ? , where T k = 10 for MNIST, CIFAR-10 and CIFAR100, T k = 5 for Clothing1M. If ? is not known in advance, ? can be inferred using validation sets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-Arts</head><p>Results on MNIST . At the top of <ref type="figure" target="#fig_2">Figure 3</ref>, it shows test accuracy vs. epochs on MNIST. In all four plots, we can see the memorization effect of networks, i.e., test accuracy of Standard first reaches a very high level and then gradu-  We can compare the test accuracy of different algorithms in detail in <ref type="table" target="#tab_2">Table 2</ref>. In the most natural Symmetry-20% case, all new approaches work better than Standard obviously, which demonstrates their robustness. Among them, JoCoR and Co-teaching+ work significantly better than other methods. When it goes to Symmetry-50% case and Asymmetry-40% case, Decoupling begins to fail while other methods still work fine, especially JoCoR and Co-teaching+. However, Co-teaching+ cannot combat with the hardest Symmetry-80% case, where it only achieves 58.92%. In this case, JoCoR achieves the best average classification accuracy (84.89%) again.</p><p>To explain such excellent performance, we plot label precision vs. epochs at the bottom of <ref type="figure" target="#fig_2">Figure 3</ref>. Only Decoupling, Co-teaching, Co-teaching+ and JoCoR are considered here, as they include example selection during training. First, we can see both JoCoR and Co-teaching can successfully pick clean instances out. Note that JoCoR not only reaches high label precision in all four cases but also performs better and better with the increase of epochs while Co-teaching declines gradually after reaching the top. This shows that our approach is better at finding clean instances. Then, Decoupling and Co-teaching+ fail in selecting clean examples. As mentioned in Related Work, very few examples are utilized by Co-teaching+ in the training process when noise rate goes to be extremely high. In this way, we can understand why Co-teaching+ performs poorly on the hardest case. Results on CIFAR-10 . <ref type="table" target="#tab_3">Table 3</ref> shows test accuracy on CIFAR-10. As we can see, JoCoR performs the best in all four cases again. In the Symmetric-20% case, JoCoR works much better than all other baselines and Co-teaching+ performs better than Co-teaching and Decoupling. In the other three cases, JoCoR is still the best and Co-teaching+ cannot even achieve comparable performance with Co-teaching. <ref type="figure" target="#fig_4">Figure 5</ref> shows test accuracy and label precision vs. epochs. JoCoR outperforms all the other comparing approaches on both test accuracy and label precision. On label precision, while Decoupling and Co-teaching+ fail to find clean instances, both JoCoR and Co-teaching can do this. An interesting phenomenon is that in the Asymmetry-40% case, although Co-teaching can achieve better performance than JoCoR in the first 100 epochs, JoCoR consistently outperforms it in all the later epochs. The result shows that JoCoR has better generalization ability than Co-teaching. Results on CIFAR-100 . Then, we show our results on CIFAR-100. The test accuracy is shown in <ref type="table" target="#tab_4">Table 4</ref>. Test accuracy and label precision vs. epochs are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Note that there are only 10 classes in MNIST and CIFAR-10 datasets. Thus, overall the accuracy is much Results on Clothing1M . Finally, we demonstrate the efficacy of the proposed method on the real-world noisy labels using the Clothing1M dataset. As shown in <ref type="table" target="#tab_5">Table 5</ref>, best denotes the scores of the epoch where the validation accuracy is optimal, and last denotes the scores at the end of training. The proposed JoCoR method gets better result than state-of-the-art methods on best. After all epochs, JoCoR achieves a significant improvement in accuracy of +5.11 over Standard, and an improvement of +1.28 over the best baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To conduct ablation study for analyzing the effect of Co-Regularization, we set up the experiments above MNIST and CIFAR-10 with Symmetry-50% noise. For implementing Joint Training without Co-Regularization (Joint-only), we set the ? in (1) to 0. Besides, to verify the effect of the Joint Training paradigm, we introduce Co-teaching and Standard enhanced by "small-loss" selection (Standard+) to join the comparison. Recall that the joint-training method selects examples by the joint loss while Co-teaching uses cross-update method to reduce the error flow <ref type="bibr" target="#b11">[12]</ref>, these two methods should play a similar role during training according to the previous analysis.</p><p>The test accuracy and label precision vs. epochs on MNIST are shown in <ref type="figure">Figure 7</ref>. As we can see, JoCoR performs much better than the others on both test accuracy and label precision. The former keeps almost no decrease while the latter decline a lot after reaching the top. This observation indicates that Co-Regularization strongly hinders neural networks from memorizing noisy labels.</p><p>The test accuracy and label precision vs. epochs on CIFAR-10 are shown in <ref type="figure">Figure 8</ref>. In this figure, JoCoR still maintains a huge advantage over the other three meth- ods on both test accuracy and label precision while Jointonly, Co-teaching and Standard+ remain the same trend as these for MNIST, keeping a downward tendency after increasing to the highest point. These results show that Co-Regularization plays a vital role in handling noisy labels. Moreover, Joint-only achieves a comparable performance with Co-teaching on test accuracy and performs better than Co-teaching and Standard+ on label precision. It shows that Joint Training is a more efficient paradigm to help select clean examples than cross-update in Co-teaching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>The paper proposes an effective approach called JoCoR to improve the robustness of deep neural networks with noisy labels. The key idea of JoCoR is to train two classifiers simultaneously with one joint loss, which is composed of regular supervised part and Co-Regularized part. Similar to Co-teaching+, we also select small-loss instances to update networks in each mini-batch data by the joint loss. We conduct experiments on MNIST, CIFAR-10, CIFAR-100 and Clothing1M to demonstrate that, JoCoR can train deep models robustly with the slightly and extremely noisy supervision. Furthermore, the ablation studies clearly demonstrate the effectiveness of Co-Regularization and Joint Training. In future work, we will explore the theoretical foundation of JoCoR based on the view of traditional Cotraining algorithms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>the detailed characteristics of these datasets are shown in <ref type="table" target="#tab_6">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architecture</head><p>The network architectures of the MLP and CNN models are shown in <ref type="table" target="#tab_7">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Parameter Sensitivity Analysis</head><p>To conduct the sensitivity analysis on parameter ?, we set up the experiments above MNIST with symmetry-50% noise. Specifically, we compare ? in the range of [0.05, 0.35, 0.65, 0.95]. The larger the ? is, the less the divergence of two classifiers in JoCoR would be.  The test accuracy and label precision vs. number of epochs are in <ref type="figure" target="#fig_8">Figure 9</ref>. Obviously, As the ? increases, the performance of our algorithm on test accuracy gets better and better. When ? = 0.95, JoCoR achieves the best performance. We can see the same trends on label precision, which means that JoCoR can select clean example more precisely with a larger ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of error flow among MentorNet (M-Net)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>JoCoR schematic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Results on MNIST dataset. Top: test accuracy(%) vs. epochs; bottom: label precision(%) vs. epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Example of noise transition matrix T (taking 6 classes and noise ratio 0.4 as an example)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results on CIFAR-10 dataset. Top: test accuracy(%) vs. epochs; bottom: label precision(%) vs. epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Results on CIFAR-100 dataset. Top: test accuracy(%) vs. epochs; bottom: label precision(%) vs. epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Results of ablation study on MNIST Results of ablation study on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Results of JoCoR with different ? on MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>learning rate ?, fixed ? , epoch T k and T max , iteration I max ; 1: for t = 1,2,. . . ,T max do</figDesc><table><row><cell>2:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average test accuracy (%) on MNIST over the last 10 epochs. Asymmetry-40% 79.00 ? 0.28 89.77 ? 0.96 81.84 ? 0.38 90.28 ? 0.27 93.28 ? 0.43 95.24 ? 0.10</figDesc><table><row><cell>Flipping-Rate</cell><cell cols="2">Standard</cell><cell>F-correction</cell><cell>Decoupling</cell><cell>Co-teaching</cell><cell>Co-teaching+</cell><cell>JoCoR</cell></row><row><cell>Symmetry-20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Symmetric Noise 0.4</cell><cell></cell><cell></cell><cell>Asymmetric Noise 0.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">60% 8% 8% 8% 8% 8%</cell><cell cols="2">100% 0% 0% 0% 0% 0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8% 60% 8% 8% 8% 8%</cell><cell cols="2">0% 60% 0% 0% 40% 0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8% 8% 60% 8% 8% 8%</cell><cell cols="2">40% 0% 60% 0% 0% 0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8% 8% 8% 60% 8% 8%</cell><cell cols="2">0% 0% 0% 100% 0% 0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8% 8% 8% 8% 60% 8%</cell><cell cols="2">0% 0% 0% 0% 100% 0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">8% 8% 8% 8% 8% 60%</cell><cell cols="2">0% 0% 40% 0% 0% 60%</cell><cell></cell><cell></cell><cell></cell></row></table><note>79.56 ? 0.44 95.38 ? 0.10 93.16 ? 0.11 95.10 ? 0.16 97.81 ? 0.03 98.06 ? 0.04 Symmetry-50% 52.66 ? 0.43 92.74 ? 0.21 69.79 ? 0.52 89.82 ? 0.31 95.80 ? 0.09 96.64 ? 0.12 Symmetry-80% 23.43 ? 0.31 72.96 ? 0.90 28.51 ? 0.65 79.73 ? 0.35 58.92 ? 14.73 84.89 ? 4.55</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Average test accuracy (%) on CIFAR-10 over the last 10 epochs. ? 0.52 68.74 ? 0.20 69.32 ? 0.40 78.23 ? 0.27 78.71 ? 0.34 85.73 ? 0.19 Symmetry-50% 42.71 ? 0.42 42.19 ? 0.60 40.22 ? 0.30 71.30 ? 0.13 57.05 ? 0.54 79.41 ? 0.25 Symmetry-80% 16.24 ? 0.39 15.88 ? 0.42 15.31 ? 0.43 26.58 ? 2.22 24.19 ? 2.74 27.78 ? 3.06 Asymmetry-40% 69.43 ? 0.33 70.60 ? 0.40 68.72 ? 0.30 73.78 ? 0.22 68.84 ? 0.20 76.36 ? 0.49 (v) As a simple baseline, we compare JoCoR with the standard deep network that directly trains on noisy datasets (abbreviated as Standard).</figDesc><table><row><cell>Flipping-Rate</cell><cell>Standard</cell><cell>F-correction</cell><cell>Decoupling</cell><cell>Co-teaching Co-teaching+</cell><cell>JoCoR</cell></row><row><cell>Symmetry-20%</cell><cell>69.18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average test accuracy (%) on CIFAR-100 over the last 10 epochs. ? 0.44 37.95 ? 0.10 33.10 ? 0.12 43.73 ? 0.16 49.27 ? 0.03 53.01 ? 0.04 Symmetry-50% 16.97 ? 0.40 24.98 ? 1.82 15.25 ? 0.20 34.96 ? 0.50 40.04 ? 0.70 43.49 ? 0.46 Symmetry-80% 4.41 ? 0.14 2.10 ? 2.23 3.89 ? 0.16 15.15 ? 0.46 13.44 ? 0.37 15.49 ? 0.98 Asymmetry-40% 27.29 ? 0.25 25.94 ? 0.44 26.11 ? 0.39 28.35 ? 0.25 33.62 ? 0.39 32.70 ? 0.35 ally decreases. Thus, a good robust training method should stop or alleviate the decreasing process. On this point, Jo-CoR consistently achieves higher accuracy than all the other baselines in all four cases.</figDesc><table><row><cell>Flipping-Rate</cell><cell>Standard</cell><cell>F-correction</cell><cell>Decoupling</cell><cell>Co-teaching Co-teaching+</cell><cell>JoCoR</cell></row><row><cell>Symmetry-20%</cell><cell>35.14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Classification accuracy (%) on the Clothing1M test set</figDesc><table><row><cell>Methods</cell><cell>best</cell><cell>last</cell></row><row><cell>Standard</cell><cell>67.22</cell><cell>64.68</cell></row><row><cell>F-correction</cell><cell>68.93</cell><cell>65.36</cell></row><row><cell>Decoupling</cell><cell>68.48</cell><cell>67.32</cell></row><row><cell>Co-teaching</cell><cell>69.21</cell><cell>68.51</cell></row><row><cell>Co-teaching+</cell><cell>59.32</cell><cell>58.79</cell></row><row><cell>JoCoR</cell><cell>70.30</cell><cell>69.79</cell></row><row><cell cols="3">lower than previous ones in Tables 2 and 3. But JoCoR</cell></row><row><cell cols="3">still achieves high test accuracy on this datasets. In the</cell></row><row><cell cols="3">easiest Symmetry-20% and Symmetry-50% cases, JoCoR</cell></row><row><cell cols="3">works significantly better than Co-teaching+, Co-teaching</cell></row><row><cell cols="3">and other methods. In the hardest Symmetry-80% case,</cell></row><row><cell cols="3">JoCoR and Co-teaching tie together but JoCoR still gets</cell></row><row><cell cols="3">higher testing accuracy. When it turns to Asymmetry-40%</cell></row><row><cell cols="3">case, JoCoR and Co-teaching+ perform much better than</cell></row><row><cell cols="3">other methods. On label precision, JoCoR keeps the best</cell></row><row><cell cols="2">performance in all four cases.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Summary of datasets used in the experiments.</figDesc><table><row><cell></cell><cell># of training</cell><cell># of test</cell><cell># of class</cell><cell>size</cell></row><row><cell>MNIST</cell><cell>60,000</cell><cell>10,000</cell><cell>10</cell><cell>28 ? 28</cell></row><row><cell>CIFAR-10</cell><cell>50,000</cell><cell>10,000</cell><cell>10</cell><cell>32 ? 32</cell></row><row><cell>CIFAR-100</cell><cell>50,000</cell><cell>10,000</cell><cell>100</cell><cell>32 ? 32</cell></row><row><cell>Clothing1M</cell><cell>1,000,000</cell><cell>10,000</cell><cell>14</cell><cell>224 ? 224</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The models used on MNIST, CIFAR-10 and CIFAR-100 MLP on MNIST CNN on CIFAR-10 &amp; CIFAR-100 28 ? 28 Gray Image 32 ? 32 RGB Image Dense 28 ? 28 ?? 256, ReLU 3 ? 3, 64 BN, ReLU 3 ? 3, 64 BN, ReLU 2 ? 2 Max-pool 3 ? 3, 128 BN, ReLU 3 ? 3, 128 BN, ReLU 2 ? 2 Max-pool 3 ? 3, 196 BN, ReLU 3 ? 3, 196 BN, ReLU 2 ? 2 Max-pool Dense 256 ?? 10 Dense 256 ?? 100</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research is supported by Singapore National Research Foundation projects AISG-RP-2019-0013, NSOE-TSS2019-01, and NTU. We gratefully acknowledge the support of NVAITC (NVIDIA AI Tech Center) for our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification from pairwise similarity and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Noisetolerant learning, the parity problem, and the statistical query model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="506" to="519" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh Annual Conference on Computational Learning Theory</title>
		<meeting>the eleventh Annual Conference on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of learning from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Leveraging latent label distributions for partial label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2107" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Partial label learning by semantic difference maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2294" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Partial label learning with self-guided retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3542" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representation</title>
		<meeting>the 5th International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning with inadequate and incorrect supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="889" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep selflearning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Binary classification for positive-confidence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5917" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05055</idno>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Positive-unlabeled learning with nonnegative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marthinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coregularization based semi-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the minimal supervision for training any binary classifier from only unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representation</title>
		<meeting>the International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from corrupted binary labels via class-probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1196" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Theoretical comparisons of positive-unlabeled learning against positive-negative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1199" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wittawat</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<title level="m">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification based on classification from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marthinus</forename><surname>Christoffel Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2998" to="3006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Class proportion estimation with application to multiclass anomaly rejection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="850" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A co-regularization approach to semi-supervised learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Learning With Multiple Views</title>
		<meeting>ICML Workshop on Learning With Multiple Views</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Are anchor points really indispensable in label-noise learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6835" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mer</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramanathan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="291" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07788</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">How does disagreement benefit co-teaching? arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kayhan Batmanghelich, and Dacheng Tao. An efficient and provable approach for mixture proportion estimation using linear independence assumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4480" to="4489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning with biased complementary labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8778" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A brief introduction to weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Science Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
