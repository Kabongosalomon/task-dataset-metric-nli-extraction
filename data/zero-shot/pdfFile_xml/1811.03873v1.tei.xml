<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Short-Term Memory with Dynamic Skip Connections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
							<email>ljzhao16@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaosong</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
							<email>mlpeng16@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
							<email>jjgong@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<email>xjhuang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University Shanghai Insitute of Intelligent Electroics &amp; Systems</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Short-Term Memory with Dynamic Skip Connections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, long short-term memory (LSTM) has been successfully used to model sequential data of variable length. However, LSTM can still experience difficulty in capturing long-term dependencies. In this work, we tried to alleviate this problem by introducing a dynamic skip connection, which can learn to directly connect two dependent words.</p><p>Since there is no dependency information in the training data, we propose a novel reinforcement learning-based method to model the dependency relationship and connect dependent words. The proposed model computes the recurrent transition functions based on the skip connections, which provides a dynamic skipping advantage over RNNs that always tackle entire sentences sequentially. Our experimental results on three natural language processing tasks demonstrate that the proposed method can achieve better performance than existing methods. In the number prediction experiment, the proposed model outperformed LSTM with respect to accuracy by nearly 20%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Recurrent neural networks (RNNs) have achieved significant success for many difficult natural language processing tasks, e.g., neural machine translation , conversational/dialogue modeling <ref type="bibr" target="#b20">(Serban et al. 2016)</ref>, document summarization <ref type="bibr" target="#b19">(Nallapati et al. 2016)</ref>, sequence tagging (Santos and Zadrozny 2014), and document classification <ref type="bibr" target="#b10">(Dai and Le 2015)</ref>. Because of the need to model long sentences, an important challenge encountered by all these models is the difficulty of capturing longterm dependencies. In addition, training RNNs using the "Back-Propagation Through Time" (BPTT) method is vulnerable to vanishing and exploding gradients.</p><p>To tackle the above challenges, several variations of RNNs have been proposed using new RNN transition functional units and optimization techniques, such as gated recurrent unit (GRU) <ref type="bibr" target="#b9">(Chung et al. 2014</ref>) and long short-term memory (LSTM) <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997)</ref>. Recently, many of the existing methods have focused on the connection architecture, including "stacked RNNs" <ref type="bibr" target="#b12">(El Hihi and Bengio 1996)</ref> and "skip RNNs" .  introduced a general formulation Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p><p>The exact amounts spent depend to some extent on appropriations legislation.</p><p>You may depend on the accuracy of the report.</p><p>The man who wore a Stetson on his head went inside. <ref type="figure">Figure 1</ref>: Examples of dependencies with variable length in the language. The same phrase "depend on" in different sentences would have dependencies with different lengths. The use of clauses also makes the dependency length uncertain. Therefore, the models using a plain LSTM or an LSTM with fixed skip connections would be difficult to capture such information.</p><p>for RNN architectures and proposed three architectural complexity measures: recurrent skip coefficients, recurrent depth, and feedforward depth. In addition, the skip coefficient is defined as a function of the shortest path from one time to another. In particular, they found empirical evidence that increasing the feedforward depth might not help with long-term dependency tasks, while increasing the recurrent skip coefficient could significantly improve the performance on long-term dependency tasks.</p><p>However, these works on recurrent skip coefficients adopted fixed skip lengths <ref type="bibr" target="#b7">Chang et al. 2017)</ref>. Although quite powerful given their simplicity, the fixed skip length is constrained by its inability to take advantage of the dependencies with variable lengths in the language, as shown in <ref type="figure">Figure 1</ref>. From this figure, we can see that the same phrase "depend on" in different sentences would have dependencies with different lengths. The use of clauses also makes the dependency length uncertain. In addition, the meaning of a sentence is often determined by words that are not very close. For example, consider the sentence "The man who wore a Stetson on his head went inside." This sentence is really about a man going inside, not about the Stetson. Hence, the models using a plain LSTM or an LSTM with a fixed skip would be difficult to capture such information and insufficient to fully capture the semantics of the natural language.   <ref type="figure">Figure 2</ref>: Architecture of the proposed model. At time step t, the agent selects one of the past few states based on the current input x t and the previous hidden state h t?1 . The agent's selections will influence the log-likelihood of the ground truth, which will be a reward or penalty to optimize the agent. Take the phrase "depend to some extent on" as an example, the agent should learn to select the hidden state from "depend" not "extend" to predict "on," because selecting "depend" receives a larger reward.</p><formula xml:id="formula_0">d I m q N v y Y h 4 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z d 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z A y O o g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 g M E V X I 5 n P y 8 / z V r q d I m q N v y Y h 4 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z d 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z A y O o g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 g M E V X I 5 n P y 8 / z V r q d I m q N v y Y h 4 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z d 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z A y O o g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 g M E V X I 5 n P y 8 / z V r q d I m q N v y Y h 4 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z d 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z A y O o g = = &lt; / l a t e x i t &gt; x 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B S o 1 s b t b Y 8 n J y L P q Z l W O 7 Y z Z 2 2 g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z b 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z Z G O o w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B S o 1 s b t b Y 8 n J y L P q Z l W O 7 Y z Z 2 2 g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z b 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z Z G O o w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B S o 1 s b t b Y 8 n J y L P q Z l W O 7 Y z Z 2 2 g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z b 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z Z G O o w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B S o 1 s b t b Y 8 n J y L P q Z l W O 7 Y z Z 2 2 g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z T t q l m 0 3 Y 3 Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q 4 N q 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R S y e Z Y u i z R C S q E 1 K N g k v 0 D T c C O 6 l C G o c C 2 + H 4 d u a 3 H 1 F p n s g H M 0 k x i O l Q 8 o g z a q z k P / V z b 9 q v 1 t y 6 O w d Z J V 5 B a l C g 2 a 9 + 9 Q Y J y 2 K U h g m q d d d z U x P k V B n O B E 4 r v U x j S t m Y D r F r q a Q x 6 i C f H z s l Z 1 Y Z k C h R t q Q h c / X 3 R E 5 j r S d x a D t j a k Z 6 2 Z u J / 3 n d z E T X Q c 5 l m h m U b L E o y g Q x C Z l 9 T g Z c I T N i Y g l l i t t b C R t R R Z m x + V R s C N 7 y y 6 u k d V H 3 3 L p 3 f 1 l r 3 B R x l O E E T u E c P L i C B t x B E 3 x g w O E Z X u H N k c 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A z Z G O o w = = &lt; / l a t e x i t &gt; x 2 &lt; l a t</formula><p>To overcome this limitation, in this paper, we consider the sequence modeling problem with dynamic skip connections. The proposed model allows "LSTM cells" to compute recurrent transition functions based on one optimal set of hidden and cell states from the past few states. However, in general, we do not have the labels to guide which two words should be connected. To overcome this problem, we propose the use of reinforcement learning to learn the dependent relationship through the exploring process. The main benefit of this approach is the better modeling of dependencies with variable length in the language. In addition, this approach also mitigates vanishing and exploding gradient problems with a shorter gradient backpropagation path. Through experiments, We find the empirical evidences (see Experiments and Appendix) that our model is better than that using attention mechanism to connect two words, as reported in <ref type="bibr" target="#b11">(Deng et al. 2018)</ref>. Experimental results also show that the proposed method can achieve competitive performance on a series of sequence modeling tasks.</p><p>The main contributions of this paper can be summarized as follows: 1) we study the sequence modeling problem incorporating dynamic skip connections, which can effectively tackle the long-term dependency problems; 2) we propose a novel reinforcement learning-based LSTM model to achieve the task, and the proposed model can learn to choose one optimal set of hidden and cell states from the past few states; and 3) several experimental results are given to demonstrate the effectiveness of the proposed method from different aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>In this work, we propose a novel LSTM network, which is a modification to the basic LSTM architectures. By using dynamic skip connections, the proposed model can choose an optimal set of hidden and cell states to compute recurrent transition functions. For the sake of brevity, we use State to represent both the hidden state and cell state. Because of the non-differentiability of discrete selection, we adopted reinforcement learning to achieve the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Overview</head><p>Taking language modeling as an example, given an input sequence x 1:T with length T , at each time step t, the model takes a word embedding x t as input, and aims to output a distribution over the next word, which is denoted by y t . However, in the example shown in <ref type="figure">Figure 2</ref>, in standard RNN settings, memorizing long-term dependency (depend ... ?on) while maintaining short-term memory (to ?some ?extent) is difficult ). Hence, we developed a skipping technique that learns to choose the most relevant State at time step t ? 3 to predict the word "on" to tackle the long-term dependency problem.</p><p>The architecture of the proposed model is shown in <ref type="figure">Figure 2</ref>. At time step t, the agent takes previous hidden state h t?1 as input, and then computes the skip softmax that determines a distribution over the skip steps between 1 and K. In our setting, the maximum size of skip K is chosen ahead of time. The agent thereby samples from this distribution to decide which State is transferred to a standard LSTM cell for recurrent transition computation. Then, the standard LSTM cell will encode the newly selected State and x t to the hidden state h t . At the end of each time step, each hidden state h t is further used for predicting the next word based on the same method as standard RNNs. Especially, such a model can be fully applied to any sequence modeling problem. In the following, we will detail the architecture and the training method of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Skip with REINFORCE</head><p>The proposed model consists of two components: (1) a policy gradient agent that repeatedly selects the optimal State from the historical State set, and (2) a standard LSTM cell (Hochreiter and Schmidhuber 1997) using a newly selected State to achieve a task. Our goal for training is to optimize the parameters of the policy gradient agent ? a , together with the parameters of standard LSTM and possibly other parameters including word embeddings denoted as ? l .</p><p>The core of the proposed model is a policy gradient agent. Sequential words x 1:T with length T correspond to the sequential inputs of one episode. At each time step t, the agent interacts with the environment s t to decide an action a t (transferring a certain State to a standard LSTM cell). Then, the standard LSTM cell uses the newly selected State to achieve the task. The model's performance based on the current selections will be treated as a reward to update the parameters of the agent.</p><p>Next, we will detail the four key points of the agent, including the environment representation s t , the action a t , the reward function, and the recurrent transition function. Environment representation. Our intuition in formulating an environment representation is that the agent should select an action based on both the historical and current information. We therefore incorporate the previous hidden state h t?1 and the current input x t to formulate the agent's environment representation as follows:</p><formula xml:id="formula_1">s t = h t?1 ? x t ,</formula><p>where ? refers to the concatenation operation. At each time step, the agent observes the environment s t to decide an action. Actions. After observing the environment s t , the agent should decide which State is optimal for the downstream LSTM cell. Formally, we construct a State set S K , which preserves the K recently obtained State, and the maximum size K is set ahead of time. The agent takes an action by sampling an optimal State in S K from a multinomial distribution ? K (k|s t ) as follows:</p><formula xml:id="formula_2">P = sof tmax(M LP (s t )) ? K (k|s t ) = P r(K = k|s t ) = K i=1 p [k=i] i ,<label>(1)</label></formula><p>where [k = i] evaluates to 1 if k = i, 0 otherwise. M LP represents a multilayer perceptron to transform s t to a vector with dimensionality K, and the sof tmax function is used to transform the vector to a probability distribution P . p i is the i-th element in P . Then, the State t?k is transferred to the LSTM cell for further computation. Reward function. Reward function is an indicator of the skip utility. A suitable reward function could guide the agent to select a series of optimal skip actions for training a better predictor. We capture this intuition by setting the reward to be the predicted log-likelihood of the ground truth, i.e., R = log P r(? T |h T ). Therefore, by interacting with the environment through the rewards, the agent is incentivized to select the optimal skips to promote the probability of the ground truth. Recurrent transition function. Based on the previously mentioned technique, we use a standard LSTM cell to encode the selected State t?k , where k ? {1, 2, ..., K}. In the text classification experiments, we found that adding the additional immediate previous state State t?1 usually led better results, although State t?1 is a particular case of State t?k . However, in our sequence labeling tasks, we found that just using State t?k is almost the optimal solution. Therefore, In our model, we use a hyperparameter ? to incorporate these two situations, as shown in <ref type="figure">Figure 3</ref>. For-</p><formula xml:id="formula_3">? ? ? h t 1 h t k c t k c t 1 x t h t c t Forget Gate</formula><p>Input Gate Output Gate <ref type="figure">Figure 3</ref>: Schematic of the recurrent transition function encoding both the selected hidden/cell states and the previous hidden/cell states. ? refers to the function ?a + (1 ? ?)b. ? and ? refer to the sigmoid and tanh functions, respectively.</p><p>mally, we give the LSTM function as follows:</p><formula xml:id="formula_4">h t?1 = ?h t?k + (1 ? ?)h t?1 c t?1 = ?c t?k + (1 ? ?)c t?1 ? ? ? g t i t f t o t ? ? ? = ? ? ? ? ? ? ? ? W g x , W g h W i x , W i h W f x , W f h W o x , W o h ? ? ? ? ? x t h t?1 + ? ? ? b g b i b f b o ? ? ? ? ? ? ? c t = ?(g t ) ?(i t ) + c t?1 ?(f t ) h t = ?(o t ) ? (c t ) ,<label>(2)</label></formula><p>where k ? {1, 2, ..., K}. ? is the tanh operator, and ? is the sigmoid operator. and ? represent the Hadamard product and the matrix product, respectively. We assume that y is one of {g, i, f, o}. The LSTM has N h hidden units and N x is the dimensionality of word representation x i . Then, W y x ? R N h ?Nx , W y h ? R N h ?N h , and b y ? R N h are the parameters of the standard LSTM cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Methods</head><p>Our goal for training is optimizing the parameters of the policy gradient agent ? a , together with the parameters of standard LSTM and possibly other parameters denoted as ? l . Optimizing ? l is straightforward and can be treated as a classification problem. Because the cross entropy loss J 1 (? l ) is differentiable, we can apply backpropagation to minimize it as follows:</p><formula xml:id="formula_5">J 1 (? l ) = ?[y i log? i + (1 ? y i ) log(1 ?? i )],<label>(3)</label></formula><p>where? i is the output of the model. The objective of training the agent is to maximize the expected reward under the skip policy distribution plus an entropy regularization <ref type="bibr" target="#b19">(Nachum et al. 2017)</ref>.</p><formula xml:id="formula_6">J 2 (? a ) = E ?(a 1:T ) [R] + H(?(a 1:T )),<label>(4)</label></formula><p>where ?(a 1:T ) = T t=1 P r(a t |s t ; ? a ), and R = log P r(? T |h T ). H(?(a 1:T )) = ?E ?(a 1:T ) [log ?(a 1:T )] is an entropy term, which can prevent premature entropy collapse and encourage the policy to explore more diverse space. We provide evidence that using the reinforcement learning with an entropy term can model sentence better than attention-based connections, as shown in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Level Vocab #Train #Dev #Test #class Named Entity <ref type="table" target="#tab_0">Recognition  CoNLL2003  word  30,290 204,567 51,578 46,666  17  Language Modeling  Penn Treebank  word  10K  929,590 73,761 82,431  10K  Sentiment Analysis  IMDB  sentence 112,540 21,250  3,750 25,000  2  Number Prediction  synthetic  word  10</ref> 100,000 10,000 10,000 10 Because of the non-differentiable nature of discrete skips, we adopt a policy gradient formulation referred to as REIN-FORCE method <ref type="bibr" target="#b25">(Williams 1992)</ref> to optimize ? a :</p><formula xml:id="formula_7">? ?a J 2 (? a ) =E ?(a 1:T ) [ T t=1 ? ?a log P r(a t |s t ; ? a ) * (R ? T t=1</formula><p>log P r(a t |s t ; ? a ) ? 1)].</p><p>(5) By applying the above algorithm, the loss J 2 (? a ) can be computed by standard backpropagation. Then, we can get the final objective by minimizing the following function:</p><formula xml:id="formula_8">J(? a , ? l ) = 1 M [ M m=1 (J 1 (? l ) ? J 2 (? a ))],<label>(6)</label></formula><p>where M denotes the quantity of the minibatch, and the objective function is fully differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>In this section, we present the experimental results of the proposed model for a variety of sequence modeling tasks, such as named entity recognition, language modeling, and sentiment analysis. In addition to the evaluation matrices for each task, in order to better understand the advantages of our model, we visualize the behavior of skip actions and make a comparison about how the gradients get changed between the LSTM and the proposed model. In addition, we also evaluate the model on the synthetic number prediction tasks, and verify the proficiency in long-term dependencies. The datasets used in the experiments are listed in <ref type="table" target="#tab_0">Table 1</ref>. General experiment settings. For the fair comparison, we use the same hyperparameters and optimizer with each baseline model of different tasks, which will be detailed in each experiment. As for the policy gradient agent, we use single layer MLPs with 50 hidden units. The maximum size of skip K and the hyperparameter ? are fixed during both training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named Entity Recognition</head><p>We now present the results for a sequence modeling task, Named Entity Recognition (NER). We performed experiments on the English data from CoNLL 2003 shared task <ref type="bibr" target="#b23">(Tjong Kim Sang and De Meulder 2003)</ref>. This data set contains four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categories. The corpora statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>. We used the Model F1 <ref type="bibr" target="#b14">Huang, Xu, and Yu (2015)</ref> 90.10 <ref type="bibr" target="#b9">Chiu and Nichols (2015)</ref> 90. <ref type="bibr">91?0.20 Lample et al. (2016)</ref> 90.94 <ref type="bibr" target="#b15">Ma and Hovy (2016)</ref> 91.21 <ref type="bibr" target="#b21">Strubell et al. (2017</ref><ref type="bibr">) ? 90.54 ? 0.18 Strubell et al. (2017</ref> 90 <ref type="formula">.</ref>  Following (Ma and Hovy 2016), we use 100-dimension GloVe word embeddings 1 and unpretrained character embeddings as initialization. We use a forward and backward new LSTM layer with ? = 1, K = 5 and a CRF layer to achieve this task. The reward is probability of true label sequence in CRF. We use early stopping based on performance on validation sets. <ref type="bibr" target="#b15">Ma and Hovy (2016)</ref> reported the best result appeared at 50 epochs and the model training required 8 hours. In our experiment, because of more exploration, the best result appeared at 65 epochs, the proposed method training required 9.98 hours. <ref type="table" target="#tab_2">Table 2</ref> shows the F1 scores of previous models and our model for NER on the test dataset from the CoNLL 2003 shared task. To our knowledge, the previous best F1 score (91.21) was achieved by using a combination of bidirectional LSTM, CNN, and CRF to obtain both word-and character-level representations automatically (Ma and Hovy 2016). By adding a bidirectional LSTM with a fixed skip, the model's performance would not improve. By contrast, the model using dynamic skipping technique improves the performance by average of 0.35%, and error reduction rate is more than 4%. Our model also outperforms the attention model, because the attention model employs a deterministic network to compute an expectation over the alignment variable, i.e., log f (H, E k [k]), not the expectation over the features, i.e., log E k (H, k). However, the gap between the above two expectations may be large <ref type="bibr" target="#b11">(Deng et al. 2018</ref>). Our <ref type="table">Table 3</ref>: Perplexity on validation and test sets for the Penn Treebank language modeling task. PPL refers to the average perplexity (lower is better) in ten runs. Size refers to the approximate number of parameters in the model. The models marked with ? have the same configuration which features a hidden size of 650 and a two layer LSTM. The models marked with ? are equivalent to the proposed model with hyperparameters ? = 0, and K = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM LSTM with dynamic skip</head><p>Average Gradient Norm  model is equivalent to the "Categorical Alignments" model in <ref type="bibr" target="#b11">(Deng et al. 2018)</ref>, which can effectively tackle the above deficiency. The detailed proof is given in Appendix.</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref>, we investigate the problem of vanishing gradient by comparing the long-term gradient values between standard LSTM and the proposed model. Following (Mujika, Meier, and Steger 2017), we compute the average gradient norms ?L T ?ht of the loss at time T with respect to hidden state h t at each time step t. We visualize the gradient norms in the first 20 time steps by normalizing the average gradient norms by the sum of average gradient norms for all time steps. Evidently, at the initial time steps, the proposed model still preserves effective gradient backpropagation. The average gradient norm in the proposed model is hundreds of times larger than that in the standard LSTM, indicating that the proposed model captures long-term dependencies, whereas the standard LSTM basically stores shortterm information <ref type="bibr" target="#b18">(Mujika, Meier, and Steger 2017)</ref>. The same effect was observed for cell states c t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling</head><p>We also evaluate the proposed model on the Penn Treebank language model corpus <ref type="bibr" target="#b17">(Marcus, Marcinkiewicz, and Santorini 1993)</ref>. The corpora statistics are shown in <ref type="table" target="#tab_0">Table 1</ref>. The model is trained to predict the next word (evaluated on perplexity) in a sequence.</p><p>To exclude the potential impact of advanced models, we restrict our comparison among the RNNs models. We replicate settings from Regularized LSTM (Zaremba, Sutskever, and Vinyals 2014) and <ref type="bibr">CharLM (Kim et al. 2016)</ref>. The above networks both have two layers of LSTM with 650 units, and the weights are initialized uniformly [-0.05, +0.05]. The gradients backpropagate for 35 time steps using stochastic gradient descent, with a learning rate initially set to 1.0. The norm of the gradients is constrained to be below five. Dropout with a probability of 0.5 on the LSTM input-tohidden layers and the hidden-to-output softmax layer is applied. The main difference between the two models is that the former model uses word embeddings as inputs, while the latter relies only on character-level inputs.</p><p>We replace the second layer of LSTM in the above baseline models with a fixed skip LSTM or our proposed model. The testing results are listed in <ref type="table">Table 3</ref>. We can see that the performance of the LSTM with a fixed skip may be even worse than that of the standard LSTM in some cases. This verifies that in some simple tasks such as the adding problem, copying memory problem, and sequential MNIST problem, the LSTM with a fixed skip length may be quite powerful , whereas in a complex language environment, the fixed skip length is constrained by its inability to take advantage of the dependencies with variable lengths. Hence, by adding a dynamic skip on the recurrent connections to the LSTM, our model can effectively tackle the above problem. Both the models with dynamic skip connections outperform baseline models, and the best model is able to improve the average test perplexity from 82.7 to 78.5, and 78.9 to 76.5, respectively. We also investigated how the hyperparameters ? and K affect the performance of proposed model as shown in <ref type="figure">Figure 5</ref>. ? is a weight to balance the utility between the newly selected State and the previous State. K represents the size of the skip space. From both figures we can see that, in general, the proposed model with larger ? and K values should obtain better PPL, while producing a larger variance because of the balance favoring the selected State and a larger searching space.</p><p>To verify the effectiveness of dynamic skip connections, we visualize the behavior of the agent in a situation where it predicts a preposition based on a long-term dependency. Some typical examples are shown in <ref type="figure">Figure 6</ref>. From the figure, we can see that in the standard LSTM setting, predicting the three prepositions in the figure is difficult just based on the previous State. By introducing the long-term and the most relevant State using dynamic skip connections, the proposed model is able to predict the next word with better performance in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis on IMDB</head><p>Two similar works to ours are Jump LSTM <ref type="bibr" target="#b26">(Yu, Lee, and Le 2017)</ref> and <ref type="bibr">Skip LSTM (Campos et al. 2017)</ref>, which are excellent modifications to the LSTM and achieve great performance on text classification. However, the above model cannot be applied to some sequence modeling tasks, such as language modeling and named entity recognition, because the jumping characteristics and the nature of skipping state updates make the models cannot produce LSTM outputs for skipped tokens and cannot update the hidden state, respectively. For better comparison with the above two models, we apply the proposed model to a sentiment analysis task. The IMDB dataset <ref type="bibr" target="#b16">(Maas et al. 2011</ref>) contains 25,000 training and 25,000 testing movie reviews annotated into positive or negative sentiments, where the average length of text is 240 words. We randomly set aside about 15% of the training data for validation. The proposed model, Jump LSTM, Skip LSTM, and LSTM all have one layer and 128 hidden units, and the batch size is 50. We use pretrained word2vec embeddings as initialization when available, or random vectors drawn from U(?0.25, +0.25). Dropout with a rate of 0.2 is applied between the last LSTM state and the classification layer. We either pad a short sequence or crop a long sequence to 400 words. We set ? and K to 0.5 and 3, respectively.</p><p>The result is reported in <ref type="table" target="#tab_4">Table 4</ref>. <ref type="bibr" target="#b8">(Chen et al. 2016</ref>) employed the idea of local semantic attention to achieve the task. <ref type="bibr" target="#b15">(Long et al. 2017</ref>) proposed a cognition based attention model, which needed additional eye-tracking data. From this result, we can see that our model also exhibits a strong performance on the text classification task. The proposed model is 1% better than the standard LSTM model. In addition, our model outperforms Skip LSTM and Jump LSTM models with accuracy at 90.1%. Therefore, the proposed model not only can achieve sequence modeling tasks such as language modeling and named entity recognition, but it also has a stronger ability for text classification tasks than Jump LSTM and Skip LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number Prediction with Skips</head><p>For further verification that the Dynamic LSTM is indeed able to learn how to skip if a clear skipping signal is given in the text, similar to <ref type="bibr" target="#b26">(Yu, Lee, and Le 2017)</ref>, we also inves-  <ref type="table">Table 5</ref>: Accuracies of different methods on number prediction dataset.</p><p>tigate a new task, where the network is given a sequence of L positive integers x 0:T ?1 , and the label is y = x x T ?1 . Here are two examples to illustrate the idea: input1: 8, 5, 1, 7, 4, 3. label: 7 input2: 2, 6, 4, 1, 3, 2. label: 4</p><p>As the examples show, x T ?1 is the skipping signal that guides the network to introduce the x T ?1 -th integer as the input to predict the label. The ideal network should learn to ignore the remaining useless numbers and learn how to skip from the training data.</p><p>According to above rule, we generate 100,000 training, 10,000 validation, and 10,000 test examples. Each example with a length T = 11 is formed by randomly sampling 11 numbers from the integer set {0, 1, ..., 9}, and we set x x11 as the label of each example. We use ten dimensional onehot vectors to represent the integers as the sequential inputs of LSTM or Dynamic LSTM, of which the last hidden state is used for prediction. We adopt one layer of LSTM with 200 hidden neurons. The Adam optimizer (Kingma and Ba 2014) trained with cross-entropy loss is used with 0.001 as the default learning rate. The testing result is reported in Table 5. It is interesting to see that even for a simple task, the LSTM model cannot achieve a high accuracy. However, the LSTM with dynamic skip is able to learn how to skip from the training examples to achieve a much better performance.</p><p>Taking this one step further, we increase the difficulty of the task by using two skips to find the label, i.e., the label is y = x x , x = x x T ?1 . To accord with the nature of skip, we force x &lt; x T ?1 , Here is an example: input: <ref type="bibr">8, 5, 1, 7, 1, 3, 3, 4, 7, 9, 4. label: 5</ref> Similar to the former method, we construct a dataset with the same size, 100,000 training, 10,000 validation, and 10,000 test examples. Each example with length T = 21 is also formed by randomly sampling 21 numbers from the integer set {0, 1, ..., 9}. We use the same model trained on the dataset. As the <ref type="table">Table 5</ref> shows, the accuracy of the LSTM with dynamic skip is vastly superior to that of LSTM. Therefore, the results indicate that the Dynamic LSTM is able to learn how to skip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Many attempts have been made to overcome the difficulties of RNNs in modeling long sequential data, such as gating mechanism <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber 1997;</ref><ref type="bibr" target="#b9">Chung et al. 2014</ref>), Multi-timescale mechanism (Chung, Ahn, and Bengio 2016). Recently, many works have explored the use of skip connections across multiple time steps <ref type="bibr" target="#b7">Chang et al. 2017)</ref>.  introduced the recurrent skip coefficient, which captures how quickly the information propagates over time, and found that raising the recurrent skip coefficient can mostly improve the performance of models on long-term dependency tasks. Note that previous research on skip connections all focused on a fixed skip length, which is set in advance. Different from these methods, this work proposed a reinforcement learning method to dynamically decide the skip length.</p><p>Other relevant works that introduce reinforcement learning to recurrent neural networks are Jump LSTM <ref type="bibr" target="#b26">(Yu, Lee, and Le 2017)</ref>, Skip RNN <ref type="bibr" target="#b20">(Seo et al. 2017)</ref>, and Skim RNN <ref type="bibr" target="#b20">(Seo et al. 2017)</ref>. The Jump LSTM aims to reduce the computational cost of RNNs by skipping irrelevant information if needed. Their model learns how many words should be omitted, which also utilizes the REINFORCE algorithm. Also, the Skip RNN and Skim RNN can learn to skip (part of) state updates with a fully differentiable method. The main differences between our method and the above methods are that Jump LSTM can not produce LSTM outputs for the skipped tokens and the Skip (Skim) RNN would not update (part of) hidden states. Thus three models would be difficult to be used for sequence labeling tasks. In contrast to them, our model updated the entire hidden state at each time step, and can be suitable for sequence labeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this work, we propose a reinforcement learning-based LSTM model that extends the existing LSTM model with dynamic skip connections. The proposed model can dynamically choose one optimal set of hidden and cell states from the past few states. By means of the dynamic skip connections, the model has a stronger ability to model sentences than those with fixed skip, and can tackle the dependency problem with variable lengths in the language. In addition, because of the shorter gradient backpropagation path, the model can alleviate the challenges of vanishing gradient. Experimental results on a series of sequence modeling tasks demonstrate that the proposed method can achieve much better performance than previous methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 9 d 9 A t o Y 1 + J M q 1 Y b L J y y i 5 J M U e J g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y b z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A t R G O k w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 d 9 A t o Y 1 + J M q 1 Y b L J y y i 5 J M U e J g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y b z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f Fe X c + l q 0 l p 5 g 5 h T 9 w P n 8 A t R G O k w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 d 9 A t o Y 1 + J M q 1 Y b L J y y i 5 J M U e J g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y b z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A t R G O k w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 d 9 A t o Y 1 + J M q 1 Y b L J y y i 5 J M U e J g = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y b z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A t R G O k w = = &lt; / l a t e x i t &gt;h 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 1 6 T 1 9 p 8 V i a s q s m v 8 0 z 1 r c d y i g Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y d z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A s 4 y O k g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 1 6 T 1 9 p 8 V i a s q s m v 8 0 z 1 r c d y i g Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y d z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A s 4 y O k g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 1 6 T 1 9 p 8 V i a s q s m v 8 0 z 1 r c d y i g Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y d z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A s 4 y O k g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X 1 6 T 1 9 p 8 V i a s q s m v 8 0 z 1 r c d y i g Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z n b R L N 5 u w u x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l R w b V z 3 2 y l t b G 5 t 7 5 R 3 K 3 v 7 B 4 d H 1 e O T t k 4 y x d B n i U h U N 6 Q a B Z f o G 2 4 E d l O F N A 4 F d s L J 3 d z v P K H S P J G P Z p p i E N O R 5 B F n 1 F j J H w 9 y d z a o 1 t y 6 u w B Z J 1 5 B a l C g N a h + 9 Y c J y 2 K U h g m q d c 9 z U x P k V B n O B M 4 q / U x j S t m E j r B n q a Q x 6 i B f H D s j F 1 Y Z k i h R t q Q h C / X 3 R E 5 j r a d x a D t j a s Z 6 1 Z u L / 3 m 9 z E Q 3 Q c 5 l m h m U b L k o y g Q x C Z l / T o Z c I T N i a g l l i t t b C R t T R Z m x + V R s C N 7 q y + u k f V X 3 3 L r 3 c F 1 r 3 h Z x l O E M z u E S P G h A E + 6 h B T 4 w 4 P A M r / D m S O f F e X c + l q 0 l p 5 g 5 h T 9 w P n 8 A s 4 y O k g = = &lt; / l a t e x i t &gt; h 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u u b W 2 9 O B 2 Y V v f x 9 J 0 K H d e 9 I H i 6 Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l rI x L 1 i N O U B z E d K R E J R t F K / n i Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A t p a O l A = = &lt; / l a t e x i t &gt; &lt; l at e x i t s h a 1 _ b a s e 6 4 = " u u b W 2 9 O B 2 Y V v f x 9 J 0 K H d e 9 I H i 6 Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l rI x L 1 i N O U B z E d K R E J R t F K / n i Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A t p a O l A = = &lt; / l a t e x i t &gt; &lt; l at e x i t s h a 1 _ b a s e 6 4 = " u u b W 2 9 O B 2 Y V v f x 9 J 0 K H d e 9 I H i 6 Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A t p a O l A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u u b W 2 9 O B 2 Y V v f x 9 J 0 K H d e 9 I H i 6 Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A t p a O l A = = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r E 5 V O + S m g H M H d j 5 q N U N F x I p 4 6 s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n S Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A 0 J 6 O p Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r E 5 V O + S m g H M H d j 5 q N U N F x I p 4 6 s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n S Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A 0 J 6 O p Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r E 5 V O + S m g H M H d j 5 q N U N F x I p 4 6 s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n S Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A 0 J 6 O p Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H r E 5 V O + S m g H M H d j 5 q N U N F x I p 4 6 s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K o N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n S Q N 2 a D a s 2 t u w u Q d e I V p A Y F W o P q V 3 + Y s C z m C p m k x v Q 8 N 8 U g p x o F k 3 x W 6 W e G p 5 R N 6 I j 3 L F U 0 5 i b I F 8 f O y I V V h i R K t C 2 F Z K H + n s h p b M w 0 D m 1 n T H F s V r 2 5 + J / X y z C 6 C X K h 0 g y 5 Y s t F U S Y J J m T + O R k K z R n K q S W U a W F v J W x M N W V o 8 6 n Y E L z V l 9 d J u 1 H 3 3 L r 3 c F V r 3 h Z x l O E M z u E S P L i G J t x D C 3 x g I O A Z X u H N U c 6 L 8 + 5 8 L F t L T j F z C n / g f P 4 A 0 J 6 O p Q = = &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v h f e x + q j f / r j d z z n P Z Y m / L B A Q Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 3 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / P G Y 6 k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v h f e x + q j f / r j d z z n P Z Y m / L B A Q Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 3 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / P G Y 6 k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v h f e x + q j f / r j d z z n P Z Y m / L B A Q Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 3 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / P G Y 6 k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v h f e x + q j f / r j d z z n P Z Y m / L B A Q Q = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 3 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / P G Y 6 k &lt; / l a t e x i t &gt; y 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p T + S B / J Y t 4 z O N w R g 7 W o u Q Y d 8 G 9 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 7 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / N l I 6 j &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p T + S B / J Y t 4 z O N w R g 7 W o u Q Y d 8 G 9 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 7 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / N l I 6 j &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p T + S B / J Y t 4 z O N w R g 7 W o u Q Y d 8 G 9 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d Da n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 7 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / N l I 6 j &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " p T + S B / J Y t 4 z O N w R g 7 W o u Q Y d 8 G 9 8 = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U 7 n R Q b 7 h N d w 6 y S r y S N K B E e 1 D / 6 g 8 T l s V c I Z P U m J 7 n p h g U V K N g k k 9 r / c z w l L I J H f G e p Y r G 3 A T F / N g p O b P K k E S J t q W Q z N X f E w W N j c n j 0 H b G F M d m 2 Z u J / 3 m 9 D K P r o B A q z Z A r t l g U Z Z J g Q m a f k 6 H Q n K H M L a F M C 3 s r Y W O q K U O b T 8 2 G 4 C 2 / v E o 6 F 0 3 P b X r 3 l 4 3 W T R l H F U 7 g F M 7 B g y t o w R 2 0 w Q c G A p 7 h F d 4 c 5 b w 4 7 8 7 H o r X i l D P H 8 A f O 5 w / N l I 6 j &lt; / l a t e x i t &gt; x 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 g M E V X I 5 n P y 8 / z V r q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>e x i t s h a 1 _ b a s e 6 4 = " J h + d 1 d q d R O S 9 X l k v z 4 N c l 4 / m I R I = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 1 F v B i 8 c K p i 2 0 o W y 2 m 3 b p Z h N 2 J 2 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n b X 1 j c 2 t 7 d J O e X d v / + C w c n T c M k m m G f d Z I h P d C a n h U i j u o 0 D J O 6 n m N A 4 l b 4 f j 2 5 n f f u T a i E Q 9 4 C T l Q U y H S k S C U b S S / 9 T P 6 9 N + p e r W 3 D n I K v E K U o U C z X 7 l q z d I W B Z z h U x S Y 7 q e m 2 K Q U 4 2 C S T 4 t 9 z L D U 8 r G d M i 7 l i o a c x P k 8 2 O n 5 N w q A x I l 2 p Z C M l d / T + Q 0 N m Y S h 7 Y z p j g y y 9 5 M / M / r Z h h d B 7 l Q a Y Z c s c W i K J M E E z L 7 n A y E 5 g z l x B L K t L C 3 E j a i m j K 0 + Z R t C N 7 y y 6 u k V a 9 5 b s 2 7 v 6 w 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C T 4 w E P A M r / D m K O f F e X c + F q 1 r T j F z A n / g f P 4 A z x a O p A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J h + d 1 d q d R O S 9 X l k v z 4 N c l 4 / m I R I = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 1 F v B i 8 c K p i 2 0 o W y 2 m 3 b p Z h N 2 J 2 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n b X 1 j c 2 t 7 d J O e X d v / + C w c n T c M k m m G f d Z I h P d C a n h U i j u o 0 D J O 6 n m N A 4 l b 4 f j 2 5 n f f u T a i E Q 9 4 C T l Q U y H S k S C U b S S / 9 T P 6 9 N + p e r W 3 D n I K v E K U o U C z X 7 l q z d I W B Z z h U x S Y 7 q e m 2 K Q U 4 2 C S T 4 t 9 z L D U 8 r G d M i 7 l i o a c x P k 8 2 O n 5 N w q A x I l 2 p Z C M l d / T + Q 0 N m Y S h 7 Y z p j g y y 9 5 M / M / r Z h h d B 7 l Q a Y Z c s c W i K J M E E z L 7 n A y E 5 g z l x B L K t L C 3 E j a i m j K 0 + Z R t C N 7 y y 6 u k V a 9 5 b s 2 7 v 6 w 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C T 4 w E P A M r / D m K O f F e X c + F q 1 r T j F z A n / g f P 4 A z x a O p A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J h + d 1 d q d R O S 9 X l k v z 4 N c l 4 / m I R I = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 1 F v B i 8 c K p i 2 0 o W y 2 m 3 b p Z h N 2 J 2 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n b X 1 j c 2 t 7 d J O e X d v / + C w c n T c M k m m G f d Z I h P d C a n h U i j u o 0 D J O 6 n m N A 4 l b 4 f j 2 5 n f f u T a i E Q 9 4 C T l Q U y H S k S C U b S S / 9 T P 6 9 N + p e r W 3 D n I K v E K U o U C z X 7 l q z d I W B Z z h U x S Y 7 q e m 2 K Q U 4 2 C S T 4 t 9 z L D U 8 r G d M i 7 l i o a c x P k 8 2 O n 5 N w q A x I l 2 p Z C M l d / T + Q 0 N m Y S h 7 Y z p j g y y 9 5 M / M / r Z h h d B 7 l Q a Y Z c s c W i K J M E E z L 7 n A y E 5 g z l x B L K t L C 3 E j a i m j K 0 + Z R t C N 7 y y 6 u k V a 9 5 b s 2 7 v 6 w 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C T 4 w E P A M r / D m K O f F e X c + F q 1 r T j F z A n / g f P 4 A z x a O p A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J h + d 1 d q d R O S 9 X l k v z 4 N c l 4 / m I R I = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 1 F v B i 8 c K p i 2 0 o W y 2 m 3 b p Z h N 2 J 2 I J / Q 1 e P C j i 1 R / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n b X 1 j c 2 t 7 d J O e X d v / + C w c n T c M k m m G f d Z I h P d C a n h U i j u o 0 D J O 6 n m N A 4 l b 4 f j 2 5 n f f u T a i E Q 9 4 C T l Q U y H S k S C U b S S / 9 T P 6 9 N + p e r W 3 D n I K v E K U o U C z X 7 l q z d I W B Z z h U x S Y 7 q e m 2 K Q U 4 2 C S T 4 t 9 z L D U 8 r G d M i 7 l i o a c x P k 8 2 O n 5 N w q A x I l 2 p Z C M l d / T + Q 0 N m Y S h 7 Y z p j g y y 9 5 M / M / r Z h h d B 7 l Q a Y Z c s c W i K J M E E z L 7 n A y E 5 g z l x B L K t L C 3 E j a i m j K 0 + Z R t C N 7 y y 6 u k V a 9 5 b s 2 7 v 6 w 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C T 4 w E P A M r / D m K O f F e X c + F q 1 r T j F z A n / g f P 4 A z x a O p A = = &lt; / l a t e x i t &gt; x t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E K + 1 W j I e M U M n W A E P G D 4 Z 6 L f W m v s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y n Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o Z Z J M M + 6 z R C a 6 E 1 L D p V D c R 4 G S d 1 L N a R x K 3 g 7 H t z O / / c i 1 E Y l 6 w E n K g 5 g O l Y g E o 2 g l / 6 m f 4 7 R f r b l 1 d w 6 y S r y C 1 K B A s 1 / 9 6 g 0 S l s V c I Z P U m K 7 n p h j k V K N g k k 8 r v c z w l L I x H f K u p Y r G 3 A T 5 / N g p O b P K g E S J t q W Q z N X f E z m N j Z n E o e 2 M K Y 7 M s j c T / / O 6 G U b X Q S 5 U m i F X b L E o y i T B h M w + J w O h O U M 5 s Y Q y L e y t h I 2 o p g x t P h U b g r f 8 8 i p p X d Q 9 t + 7 d X 9 Y a N 0 U c Z T i B U z g H D 6 6 g A X f Q B B 8 Y C H i G V 3 h z l P P i v D s f i 9 a S U 8 w c w x 8 4 n z 8 z b 4 7 m &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E K + 1 W j I e M U M n W A E P G D 4 Z 6 L f W m v s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y n Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o Z Z J M M + 6 z R C a 6 E 1 L D p V D c R 4 G S d 1 L N a R x K 3 g 7 H t z O / / c i 1 E Y l 6 w E n K g 5 g O l Y g E o 2 g l / 6 m f 4 7 R f r b l 1 d w 6 y S r y C 1 K B A s 1 / 9 6 g 0 S l s V c I Z P U m K 7 n p h j k V K N g k k 8 r v c z w l L I x H f K u p Y r G 3 A T 5 / N g p O b P K g E S J t q W Q z N X f E z m N j Z n E o e 2 M K Y 7 M s j c T / / O 6 G U b X Q S 5 U m i F X b L E o y i T B h M w + J w O h O U M 5 s Y Q y L e y t h I 2 o p g x t P h U b g r f 8 8 i p p X d Q 9 t + 7 d X 9 Y a N 0 U c Z T i B U z g H D 6 6 g A X f Q B B 8 Y C H i G V 3 h z l P P i v D s f i 9 a S U 8 w c w x 8 4 n z 8 z b 4 7 m &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "E K + 1 W j I e M U M n W A E P G D 4 Z 6 L f W m v s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y n Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o Z Z J M M + 6 z R C a 6 E 1 L D p V D c R 4 G S d 1 L N a R x K 3 g 7 H t z O / / c i 1 E Y l 6 w E n K g 5 g O l Y g E o 2 g l / 6 m f 4 7 R f r b l 1 d w 6 y S r y C 1 K B A s 1 / 9 6 g 0 S l s V c I Z P U m K 7 n p h j k V K N g k k 8 r v c z w l L I x H f K u p Y r G 3 A T 5 / N g p O b P K g E S J t q W Q z N X f E z m N j Z n E o e 2 M K Y 7 M s j c T / / O 6 G U b X Q S 5 U m i F X b L E o y i T B h M w + J w O h O U M 5 s Y Q y L e y t h I 2 o p g x t P h U b g r f 8 8 i p p X d Q 9 t + 7 d X 9 Y a N 0 U c Z T i B U z g H D 6 6 g A X f Q B B 8 Y C H i G V 3 h z l P P i v D s f i 9 a S U 8 w c w x 8 4 n z 8 z b 4 7 m &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E K + 1 W j I e M U M n W A E P G D 4 Z 6 L f W m v s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m L b Q h r L Z b t q l m 0 3 Y n Y g l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S q F Q d f 9 d k p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f C o Z Z J M M + 6 z R C a 6 E 1 L D p V D c R 4 G S d 1 L N a R x K 3 g 7 H t z O / / c i 1 E Y l 6 w E n K g 5 g O l Y g E o 2 g l / 6 m f 4 7 R f r b l 1 d w 6 y S r y C 1 K B A s 1 / 9 6 g 0 S l s V c I Z P U m K 7 n p h j k V K N g k k 8 r v c z w l L I x H f K u p Y r G 3 A T 5 / N g p O b P K g E S J t q W Q z N X f E z m N j Z n E o e 2 M K Y 7 M s j c T / / O 6 G U b X Q S 5 U m i F X b L E o y i T B h M w + J w O h O U M 5 s Y Q y L e y t h I 2 o p g x t P h U b g r f 8 8 i p p X d Q 9 t + 7 d X 9 Y a N 0 U c Z T i B U z g H D 6 6 g A X f Q B B 8 Y C H i G V 3 h z l P P i v Ds f i 9 a S U 8 w c w x 8 4 n z 8 z b 4 7 m &lt; / l a t e x i t &gt; y t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q G S o 7 T I 0 l 9 E X 8 X k / w T O j x F w J U h s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U O B 3 U G 2 7 T n Y O s E q 8 k D S j R H t S / + s O E Z T F X y C Q 1 p u e 5 K Q Y F 1 S i Y 5 N N a P z M 8 p W x C R 7 x n q a I x N 0 E x P 3 Z K z q w y J F G i b S k k c / X 3 R E F j Y / I 4 t J 0 x x b F Z 9 m b i f 1 4 v w + g 6 K I R K M + S K L R Z F m S S Y k N n n Z C g 0 Z y h z S y j T w t 5 K 2 J h q y t D m U 7 M h e M s v r 5 L O R d N z m 9 7 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 8 I G B g G d 4 h T d H O S / O u / O x a K 0 4 5 c w x / I H z + Q M 0 9 4 7 n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q G S o 7 T I 0 l 9 E X 8 X k / w T O j x F w J U h s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U O B 3 U G 2 7 T n Y O s E q 8 k D S j R H t S / + s O E Z T F X y C Q 1 p u e 5 K Q Y F 1 S i Y 5 N N a P z M 8 p W x C R 7 x n q a I x N 0 E x P 3 Z K z q w y J F G i b S k k c / X 3 R E F j Y / I 4 t J 0 x x b F Z 9 m b i f 1 4 v w + g 6 K I R K M + S K L R Z F m S S Y k N n n Z C g 0 Z y h z S y j T w t 5 K 2 J h q y t D m U 7 M h e M s v r 5 L O R d N z m 9 7 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 8 I G B g G d 4 h T d H O S / O u / O x a K 0 4 5 c w x / I H z + Q M 0 9 4 7 n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q G S o 7 T I 0 l 9 E X 8 X k / w T O j x F w J U h s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U O B 3 U G 2 7 T n Y O s E q 8 k D S j R H t S / + s O E Z T F X y C Q 1 p u e 5 K Q Y F 1 S i Y 5 N N a P z M 8 p W x C R 7 x n q a I x N 0 E x P 3 Z K z q w y J F G i b S k k c / X 3 R E F j Y / I 4 t J 0 x x b F Z 9 m b i f 1 4 v w + g 6 K I R K M + S K L R Z F m S S Y k N n n Z C g 0 Z y h z S y j T w t 5 K 2 J h q y t D m U 7 M h e M s v r 5 L O R d N z m 9 7 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 8 I G B g G d 4 h T d H O S / O u / O x a K 0 4 5 c w x / I H z + Q M 0 9 4 7 n &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q G S o 7 T I 0 l 9 E X 8 X k / w T O j x F w J U h s = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 8 F L x 4 r m F p o Q 9 l s N + 3 S z S b s T o Q Q + h u 8 e F D E q z / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 M J X C o O t + O 5 W 1 9 Y 3 N r e p 2 b W d 3 b / + g f n j U M U m m G f d Z I h P d D a n h U i j u o 0 D J u 6 n m N A 4 l f w w n t z P / 8 Y l r I x L 1 g H n K g 5 i O l I g E o 2 g l P x 8 U O B 3 U G 2 7 T n Y O s E q 8 k D S j R H t S / + s O E Z T F X y C Q 1 p u e 5 K Q Y F 1 S i Y 5 N N a P z M 8 p W x C R 7 x n q a I x N 0 E x P 3 Z K z q w y J F G i b S k k c / X 3 R E F j Y / I 4 t J 0 x x b F Z 9 m b i f 1 4 v w + g 6 K I R K M + S K L R Z F m S S Y k N n n Z C g 0 Z y h z S y j T w t 5 K 2 J h q y t D m U 7 M h e M s v r 5 L O R d N z m 9 7 9 Z a N 1 U 8 Z R h R M 4 h X P w 4 A p a c A d t 8 I G B g G d 4 h T d H O S / O u / O x a K 0 4 5 c w x / I H z + Q M 0 9 4 7 n &lt; / l a t e x i t &gt; h t k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y U V v j j e J S z S u G b 7 n E C U w 9 I y v i W o = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B i y U R Q b 0 V v H i s Y D + g D W W z 3 b R L N p u w O x F K 6 I / w 4 k E R r / 4 e b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l Q K g 6 7 7 7 Z T W 1 j c 2 t 8 r b l Z 3 d v f 2 D 6 u F R 2 y S Z Z r z F E p n o b k A N l 0 L x F g q U v J t q T u N A 8 k 4 Q 3 c 3 8 z h P X R i T q E S c p 9 2 M 6 U i I U j K K V O u N B j h f R d F C t u X V 3 D r J K v I L U o E B z U P 3 q D x O W x V w h k 9 S Y n u e m 6 O d U o 2 C S T y v 9 z P C U s o i O e M 9 S R W N u / H x + 7 p S c W W V I w k T b U k j m 6 u + J n M b G T O L A d s Y U x 2 b Z m 4 n / e b 0 M w x s / F y r N k C u 2 W B R m k m B C Z r + T o d C c o Z x Y Q p k W 9 l b C x l R T h j a h i g 3 B W 3 5 5 l b Q v 6 5 5 b 9 x 6 u a o 3 b I o 4 y n M A p n I M H 1 9 C A e 2 h C C x h E 8 A y v 8 O a k z o v z 7 n w s W k t O M X M M f + B 8 / g B P a Y + C &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " R T Z U / I B B N R d e C u l L K N o p I k C t P B Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q 4 2 x Q r b l 1 d w G y T r y C 1 K B A a 1 D 9 6 g 8 T l s V c I Z P U m J 7 n p h j k V K N g k s 8 q / c z w l L I J H f G e p Y r G 3 A T 5 4 t g Z u b D K k E S J t q W Q L N T f E z m N j Z n G o e 2 M K Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i F X b L k o y i T B h M w / J 0 O h O U M 5 t Y Q y L e y t h I 2 p p g x t P h U b g r f 6 8 j p p X 9 U 9 t + 4 9 X N e a t 0 U c Z T i D c 7 g E D x r Q h H t o g Q 8 M B D z D K 7 w 5 y n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x 8 a 7 4 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R T Z U / I B B N R d e C u l L K N o p I k C t P B Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q 4 2 x Q r b l 1 d w G y T r y C 1 K B A a 1 D 9 6 g 8 T l s V c I Z P U m J 7 n p h j k V K N g k s 8 q / c z w l L I J H f G e p Y r G 3 A T 5 4 t g Z u b D K k E S J t q W Q L N T f E z m N j Z n G o e 2 M K Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i F X b L k o y i T B h M w / J 0 O h O U M 5 t Y Q y L e y t h I 2 p p g x t P h U b g r f 6 8 j p p X 9 U 9 t + 4 9 X N e a t 0 U c Z T i D c 7 g E D x r Q h H t o g Q 8 M B D z D K 7 w 5 y n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x 8 a 7 4 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R T Z U / I B B N R d e C u l L K N o p I k C t P B Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q 4 2 x Q r b l 1 d w G y T r y C 1 K B A a 1 D 9 6 g 8 T l s V c I Z P U m J 7 n p h j k V K N g k s 8 q / c z w l L I J H f G e p Y r G 3 A T 5 4 t g Z u b D K k E S J t q W Q L N T f E z m N j Z n G o e 2 M K Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i F X b L k o y i T B h M w / J 0 O h O U M 5 t Y Q y L e y t h I 2 p p g x t P h U b g r f 6 8 j p p X 9 U 9 t + 4 9 X N e a t 0 U c Z T i D c 7 g E D x r Q h H t o g Q 8 M B D z D K 7 w 5 y n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x 8 a 7 4 7 W &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R T Z U / I B B N R d e C u l L K N o p I k C t P B Y = " &gt; A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q N 4 K X j x W M G 2 h D W W z 3 b R L N 5 u w O x F K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w l Q K g 6 7 7 7 Z Q 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J 2 y S Z Z t x n i U x 0 N 6 S G S 6 G 4 j w I l 7 6 a a 0 z i U v B N O 7 u Z + 5 4 l r I x L 1 i N O U B z E d K R E J R t F K / n i Q 4 2 x Q r b l 1 d w G y T r y C 1 K B A a 1 D 9 6 g 8 T l s V c I Z P U m J 7 n p h j k V K N g k s 8 q / c z w l L I J H f G e p Y r G 3 A T 5 4 t g Z u b D K k E S J t q W Q L N T f E z m N j Z n G o e 2 M K Y 7 N q j c X / / N 6 G U Y 3 Q S 5 U m i F X b L k o y i T B h M w / J 0 O h O U M 5 t Y Q y L e y t h I 2 p p g x t P h U b g r f 6 8 j p p X 9 U 9 t + 4 9 X N e a t 0 U c Z T i D c 7 g E D x r Q h H t o g Q 8 M B D z D K 7 w 5 y n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x 8 a 7 4 7 W &lt; / l a t e x i t &gt; Hidden State State Set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Normalized long-term gradient values ?L T ?ht tested on CoNLL 2003 dataset. At the initial time steps, the proposed model still preserves effective gradients, which is hundreds of times larger than those in the standard LSTM, indicating that the proposed model have stronger ability to capture long-term dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Test set perplexities (lower is better) on Penn Treebank language model corpus with standard deviation for K from 2 to 10 with ? = 0.5 (left), and ? from 0.1 to 1.0 with K = 5 (right).... seek to prevent executive branch officials from ... from ... who devote most of their time to practicing ... ... taking flights from san francisco late yesterday to ... LSTM 0.03, LSTM with dynamic skip 0.21 Prediction Probability: LSTM 0.04, LSTM with dynamic skip 0.31 Prediction Probability: LSTM 0.33, LSTM with dynamic skip 0.91 Examples of the proposed model applied to language modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the CoNLL2003, Penn Treebank, IMDB, and synthetic datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: F1-measure of different methods applied to the</cell></row><row><cell>CoNLL 2003 dataset. The model that does not use character</cell></row><row><cell>embeddings is marked with  ?. "LSTM with attention" refers to the LSTM model using attention mechanism to connect</cell></row><row><cell>two words.</cell></row><row><cell>BIOES tagging scheme instead of standard BIO2, as previ-</cell></row><row><cell>ous studies have reported meaningful improvement with this</cell></row><row><cell>scheme (Lample et al. 2016; Ma and Hovy 2016).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on the IMDB test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>LSTM with dynamic skip, ?=1, K=10 79.6 80.5 LSTM with dynamic skip, ?=0.5, K=10 90.4 90.5 LSTM with dynamic skip, ?=1, K=10 77.6 77.7 LSTM with dynamic skip, ?=0.5, K=10 87.7 88.5</figDesc><table><row><cell>sequence length 11</cell><cell></cell></row><row><cell>Model</cell><cell>Dev. Test</cell></row><row><cell>LSTM</cell><cell>69.6 70.4</cell></row><row><cell>LSTM with attention</cell><cell>71.3 72.5</cell></row><row><cell>sequence length 21</cell><cell></cell></row><row><cell>Model</cell><cell>Dev. Test</cell></row><row><cell>LSTM</cell><cell>26.2 26.4</cell></row><row><cell>LSTM with attention</cell><cell>26.7 26.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&amp;D Program (No. 2017YFB1002104, 2018YFC0831105), National Natural Science Foundation of China (No. 61532011,61751201, 61473092, and 61472088), and STCSM (No.16JC1420401, 17JC1420200).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Notations</head><p>Let H ? R d?T be an matrix formed by a set of members {h 1 , h 2 , . . . , h T }, where h t ? R d is vector-valued and T is the cardinality of the set. Let s t be an arbitrary "query" for attention computation or the state of reinforcement learning agent. In the paper, the s t is defined by incorporating the previous hidden state h t?1 and the current input x t as follows:</p><p>Then, we use s t to operate on H to predict the label y ? Y. The process can be formally defined as follows:</p><p>where g is a function to produce an alignment distribution D. f is another function mapping H over the distribution z to the label y. Our goal is to optimize the parameters ? by maximizing the log marginal likelihood:</p><p>Directly maximizing this log marginal likelihood is often difficult due to the expectation <ref type="bibr" target="#b11">(Deng et al. 2018)</ref>. To tackle this challenge, previous works focus on using the attention model as an alternative solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deficiency of Attention model</head><p>Attention networks use a deterministic network to compute an expectation over the distribution variable z. We can write this model as follows:</p><p>The attention networks compute the expectation before f without computing an integral over f , which enhance the efficiency of computation. Although many works use attention as an approximation of alignment <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REINFORCE with Entropy Regularization</head><p>In the previous section, we have show that the logprobability of label y can be mediated through a latent alignment variable z:</p><p>Through variational inference, the above function can be rewritten as:</p><p>The second term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is nonnegative, the first term is called the (variational) lower bound L(?, ?; y) and can be written as:</p><p>which can also be written as:</p><p>where the first term is the prediction loss, or expected loglikelihood of the label. The expectation is taken with respect to the encoders distribution over the representations. This term encourages the decoder to learn to predict the true label. The second term is a regularizer. This is the KL divergence between the encoders distribution q ? (z|H, s t ) and p ? (z).</p><p>In order to tighten the gap between the lower bound and the likelihood of the label, we should maximize the variational lower bound:</p><p>In our LSTM with dynamic skip setting, let the random variable z be the trajectory variable ? . Then the function 14 can be rewritten as follows:</p><p>where ? a is the parameters of agent, and ? l is the parameters of LSTM units. For simplicity, we omit the condition of q ?a (? |H, s t ) to be q ?a (? ), and treat the log-likelihood of ground truth log p ? l (y|? ) as the rewards R ? l (? ). We make the prior p ? (z) be uniform distribution. Then the gradients of function 15 can be computed as follows:</p><p>which has the same form as the loss function in the paper. Hence, using REINFORCE with entropy regularization can optimize the model towards the direction of tightening the gap between the lower bound and the likelihood of the label, while the attention model may not tackle this gap. This is why the performance of REINFORCE with an entropy term is better than attention. Meanwhile, optimizing the parameters of standard LSTM ? l is straightforward and can be treated as a classification problem as shown in our paper. Therefore, our model can be trained end-to-end with standard back-propagation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Model Dev.(PPL) Test(PPL) Size RNN (Mikolov and Zweig 2012) -124.7 6 m RNN-LDA (Mikolov and Zweig</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Zoneout + Variational LSTM (medium) (Merity et al. 2016) ? 84.4 80.6 20 m Variational LSTM (medium) (Gal and Ghahramani</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Variational LSTM (medium, MC) (Gal and Ghahramani 2016) ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lstm (</forename><surname>Regularized</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutskever</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">? ?</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Charlm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Charlm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>fixed skip = 5</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Campos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08308</idno>
		<idno>arXiv:1412.3555</idno>
	</analytic>
	<monogr>
		<title level="m">Named entity recognition with bidirectional lstm-cnns</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03756</idno>
		<title level="m">Latent alignment and variational attention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio ; El Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu ;</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<idno>arXiv:1603.01360</idno>
	</analytic>
	<monogr>
		<title level="m">Neural architectures for named entity recognition</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>A cognition based attention model for sentiment analysis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcinkiewicz</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
	</analytic>
	<monogr>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>SLT</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast-slow recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meier</forename><surname>Mujika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steger ; Mujika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5917" to="5926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<idno>arXiv:1312.6026</idno>
	</analytic>
	<monogr>
		<title level="m">How to construct deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICML-14</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building end-toend dialogue systems using generative hierarchical neural network models</title>
		<idno type="arXiv">arXiv:1711.02085</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Neural speed reading via skim-rnn</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strubell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2670" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><forename type="middle">;</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>[tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Tjong Kim</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2003</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Show, attend and tell: Neural image caption generation with visual attention</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le ;</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
	</analytic>
	<monogr>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1822" to="1830" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
