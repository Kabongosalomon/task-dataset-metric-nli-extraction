<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Conversational Recommendation Systems&apos; Quality with Context-Aware Item Meta Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Conversational Recommendation Systems&apos; Quality with Context-Aware Item Meta Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational recommendation systems (CRS) engage with users by inferring user preferences from dialog history, providing accurate recommendations, and generating appropriate responses. Previous CRSs use knowledge graph (KG) based recommendation modules and integrate KG with language models for response generation. Although KG-based approaches prove effective, two issues remain to be solved. First, KG-based approaches ignore the information in the conversational context but only rely on entity relations and bag of words to recommend items. Second, it requires substantial engineering efforts to maintain KGs that model domain-specific relations, thus leading to less flexibility. In this paper, we propose a simple yet effective architecture comprising a pre-trained language model (PLM) and an item metadata encoder. The encoder learns to map item metadata to embeddings that can reflect the semantic information in the dialog context. The PLM then consumes the semantic-aligned item embeddings together with dialog context to generate high-quality recommendations and responses. Instead of modeling entity relations with KGs, our model reduces engineering complexity by directly converting each item to an embedding. Experimental results on the benchmark dataset REDIAL show that our model obtains state-of-the-art results on both recommendation and response generation tasks 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An automated conversational recommendation system (CRS) <ref type="bibr" target="#b30">(Li et al., 2019;</ref> is intended to interact with users and provide accurate product recommendations (e.g., movies, songs, and consumables). It has been a focal point of research lately due to its potential applications in the e-commerce industry. Traditional recommendation systems collect user preferences from implicit feedback such as click-through-rate <ref type="bibr" target="#b32">(Zhou et al., 2018)</ref> or purchase history and apply collaborative filtering <ref type="bibr" target="#b24">(Su and Khoshgoftaar, 2009;</ref><ref type="bibr" target="#b21">Shi et al., 2014)</ref> or deep learning models <ref type="bibr" target="#b1">(Covington et al., 2016;</ref><ref type="bibr" target="#b5">He et al., 2017)</ref> to construct latent spaces for user preferences. Unlike traditional recommendation systems, CRSs directly extract user preferences from live dialog history instead of implicit interactive records, thus can provide better recommendations that precisely address the users' needs.</p><p>Although some progress has been made in this area, there is still room for improvement. First, previous CRSs <ref type="bibr" target="#b0">(Chen et al., 2019;</ref> track entities mentioned in the dialog context, and then search related items in knowledge graphs to recommend to users. However, these systems require a named-entity recognition (NER) module to extract mentioned entities from the dialog context, thus we need to collect additional domain-specific data to train the NER module. In practice, such NER modules have deficient performance, leading to a bad accuracy of CRS. Additionally, entity-dependent recommendation modules lack of contextual information. For example, a user may say: "I watched Ant-Man but did not enjoy it.". In this case, Ant-Man is extracted as a mentioned entity and the system will recommend a similar movie to Ant-Man, which is not an appropriate recommendation. Moreover, in some domains, users are more inclined to express their needs with pure language. Since corpora in such domains barely contain named entities, existing entity-based CRSs perform poorly on these domains. Ideally, a CRS should condition its recommendation on the integrated contextual information of the entire dialog context and mentioned entities. Second, existing CRSs built upon graph neural networks <ref type="bibr" target="#b20">Schlichtkrull et al., 2017)</ref> cannot quickly scale up or respond to rapid changes of the underlining entities. In e-commerce companies, items for recommendation change daily or even hourly due to constant updates of merchants and products. Existing approaches with graph neural networks require either re-training the entire system when the structure of knowledge graph changes <ref type="bibr" target="#b2">(Dettmers et al., 2018)</ref> or adding complex architectures on top to be adaptive . A more flexible architecture can help the system react to rapid changes and adapt itself to new items.</p><p>Driven by the motivations above, we present a Metadata Enhanced learning approach via Semantic Extraction from dialog context i.e. MESE. The major components of MESE contain a pre-trained language model (PLM) and an item encoder architecture. The item encoder takes item metadata as input and is jointly trained with the PLM and dialog context. After training, the item encoder can map item metadata in a systematic way such that contextual information from the dialog is reflected in the constructed embeddings. Item embeddings are then consumed together with dialog context by the self-attention mechanism of the PLM to perform recommendation and response generation. In order for the model to scale up with the size of the item database, we pose recommendation as a two-phase process with candidate selection and candidate ranking following <ref type="bibr" target="#b1">(Covington et al., 2016)</ref>.</p><p>The key contributions of this paper are summarized as follows: This paper presents MESE, a novel CRS framework that considers both item metadata and dialog context for recommendations. Our model employs a simple yet effective item metadata encoder that learns to construct item embeddings during training, thus can adapt to database changes quickly and be independent on task-specific architectures. Extensive experiments on standard dataset REDIAL demonstrate that MESE outperforms previous state-of-the-art methods on both response generation and recommendation with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Current CRS paradigm contains two major modules: a recommendation module that suggest items based on conversational context and a response generation module that generate responses based on dialog history and the recommended items. How to integrate these two modules to perform well on both tasks has been a major challenge. <ref type="bibr" target="#b0">(Chen et al., 2019)</ref> leverages external knowledge and employees graph neural networks as the backbone to model entities and entity relations in the knowledge graph (KG) to enhance the performance. In , a word-level KG (ConceptNet <ref type="bibr" target="#b23">(Speer et al., 2018)</ref>) is introduced to the system with semantic fusion  to enhance the semantic representations of words and items. Since item information and dialog context are processed separately in the above approaches, they suffer from loss of integrated sentence-level information. We propose to condition recommendation on an integrated contextual information of both dialog context and mentioned entities. More recent works try to adopt pre-trained language models (PLM) <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b18">Radford et al., 2018;</ref><ref type="bibr" target="#b31">Zhang et al., 2020)</ref> and template based methods to facilitate response generation. <ref type="bibr" target="#b9">(Liang et al., 2021)</ref> generates a response template that contains a mixture of contextual words and slot locations to better incorporate recommended items.  expands the vocabulary list of the PLM to include items to unify the process of item recommendation with response generation. We propose to enhance our PLM with an item metadata encoder to extract context-aware representations by jointly training on both recommendation and response generation tasks. We also generate response templates with slot locations to better incorporate recommended items into responses.</p><p>Our work is also inspired by studies from other areas. Recent works have shown that crossmodality training across vision and language tasks can lead to outstanding results in building multimodal representations <ref type="bibr" target="#b27">(Tan and Bansal, 2019;</ref><ref type="bibr" target="#b14">Lu et al., 2019)</ref>. In <ref type="bibr" target="#b27">(Tan and Bansal, 2019)</ref>, a largescale transformer based model is adapted with cross-modal encoders to connect visual and linguistic semantics and pre-trained on vision-language pairs to learn intra-modality and cross-modality relationships. Prompt tuning <ref type="bibr" target="#b9">(Li and Liang, 2021;</ref><ref type="bibr" target="#b3">Gao et al., 2021)</ref> methods prove that PLMs are capable of integrating different sources of information into the same embedding space and perform well on downstream tasks. In terms of using PLM as a recommendation system, <ref type="bibr" target="#b26">(Sun et al., 2019)</ref> trains a bidirectional self-attention model to predict masked items and their model outperforms previ-ous sequential models on various recommendation tasks. Inspired by the above studies, we propose to use an encoder module to map item meta information to an embedding space. By jointly training on dialog context and encoded item representations, the system can align these two streams of information by fusing the semantic spaces. The joint embedding representations are then processed by the self-attention mechanism of the PLM to perform recommendation and response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we present our framework MESE that integrates item metadata with dialog context. We first introduce how to encode item metadata and how to process dialog context with the encoded metadata. We then illustrate how the recommendation module and the response generation module are built and how the encoded metadata is incorporated into both modules. Finally, we describe the training objectives and the testing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoding Item Metadata</head><p>Instead of modeling item representations based on their relations with other items in the knowledge graphs <ref type="bibr" target="#b0">(Chen et al., 2019;</ref>, we propose to use an item encoder to directly map the metadata of each item to an embedding. In the movie recommendation setting, description on title, genre, actors, directors, and plot are collected as metadata and concatenated with a "[SEP]" token for each movie. This concatenated information is the input to the item encoder which produces a vector representation for each item. The item encoder consists of a DistilBERT <ref type="bibr" target="#b19">(Sanh et al., 2020)</ref> model that maps the input sequence to a sequence of vector embeddings, a pooling layer that condenses the sequence embeddings to a single vector embedding, and a feed-forward layer to produce the output embedding. A visualization of this module is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We construct the embeddings for all items in the database.</p><p>Next, we discuss how to incorporate items into dialog context with the encoded embeddings and the GPT-2 model <ref type="bibr" target="#b18">(Radford et al., 2018)</ref>. Previous studies have shown that KG-based frameworks cannot always integrate recommended items into generated replies . To solve this issue, we introduce a special placeholder token "[PH]" to the vocabulary list of GPT-2. Every occurrence of item name in the corpus is replaced with this "[PH]" token. This modified dialog sequence is then mapped to a sequence of word token embeddings (WTE) by the vocabulary embedding matrix of GPT-2. An instance of the item encoder is used to encode item metadata into token embeddings. The item encoder takes in item metadata and outputs an item token embedding (ITE) with the same dimensionality as a WTE of the GPT-2 model. The ITE is then concatenated with the WTEs constructed from the dialog context to be consumed by GPT-2. An example is shown in 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Have you seen Venom ?</head><p>Have you seen [PH] ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ITE WTEs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Item Encoder</head><p>Venom Metadata GPT-2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recommendation Module</head><p>Similar to <ref type="bibr" target="#b1">(Covington et al., 2016)</ref>, we pose recommendation as a two-phase process: candidate selection and candidate ranking. During candidate selection, the entire item database is traversed and narrowed down to a few hundred candidates based on a calculated similarity score between the dialog context and the item metadata. During candidate ranking, similarity scores between the dialog context and the generated candidates are calculated with finer granularity because only a few hundred items are being considered rather than the entire database. The top candidates after sorting is then used as prompts for response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Candidate Selection</head><p>In this section, we describe the training objective of candidate selection. We add a special token "[REC]" to the vocabulary embedding matrix of GPT-2. This token is used to indicate the start of the recommendation process and to summarize dialog context. At the end of each turn, a token embedding sequence is created following <ref type="figure" target="#fig_1">Figure 2</ref> in the format of an interleaving of word token embeddings (WTE) and item token embeddings (ITE) to represent all previous dialog context. When recommendation is labeled in a conversation turn in the training dataset, the WTE of "[REC]" is appended to the previous token embedding sequence to form a new sequence D. Next, GPT-2 takes in D and produces an output embedding sequence. We denote the last vector of this output embedding sequence as D R which corresponds to the appended special token "[REC]". D R summarizes dialog context and can be used to retrieve candidate items.</p><p>In order to let the model learn how to find candidates based on their relevance to dialog context, we randomly sample M items and their metadata from the database as negative examples and combine them with the ground truth item labeled in the dataset to get the training samples. Another instance of the item encoder, is used to create candi-date token embeddings for each item in the training samples. The item Encoder takes in metadata of the samples items and outputs a set of candidate vector embeddings C = (c 0 , c 1 , ..., c M ), each with the sample dimensionality as D R . The recommendation task at this phase is posed as a multi-class classification problem of predicting the ground truth item over the negative samples. The probability of each candidate item is defined in (1) and optimized by a cross-entropy loss function, denoted as L select :</p><formula xml:id="formula_0">P (i) = e c i ?D R M n=0 e cn?D R<label>(1)</label></formula><p>Note that the purpose of this learning objective is to let the model learn how to construct the D R representation instead of learning the probabilities of candidate items. The D R representation is later used in an approximate nearest neighbor search <ref type="bibr" target="#b12">(Liu et al., 2004)</ref> to select candidates from the entire database in testing 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Candidate Ranking</head><p>In this section, we describe the training objective of candidate ranking. The goal of candidate ranking is to further perform fine-grained scoring on the similarities between generated candidates and dialog context so that the final rankings of items can better reflect users' preferences. Unlikely previous studies where knowledge graphs are applied to complete this task, we propose to use GPT-2 directly.</p><p>During training, the same context token embedding sequence D and the same training sample with M negative examples are used. The ITE encoder from section 3.1 is used to map the metadata of the sample to an ITE set T = (t 0 , t 1 , ..., t M ), where the subscript of each t i corresponds to their index in the database. A concatenation of context sequence D and T are created and consumed by the same GPT-2 model used above and the output embeddings are computed. Note that the order of candidate items should not make a difference on the values of the outputs. Therefore, we add the same positional encoding to each ITE in T and remove the auto-regressive attention masks among the ITEs. We select from the output embeddings the vectors that correspond to the vectors in T . For each output vector selected, a feed-forward layer is applied to reduce each vector from a higher dimension to a single number with dimensionality equals 1. This set of numbers are denoted by Q = (q 0 , q 1 , ..., q M ) where the index of each number corresponds to their index in T . The final ranking score of each candidate item is defined in (2) and optimized by a cross-entropy loss function, denoted as L rank :</p><formula xml:id="formula_1">R(i) = e q i M n=0 e qn<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Response Generation Module</head><p>In this section, we describe how to train the model to generate responses based on the recommended items' metadata. The same token embedding sequence D is used as context and current system utterance U = (w 0 , w 1 , ..., w n ) is used as targets where each w i represents a WTE. We only optimize the GPT-2 model to reconstruct system utterances.</p><p>If the current utterance contains recommendations, we create ITEs by passing metadata of the recommended items through the item Encoder used in 2 and append the ITEs to context token embedding sequence D to obtain D . If the current utterance doesn't contain recommendations, D is simply set to be D. The GPT-2 model is trained to reconstruct the ground truth U based on D . The probability of generated response is formulated as:</p><formula xml:id="formula_2">P (U |D ) = n i=1 P (w i |w i?1 , ..., w 0 , D )<label>(3)</label></formula><p>The loss function is set to be:</p><formula xml:id="formula_3">L res = ? 1 N N i=1 log(P (U i |D ))<label>(4)</label></formula><p>Where N is the total number of system utterances in one dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Joint Learning</head><p>Finally, we use the following combined loss to jointly train both the encoders and the GPT-2 model:</p><formula xml:id="formula_4">Loss = a ? L select + b ? L rank + c ? L res<label>(5)</label></formula><p>Where a, b and c are the weights of language training and recommendation training objectives. During training, all weight parameters of the two item encoders, the GPT-2 model and relevant feedforward layers participate in back-propagation. An overview of training is shown in <ref type="figure">Figure 3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Testing</head><p>During testing, a candidate embedding set over the entire item database is built by running metadata through the item encoder used in section 3.2.1 and stored with a nearest neighbor index (NNI) <ref type="bibr" target="#b16">(Muja and Lowe, 2014)</ref>. During response generation, when a "[REC]" token is generated, candidate selection 3.2.1 is activated. An approximate nearest neighbor search is conducted over the NNI and K closest candidates are selected based on their similarities from the D R vector. Candidate ranking is then activated and the GPT-2 and the item encoder from 2 are used to generate a score for each candidate. When ranking finishes, the ITE that receives the highest ranking score is appended to the dialog context D and response generation continues until the end of sentence token is generated. After generation is completed, we replace the occurrence of the placeholder token "[PH]" with the title of the recommended item to form the final response. Note that when there is no need for recommendation, our GPT-2 model simply generates a clarification question or a chitchat response with no placeholder tokens. We only present the case when there's only one ground truth recommendation in the utterance. However, it's easy to extend the above approach to multiple recommendations. An overview of testing is shown in <ref type="figure">Figure 3</ref> 4 Experiments</p><p>In this section, we discuss the datasets used, experiment setup, experiment results on both recommendation and language metrics, and report analysis results with ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluated our model on two datasets: ReDial dataset <ref type="bibr" target="#b30">(Li et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">baselines</head><p>The baseline models for evaluation on the ReDial dataset is described below:</p><p>ReDial <ref type="bibr" target="#b30">(Li et al., 2019)</ref>: A dialogue generation model using HRED <ref type="bibr" target="#b22">(Sordoni et al., 2015)</ref> as backbone for dialog module KBRD <ref type="bibr" target="#b0">(Chen et al., 2019)</ref>: The dialog generation module based on the Transformer architecture <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>. It exploits external knowledge to perform recommendations and language generation.</p><p>KGSF : Concept-net is used alongside knowledge graph to perform semanticaware recommendations.</p><p>CR-Walker <ref type="bibr" target="#b15">(Ma et al., 2021)</ref>: performs treestructured reasoning on a knowledge graph and guides language generation with dialog acts CRFR : conversational context-based reinforcement learning model with multi-hop reasoning on KGs.</p><p>NTRD <ref type="bibr" target="#b9">(Liang et al., 2021)</ref>: an encoder-decoder model is used to generate a response template with slot locations to be filled in with recommended items using a sufficient attention mechanism.</p><p>RID : pre-trained language model and knowledge graph are used to improve CRS performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Implementation Details</head><p>We employed GPT-2 model <ref type="bibr" target="#b18">(Radford et al., 2018)</ref> as the backbone of MESE for dialog generation, which contains 12 layers, 768 hidden units, 12 heads, with 117M parameters. We recruited 2 item encoders <ref type="bibr" target="#b19">(Sanh et al., 2020)</ref> to encoder items in candidate generation 3.2.1 and candidate ranking 3.2.2, respectively, each has a distil-bert model with 6 layers, 768 hidden units, 12 heads, with 66M parameters. We used the AdamW optimizer <ref type="bibr" target="#b13">(Loshchilov and Hutter, 2019)</ref> with epsilon set to 1e ?6 , learning rate set to 3e ?5 . The model was trained for 8 epochs on ReDial dataset, and the first epoch was dedicated to warm up with a linear scheduler. We set the sample size M during candidate generation and candidate ranking to be 150. We chose K = 500 for the number of candidates during testing. We set a=0.3, b = 1.0 and c = 0.5 as coefficients for 3 loss functions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Evaluation Metrics</head><p>We performed two evaluations, recommendation evaluation and dialog evaluation, for the model. For recommendation evaluation, we used Recall@X (R@X), which shows whether the top X items recommended by the system include the ground truth item suggested by human recommenders. In particular, we chose R@1, R@10 and R@50 following previous works <ref type="bibr" target="#b0">(Chen et al., 2019;</ref>. We also define recall accuracy of MESE to be the percentage of ground truth items that appear among the 500 generated candidates in the candidate generation phase 3.2.1 and ranking accuracy to be the percentage of items that appear in the top k (k=1, 10, 50) position of the sorted candidates in the candidate ranking phase 3.2.2. The product of the recall and ranking accuracy is the final recommendation accuracy of MESE. We also adopted end-to-end response evaluation following . We computed response recall (ReR) as whether the final response contains the target items recommended by human annotators. For dialog evaluation, we measured perplexity, distinct n-grams <ref type="bibr" target="#b7">(Li et al., 2016)</ref>, and BLEU score <ref type="bibr" target="#b17">(Papineni et al., 2002</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Results</head><p>We first report recall, ranking, and final accuracy on REDIAL dataset of MESE in table 3. From the results, it can be seen that candidate ranking has remarkable performance gains in scoring the items. It demonstrates that pre-trained language model have great potential in making recommendations. One possible reason behind this is that the selfattention mechanism is effective in learning the discrepancies between item semantics and dialog semantics.</p><p>top k Ranking Acc Recall Acc Final Acc @1 7.2 0.778 5.6 @10 33.0 0.778 25.6 @50 58.5 0.778 45.5  <ref type="table" target="#tab_3">Table 2</ref> compares different models on REDIAL dataset. The superiority of MESE persists across recommendation and language generation. On all recommendation metrics, including R@1, R@10, and R@50, MESE outperforms the state-of-the-art models by a large margin. We argue in 5.2 that this significant gain of performance is due to the effectiveness of the item encoder. MESE also performs well on the ReR score, which indicates that the filling placeholder tokens can help integrate recommended items into responses. For language generation, MESE also achieves significantly better performance than all other models on distinct ngrams and bleu scores with the exception that the PPL is worse than those of KGSF and NTRD. This indicates that MESE can generate more diverse responses while sticking to the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies and Analysis</head><p>In this section, we first analyze the reason behind the performance gain of our recommendation module by analyzing the embeddings learned by the item encoder.</p><p>How much does metadata help with recommendation? We argue that our training objectives on recommendation enable the item encoder to selectively extract useful features pertinent to the recommendation task from item metadata and construct item representations that resonate with instructional semantic properties in the dialog histories. For example, in REDIAL dataset, movie genre information is the most frequently mentioned property in dialog histories and human recommenders often make recommendation decisions based on this property. Although other properties like actors also help with recommendation, they do not appear in the corpus as often as genres or movie plots. We designed the following experiments to test our hypothesis. First, we train MESE with movie genre and plot information removed from the metadata, which we refer to as MESE w/o content, and compare its recommendation performance with MESE in <ref type="table" target="#tab_5">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>R@1 R@10 R@50 MESE w/o content 3.9 19.5 37.9 MESE 5.6 25.6 45.5 As we can see from the table, there is a significant performance decrease after we remove genre and plot information, which indicates that MESE depends on the input features to make high quality recommendations. We also point out that movie titles contain weak genre information but are not able to provide adequate features for the item encoder to extract from.</p><p>How does the item encoder help with recommendation? Specifically, we select all movie items with only one genre as our candidates, resulting in a subset of~600 movies. We then select 2 item encoders (section 3.2.2) from MESE, MESE w/o content, and the item encoder before training (MESE raw), respectively, and obtain 3 sets of item embeddings by passing the movie subset to these encoders. On each set of embeddings, we run a K-means clustering algorithm with K being set to be 3, 4, and 5, respectively. For each cluster obtained, we calculated the proportion of the majority genre among all item candidates. This process is repeated 20 times for each K and the average accuracy is reported in  As we can see from the table, without training, MESE raw, being the least sensitive to genre information, achieves the lowest accuracy scores on all clusters. MESE w/o content, although deprived of genre and plot, still has slightly higher accuracy than MESE raw due to its exposure to REDIAL conversations. MESE is most sensitive to genre information. This is an indication that by aligning recommendation related information in both dialog context and item metadata, our model is able to generate meaningful representations for the task, which can facilitate the language model to produce better rankings through its multi-head attention mechanism and result in better recommendation performance. Previous KG-based recommendation module, although also modeled metadata as relations with Graph Convolutional Networks, did not compose item representations in a systematic way.</p><p>What if we remove mentioned entities from dialog context? Mentioned entities are crucial to previous approaches <ref type="bibr" target="#b0">(Chen et al., 2019;</ref>   We can see removing the entities led to an average of 26.3% performance drop on REDIAL and an average of 11.2% performance drop on INSPIRED. The recommendation performance on REDIAL are more impacted by the removal of entities. That is because the conversations in REDIAL are rich with entities and weak in semantic information, whereas, INSPIRED is more sparse on entities but contains more semantics. REDIAL has 10006 conversations, in which there is 1 mentioned movies among every 21.85 word tokens. Its sentence level distinct 1-grams and 3-grams are 0.15 and 2.81. However, INSPIRED dataset has 1001 conversations, in which there is 1 mentioned movies among every 63.54 word tokens. Its sentence level distinct 1-grams and 3-grams are 0.59 and 6.84. This proves that our model can efficiently infer user interests from texts to make high-quality recommendations without explicitly using mentioned entities. This property could be useful in an e-commerce setting where users tend to convey their requirements more with texts than entities. It could also be useful in a cold start scenario where we don't have many entities in the context and are forced to only rely on semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we introduced MESE, a novel CRS framework. By utilizing item encoders to construct embeddings from metadata, MESE can provide high-quality recommendations that align with the dialog history. We also analyzed various behaviors of MESE to better understand its underlining mechanisms. Our approach yields better performance than existing state-of-the-art models. As for future work, we will consider applying this approach to a broader domain of CRS datasets. Currently, we only experiment on movie recommendations. As mentioned in section 5.2, MESE is capable of efficiently utilizing dialog history as a whole and construct item embeddings that reflects user preferences. It has the potential to work well with cross-modal tasks. For example, multimodal works can be explored in the e-commerce domain with MESE based architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Item Encoder takes in the metadata of an item and outputs an embedding of certain dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Dialog context is represented as a concatenation of WTEs and ITEs to be consumed by GPT-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Overview of MESE. During training, M examples are sampled from the item database and participate in computing the joint loss L select and L rank , which are then combined with the response generation loss L res and jointly optimized. During testing, the entire metadata DB is stored as a nearest neighbor index (NNI). An approximate nearest neighbor search is performed on D R to get candidate items, which is then fed to the ITE Encoder to compute final scores and the the highest-ranked candidate will be presented to the user in the generated response.</figDesc><table><row><cell cols="2">Dialog Context</cell><cell></cell><cell cols="2">Training</cell><cell>Venom 0.3 Titanic 0.2</cell><cell></cell><cell>?select</cell></row><row><cell>User: Hi!</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>Venom 0.8 Avatar 0.1 ?</cell><cell>?rank</cell></row><row><cell></cell><cell cols="2">System: Hello!</cell><cell></cell><cell></cell><cell>Similarity</cell><cell></cell><cell>?res</cell></row><row><cell>User: I like action films.</cell><cell></cell><cell></cell><cell>S 0</cell><cell>?</cell><cell>S M</cell><cell>D R</cell><cell>FFN &amp; Softmax</cell></row><row><cell cols="3">System: Have you seen Venom ?</cell><cell cols="3">Item Encoder</cell><cell>GPT-2</cell><cell>GPT-2</cell></row><row><cell>User: ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">System: ?</cell><cell></cell><cell></cell><cell cols="2">I like action films REC</cell><cell>T 0 T 1 ? T M</cell><cell>I like action films REC T Venom Have you seen [PH] ?</cell></row><row><cell>Item DB</cell><cell cols="2">Item Encoder</cell><cell cols="3">M samples from Item DB + ground truth</cell><cell></cell><cell>Item Encoder</cell><cell>Venom (Ground Truth)</cell><cell>Item Encoder</cell></row><row><cell>Testing Step (1)</cell><cell>D R</cell><cell>NNI</cell><cell></cell><cell></cell><cell>Testing Step (2)</cell><cell cols="2">FFN &amp; Softmax</cell><cell>Testing Step (3)</cell><cell>Have you seen ?</cell></row><row><cell cols="2">GPT-2</cell><cell></cell><cell></cell><cell></cell><cell>GPT-2</cell><cell></cell><cell>GPT-2</cell></row><row><cell cols="2">I like action films REC</cell><cell></cell><cell></cell><cell></cell><cell>I like action films REC</cell><cell cols="2">T 0 T 1 ? T K-1</cell><cell>I like action films REC</cell><cell>T Venom Have you ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>K Nearest Candidates</cell><cell cols="2">Item Encoder</cell><cell>Venom Rank #1 Item</cell><cell>Item Encoder</cell></row><row><cell cols="2">Candidate Selection</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Candidate Ranking</cell><cell>Response Generation</cell></row><row><cell>Figure 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Model</cell><cell cols="4">Recommendation metrics R@1 R@10 R@50 ReR</cell><cell cols="6">Language generation metrics PPL Dist2 Dist3 Dist4 Bleu2 Bleu4</cell></row><row><cell>ReDial</cell><cell>2.4</cell><cell>14.0</cell><cell>32.0</cell><cell>0.7</cell><cell cols="6">28.1 0.225 0.236 0.228 0.178 0.074</cell></row><row><cell>KBRD</cell><cell>3.1</cell><cell>15.0</cell><cell>33.6</cell><cell>0.8</cell><cell cols="6">17.9 0.263 0.368 0.423 0.185 0.074</cell></row><row><cell>KGSF</cell><cell>3.9</cell><cell>18.3</cell><cell>37.8</cell><cell>0.9</cell><cell cols="6">5.6 0.289 0.434 0.519 0.164 0.074</cell></row><row><cell>CR-Walker</cell><cell>4.0</cell><cell>18.7</cell><cell>37.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CRFR</cell><cell>4.0</cell><cell>20.2</cell><cell>39.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RID</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.1</cell><cell cols="6">54.1 0.518 0.624 0.598 0.204 0.110</cell></row><row><cell>NTRD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.8</cell><cell cols="4">4.4 0.578 0.820 1.005</cell><cell>-</cell><cell>-</cell></row><row><cell>MESE</cell><cell>5.6</cell><cell>25.6</cell><cell>45.5</cell><cell>6.4</cell><cell cols="6">12.9 0.822 1.152 1.313 0.246 0.143</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results and comparison with the literature on REDIAL.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Recall, Ranking and Final Accuracy of MESE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison Results of MESE and MESE w/o content.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The reasoning behind this setup is that the more features a set of embeddings contain about genre, the closer the clusters should be towards its central point, thus the higher the accuracy should be for its majority genre.</figDesc><table><row><cell>Model</cell><cell>K=3</cell><cell>K=4</cell><cell>K=5</cell></row><row><cell>MESE raw</cell><cell cols="3">0.492 0.514 0.574</cell></row><row><cell cols="4">MESE w/o content 0.555 0.589 0.606</cell></row><row><cell>MESE</cell><cell cols="3">0.695 0.725 0.738</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Item Encoders Clustering Accuracy</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>in terms of recommendations. We train MESE with mentioned entities removed from dialog history and compare its performance with MESE on REDIAL dataset and INSPIRED dataset in table 6.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="3">R@1 R@10 R@50</cell></row><row><cell>REDIAL</cell><cell>MESE w/o item</cell><cell>3.4</cell><cell>18.1</cell><cell>38.7</cell></row><row><cell></cell><cell>MESE</cell><cell>5.6</cell><cell>25.6</cell><cell>45.5</cell></row><row><cell cols="2">INSPIRED MESE w/o item</cell><cell>4.3</cell><cell>11.9</cell><cell>26.7</cell></row><row><cell></cell><cell>MESE</cell><cell>4.8</cell><cell>13.5</cell><cell>30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results of MESE and MESE w/o on REDIAL and INSPIRED.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available online https://github.com/ by2299/MESE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards knowledge-based recommender dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1189</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1803" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IN-SPIRED: Toward sociable recommendation dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiaoyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.654</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8142" to="8152" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<title level="m">Neural collaborative filtering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Chris Pal. 2019. Towards deep conversational recommendations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DEUX: an attribute-guided framework for sociable recommendation dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirley</forename><forename type="middle">Anugrah</forename><surname>Hayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2105.00825</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zujie</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Xiubo Geng, Fan Liang, and Daxin Jiang. 2021. Learning neural templates for recommender dialogue system</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An investigation of practical approximate nearest neighbor algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cr-walker: Tree-structured graph reasoning and dialog acts for conversational recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbor algorithms for high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2227" to="2240" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556270</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A hierarchical recurrent encoder-decoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Artif. Intell</title>
		<imprint>
			<biblScope unit="page" from="421425" to="421426" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semisupervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhua</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">Sequential recommendation with bidirectional encoder representations from transformer</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LXMERT: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5100" to="5111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Finetuning largescale pre-trained language models for conversational recommendation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficiently embedding dynamic knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DIALOGPT : Largescale generative pre-training for conversational response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.30</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<title level="m">Deep interest network for clickthrough rate prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CRFR: Improving conversational recommender systems via flexible fragments reasoning on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021</title>
		<meeting>the 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Ruifang He, and Yuexian Hou</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Online and Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4324" to="4334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving conversational recommender systems via knowledge graph based semantic fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
