<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Sensetime Group Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Various deep learning techniques have been proposed to solve the single-view 2D-to-3D pose estimation problem. While the average prediction accuracy has been improved significantly over the years, the performance on hard poses with depth ambiguity, self-occlusion, and complex or rare poses is still far from satisfactory. In this work, we target these hard poses and present a novel skeletal GNN learning solution. To be specific, we propose a hop-aware hierarchical channel-squeezing fusion layer to effectively extract relevant information from neighboring nodes while suppressing undesired noises in GNN learning. In addition, we propose a temporal-aware dynamic graph construction procedure that is robust and effective for 3D pose estimation. Experimental results on the Human3.6M dataset show that our solution achieves 10.3% average prediction accuracy improvement and greatly improves on hard poses over state-of-the-art techniques. We further apply the proposed technique on the skeleton-based action recognition task and also achieve state-of-the-art performance. Our code is available at https://github. com/ailingzengzzz/Skeletal-GNN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single-view skeleton-based 3D pose estimation problem plays an important role in numerous applications, such as human-computer interaction, video understanding, and human behavior analysis. Given the 2D skeletal positions detected by a 2D keypoint detector (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">27,</ref><ref type="bibr">38]</ref>), this task aims to regress the 3D positions of the corresponding joints. It is a challenging task and has drawn lots of attention from academia in recent years. Even though the average performance increases steadily over the years, the prediction errors on some poses are still quite high. <ref type="figure" target="#fig_1">Fig. 1</ref> shows some examples in a widely used dataset, Human3.6M <ref type="bibr" target="#b8">[9]</ref>. Some actions (e.g., "Sit" and "Sit Down") contain many poses with depth ambiguity, self-occlusion, or complex poses. Also, there inevitably exist some poses rarely seen in the training dataset. Similar to the definition of hard examples in object detection <ref type="bibr" target="#b16">[17]</ref> and semantic seg-mentation <ref type="bibr" target="#b14">[15]</ref>, we collectively regard those poses with high prediction errors as hard poses.</p><p>Early attempts [24, 29] simply use fully-connected networks (FCN) to lift the 2D keypoints into 3D space. However, the dense connection of FCN is prone to overfit, leading to relatively poor performance. To tackle this problem, geometric dependencies are incorporated into the network in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">28,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b23">46]</ref>, which significantly improve prediction accuracy. As articulated human body can be naturally modeled as a graph, with the recent development of graph neural networks (GNN) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr">41,</ref><ref type="bibr" target="#b24">47]</ref>, various GNN-based methods <ref type="bibr" target="#b26">[49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">51]</ref> are proposed in the literature for 2D-to-3D pose estimation.</p><p>GNN-based solutions naturally capture the relationship between body joints. For a target node, aggregating features from its neighboring nodes facilitates bringing in semantic information to relieve the uncertainty in estimating its 3D position. In other words, for the estimation of a particular node in the graph (e.g., left hand), both its direct neighbor (i.e., left elbow) and other nodes that are multiple hops away in the graph (e.g., left shoulder and even right foot in some poses) may provide useful information that contributes to the position estimation of the target node, and the learning of skeletal graph neural networks is to capture such context information for better 2D-to-3D pose estimation. However, existing GNN-based solutions do not fully tap the potential of the skeleton graph. The reasons are two-fold:</p><p>? The power of graph neural networks lies in the aggregation of neighboring nodes, which, however, contributes both useful information and undesired noises. On the one hand, aggregating distant nodes in the skeletal graph does provide useful information; On the other hand, the more distant the nodes, the more likely undesired noises are introduced into the aggregation procedure. Existing works do not consider such signalto-noise issues in message passing over the GNN.</p><p>? The relationship between body joints varies with different poses. For example, for poses in "running", the hand-foot joints are closely related, while for poses in "Sitting", there is no such strong relationship. It is rather difficult to capture such information with a static <ref type="bibr" target="#b4">(5)</ref> Complex Pose (6) Rare Pose (4) Self-occlusion (3) Depth Ambiguity (1) Easy Poses (2) Moderate Pose <ref type="figure" target="#fig_1">Figure 1</ref>: The examples of two easy poses, one moderate pose, and four kinds of hard poses in 2D-to-3D pose estimation with 2D detected poses as inputs (shown in the images). Although rapid progress has been made in this field, both non-graph methods and graph-based ones yield large prediction error on these hard poses.</p><p>skeleton graph across all poses.</p><p>This work proposes novel skeletal GNN learning solutions to mitigate the above problems, especially for hard poses. Our contributions are summarized as follows:</p><p>? We propose a hop-aware hierarchical channelsqueezing fusion layer to extract relevant information from neighboring nodes effectively while suppressing undesired noises. This is inspired by the feature squeezing works <ref type="bibr" target="#b27">[50,</ref><ref type="bibr" target="#b21">44,</ref><ref type="bibr" target="#b22">45]</ref>, wherein channel size is reduced to keep valuable information in each layer. Specifically, we squeeze long-range context features (i.e., information from distant nodes) and fuse them with short-range features in a hierarchical manner.</p><p>? Inspired by GNN-based action recognition work [36, <ref type="bibr" target="#b20">43,</ref><ref type="bibr">21]</ref>, we build dynamic skeletal graphs, wherein the edges between nodes are not only from the fixed human skeleton topology but also the node features to capture action-specific poses. To cope with the change of dynamic graphs over time and relieve outliers from frame-level features, we further integrate temporal cues into the learning process of dynamic graphs. The proposed temporal-aware dynamic graph construction procedure is robust and effective for 2Dto-3D pose estimation.</p><p>We conduct experiments on Human3.6M dataset <ref type="bibr" target="#b8">[9]</ref>, and the proposed solution outperforms state-of-the-art techniques <ref type="bibr" target="#b23">[46]</ref> in 3D pose estimation by 10.3% on average, and greatly improves on hard poses. Compared to stateof-the-art GNN-based solutions, we surpass [6] by 16.3%. As the proposed method is a plug-and-play module, we further integrate it into the skeleton-based action recognition framework, achieving state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries and Motivation</head><p>This work focuses on GNN-based 3D pose estimation. We first describe the general skeletal GNN construction Hop-2:</p><p>Dynamic Edge:</p><p>Target Node: Hop-1:</p><p>Physical Edge:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Skeletal Graph Neural Network</head><p>The human skeleton can be naturally modeled as a graph as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>(a). The nodes in the graph represent the 2D positions of human joints, and the edges between two joints denote bone connections. Hop-k demonstrates the shortest path k between two nodes. For instance, take node 1 (right shoulder) as the target node, node 2 (right elbow) and node 3 (neck) are its hop-1 neighbors (masked in dark blue); while nodes 4 (right hand), 5 (head), 6 (left shoulder), and 7 (spine) are its hop-2 neighbors (masked in light blue).</p><p>In a graph neural network, adjacency matrix determines the information passing among nodes, and the objective of the learning procedure is to obtain each node's features by aggregating features from its neighboring nodes. As hop-k increases during message passing, the information passed from the corresponding neighbors varies from short-range context to long-range context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">GNN-Based 3D Human Pose Estimation</head><p>weight sharing strategy restricts the representation power, a locally connected network (LCN) <ref type="bibr" target="#b5">[6]</ref> is proposed to learn the weight of each node individually for enhancing the representation differences among nodes, achieving better generalization capability.</p><p>High-order GCN <ref type="bibr" target="#b28">[51]</ref> explores different aggregation methods on high-order neighbors to capture the long-range dependencies among nodes. However, it may introduce more noises from less-related nodes without differentiating the impacts between short-range and long-range contexts.</p><p>Furthermore, some recent works <ref type="bibr" target="#b26">[49,</ref><ref type="bibr" target="#b5">6]</ref> try to learn the edge weights of the skeletal graph. However, without changing the graph topology, the effectiveness of such a dynamic solution is limited, especially for rare poses.  <ref type="bibr">[24,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b23">46,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">51]</ref>. As can be observed from this figure, most of them suffer from poor performance for some complex actions, such as, "Sit," "Sit Down," and "Take Photos." We attribute this phenomenon to that these hard poses require both short-range and long-range context information for better estimation. Meanwhile, existing solutions do not fuse them effectively in the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Observation and Motivation</head><p>Distant neighbors pass not only valuable semantic information but also irrelevant noise. Existing GNN-based methods try to aggregate the semantic information in both short-range and long-range neighbors. However, they ignore that the messages passed from distant neighbors also contain irrelevant noise. For example, "Walking" has a clear pattern, which contains strong correlation between arm and leg. Intuitively, we need to take both of them into account. However, due to some personalized style, besides the action pattern, there is also some heterogeneous noise. Therefore, it is necessary to suppress such irrelevant noise. Interestingly, through experiments, we observe that such noise is sensitive to the channel dimension. The channel dimension can constrain the amount of information and noise passed among nodes in a skeleton graph. In other words, an effective channel squeezing strategy could filter out undesired noise while keeping valuable information. Consequently, we propose a hop-aware hierarchical channelsqueezing transform on long-range features to improve aggregation effectiveness in skeletal GNN learning.</p><p>Dynamic graph construction is useful but should be delicately designed. Existing GNN-based methods construct the graph based on the physical skeleton topology <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">49]</ref>, like <ref type="figure" target="#fig_0">Fig. 2(a)</ref>. However, the strong hidden relationships among nodes vary with actions. By constructing a dynamic graph as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>, such a relationship might be more useful than physical features. Although the dynamic graphs seem intuitive for representing different motion-specific relations, it usually seems vulnerable to the A + + <ref type="figure" target="#fig_2">Figure 3</ref>: The framework of our method. The key is a specially designed module called Dynamic Hierarchical Channel-Squeezing Fusion layer (D-HCSF), shown in <ref type="figure" target="#fig_3">Fig. 4</ref> with details.</p><p>single-frame outliers. Thus, we introduce temporal information to make dynamic graph learning robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, our goal is to reduce errors of 3D human pose estimation, especially on hard poses. More specifically, given 2D keypoints X ? R N ?2 , with N nodes, the model outputs better 3D positions Y ? R N ?3 . The framework is designed based on the observations and motivations in Sec. 2.3, and shown in  <ref type="figure" target="#fig_3">(Fig 4)</ref>, and a temporal-aware dynamic graph learning component for updating the dynamic graph.</p><p>In this section, we first revisit the formulation of generic GCN <ref type="bibr" target="#b10">[11]</ref> and LCN <ref type="bibr" target="#b5">[6]</ref> in Sec. 3.1. Then, we introduce our hierarchical channel-squeezing fusion scheme in Sec. 3.2. Finally, we propose the dynamic graph learning and consider temporal-aware strategy in this process in Sec 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Vanilla Graph Neural Network</head><p>Given a graph G = (V, E), it consists of the nodes V and the edges E. We revisit a generic GCN <ref type="bibr" target="#b10">[11]</ref> layer defined as follows:</p><formula xml:id="formula_0">H = ?(?XW),<label>(1)</label></formula><p>where A ? R N ?N is an adjacency matrix with N nodes, indicating the connections between nodes. If the joint j is dependent on the joint i, then a ij = 1. Otherwise, the connections are set to zero a ij = 0. We denote the input node features as X ? R N ?Cin , the learnable weight matrix as W ? R Cin?Cout , and the activation function as ?(?). For simplification, we ignore the ?(?) in the following formulas. The GCN's representation power is limited by weight sharing strategy in node regression problem, while Ci et al. <ref type="bibr" target="#b5">[6]</ref> propose a locally connected network (LCN), which introduces the node-wise trainable parameters to enhance the differences among node features. This aggregation scheme learns different relations among different nodes. Accordingly, we recap its basic formulation. For clarity purposes, we take the embedding learning of node i from the direct neighbors in a layer as an example:</p><formula xml:id="formula_1">,? (1, ) ? ? ? ? ] [ ? ? ? ] Tanh [ ? ] + [ - ] [ ? ] [ ? ? ] [ , ? ? ] X: [ ? ? ] ? ? [ ? ]</formula><p>Hierarchical Fusion Layer Each dotted box consists of two streams: a weighted graph learning branch based on fixed physical edges (blue lines) and a dynamic graph learning branch to update the graph based on the node features adaptively (orange lines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel-Squeezing</head><formula xml:id="formula_2">h i = j?N1,i (? ij x j W ij ),<label>(2)</label></formula><p>where N 1,i contains the self-node and direct neighbors (hop=1) of the node i. We denote? ij as the value of i th row and j th column in the adjacency matrix?, which distinctly aggregates features among neighbors. x j is the inputs of the neighbor j. W ij ? R Cin?Cout denotes the learnable weights between the node pair (i, j), and h i is the updated features of the node i. Hence, the final output H is represented by the concatenation of all node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical Channel-Squeezing Fusion Layer</head><p>Inspired by the first observation in Sec. 2.3, we find (i) hierarchical spatial features are important to capture better short-to-long range context; (ii) distinguishing short-range and long-range context in fusion strategies is necessary to relieve irrelevant long-range context while keeping their essential components in hard pose estimation. Thus, as illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref> (c), we propose a hierarchical channelsqueezing fusion layer to reach the above hypothesis.</p><p>Hierarchical fusion layer. Accordingly, we take nodewise aggregation LCN <ref type="bibr" target="#b5">[6]</ref> as a baseline. To capture different ranges in spatial context, we generalize Eq. 2 by modifying the direct neighbors N 1,i to hop-k neighbors N k,i . Then, we can get updated features h k,i from hop-k neighbors.</p><p>To integrate multi-hop features in a layer, we propose a hierarchical fusion block as follows. It consists of two parts. First, we consider the short-range features h S,i within hop-S, which contain the most essential context of the target node i. We thus keep the whole information without squeezing them. We then define a farthest hop L to obtain the potentially useful information as a set of long-range context H L,i defined as:</p><formula xml:id="formula_3">h S,i = j?N S,i (? ij x j W ij ),<label>(3)</label></formula><formula xml:id="formula_4">H L,i = {h k,i |k = S + 1, ..., L},<label>(4)</label></formula><p>where S is less than or equal to L, set empirically. To fuse features from different contexts, we introduce two fusion functions, namely F k and F a , to form a twostage procedures. F k first transforms a set of long-range features H L,i to obtain a fused long-range features h L,i , and then F a fuses h L,i with short-range features h S,i to get output h a,i . We refer to such two-step fusion scheme as hierarchical fusion block. Finally, we process the feature h a,i through a transformation W a to obtain an final output h i with pre-defined dimension. Formally, the final output h i of this fusion layer is defined as:</p><formula xml:id="formula_5">h i = F a [h S,i , F k (H L,i )]W a .<label>(5)</label></formula><p>Channel-Squeezing block. To retain useful information while suppressing the irrelevant information of long-range context, we hypothesize that the context contains less relevant information. Hence, we propose a set of bottleneck transformations, named Channel-Squeezing Blocks to filter irrelevance by an end-to-end learning scheme. To reach the above hypothesis, we distinguish the learnable matrix W ij ? R Cin?C k,out from its output size C k,out of long-range context H L,j . Moreover, based on the theoretical Information Gain (IG) analysis on the hop-k <ref type="bibr" target="#b7">[8]</ref> measured by the average KL-divergence, where the information gain IG(k) of hop k always fits well on the function ae ?bk , where a, b &gt; 0. It means the information gain from long-range context will be decreased exponentially. In our condition, we propose a simplified relation function to decide the output size C k,out for each long-range context to reflect the decay of useful information as k increases.</p><formula xml:id="formula_6">C k,out = d (k?L) * C in ,<label>(6)</label></formula><p>where d ? [0, 1] indicates the channel squeezing ratio, and k ? [S + 1, L]. For short-range context h S,i , the output channels of W i,j keep the same as C in without squeezing to reduce the loss of useful features. Compared with Eq. 2, LCN is a special kind of HCSF module when C k,out is a constant across hops, and F k , F a are summations.  <ref type="bibr" target="#b10">[11]</ref>, (b) Locally Connected Network (LCN) <ref type="bibr" target="#b5">[6]</ref>, and (c) Our Hierarchical Channel-Squeezing Fusion (HCSF). We take the feature updating of node 1 as an example. The index of the node corresponds to <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Please noted that the HCSF layer could also adapt to other graph-based frameworks, like GCN <ref type="bibr" target="#b10">[11]</ref>, Pre-Agg, Post-Agg <ref type="bibr" target="#b19">[20]</ref>, the difference lies in weight sharing schemes, which is orthogonal to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporal-aware Dynamic Graph Learning</head><p>In this subsection, we present a framework when dealing with dynamic skeleton topology. Based on the second observation in Sec. 2.3, with a static graph, the model is hard to properly reflect the relations, especially for hard poses.</p><p>Learning dynamic graph. We learn the dynamic graph from two streams, where one is to update by physical skeleton topology, and the other is to update through features of nodes. For the first stream ( <ref type="figure" target="#fig_3">Fig. 4</ref> blue lines), we initialize the graph M k as the physical skeleton topology following SemGCN <ref type="bibr" target="#b26">[49]</ref>. Then, this graph will be updated during training. After training, this graph M k keeps fixed during inference and reflects the distribution of the whole dataset.</p><p>As the poses change, the connections should be varied during testing to capture motion-specific relations. Hence, we introduce a scheme for the second stream ( <ref type="figure" target="#fig_3">Fig. 4</ref> orange lines) to learn the graph from input features dynamically. Then, the graph can be adapted to the input pose during the inference. The input feature X is separately transformed by W k,? and W k,? for each branch. After multiplying two transformed features, we obtain a N ? N matrix indicating relations among nodes.</p><formula xml:id="formula_7">O k = F{[(X) tr W k,? ][(W k,? ) tr X]},<label>(7)</label></formula><p>where F denotes the activation function (i.e., Tanh), and tr denotes the transpose of a matrix. The output O k generates unique connections changing adaptively with input X.</p><p>Since learning dynamic graphs directly from input features may be sensitive to outliers (e.g., jitter and missing joints). We regard O k as dynamic offsets to refine the weighted graph M k . Hence, the final formulation of the dynamic graph of Hop-k is as follows:</p><formula xml:id="formula_8">A k = M k + ?O k ,<label>(8)</label></formula><p>where ? is a learnable scalar to adjust the scale of dynamic offsets. Then, we can aggregate the hop-k features by A k , and the following fusion block is the same as Eq. 5.</p><p>Temporal-aware dynamic graph. The connections between nodes may naturally vary over space and time, and the frame-level dynamic graph will suffer from unstable inputs, it is potential to optimize the dynamic graph learning process combining spatial and temporal context as a whole.</p><p>In terms of introducing temporal information, the input will add a time dimension, where X ? R Cin?Tin?N . We develop a temporal-aware scheme in Eq. 7. Instead of only applying linear transformations in spatial domain, we integrate temporal context by 1D temporal convolution layer with a kernel size of 1 ? F . It can smooth and filter the spatial outliers in input X to make the learning process robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we perform experimental studies to demonstrate the effectiveness of the proposed solution. <ref type="bibr" target="#b8">[9]</ref> consists of 3.6 million video frames with 15 actions from 4 camera viewpoints, where accurate 3D human joint positions are captured from high-speed motion capture system. Following previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">46</ref>, 24], we adopt the standard cross-subject protocol with 5 subjects (S1, S5, S6, S7, S8) as training set and another 2 subjects (S9, S11) as test set. It is commonly evaluated by two metrics, namely the mean per joint position error (MPJPE) with 17 joints of each subject and the Procrustes Analysis MPJPE (PA-MPJPE) to relieve the inherent scale, rotation, and translation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Human Pose Estimation</head><formula xml:id="formula_9">4.1.1 Data Description -Human3.6M</formula><p>-MPI-INF-3DHP [25, 26] contains both constrained indoor scenes and complex outdoor scenes, covering a greater diversity of poses and actions, where it is usually taken as a cross-dataset setting to verify the generalization ability of the proposed methods. For evaluation, we follow common  <ref type="table" target="#tab_15">Table 1</ref>: Comparison of single-frame 3D pose estimation in terms of MPJPE on Human3.6M. Works above the double line show results from detected 2D poses, and the below results are from 2D ground truth inputs to explore the upper bound of these methods. We highlight the graph-based methods by ?. w/A denotes using dynamic graphs. Best results in bold.</p><p>practice <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b23">46]</ref> by using the Percentage of Correct Keypoints (PCK) with a threshold of 150mm and the Area Under Curve (AUC) for a range of PCK thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Method Comparison</head><p>In terms of the frames of inputs, single-view 3D pose estimation can be divided into single-frame and temporal settings. We first compare our HCSF module and Dynamic graph learning with other previous works under the singleframe setting. Then, we extend to the temporal setting to compare related works with our Temporal-aware dynamic graph learning with HCSF scheme.</p><p>Comparison with Single-frame methods. As shown in Tab. 1, we compare our methods with other baselines. Under the standard protocol with 2D detected inputs <ref type="bibr" target="#b2">[3]</ref>, our methods can improve the graph-based method <ref type="bibr">[</ref> With temporal-aware dynamic graph construction, the proposed solution further improves the result by 0.7mm. Improvements on hard poses. As discussed earlier, we define hard poses as those with high prediction errors and they are model-specific. That is, while hard poses have some inherent characteristics (e.g., depth ambiguity and self-occlusion), they are handled differently with different models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b23">46,</ref><ref type="bibr" target="#b28">51]</ref>. Consequently, one pose that exhibits large errors on one model may show satisfactory results on another model and vice versa (we show more detailed analyses in the supplemental material). However, statistically speaking, if a model handles hard poses better, it would have the following properties: (1) those actions with high prediction errors would be improved more; (2) the proportion of poses with high errors would be smaller; (3) the upper bound of high-error poses would be smaller. Compared with state-of-the-art solution <ref type="bibr" target="#b23">[46]</ref>, our method reduces the prediction errors by 7.9mm, 6.1mm, 5.8mm, and 5.8mm (relative 17.2%, 18.5%, 14.9%, 13.6% improvements) on the actions "SitDown", "Direct", "Sit" and "Photo", respectively. The average improvement of the hard poses is 16.1% in Tab. 1. Next, in <ref type="figure" target="#fig_5">Fig. 6</ref>, we compare the error distribution in the test set with four existing solutions [24, <ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">46]</ref>. We can observe that there are much fewer poses with high prediction errors with our proposed  <ref type="table" target="#tab_16">Table 2</ref>: Comparison on temporal 2D detected poses input with similar input frames (5, 3, 9, 7, 10, 9, 9 frames, individually) for comparison. The noted w/o T denotes using dynamic graphs without temporal-aware scheme.</p><p>solution. Specially, there are only 3.6% cases with MPJPE above 60mm with our solution, while it is more than 6% with all the other methods. In fact, the number of cases with MPJPE above 40mm is consistently lower, and the number of cases with MPJPE less than 30mm is consistently higher with our solution than that with other methods.  Last, we present the mean errors for the top 5% hardest poses of five methods in <ref type="figure" target="#fig_6">Fig. 7</ref>, ours is 70.7mm, which is 13.8% and 17.1% smaller than the SOTA methods LCN (82.0mm) and SRNet (85.3mm), respectively. We also show visualization results in <ref type="figure">Fig. 8</ref>, compared with SOTA methods (upper LCN, below SRNet). In summary, the above results demonstrate the benefits of the proposed technique on hard poses.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>To study some important design choices in the proposed method, we take 2D ground truth poses as inputs and adopt MPJPE as an evaluation metric for analysis. Impact of hierarchical fusion. Tab. 4 shows that (1) two hierarchical design consistently outperforms Non-hierarchy by 2mm?3.7mm under different L, indicating that it is not appropriate to fuse long-range and short-range information in a single stage; (2) without considering different contributions from different hops, the performance of Hierarchy w/o hop-aware is inferior to Hierarchy, leading to a consistent performance drop of 1.6mm?1.8mm. It illustrates that the importance of processing long-range contexts according to hops. We fix S=1, d=1/16 by default. Impact of squeezing ratio d. We set S = 1, L = 2 to study the influence of the squeezing ratio d. Tab. 6 shows that as d decreases, the corresponding MPJPE first decreases and then increases. As d controls the output channel size of different hop features, the small value of d indicates the small output dimension for channel-squeezing transform. The decrease of d may first reduce the irrelevant information from long-range context, and thus reducing the pose estimation error. When d takes an extreme value <ref type="bibr">1 16</ref> , the useful information may also be substantially squeezed, leading to a performance drop. Impact of S and L. As shown in Tab. 7, S = 1 yields consistently good results under different L. As features   <ref type="table" target="#tab_25">Table 6</ref>: Influence of the squeezing ratio d.</p><p>within hop-S will not be squeezed, it is in line with our intuition that the direct neighbors provide the most relevant information to the target node. Besides, a random combination of S and L surpasses the strong baseline LCN <ref type="bibr" target="#b5">[6]</ref>, demonstrating the effectiveness of our design. Impact of the dynamic graph learning In Sec. 3.3, we introduce a dynamic graph A k consisting of a learnable graph M k and a graph offset O k learned from input features. As Tab. 5a, 5b and 5c illustrate, the initialization of the dynamic graph learning is important. The result shows that it is hard for all connected initialization (Tab. 5b) and random (Tab. 5c) to converge well. On the other hand, taking the physical topology as an initial graph (Tab. 5a) can achieve better results (32.1mm). Only learning the dynamic offsets (Tab. 5d) leads to severe performance drop. Relying only on input features may weaken its capability of dealing with data noise.</p><p>Tab. 5e adds the weighted graph M k and dynamic offsets O k , obtaining a 0.3mm performance gain over baseline. Moreover, although the dynamic graph shows priority in representing different motion-specific relations, it is usually vulnerable to the single-frame outliers. After considering the temporal context, we can further improve the baseline from 30.8mm to 29.7mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Skeleton-based Action Recognition</head><p>As a plug-and-play module, we integrate the proposed HCSF layer and temporal-aware dynamic graph learning into skeleton-based action recognition. Given a sequence of human skeleton coordinates in videos, this task categorizes them into a predefined action class. We conduct experiments on two commonly used datasets, namely, NTU RGB+D 60 [35] and NTU RGB+D 120 <ref type="bibr" target="#b17">[18]</ref>. Due to the limits of contents, we leave detailed datasets, implementation details, and ablation study in the supplemental material.</p><p>Following related works [42, 36, 37, 21, 5, 43], we employ a GNN model with ten spatial-temporal convolution neural layers as the baseline framework. We adopt the pro-   <ref type="table">Table 7</ref>: The influence of the hop-S and the hop-L. posed framework shown in <ref type="figure" target="#fig_2">Fig. 3</ref> to this task by changing the final regression branch into classification branch. Our results use the multi-stream framework as in <ref type="bibr">[37,</ref><ref type="bibr" target="#b4">5]</ref>. Single stream results are in the supplemental material.</p><p>Tab. 8 shows that our solution outperforms SOTA on both benchmarks. In particular, the top-1 accuracy is 89.2% and 87.5% in the X-Set and X-Sub settings on the more complex dataset <ref type="bibr" target="#b17">[18]</ref>, surpassing state-of-the-art solutions irrespective of the fact that they employ sophisticated attention modules [37] and temporal-dilated combinations in each layer <ref type="bibr">[21]</ref>. Our method can construct robust dynamic relations through the proposed HCSF layer and temporalaware scheme. Therefore, those inherent relations among joints are better captured, enhancing the capability to distinguish different actions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we explore a novel skeleton-based representation learning method for better dealing with hard poses in 3D pose estimation. The proposed Hierarchical Channel-Squeezing Fusion module can encode short-range and longrange contexts, keeping essential information while reducing irrelevant noises. Besides, we learn dynamic graphs that adaptively evolved based on the input poses rather than relying on a fixed pre-defined graph. Our method surpasses all non-graph methods by 10.3% and enhances the effectiveness of graph-based methods. We hope that our method would inspire the field of skeleton-based representation learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation</head><p>This supplementary material presents more experimental details, including data pre-processing, implementation details, additional experimental results, and ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">3D Human Pose Estimation</head><p>In this section, we demonstrate more detailed results on 3D human pose estimation. Sec. 1.1 gives more details on experiment settings. Second, Sec. 1.2 analyzes features of hard poses in this task. Third, Sec. 1.3 compares existing methods by the metric of PA-MPJPE. Finally, Sec. 1.4 shows the ablation study of only using a dynamic graph with HCSF module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Dataset and Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">Dataset Pre-processing</head><p>We follow our baseline <ref type="bibr" target="#b5">[6]</ref> to transform the 3D joint position under the camera coordinate system into the pixel coordinate system to remove the influence of pose scales for the single-view pose estimation. Following previous works <ref type="bibr">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">46]</ref>, we normalize 2D input poses in the range of [-1, 1] according to the width and height of images. The furthest hop is 6 in our pre-defined topology. Meanwhile, we set the entry values of the adjacency matrix to be ones if two nodes are physically connected and zero if not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Training Details</head><p>We build a six-layer network as the basic setting, including the first layer, two cascaded blocks, and the last layer. For a single-frame setting, each cascaded block consists of two HCSF layers followed by BN, LeakyReLU (alpha is 0.2), and dropout (random drop probability is 0.25). Besides, each block is wrapped with a residual connection, as shown in <ref type="figure" target="#fig_2">Fig.3</ref> in the main paper. The channel size of each layer we report in the final result is 128. In the ablation study, we set all output channels as 64 for each node. The above framework is a common structure that is also used in those works <ref type="bibr">[24,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b23">46]</ref>. For temporal settings, each cascaded block consists of one HCSF layer and one TCN layer. The fusion functions F k and F a are concatenation operators by default, which can also be addition, multiplication. L1 regression loss is used between the ground truth and outputs. Moreover, we train our model for 80 epochs using Adam <ref type="bibr" target="#b9">[10]</ref> optimizer. The initial learning rate is set as 0.001, and the exponential decay rate is 0.95. The mini-batch size is 256. For data augmentation, we follow <ref type="bibr">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b23">46]</ref> and use horizontal flip data augmentation at both training and test stages. Then, we evaluate our method with standard protocol following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b23">46</ref>, 29].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Further Analysis on Model-Specific Hard Poses</head><p>We define high-error poses as hard poses in the 2D-3D pose regression task. After analyzing the error distribution of hard poses in recent works <ref type="bibr">[24,</ref><ref type="bibr" target="#b26">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">46]</ref>, we could conclude they are model-specific. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, we illustrate the comparison of the (50% ? 5%) hardest poses from each method. For example, <ref type="figure" target="#fig_1">Fig. 1(a)</ref> shows the (50% ? 5%) hardest poses from the fully connected network [24], and we compare the results with the other four methods under the same poses.</p><p>We can observe: (1) The hardest 10% poses of each method is different, indicating that hard poses are modelspecific; (2) as the poses become increasingly difficult, the errors of all methods rise to some extent; (3) our method obtains the best results for the hardest poses of all the other four methods; (4) the error gap in <ref type="figure" target="#fig_1">Fig. 1(e)</ref> is smaller than <ref type="figure" target="#fig_1">Fig. 1(a?d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Comparison in PA-MPJPE</head><p>In Tab. 1, we compare our methods with other related works using the PA-MPJPE metric where available. We show the results from different 2D inputs, using detected poses or ground truth poses. Our approach achieves the new state-of-the-art with different inputs. Specifically, we surpass <ref type="bibr" target="#b23">[46]</ref> from 27.8mm to 24.8mm (relative 10.8% improvement) with 2D ground truth input. Moreover, we improve upon <ref type="bibr" target="#b18">[19]</ref> from 41.2mm to 39.0mm (relative 5.3% improvement) with 2D keypoint detection input. Our method can also show the superior in this metric, indicating the effectiveness of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Ablation Study on Dynamic Graph</head><p>This work has two main contributions: Hierarchical Channel-Squeezing Fusion (HCSF) and temporal-aware dynamic graph learning. We further explore how temporalaware dynamic graph alone influences the regression results. The 2D inputs are 2D ground truth to explore the upper bound of our method to avoid some irrelevant noises from detected 2D poses. Effects of dynamic graph learning. Dynamic graph learning shows different action-related connectivity with different inputs. It can be more flexible to extract specific-action patterns, especially for hard poses. We have demonstrated the influence on both HCSF and dynamic graph learning in the main paper. Accordingly, we study the effects of dynamic graph learning alone. We take the Non-hierarchy strategy LCN with the static graph aggregating with hop-2 as a baseline. Similar to the Tab.6 in the main paper, the Tab. 2a, 2b, 2c shows that M k (ori), using the physical topology as an initial connections, is better than M k (dense) and    Effects of the temporal scale. The uncertainty in singleframe poses will affect the regression results, making dynamic graph learning unstable and misleading. Hence, it is essential to introduce temporal consistency to make the process effective. We then explore how different settings in the temporal-aware scheme impact the performance. The temporal-aware schemes are different from the receptive fields. We fix S=1, L=2, d=1/8. The channel size of each layer is 128. And the frame of input is 9. From Tab. 3, we can find that using the 3 ? 1 kernel size will be better than other settings. And using temporal information will consistently improve the single-frame results by 0.1 ? 0.4mm. Thus, we report our final results using the 3 ? 1 kernel size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Skeleton-based Human Action Recognition</head><p>In this section, we present the experimental details, more results and ablation study of skeleton-based action recognition in Sec.  <ref type="bibr">[36,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b20">43,</ref><ref type="bibr">31</ref>, 42], we use skeleton sequences with 25 body joints captured by Kinect V.2 as inputs, and take two evaluation settings in NTU RGB+D 60: (1) Cross-Subject (X-Sub), where 20 subjects each for training and testing, respectively; (2) Cross-View (X-View), where 2 camera views for training and 1 camera view for testing. We perform the ablation study in Sec. 2.3 on the X-View setting. NTU RGB+D 120 <ref type="bibr" target="#b17">[18]</ref> collects 120 various actions by 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. We also follow some previous works <ref type="bibr" target="#b20">[43,</ref><ref type="bibr">21,</ref><ref type="bibr">31,</ref><ref type="bibr">30]</ref>, using two evaluation settings: (1) Cross-Setup (X-Set), training on 16 camera setups and testing on other 16 camera setups; (2) Cross-Subject (X-Sub), half subjects for training and half for testing. We report the top-1 accuracy on both benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Data Pre-processing</head><p>The procedure for both datasets follows <ref type="bibr">[36,</ref><ref type="bibr">37,</ref><ref type="bibr">21]</ref>. Each video has a maximum of 300 frames, and if it is shorter than    300, we repeat some frames to make up for it. Since there are at most two people in both datasets, we pad the second body with zeros to keep the same shape of inputs when the second body does not appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Training Details</head><p>We build a ten-layer network, including nine cascaded blocks that consist of one HCSF layer followed by BN, ReLU, temporal convolution layer (TCN), BN and ReLU. Each temporal 1D convolution layer conducts 9 ? 1 convolution on the feature maps. Each block is wrapped with a residual connection. The output dimension for each block are 64, 64, 64, 128, 128, 128, 256, 256 and 256. A global average pooling layer and a fully-connected layer are used to aggregate extracted features, and then, feed them into a softmax classifier to obtain the action class. The above framework is also a common setting as in <ref type="bibr">[42,</ref><ref type="bibr">36,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b20">43]</ref>. For multi-stream networks [37], we use four modalities, e.g., joints, bones and their motions, as inputs for each stream, and average their softmax scores to obtain the final prediction. Cross-entropy is used as the classification loss function to back-propagate gradients. We set the entry values in the adjacency matrix to be ones if two nodes are physically connected and zero if not.</p><p>For the training settings, we train our model for 60 epochs using the SGD optimizer with mini-batch size 64. The initial learning rate is 0.1 and it reduces by 10 times in both the 35 th and 45 th epoch, respectively. The weight decay is set as 0.0005. All data augmentation is the same as [36, 37].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Results of Single-Stream Framework</head><p>Due to space limitations, we only report the accuracy of the multi-stream framework [37] for the skeleton-based human action recognition task in the main paper. Specifically, the multi-stream network comprises four different modality inputs: the 3D skeleton joint position, the 3D skeleton bone vector, the motion of the 3D skeleton joint, and the motion of the 3D skeleton bone. Here, we report the performance of each modality input in Tab. 4 for the ease of comparison with existing works.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Ablation Study</head><p>We investigate the proposed methods on the NTU RGB-D X-View setting with 3D joint positions as inputs.</p><p>Effects of hierarchical channel-squeezing fusion block. From Tab. 5, our method improves the accuracy of 0.7% steadily under all three graph settings, static graphs G k and two dynamic graphs M k and A k in Eq.8. Basically, better results can be achieved when d=1/8. Moreover, we get the best results when using HCSF with dynamic graph A k , which validates the effectiveness of the proposed structure.</p><p>Furthermore, in Tab. 6, we demonstrate the performance of different methods concerning the number of hops. Since the skeleton topology in NTU-RGBD datasets is different from Human3.6M, it has more keypoints and further hops. The furthest hop is 13 in our pre-defined topology. We set S=5, L=7 and d=1/8. k-hop (k=1, 5, 7) means aggregating the neighbors within the distance k (1-hop with a static graph is ST-GCN [42]). Mixhop <ref type="bibr" target="#b0">[1]</ref> means that it concatenates the k-hop (k=1, 5, 7) features as the output of a layer, and the output size of the k-hop feature is one-third of the final output. MS-Hop means that it averages the khop (k=1, 5, 7) features, and the output size of the k-hop feature is the same as the final output.</p><p>As illustrated in Tab. 6, though MixHop and MS-Hop show improvements on k-hop strategies, they have no distinction in handling distant and close neighbors, which over-mix the useful and noisy information. Our approaches outperform all other baselines, which indicates the effectiveness of the hierarchical channel-squeezing fusion strategy.</p><p>Additionally, we explore the effects of other hyperparameters in the HCSF. We have the following observations. First, when using a dynamic graph A k in Eq.8 and fixing the hyper-parameters squeezing ratio d and the output channel size C in a layer, we find little effects on the results that S and L has. The accuracy is stable around 95.1% (? 0.2%). It indicates that the HCSF is robust to the noise in the graph. Second, as the number of hops increases, the performance first improves and then becomes stable. Since adding more hops leads to extra computations, to balance the computation efficiency and performance, our final setting for each layer is S=5, L=7, d=1/8, C of each layer is the same as <ref type="bibr">[42,</ref><ref type="bibr">36,</ref><ref type="bibr">37]</ref>. Last, we also explore to automatically learn the relations between hops and dimensions with   <ref type="table">Table 7</ref>: The impact of settings of temporal convolution in dynamic graph learning of skeleton-based action recognition. st. is an abbreviation for stride, and di. is dilation.</p><p>the guidance of channel attention. However, we find that the exponentially decaying in dimension consistently yields better results than the soft attention, which may be because the soft attention mechanism introduces more uncertainty and complexity.</p><p>Effects of the temporal-aware dynamic graph learning.</p><p>The jitter and missing inputs will make dynamic graph learning unreliable, making it difficult to distinguish between similar actions, e.g., "eat a meal" and "brushing teeth." Such problems are serious in using single-frame features, but they can be improved by involving temporal information. From Tab. 7, we can observe that when using three frames into a temporal convolution, it can improve the single-frame setting by 0.6%. While the settings of temporal aggregation are important, the longer temporal contexts will also degrade the performance, and use three frames will be the optimal setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The illustration of a skeletal graph with human physical edges and action-specific dynamic edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>shows the prediction accuracy of recent 3D pose estimation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 3 .</head><label>3</label><figDesc>The core of our framework is the module: a Dynamic Hierarchical Channel-Squeezing Fusion Layer (D-HCSF) shown in Fig 4. It contains a hierarchical channel-squeezing fusion scheme for updating features of each node</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The architecture of our Dynamic Hierarchical Channel-Squeezing Fusion (D-HCSF) layer under k hops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The architectures of (a) Graph Convolution Network(GCN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>MPJPE distribution on the testset of Human3.6M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>] SemGCN<ref type="bibr" target="#b26">[49]</ref> LCN<ref type="bibr" target="#b5">[6]</ref> SRNet<ref type="bibr" target="#b23">[46]</ref> Ours Mean-Error comparison of the 5% Hardest Poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>L</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 :</head><label>1</label><figDesc>(e) The hard poses of Ours The comparison of the hard poses in terms of each method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2] from 50.6mm to 47.9mm (relative 5.3% improvement), and surpass the non-graph based method<ref type="bibr" target="#b23">[46]</ref> by 2.0mm (relative 4.0% improvement). Since the results of 2D detected poses would affect uncertainty, it is better to consider using 2D ground truth as input to explore the upper bound of these methods. Accordingly, with 2D ground truth inputs, our proposed model improves the graph-based stateof-the-art<ref type="bibr" target="#b5">[6]</ref> from 36.3mm to 30.4mm (relative 16.3% improvement). Although LCN<ref type="bibr" target="#b5">[6]</ref> aggregates long-range (L = 3) information to relieve depth ambiguities, it ignores the fact that distant joints may bring more disruptions while they still contain certain useful information.</figDesc><table /><note>The proposed HCSF module considers this effect by squeezing the dif- ferent hop features into different latent spaces and then hi- erarchically fusing them. Moreover, our method surpasses state-of-the-art non-GNN method [46] by 3.5mm (relative 10.3% improvement), which further proves the effective- ness among the general methods. Comparison with Temporal methods. We compare with temporal methods with nine frames as inputs and Tab. 2 shows the comparison in terms of average error. For all methods, we select their reported results with similar input frames for comparison. The result shows that the proposed method can outperform previous approaches consistently.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method Hossain et al. [34] Lee et al. [12] Pavllo et al. [29] Cai et al. [2] Lin et al. [16] Ours w/o T</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell>MPJPE (mm)</cell><cell>58.3</cell><cell>52.8</cell><cell>49.8</cell><cell>48.8</cell><cell>48.8</cell><cell>46.4</cell><cell>45.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Comparison on MPI-INF-3DHP. We further test our model trained with the Human3.6M dataset on the MPI-INF-3DHP dataset to verify its generalization capability. Tab. 3 shows about 5.5% improvements on all metrics over related methods.</figDesc><table><row><cell>Method</cell><cell>FCN [24]</cell><cell>OriNet [22]</cell><cell>LCN [6]</cell><cell>SRNet [46]</cell><cell>Ours</cell></row><row><cell>Outdoor</cell><cell>31.2</cell><cell>65.7</cell><cell>77.3</cell><cell>80.3</cell><cell>84.6 ? 5.4%</cell></row><row><cell>All PCK</cell><cell>42.5</cell><cell>65.6</cell><cell>74.0</cell><cell>77.6</cell><cell>82.1 ? 5.8%</cell></row><row><cell>All AUC</cell><cell>17.0</cell><cell>33.2</cell><cell>36.7</cell><cell>43.8</cell><cell>46.2 ? 5.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>(1) Depth Ambiguity</cell><cell></cell><cell></cell><cell>(2) Self-Occlusion</cell><cell></cell><cell></cell></row><row><cell>LCN: 178.9mm</cell><cell>Ours: 29.6mm</cell><cell></cell><cell>LCN: 76.1mm</cell><cell cols="2">Ours: 27.2mm</cell></row><row><cell>(3) Rare Pose</cell><cell></cell><cell></cell><cell>(4) Complex Pose</cell><cell></cell><cell></cell></row><row><cell>SRNet: 128.7mm</cell><cell>Ours: 51.3mm</cell><cell></cell><cell>SRNet: 80.7mm</cell><cell cols="2">Ours: 31.4mm</cell></row><row><cell cols="6">Figure 8: Qualitative results of hard poses. 3D ground truth,</cell></row><row><cell cols="5">SOTA methods, and ours are black, red, blue in order.</cell><cell></cell></row><row><cell>L</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell></row><row><cell>Non-hierarchy</cell><cell cols="5">37.2 36.2 37.7 37.4 39.9</cell></row><row><cell cols="6">Hierarchy w/o hop-aware 34.6 34.2 34.7 35.4 36.2</cell></row><row><cell>Hierarchy</cell><cell>-</cell><cell cols="4">32.6 32.9 33.7 34.4</cell></row></table><note>Cross-dataset results on MPI-INF-3DHP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison of aggregating without hierarchy strat- egy (Non-hierarchy), with hierarchy scheme but regarding all hops information equally (Hierarchy w/o hop-aware) and our full design (Hierarchy).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison on the effects of dynamic graph learning A. ori is the static graph with physical connections. Baseline takes A k as ori. Only M k (?) denotes applying M k with different initialization. Only O k keeps the dynamic offset in Eq. 8. M k + O k equals to set ? = 1 in Eq. 8. w/T represents the temporal-aware scheme defined in Sec. 3.3.</figDesc><table><row><cell>d</cell><cell>1</cell><cell>1/2 1/4 1/8 1/16</cell></row><row><cell cols="3">MPJPE (mm) 35.4 34.7 33.8 31.4 32.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison against state-of-the-art methods on the NTU RGB+D 60 and 120 Skeleton dataset in terms of Top-1 accuracy(%). Best results in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>[ 21 ]</head><label>21</label><figDesc>Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang. Disentangling and unifying graph convolutions for skeleton-based action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</figDesc><table><row><cell></cell><cell>the 28th ACM International Conference on Multime-</cell></row><row><cell></cell><cell>dia, pages 1432-1440, 2020. 8, 1</cell></row><row><cell></cell><cell>[32] Huy Hieu Pham, Houssam Salmane, Louahdi</cell></row><row><cell></cell><cell>Khoudour, Alain Crouzil, Pablo Zegers, and Sergio A</cell></row><row><cell>pages 143-</cell><cell>Velastin. A unified deep framework for joint 3d pose</cell></row><row><cell>152, 2020. 2, 8, 1</cell><cell>estimation and action recognition from a single rgb</cell></row><row><cell>[22] Chenxu Luo, Xiao Chu, and Alan Yuille. Orinet: A</cell><cell>camera. arXiv preprint arXiv:1907.06968, 2019. 3,</cell></row><row><cell>fully convolutional network for 3d human pose esti-</cell><cell>6</cell></row><row><cell>mation. arXiv preprint arXiv:1811.04989, 2018. 7</cell><cell>[33] Chiara Plizzari, Marco Cannici, and Matteo Mat-</cell></row><row><cell>[23] Diogo C Luvizon, David Picard, and Hedi Tabia.</cell><cell>teucci. Spatial temporal transformer network for</cell></row><row><cell>2d/3d pose estimation and action recognition using</cell><cell>skeleton-based action recognition. arXiv preprint</cell></row><row><cell>multitask deep learning. In Proceedings of the IEEE</cell><cell>arXiv:2008.07404, 2020. 8</cell></row><row><cell>Conference on Computer Vision and Pattern Recogni-</cell><cell>[34] Mir Rayat Imtiaz Hossain and James J Little. Exploit-</cell></row><row><cell>tion, pages 5137-5146, 2018. 6</cell><cell>ing temporal information for 3d human pose estima-</cell></row><row><cell>[24] Julieta Martinez, Rayat Hossain, Javier Romero, and</cell><cell>tion. In Proceedings of the European Conference on</cell></row><row><cell>James J Little. A simple yet effective baseline for 3d</cell><cell>Computer Vision (ECCV), pages 68-84, 2018. 7, 1</cell></row><row><cell>human pose estimation. In Proceedings of the IEEE</cell><cell>[35] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang</cell></row><row><cell>International Conference on Computer Vision, pages</cell><cell>Wang. Ntu rgb+ d: A large scale dataset for 3d human</cell></row><row><cell>2640-2649, 2017. 1, 3, 5, 6, 7, 0, 2</cell><cell>activity analysis. In Proceedings of the IEEE con-</cell></row><row><cell>[25] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal</cell><cell>ference on computer vision and pattern recognition,</cell></row><row><cell>Fua, Oleksandr Sotnychenko, Weipeng Xu, and Chris-</cell><cell>pages 1010-1019, 2016. 8, 1</cell></row><row><cell>tian Theobalt. Monocular 3d human pose estimation</cell><cell>[36] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.</cell></row><row><cell>in the wild using improved cnn supervision. In 2017</cell><cell>Skeleton-based action recognition with multi-stream</cell></row><row><cell>international conference on 3D vision (3DV), pages</cell><cell>adaptive graph convolutional networks. arXiv preprint</cell></row><row><cell>506-516. IEEE, 2017. 5</cell><cell>arXiv:1912.06971, 2019. 2, 8, 1, 3</cell></row><row><cell>[26] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotny-</cell><cell>[37] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.</cell></row><row><cell>chenko, Helge Rhodin, Mohammad Shafiei, Hans-</cell><cell>Skeleton-based action recognition with multi-stream</cell></row><row><cell>Peter Seidel, Weipeng Xu, Dan Casas, and Christian</cell><cell>adaptive graph convolutional networks. IEEE Trans-</cell></row><row><cell>Theobalt. Vnect: Real-time 3d human pose estima-</cell><cell>actions on Image Processing, 29:9532-9545, 2020. 8,</cell></row><row><cell>tion with a single rgb camera. ACM Transactions on</cell><cell>1, 2, 3</cell></row><row><cell>Graphics (TOG), 36(4):1-14, 2017. 5</cell><cell>[38] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.</cell></row><row><cell>[27] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked</cell><cell>Deep high-resolution representation learning for hu-</cell></row><row><cell>hourglass networks for human pose estimation. In</cell><cell>man pose estimation. In Proceedings of the IEEE Con-</cell></row><row><cell>European conference on computer vision, pages 483-</cell><cell>ference on Computer Vision and Pattern Recognition,</cell></row><row><cell>499. Springer, 2016. 1</cell><cell>pages 5693-5703, 2019. 1</cell></row><row><cell>[28] Sungheon Park and Nojun Kwak. 3d human pose</cell><cell>[39] Luyang Wang, Yan Chen, Zhenhua Guo, Keyuan</cell></row><row><cell>estimation with relational networks. arXiv preprint</cell><cell>Qian, Mude Lin, Hongsheng Li, and Jimmy S Ren.</cell></row><row><cell>arXiv:1805.08961, 2018. 1, 6</cell><cell>Generalizing monocular 3d human pose estimation in</cell></row><row><cell>[29] Dario Pavllo, Christoph Feichtenhofer, David Grang-</cell><cell>the wild. arXiv preprint arXiv:1904.05512, 2019. 1, 6</cell></row><row><cell>ier, and Michael Auli. 3d human pose estima-</cell><cell>[40] Hailun Xia and Xinkai Gao. Multi-scale mixed dense</cell></row><row><cell>tion in video with temporal convolutions and semi-</cell><cell>graph convolution network for skeleton-based action</cell></row><row><cell>supervised training. In Proceedings of the IEEE Con-</cell><cell>recognition. IEEE Access, 2021. 8</cell></row><row><cell>ference on Computer Vision and Pattern Recognition, pages 7753-7762, 2019. 1, 6, 7, 0</cell><cell>[41] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomo-hiro Sonobe, Ken-ichi Kawarabayashi, and Ste-</cell></row><row><cell>[30] Wei Peng, Xiaopeng Hong, Haoyu Chen, and Guoy-</cell><cell>fanie Jegelka. Representation learning on graphs</cell></row><row><cell>ing Zhao. Learning graph convolutional network for</cell><cell>with jumping knowledge networks. arXiv preprint</cell></row><row><cell>skeleton-based human action recognition by neural</cell><cell>arXiv:1806.03536, 2018. 1</cell></row><row><cell>searching. In AAAI, pages 2669-2676, 2020. 8, 1</cell><cell>[42] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spa-</cell></row><row><cell>[31] Wei Peng, Jingang Shi, Zhaoqiang Xia, and Guoying</cell><cell>tial temporal graph convolutional networks for</cell></row><row><cell>Zhao. Mix dimension in poincar? geometry for 3d</cell><cell>skeleton-based action recognition. arXiv preprint</cell></row><row><cell>skeleton-based action recognition. In Proceedings of</cell><cell>arXiv:1801.07455, 2018. 8, 1, 2, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 1 :</head><label>1</label><figDesc>Comparison results regarding PA-MPJPE after rigid transformation from the ground truth. We highlight the graphbased methods by ?. ? donates the use of 2D ground truth poses as input. Best results in bold.</figDesc><table><row><cell>Method</cell><cell>LCN</cell><cell>a</cell><cell>b</cell><cell>c</cell><cell>d</cell><cell>e</cell><cell>f</cell><cell>g</cell></row><row><cell>A k</cell><cell>ori</cell><cell cols="7">Only M k (ori) Only M k (dense) Only M k (rand) Only O k M k + O k Eq.8 Eq.8 w/T</cell></row><row><cell>MPJPE(mm)</cell><cell>35.7</cell><cell>34.8</cell><cell>35.5</cell><cell>41.2</cell><cell>46.1</cell><cell>34.3</cell><cell>34.0</cell><cell>33.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the effects of dynamic graph learning A in a Non-hierarchy strategy. ori is the static graph with physical connections, shown in LCN<ref type="bibr" target="#b5">[6]</ref>. Baseline takes A k as ori. Only M k (?) denotes applying M k with different initialization. Only O k keeps the dynamic offset in Eq.8. M k + O k equals to set ? = 1 in Eq.8. w/T represents the temporal-aware scheme defined in Sec.3.3.</figDesc><table><row><cell>F</cell><cell>(1,1)</cell><cell>(3,1)</cell><cell>(3,1) w/st.=2</cell><cell>(3,1) w/di.=2</cell><cell>(5,1)</cell><cell>(7,1)</cell></row><row><cell>HCSF</cell><cell>30.8</cell><cell>30.4</cell><cell>30.7</cell><cell>30.7</cell><cell>30.6</cell><cell>30.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 3 :</head><label>3</label><figDesc>The impact of settings F of temporal convolution in dynamic graph learning of 3D human pose estimation. st. is an abbreviation for stride, and di. is dilation.M k (rand). The weighted graph M k (ori) can also surpass the same weighted graph in LCN. Moreover, only learning graph structure from features increase the error from 35.7mm to 46.1, which is infeasible. After combining the weighed graph M k (ori) with the dynamic offset O k , we can obtain 0.5mm improvement. Furthermore, considering a dynamic scale ? to control the influence of the dynamic offsets, which is the formula in Eq.8, will be helpful. Last, we can observe that the temporal-aware scheme can boost the performance, decreasing the MPJPE from 34.0mm to 33.5mm.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>2.1, Sec. 2.2 and Sec. 2.3, respectively. 2.1. Dataset and Implementation Details 2.1.1 Data Description NTU RGB+D 60 [35] is one of the most widely used indoor RGB+Depth action recognition dataset with 60 actions. They include daily, mutual, and health-related actions. NTU RGB+D 60 has 40 subjects under three cameras. Following</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracy (%) is used as the evaluation metric. The best result in each K is in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 5 :</head><label>5</label><figDesc>The impact of decay rate d under static matrix G, dynamic graph from M k , and dynamic graph from A k in Eq.8.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 6 :</head><label>6</label><figDesc>Comparison on various multiple hop structures under static matrix G K , dynamic graph from M k , and a dynamic graph from A k . Top-1 accuracy is used as the evaluation metric.</figDesc><table><row><cell>F</cell><cell>(1,1)</cell><cell>(3,1)</cell><cell>(3,1) w/st.=2</cell><cell>(3,1) w/di.=2</cell><cell>(5,1)</cell><cell>(7,1)</cell></row><row><cell>HCSF</cell><cell>94.7</cell><cell>95.3</cell><cell>95.0</cell><cell>94.8</cell><cell>95.1</cell><cell>94.7</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Innovation and Technology Fund (ITF) under Grant No. MRP/022/20X, Innovation and Technology Commission, Hong Kong S.A.R.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00067</idno>
		<title level="m">Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decoupling gcn with dropgraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lookhops: light multi-order convolution and pooling for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li S Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15741,2020.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficultyaware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08289</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">Lisboa</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning global pose features in graph convolutional networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic gcn: Contextenriched topology learning for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanfan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14690</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Autoslim: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Hop-aware dimension optimization for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14490</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09389</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11147</idno>
		<title level="m">Hop-hop relation-aware graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semanticsguided neural networks for efficient skeleton-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1112" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhulin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanguang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01861</idno>
		<title level="m">Rethinking the number of channels for the convolutional neural network</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High-order graph convolutional networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
