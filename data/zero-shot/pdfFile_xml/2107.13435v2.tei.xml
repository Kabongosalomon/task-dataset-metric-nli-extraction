<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Liang</surname></persName>
							<email>zliang6@nd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
							<email>xzhang33@nd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qin</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
							<email>yslan@dase.ecnu.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
							<email>shaojie@uestc.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">Science and Technology of China</orgName>
								<orgName type="institution">University of Electronic</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>jzhanggr@conect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MWP-BERT: Numeracy-Augmented Pre-training for Math Word Problem Solving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model (PLM) techniques and build MWP-BERT, an effective contextual number representation PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and several MWP-specific understanding tasks on both English and Chinese benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent works in math word problem (MWP) solving <ref type="bibr" target="#b32">(Wang et al., 2018</ref><ref type="bibr" target="#b22">Liu et al., 2019a;</ref><ref type="bibr" target="#b38">Xie and Sun, 2019;</ref><ref type="bibr" target="#b42">Zhang et al., 2020b;</ref><ref type="bibr" target="#b35">Wu et al., 2020;</ref><ref type="bibr" target="#b24">Qin et al., 2021;</ref><ref type="bibr" target="#b12">Huang et al., 2021;</ref><ref type="bibr" target="#b36">Wu et al., 2021a;</ref><ref type="bibr" target="#b39">Yu et al., 2021;</ref><ref type="bibr" target="#b26">Shen et al., 2021)</ref> arrange the pipeline into a sequenceto-sequence framework. In brief, they use deep representation and gradient optimization as well as symbolic constraints to discover discrete symbolic combinations of operators and variants. Fundamentally, MWP solving system aims to perform symbolic reasoning by searching through a combinatorial solution space given the text description evidences. Thus, these neurosymbolic methods mainly focus on getting more effective semantic representations <ref type="bibr" target="#b42">Zhang et al., 2020b;</ref><ref type="bibr" target="#b35">Wu et al., 2020</ref><ref type="bibr" target="#b36">Wu et al., , 2021a</ref><ref type="bibr" target="#b39">Yu et al., 2021)</ref>, injecting symbolic constraints <ref type="bibr" target="#b32">(Wang et al., 2018</ref><ref type="bibr" target="#b22">Liu et al., 2019a;</ref><ref type="bibr" target="#b38">Xie and Sun, 2019)</ref> and how to align semantic space (text descriptions) and huge combinatorial symbolic space (symbolic solutions) <ref type="bibr" target="#b24">(Qin et al., 2021;</ref><ref type="bibr" target="#b26">Shen et al., 2021;</ref><ref type="bibr" target="#b12">Huang et al., 2021)</ref>. This line of methods has achieved great success and is still holding the lead in various MWP solving benchmarks <ref type="bibr" target="#b34">(Wang et al., 2017;</ref><ref type="bibr" target="#b44">Zhao et al., 2020;</ref><ref type="bibr" target="#b15">Koncel-Kedziorski et al., 2016)</ref>.</p><p>Despite the great performance achieved by the previous methods, there still exists fundamental challenges in number representation for MWP solving. More exactly, number values are required to be considered as vital evidence in solution exploration but existing works are known to be inefficient in capturing numeracy information <ref type="bibr" target="#b31">(Wallace et al., 2019)</ref>. Intuitively, we could simply treat explicit numbers in the same way with words, i.e., assign position for all numbers in the vocabulary. However, there would be an infinite number of candidates during prediction and it would be impossible to learn their deep representations. In other words, the solution space will be extremely large and the complexity is unacceptable. Therefore, almost all existing works follow the number mapping technique <ref type="bibr" target="#b34">Wang et al. (2017)</ref> to replace all numbers with symbolic placeholders (e.g., "x1", "x2"). The core idea here is to get a reasonable solution space by restricting neural networks to leave out numerical characteristics and focus on logic reasoning. However, most of the current MWP solvers do not consider the background knowledge in the context and are usually inefficient in capturing numeracy properties. An example is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Small perturbations in the problem description actually bring large variations in reasoning logic and equation. If the model simply regards "75" and "10%" as the same placeholder "x3", and does not notice the small variation in the context, a wrong solution will be generated.</p><p>To this end, we incorporate several numeracy grounded pre-training objectives to inject inductive bias about numerical constraints into dynamic representations. Compared with word candidate sets, useful points in number candidate space are scattered sparsely. However, we identify that during prediction, what matters is the reusable numerical properties of number values. What's more, these properties do not suffer from the sparsity issues of specific values in MWPs. Therefore, compared with assigning a prototype vector for each single number value like <ref type="bibr" target="#b37">(Wu et al., 2021b)</ref>, it is more reasonable to inject the reusable numerical properties in deep representations, e.g., magnitude and number type. In this work, we propose to design numeracy grounded pre-training objectives to implement soft constraints between symbolic placeholders and numbers in deep representation.</p><p>Contributions. We present a suite of numeracyaugmented pre-training tasks with consideration of reasoning logic and numerical properties. More exactly, we introduce several novel pre-training tasks with access to different levels of supervision signals to make use of more available MWP data.</p><p>? One basic group of pre-training tasks is de-signed for the self-supervised setting. Except for masked language modeling (MLM), we give extra consideration to number-related context information by designing related objectives.</p><p>? Another set of pre-training objectives is for the weakly-supervised setting that has only answer annotations but no equation solutions. With access to the answer value, we introduce several tasks to determine the type and the value of the answer.</p><p>? The final set is for the fully-supervised setting, where both solution equation and answer are available for the MWPs.</p><p>Besides, a group of numeracy grounded pretraining objectives is designed to leverage the corpus of MWP and encourage the contextual representation to capture numerical information. Experiments conducted on both Chinese and English benchmarks show the significant improvement of our proposed approach over all competitors. To our knowledge, this is the first approach that surpasses human performance  in terms of MWP solving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Math Word Problems Solving. There exist two major types of MWP, equation set MWP <ref type="bibr" target="#b34">(Wang et al., 2017;</ref><ref type="bibr" target="#b44">Zhao et al., 2020)</ref> and arithmetic MWP <ref type="bibr" target="#b11">Huang et al., 2016)</ref>. This work focuses on arithmetic MWP, which is usually paired with one unknown variable. Along the path of the MWP solver's development, the pioneer studies use traditional rule-based methods, machine learning methods and statistical methods <ref type="bibr" target="#b40">(Yuhui et al., 2010;</ref><ref type="bibr" target="#b16">Kushman et al., 2014;</ref><ref type="bibr" target="#b28">Shi et al., 2015;</ref><ref type="bibr" target="#b14">Koncel-Kedziorski et al., 2015)</ref>. Afterwards, inspired by the development of sequence-to-sequence (Seq2Seq) models, MWP solving has been formulated as a neurosymbolic reasoning pipeline of translating language descriptions to mathematical equations with encoder-decoder framework <ref type="bibr" target="#b32">(Wang et al., 2018</ref><ref type="bibr" target="#b42">Zhang et al., 2020b;</ref><ref type="bibr" target="#b39">Yu et al., 2021;</ref><ref type="bibr" target="#b36">Wu et al., 2021a)</ref>. By fusing hard constraints into decoder <ref type="bibr" target="#b3">(Chiang and Chen, 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2019a;</ref><ref type="bibr" target="#b38">Xie and Sun, 2019;</ref><ref type="bibr" target="#b27">Shen and Jin, 2020;</ref><ref type="bibr" target="#b41">Zhang et al., 2020a)</ref>, MWP solvers achieve much better performance then. Several works propose to utilize multi-stage frameworks <ref type="bibr" target="#b12">Huang et al., 2021;</ref><ref type="bibr" target="#b26">Shen et al., 2021;</ref><ref type="bibr" target="#b21">Liang and Zhang, 2021)</ref> to make more robust solvers. Also, several new works made attempts to improve MWP solver beyond supervised settings <ref type="bibr">(Hong et al., 2021a,b)</ref>. Among all these previous studies, the most relevant ones to our work can be categorized into two groups. First, it has been noted that number values and mathematical constraints play a significant role in supporting numerical reasoning. <ref type="bibr" target="#b37">Wu et al. (2021b)</ref> proposed several number value features to enhance encoder and <ref type="bibr" target="#b24">Qin et al. (2021)</ref> designed new auxiliary tasks to enhance neural MWP solvers. Compared with their work, we first introduce pretraining language model (PLM) and concentrate on representation learning to resolve numerical understanding challenges. Second, regarding the usage of pre-training techniques for MWP solving, <ref type="bibr" target="#b26">Shen et al. (2021)</ref> introduced BART-based <ref type="bibr" target="#b18">(Lewis et al., 2020)</ref> MWP solver and incorporated specialized multi-task training for obtaining more effective pre-training Seq2Seq models for MWP. Compared with them, our work focuses on the number representation learning issue of MWP and achieves a more flexible pre-training representation module for MWP solving, which can be applied in various MWP related tasks other than solution generation.</p><p>Numeracy-aware Pre-training Models. Number representation has been recognized as one of the main issues in word representation learning. Existing methods make use of value, exponent, subword and character methods <ref type="bibr" target="#b30">(Thawani et al., 2021)</ref> to obtain number representations for explicit number values. These methods are known to be less effective in extrapolation cases like testing with numbers not appearing in the training corpus.</p><p>Previous related works <ref type="bibr" target="#b1">(Andor et al., 2019;</ref><ref type="bibr" target="#b31">Wallace et al., 2019;</ref><ref type="bibr" target="#b7">Geva et al., 2020)</ref> mainly focus on shallow numerical reasoning tasks shown in DROP dataset <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>, which usually serves as a benchmark for evaluating numerical machine reading comprehension (Num-MRC) performance. Compared with MWP solving, Num-MRC's main focus is laid on extracting answer spans from a paragraph, which are more fault-tolerant with no needs to predict number tokens. Besides, their solution generation tasks only contain simple computations like addition/subtraction and there are only integers in DROP. More exactly, several research efforts have been made to deal with this kind of math-related reading comprehension task by synthesizing new training examples <ref type="bibr" target="#b7">(Geva et al., 2020)</ref>, incorporating special modules considering the numerical operation <ref type="bibr" target="#b1">(Andor et al., 2019)</ref> and designing specific tokenization strategies <ref type="bibr" target="#b43">(Zhang et al., 2020c)</ref>. Since MWP solving requires further consideration of the complex composition of reasoning logics in MWP text, the symbolic placeholder is more effective in MWP solving. Thus, instead of dealing with explicit number values, our work focuses on improving representation for symbolic placeholders by injecting numerical properties in a probabilistic way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>The input to an MWP solver is a textual description, we denote it as W with length m, thus W = {w 1 , w 2 , ..., w m }. We also define a subset W q of W which contains all the quantities appeared in W . The output is an equation showing how to get the final answer. We denote it as A with length n, where A = {a 1 , a 2 , ..., a n }. The vocabulary of A contains three parts, namely V op , V num and V cons . V op is the vocabulary for all operators, i.e. +, ?, ?, ? and ? . The vocabulary of quantities V num is constructed by number mapping <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, which transforms quantities in different MWPs into a unified representation. More specifically, V num does not contain the actual value of quantities appeared in W , and those quantities are denoted as {n 1 , n 2 , ..., n k }, where n i means the i-th number from W and k is the maximum number of quantities in V num in order to fix the size of it. V cons contains necessary constant values e.g., ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PLM Encoder</head><p>Our PLM encoder maps the problem description W into a representation matrix Z ? R m * h where h is the dimension of the hidden feature.</p><formula xml:id="formula_0">Z = encoder(W ).</formula><p>(1)</p><p>The representation vector corresponding to each word in Z will be used in the decoding process for generating the solution.</p><p>An overview of pre-training objectives and our model architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In general, pre-training objectives are designed to inject contextual priori and numerical properties as soft constraints for representation learning. They are categorized into three types given provided training signals, i.e., self-supervised, weakly-supervised, and fully-supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-supervised Objectives</head><p>In this part, we only consider input text descriptions for each example. Also, these objectives can alleviate the costs of collecting MWP corpus by constructing supervision signals without solution answers and equations.</p><p>Masked Language Modeling. We follow <ref type="bibr" target="#b5">Devlin et al. (2019)</ref> and introduce masked language modeling (MLM) for basic contextual representation modeling. Specially, we apply masks on 10% of tokens, randomly replace 10% of tokens with other tokens and keep 80% of tokens unchanged. Later, the manipulated sentence is utilized to reconstruct the original sentence.</p><p>Number Counting. Another pre-training objective is to predict the amount of numbers that appeared in MWP description. The amount of a number corresponds with the cardinality of variable sets. This also reflects the basic understanding about the difficulty of an MWP and can act as a key contextual MWP number understanding feature. Here, we introduce a regression objective with the following formulated loss function:</p><formula xml:id="formula_1">L N umCount = M SE(F F N (Z), |W q |), (2)</formula><p>where M SE stands for mean-squared-error and FFN stands for the feed-forward network which is made up of two fully-connected (FC) layers and one ReLU activation. We build a two-FC-layer block for each pre-training task (except MLM) and discard them during fine-tuning.Z ? R h is the mean vector of Z and represents the encoder's overall understanding of a single MWP text description. |W q | is the number of quantities shown in the problem description W .</p><p>Number Type Grounding. This objective aims at linking contextual number representations with corresponding number types to tell the difference between discrete and continuous concepts/entities. For numerical reasoning in MWP solving, we only need to handle whole numbers as well as noninteger numbers (decimal, fraction and percentage). Ideas here are that whole numbers usually associate with discrete entities (for example, desks, chairs and seats) while non-integer numbers often connect with continuous concepts (for example, proportions, rate, velocity). Besides, comparisons among whole numbers got different issues compared with rational numbers. Therefore, we propose a classification objective to predict if a number is a whole number or non-integer number:</p><formula xml:id="formula_2">L N T Ground = i:W i ?Wq CE(F F N (Z i ), y i ), (3)</formula><p>where W q contains all the numbers that appeared in W , and CE is the cross-entropy loss for binary classification. Here, i is the index when W i is a quantity, Z i is the corresponding representation vector, and y i is a binary label indicating if W i is a whole number or non-integer number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Weakly-supervised Objectives</head><p>Given both text descriptions of MWPs and corresponding answers, we can model dependencies among answer number and numbers in text descriptions so that contextual representation perceive the existence of the target variable number that does not appear in the text descriptions. In detail, we design 3 novel pre-training objectives specializing in value-annotated MWPs to improve number representation in our MWP-BERT.</p><p>Answer Type Prediction. Determining the type of answer number can provide us discrete/continuous nature of target entity/concept. Thus, we want to predict type (whole/non-integer) of the answer value given global representations of an MWP (embedded in Z):</p><formula xml:id="formula_3">L AT P red = CE(F F N (Z), y s ),<label>(4)</label></formula><p>where y s is the ground truth label indicating the type of answer number.</p><p>Context-Answer Type Comparison. Besides the global context feature, an MWP-BERT also needs to associate context numbers and answer number (the target number does not explicitly appear in the text). Thus, another objective is proposed to predict if the quantities appeared in the MWP text fall into the same category as the answer (i.e. they are all whole or non-integer):</p><formula xml:id="formula_4">L CAT Comp = i:W i ?Wq CE(F F N (Z i ), y i ? y s ),<label>(5)</label></formula><p>where ? stands for the exclusive-or operator between two binary labels to check if they are the same, the label of a quantity y i and the label of the solution value y s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number Magnitude Comparison.</head><p>Beyond type, the magnitude of a number serves as the foundation of numerical reasoning. By associating magnitudes evaluation with contextual representation, the model can get a better perception about variance over key reasoning cues like time, size, intensity and speed. Let? i indicate if the current quantity W i is greater than the solution value or not. Moreover, the loss function is formulated as:</p><formula xml:id="formula_5">L N umM Comp = i:W i ?Wq CE(F F N (Z i ),? i ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fully-supervised Objectives</head><p>Given both equations and answers for MWPs, we can design fully-supervised training tasks to associate number representation with reasoning flows (solution equation). Mathematical equations are known to be binary tree structures with operators on root nodes and numbers on leaf nodes. The motivation is to encourage models to learn structureaware number representations that encode the information on how to make combinations over atomic operators and numbers. We incorporate two pretraining objectives based on the solution equation tree.</p><p>Operation Prediction. The first one is a quantity-pair relation prediction task that focuses on the local feature of the equation tree. The goal is to predict the operator between two quantity nodes in the solution tree. This is in fact a classification task with 5 potential targets, i.e., +, ?, ?, ? and ?. The loss function of this task is:</p><formula xml:id="formula_6">L OP red = i,j CE(F F N ([Zi; Zj]), op(Wi, Wj)),<label>(7)</label></formula><p>where i, j are two indexes that satisfy W i , W j ? W q and [Z i ; Z j ] ? R 2h is the concatenation of Z i and Z j for the quantity W i and W j . op(W i , W j ) returns the operator between W i and W j .</p><p>Tree Distance Prediction. Another pre-training objective is to incorporate the global structure of the equation tree in a quantitative way. Inspired by <ref type="bibr" target="#b8">Hewitt and Manning (2019)</ref>, we consider the depth of each number and operator on the corresponding binary equation tree to be the key structure priori. Thus, we design another fully-supervised objective to utilize this information. More exactly, given the representation of two number nodes in an equation tree, this is a regression problem that predicts the distance (difference of their depth) between them. The loss is formulated as:</p><formula xml:id="formula_7">L T P red = i,j M SE(F F N ([Zi; Zj]), d(Wi, Wj)), (8) where d(W i , W j )</formula><p>is the distance between quantity W i and W j in the solution tree.</p><p>The final pre-training objective is the summation of Equation 2~8 and the masked language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Fine-Tuning</head><p>To investigate the mathematical understanding ability of our pre-training MWP-BERT, we evaluate our model of MWP solving, quantity tagging and 7 probing tasks. Moreover, we not only use BERT but also RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019b)</ref> as the backbone of our encoder to show the adaptiveness of proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present several empirical results with octopus evaluation settings <ref type="bibr" target="#b2">(Bender and Koller, 2020)</ref> to prove the superiority of MWP-BERT and MWP-RoBERTa solver. In section 4.1, we illustrate the application of both solvers in the generation scenario, MWP solving, by fine-tuning them with a specific decoder <ref type="bibr" target="#b38">(Xie and Sun, 2019)</ref>. Next, we present MWP probing tasks in section 4.3 to evaluate the capability of MWP-BERT and MWP-RoBERTa on "understanding" or "capturing the meanings" of MWPs. Finally, results and analysis about ablation study are illustrated in section 4.4.</p><p>Implementation Details. We pre-train our model on 4 NVIDIA TESLA V100 graphic cards and fine-tune on 1 card. The model was pre-trained for 50 epochs (2 days) and fine-tuned for 80 epochs (1 day) with a batch size of 32. Adam optimizer (Kingma and Ba, 2014) is applied with an initial learning rate of 5e-5, which would be halved every 30 epochs. Dropout rate of 0.5 is set during training to prevent over-fitting. During testing, we use 5-beam search to get reasonable solutions. The hyper-parameters setting of our BERT and RoBERTa is 12 layers of depth, 12 heads of attention and 768 dimensions of hidden features. For the Chinese pre-training model, we use an upgrade patch of Chinese BERT and RoBERTa which are pre-trained with the whole word masking (WWM) 1 <ref type="bibr" target="#b4">(Cui et al., 2020)</ref>. For the English pre-training models, we use the official source on this website 2 . Our code and data have been open-sourced on Github 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MWP Solving</head><p>Experiment Settings and Datasets. Given a textual description of a mathematical problem, which contains several known variables, MWP solving targets at getting the correct answer for the corresponding question. A solver is expected to be able to predict an equation that can exactly reach the answer value. We conduct experiment based on these benchmarks, Math23k <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, MathQA <ref type="bibr" target="#b0">(Amini et al., 2019)</ref> and Ape-210k <ref type="bibr" target="#b44">(Zhao et al., 2020)</ref>. Since there exist many noisy examples in Ape-210k, e.g., examples without equation annotations or answer values, we re-organize Ape-210k to Ape-clean and Ape-unsolvable, where the training set of Ape-clean and the whole Ape-unsolvable are used for pre-training. For the English MWP, we use the training set of MathQA <ref type="bibr" target="#b0">(Amini et al., 2019)</ref> to perform pre-training. For the implementation of our solver, MWP-BERT is adapted as an encoder to generate intermediate MWP representation for the tree-based decoder in <ref type="bibr" target="#b38">Xie and Sun (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Ape-clean Dataset</head><p>Ape210k is a recently released large MWPs dataset, including 210,488 problems. The problems in Ape210k are more diverse and difficult than those in Math23k as shown in 1. Not only the stronger requirement of common-sense knowledge for getting solutions, but also the missing of ground-truth solution equations or answers, will take extra obstacles for MWP solving. Among all these cases, 1 https://github.com/ymcui/Chinese-BERT-wwm 2 https://huggingface.co/bert-base-uncased and https:// huggingface.co/roberta-base 3 https://github.com/LZhenwen/MWP-BERT Unsolvable problem 1:</p><p>The price of a ball is 6 yuan, and the price of a basketball is less than 13 times of the ball's price. How much might the price of the basketball be? Answer:</p><p>? Unsolvable problem 2:</p><p>x is a single digit and the quotient of x72/47 is also a single digit, what is x at most? Answer:</p><p>3 Unsolvable problem 3:</p><p>In the yard there were 25 chickens and rabbits. Together they had 80 legs. How many rabbits were in the yard? Answer:</p><p>(80-25)*2/(4-2) = 15   <ref type="bibr" target="#b39">(Yu et al., 2021)</ref> builds hierarchical reasoning encoder in parallel with PLM encoder. REAL <ref type="bibr" target="#b12">(Huang et al., 2021)</ref> proposes a human-like analogical auxiliary learning strategy. EEH-G2T <ref type="bibr" target="#b36">(Wu et al., 2021a)</ref> injects edge label information and the long-range word relationship into graph network. Gen&amp;Rank <ref type="bibr" target="#b26">(Shen et al., 2021</ref>) designs a multi-task learning framework for adapting BART in MWP solving. BERT-CL  incorporates contrastive learning strategy with PLM. To avoid the implementation error that may cause unreproducible results of baseline models, we reported the results of these baselines from the papers where they were published, as many previous papers <ref type="bibr" target="#b42">(Zhang et al., 2020b;</ref><ref type="bibr" target="#b27">Shen and Jin, 2020)</ref> did. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our MWP-BERT achieves competitive results. It is worth noting that we perform strict pre-training paradigm in MWP solving, i.e., our results come from pre-training on the different annotated MWP examples that will be applied in further fine-tuning. Here, our pre-training only uses Ape-clean and Ape-unsolvable and fine-tuning only uses Math23k/MathQA. RPKHS, REAL and BERT-CL all incorporate BERT in their model architecture, which are orthogonal to our work. Our MWP-BERT can be utilized as an MWP-specific checkpoint for their encoder part to improve their performance. Besides, REAL, BERT-CL and Gen &amp; Rank are all trying to make Seq2Seq pre-training <ref type="bibr" target="#b18">(Lewis et al., 2020)</ref>, which adapt both pre-training encoder as well as pretraining decoder for MWP solving. Compared with them, our model focuses on encoder pre-training and aims at obtaining better MWP representation that can be widely applied across various MWP related tasks (like quantity tagging, MWP question generation).</p><p>Another interesting observation is that BERTbased models perform better on Chinese MWP datasets while RoBERTa-based models are good at English MWP datasets. Because the Chinese RoBERTa used in this paper is actually a BERT model that uses BERT tokenization but is trained like RoBERTa (drops the Next Sentence Prediction task). Similar behaviors can be observed in <ref type="bibr" target="#b4">Cui et al. (2020)</ref>. For English setting, RoBERTa performs better than BERT, which is consistent with conclusions raised in <ref type="bibr" target="#b23">Liu et al. (2019b)</ref>.</p><p>Evaluation on Ape-clean and Math23k when being trained by a Joint MWP Set. Moreover, we combine the training set of Math23k and Apeclean to train MWP-BERT, and then measure the accuracy on the testing set of Math23k and Apeclean separately. Results shown in <ref type="table" target="#tab_1">Table 2</ref>   <ref type="table">Table 3</ref>: Comparison of answer accuracy (%) between our proposed models and baselines when they are all trained by the combination of the training set from Apeclean and Math23k dataset.</p><p>vey interesting evaluation observations. Surprisingly, the accuracy of our models on Math23k reaches above 90%, which is marvelously high (previous state-of-the-art methods can hardly reach 80% <ref type="bibr" target="#b27">(Shen and Jin, 2020)</ref>). Compared to the results in <ref type="table" target="#tab_1">Table 2</ref>, even GTS has a higher accuracy when trained with the big joint MWP set of Ape-clean and Math23k. By comparing the performance of corresponding groups between <ref type="table" target="#tab_1">Table 2 and Table 3</ref>, we can learn that our proposed MWP-BERT pre-training paradigm can achieve more significant boosting with more training examples, which proves the effectiveness of our proposed representation learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other MWP Understanding Tasks</head><p>Standard MWP solving is an equation generation task. To make a sufficient validation of the effectiveness of our model on number representations learning, MWP-specific understanding tasks are further considered. Following Hewitt and Manning (2019); <ref type="bibr" target="#b31">Wallace et al. (2019)</ref>, we design several number probing tasks and incorporate quantity tagging <ref type="bibr" target="#b46">(Zou and Lu, 2019b)</ref> to enlarge the MWP understanding evaluation task.</p><p>Following the motivation mentioned in section 3.2, we re-run all the pre-training tasks as probing tasks to evaluate our modeling's understanding ability and test MWP-BERT in a zero-shot scenario, i.e. without fine-tuning the parameters of MWP-BERT and MWP-RoBERTa for the sake of fair comparison. We perform the probing evaluation on both Ape-clean and Ape-unsolvable, except that "OPred" and "TPred" are only evaluated on Ape-clean because they require equation solutions as the ground truth.   <ref type="table" target="#tab_6">Table 4</ref> shows the performances of 4 different PLMs on the above mentioned MWP-specific understanding tasks. Significant improvements can be observed in all the tasks, and demonstrate the effectiveness of our proposed pre-training techniques in improving number representation of PLMs.</p><p>Besides, we borrow an MWP-specific sequence labeling task, quantity tagging <ref type="bibr" target="#b46">(Zou and Lu, 2019b)</ref> ("QT"), to further compose MWP understanding evaluation settings. Quantity tagging <ref type="bibr" target="#b45">(Zou and Lu, 2019a)</ref> is firstly proposed to solve MWP examples with only addition and subtraction operators in their solutions. Briefly speaking, this task requires the model to assign "+", "-" or "None" for every quantity in the problem description and can serve as an MWP understanding evaluation tool to examine the model's understanding of each variable's logic role in the reasoning flow. More exactly, this is also a classification task with 3 possible targets. We extract the corresponding vectors of all quantities according to their positions in encoded problem Z from Equation 1. Next, a 2-layer feed-forward block is connected to output the final prediction.</p><p>Following the setting in baseline method QT (Zou and Lu, 2019a), we perform 3-fold crossvalidation and the results are given in <ref type="table" target="#tab_4">Table 5</ref>, which shows that PLMs benefits from the proposed mathematical pre-training and outperforms the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We run ablation study over the proposed training objectives to investigate the necessity for each of them. As <ref type="table">Table 6</ref> shows, all the proposed objectives can achieve improvements individually. Moreover, only using MLM results in weaker MWP solvers on Math23k (1.4% less) and Ape-clean (1.2% less),   <ref type="table">Table 6</ref>: The experimental results show the effectiveness of every pre-trained task. "Only self-supervised" means we only apply 3 tasks of self-supervised pretraining on the BERT encoder. We also investigate the influence of each task. For example, "w/o MLM" means only performing self-supervised pre-training and discarding the MLM pre-training task.</p><p>which again proves the effectiveness of our proposed pre-training tasks. Since the difficulty level of MWPs is usually in proportion to their solution length, we can easily identify that a set of MWPs exhibit a long-tail distribution over solution length, as well as the difficulty level, as shown in. <ref type="figure">Fig 3 of</ref> the Appendix. Thus, the 87% accuracy of human-level performance in Math23k  indicates that 13% of the MWPs are difficult to solve. Any solvers that can improve the accuracy above 87% are making significant contribution on solving the extremely difficult cases, such as MWPs whose solutions contain ? 4 variables or single variable being used multiple times. As neural models are known to be limited at dealing with these combinational and symbolic reasoning cases <ref type="bibr" target="#b17">(Lee et al., 2020)</ref>, we exam our model on these specially difficult cases. Due to the space limit, we attach several examples of these difficult cases, statistics about solution length distribution and performance for increasing length of solution equations in the Appendix. Besides, it is worth noting that even without MLM objective, our model is able to promote the PLM competitor. Besides, we can observe that linking equation structure and number during pre-training is certainly beneficial for solving MWPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose MWP-BERT, an MWP-specific PLM model with 8 pre-training objectives to solve the number representation issue in MWP. Also, a new dataset Ape-clean is curated by filtering out unsolvable problems from Ape210k, and the filtered MWPs are useful for self-and weakly-supervised pre-training. Experimental results show the superiority of our proposed MWP-BERT across various downstream tasks on generation and understanding. In terms of the most representative task MWP solving, our approach achieves the highest accuracy, and firstly beats human performance. Better numerical understanding ability is also demonstrated in the probing evaluation. We believe that our study can serve as a useful pre-trained pipeline and a strong baseline in the MWP community.</p><p>Problem 1:</p><p>There are 20 questions in an exam. Solving a question correctly gets 5 points, and 1 point is deducted if the answer is wrong. Jack gets 70 points. How many questions did he get right? Answer: 20-(20*5-70)/(5+1) Problem 2:</p><p>Peter is reading a book. He reads 30% of the whole book on the first day, and 15 pages on the second day. The ratio of the number of pages that has been read to the number of pages not read is 2:3. How many pages does this book have? Answer:</p><p>15/(1-((3)/(2+3))-30%) Problem 3:</p><p>There are 72% of 50 students can swim, and (3/5) of 25 girls can swim, how many percent of the boys can swim? Answer:</p><p>(50*72%-25*(3/5))/(50-25) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Accuracy w.r.t. Solution Length</p><p>To better understand the improvement of the MWP solving performance of our model, we evaluate the problems with different lengths of solutions separately. The solution distribution details can be found in <ref type="figure">Figure 3</ref> It is expected that getting longer solutions requires more comprehensive understanding and complex reasoning, like the three difficult examples shown in <ref type="table" target="#tab_7">Table 7</ref>. The results in <ref type="table" target="#tab_9">Table 8</ref> demonstrate that our proposed MWP-BERT overcomes more difficult problems than the vanilla BERT model. Although the statistical improvement from BERT to MWP-BERT is marginal, our method really enhances the mathematical understanding and reasoning ability of PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Case Study</head><p>We perform case study as shown in <ref type="table">Table 9</ref>. Firstly, we choose a difficult problem from Math23k dataset and use 3 different solvers to solve it. Both GTS <ref type="bibr" target="#b38">(Xie and Sun, 2019)</ref> and Graph2Tree <ref type="bibr" target="#b42">(Zhang et al., 2020b)</ref> fail to generate the right solution for it, while our proposed MWP-BERT solves it cor-  rectly. This example shows that our encoder has a stronger capability to understand complex MWPs to guide the tree-based decoder generate correct solutions. Secondly, when solving a pair of 2 similar problems (i.e., problem 1 and problem 2 in <ref type="table">Table  9</ref>), GTS, Graph2Tree and our MWP-BERT successfully solve the former problem. However, the baseline methods GTS and Graph2Tree both fail to solve the latter one. Our MWP-BERT generates the correct answer. This example proves that our probing tasks help the encoder to capture minor variations inside the problem description, leading to more accurate solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficult</head><p>Problem:</p><p>There are totally 48 cars and motorcycles in a parking lot. Each car has 4 wheels and each motorcycle has 3 wheels. If they have 172 wheels in total. How many motorcycles are there in the parking lot? GTS:</p><p>x = 48 + (172 ? 48)/(4 ? 3) () Graph2Tree : x = 48 ? (48 ? 172)/3 () MWP-BERT: x = (48 * 4 ? 172)/(4 ? 3) ( ) Problem 1: Team A and team B are working on a project together. Team A finished (4/15) of the project, and team B finished (2/15) more than Team A . How many percentage did the two teams finish in total? GTS:</p><p>x = (4/15) + (2/15) + (4/15) ( ) Graph2Tree : x = (4/15) + (2/15) + (4/15) ( ) MWP-BERT: x = (4/15) + (2/15) + (4/15) ( ) Problem 2:</p><p>Team A and team B are building a road. Team A builds (4/9), and team B builds (1/9) more than team A. How many percentage does Team B build? GTS:</p><p>x = (4/9) + (1/9) + (4/9) () Graph2Tree : x = (4/9) + (1/9) + (4/9) () MWP-BERT: x = (4/9) + (1/9) ( ) <ref type="table">Table 9</ref>: Our case study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The second question is obtained from the first one by minor modifications. However, their solution equation and corresponding equation tree structure are different from each other. This demonstrates the importance of considering numerical value information and reasoning logic (equation tree) in contextual modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Some workers are producing x1 clothes ... x2 days and x3 clothes ? x4 more ? The overall architecture of our BERT-based MWP solver. Our method enables the solver to learn from unlabeled, incompletely labeled and fully labeled MWPs by different pre-training tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>the problems without answers can not be used for</cell></row><row><cell>fully-supervised setting. Besides, the problems</cell></row><row><cell>without annotated equations but only answer val-</cell></row><row><cell>ues can be used in the weakly-supervised learning</cell></row><row><cell>setting. Therefore, we follow the rules below to se-</cell></row><row><cell>lect the usable problems from Ape210k to construct</cell></row><row><cell>an Ape-clean dataset, which can be used for the</cell></row><row><cell>fully-supervised learning setting. (i). We remove</cell></row><row><cell>all MWPs that have no answer values nor equa-</cell></row><row><cell>tions. (ii). We remove all MWPs that only have</cell></row><row><cell>answer values without equations. (iii). We remove</cell></row><row><cell>all MWPs with a problem length m &gt; 100 or an an-</cell></row><row><cell>swer equation length n &gt; 20, as they will bring ob-</cell></row><row><cell>stacles for training. (iv). We remove all MWPs re-</cell></row><row><cell>quiring external constants except 1 and ?. (v). We</cell></row><row><cell>remove all duplicated problems with the MWPs in</cell></row><row><cell>Math23k, because almost all problems in Math23k</cell></row><row><cell>can be found in Ape-210k. After data filtering,</cell></row><row><cell>the Ape-clean dataset contains 81,225 MWPs, in-</cell></row><row><cell>cluding 79,388 training problems and 1,837 testing</cell></row><row><cell>problems. The remaining 129,263 problems in</cell></row><row><cell>Ape210k are regarded as Ape-unsolvable, which</cell></row><row><cell>can be used in the pre-training tasks in the settings</cell></row><row><cell>of self-supervised and weakly-supervised learning.</cell></row></table><note>This table shows three kinds of discarded MWPs in Ape210k. The first one does not have a cer- tain answer, and the solution of the second one cannot be represented by equations. Solving the third problem requires external constants. Thus we filter those prob- lems out in our Ape-clean dataset.Model Comparison. We first compare our ap- proach with the most recent representative base- lines on the benchmark Math23k dataset. The first baseline is DNS which is the pioneering work us-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of answer accuracy (%) among our proposed models and different baselines. Math23k column shows the results on the public test set and Math23k</figDesc><table><row><cell>is 5-fold cross validation on Math23k</cell></row><row><cell>dataset. MathQA is adapated from Li et al. (2021);</cell></row><row><cell>Tan et al. (2021). "RoBERTa" and "BERT" repre-</cell></row><row><cell>sent results without pre-training. "MWP-RoBERTa"</cell></row><row><cell>and "MWP-BERT" represent first pre-training with</cell></row><row><cell>proposed tasks and then fine-tuning.</cell></row><row><cell>ing the Seq2Seq model to solve MWPs. Math-</cell></row><row><cell>EN (Wang et al., 2018) proposes an equation-</cell></row><row><cell>normalization method and uses vanilla Seq2Seq</cell></row><row><cell>model to get solutions. GTS (Xie and Sun, 2019)</cell></row><row><cell>proposes a goal-driven tree-based decoder and</cell></row><row><cell>achieves great results. Graph2Tree (Zhang et al.,</cell></row><row><cell>2020b) constructs two graphs during data pre-</cell></row><row><cell>processing to extract extra relationships from text</cell></row><row><cell>descriptions. KA-S2T (Wu et al., 2020) proposes</cell></row><row><cell>a novel knowledge-aware model that can incor-</cell></row><row><cell>porate background knowledge. NS-Solver (Qin</cell></row><row><cell>et al., 2021) designs several auxiliary tasks to help</cell></row><row><cell>training. NumS2T (Wu et al., 2021b) uses explicit</cell></row><row><cell>numerical values instead of symbol placeholder</cell></row><row><cell>to encode quantities. RPKHS</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of tagging accuracy (%) between our proposed models and baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>NumCount NTGround ATPred CATComp NumMComp OPred TPred QT</figDesc><table><row><cell>Metric</cell><cell>MSE ?</cell><cell>Acc ?</cell><cell>Acc ?</cell><cell>Acc ?</cell><cell>Acc ?</cell><cell>Acc ? MSE ? Acc ?</cell></row><row><cell>BERT</cell><cell>3.08</cell><cell>0.87</cell><cell>0.75</cell><cell>0.77</cell><cell>0.77</cell><cell>0.50 0.97 84.5</cell></row><row><cell>RoBERTa</cell><cell>3.20</cell><cell>0.86</cell><cell>0.76</cell><cell>0.78</cell><cell>0.77</cell><cell>0.51 0.99 84.6</cell></row><row><cell>MWP-RoBERTa</cell><cell>0.69</cell><cell>0.92</cell><cell>0.86</cell><cell>0.87</cell><cell>0.86</cell><cell>0.86 0.44 91.0</cell></row><row><cell>MWP-BERT</cell><cell>0.67</cell><cell>0.92</cell><cell>0.85</cell><cell>0.87</cell><cell>0.86</cell><cell>0.87 0.45 91.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The evaluation results on MWP-specific understanding tasks. All tasks correspond to the tasks mentioned in section 4. Note that the metric for 2 tasks is mean-squared-error, while others use classification accuracy. "QT" stands for quantity tagging.</figDesc><table><row><cell></cell><cell cols="2">Math23k Ape-clean</cell></row><row><cell>Only MLM</cell><cell>89.8</cell><cell>80.1</cell></row><row><cell>Only self-supervised</cell><cell>90.4</cell><cell>80.9</cell></row><row><cell>w/o MLM</cell><cell>90.1</cell><cell>80.6</cell></row><row><cell>w/o N umCount</cell><cell>89.9</cell><cell>80.5</cell></row><row><cell>w/o N T Ground</cell><cell>90.1</cell><cell>80.4</cell></row><row><cell>Only weakly-supervised</cell><cell>90.1</cell><cell>80.8</cell></row><row><cell>w/o AT P red</cell><cell>89.7</cell><cell>80.2</cell></row><row><cell>w/o CAT Comp</cell><cell>89.7</cell><cell>80.4</cell></row><row><cell>w/o N umM Comp</cell><cell>89.6</cell><cell>80.5</cell></row><row><cell>Only fully-supervised</cell><cell>91.0</cell><cell>80.5</cell></row><row><cell>w/o OP red</cell><cell>90.5</cell><cell>80.3</cell></row><row><cell>w/o T P red</cell><cell>90.6</cell><cell>80.5</cell></row><row><cell>MWP-BERT</cell><cell>91.2</cell><cell>81.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>This table shows three difficult problems in Math23k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>The answer accuracy of BERT and MWP-BERT on problems with different lengths in Math23k. #op denotes the number of operators in the solution. #P is the number of problems of that kind of MWPs in the public test set of Math23k.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2357" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Giving BERT a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5946" to="5951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: on meaning, form, and understanding in the age of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantically-aligned equation generation for solving and reasoning math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: Findings</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Injecting numerical reasoning skills into language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="946" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A structural probe for finding syntax in word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4129" to="4138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning by fixing: Solving math word problems with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ciao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Smart: A situation model for algebra story problems via attributed grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ciao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="887" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recall and learn: A memoryaugmented solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="786" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena</forename><surname>Dumas Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mawps: A math word problem repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1152" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mathematical reasoning in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling intra-relation in math word problems with different functional multi-head attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jierui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6162" to="6167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08464</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Solving math word problems with teacher supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tree-structured decoding for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyv</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2370" to="2379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural-symbolic solver for math word problems with auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5870" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantically-aligned universal tree-structured solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3780" to="3789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generate &amp; rank: A multi-task framework for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2269" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Solving math word problems with multi-encoders and multi-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheqing</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2924" to="2934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1132" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Investigating math word problems using pretrained multilingual language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/2105.08928</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Representing numbers in NLP: a survey and a vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Szekely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="644" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Do NLP models know numbers? probing numeracy in embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5306" to="5314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Translating a math word problem to a expression tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Template-based math word problem solvers with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Tian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7144" to="7151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep neural solver for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A knowledge-aware sequence-to-tree network for math word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7137" to="7146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An edge-enhanced hierarchical graph-to-tree network for math word problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Math word problem solving with explicit numerical values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinzhuo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5859" to="5869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A goal-driven tree-structured neural model for math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5299" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving math word problems with pre-trained knowledge and hierarchical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingpeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3384" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Frame-based calculus of solving arithmetic multi-step addition and subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Guangzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Ronghuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Second International Workshop on Education Technology and Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="476" to="479" />
			<date type="published" when="2010" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Teacher-student networks with multiple decoders for solving math word problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4011" to="4017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graphto-tree learning for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Do language embeddings capture scales?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4889" to="4896" />
		</imprint>
	</monogr>
	<note>Findings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.11506</idno>
		<title level="m">Ape210k: A large-scale and template-rich dataset of math word problems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantity tagger: A latent-variable sequence labeling approach to solving addition-subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5246" to="5251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Text2math: End-toend parsing text into math expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5330" to="5340" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
