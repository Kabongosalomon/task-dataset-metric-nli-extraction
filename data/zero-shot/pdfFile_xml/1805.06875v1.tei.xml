<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
							<email>richard@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
							<email>kuehne@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
							<email>iqbalm@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video learning is an important task in computer vision and has experienced increasing interest over the recent years. Since even a small amount of videos easily comprises several million frames, methods that do not rely on a frame-level annotation are of special importance. In this work, we propose a novel learning algorithm with a Viterbibased loss that allows for online and incremental learning of weakly annotated video data. We moreover show that explicit context and length modeling leads to huge improvements in video segmentation and labeling tasks and include these models into our framework. On several action segmentation benchmarks, we obtain an improvement of up to 10% compared to current state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A continuously growing amount of publicly available video data on YouTube or video streaming services, an increased interest in applications such as surveillance, and the need to analyze continuous video streams e.g. in the domain of autonomous driving has caused an increased interest in video learning algorithms.</p><p>While approaches for action classification on presegmented video clips already perform convincingly well <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>, realistic applications require the segmentation of temporally untrimmed videos that usually contain a large variety of different actions with different lengths. Since acquiring frame-level annotations of such videos is expensive, methods that can learn from less supervision are of particular interest. A popular type of weak supervision are transcripts <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, which provide for each training video an ordered list of actions, but not the frames where the actions occur in the video.</p><p>In order to learn a model for temporal action segmentation with such weak supervision, CNNs or RNNs have been combined with an explicit model for the intraclass temporal progression, e.g. a hidden Markov model (HMM), and the inter-class context, e.g. with a finite grammar <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. While these approaches are particularly suited for videos that contain complex actions and have a  <ref type="figure" target="#fig_0">Figure 1</ref>. The input video x T 1 is forwarded through the network and the Viterbi decoding is run on the output probabilities. The frame labels generated by the Viterbi algorithm are then used to compute a framewise cross-entropy loss based on which the network gradient is computed. huge number of distinct classes, they come with the major problem that their training requires some heuristical ground truth. They rely on a two-step approach that is iterated several times. It consists of first generating a segmentation for each training video using the Viterbi algorithm and then training the neural network as in the fully supervised case using the generated segmentation as pseudo groundtruth. Consequently, the two-step approach is sensitive to the initialization of the pseudo ground-truth and the accuracy tends to oscillate between the iterations <ref type="bibr" target="#b23">[24]</ref>. In contrast to theses methods, CTC <ref type="bibr" target="#b9">[10]</ref> is a framework for weakly supervised sequence learning. However, this approach does not allow to include explicit models for the context between classes and their temporal progression and therefore does not achieve state-of-the-art performance.</p><p>In this work, we propose a novel learning algorithm that allows for direct learning using the input video and ordered action classes only. The approach includes the Viterbidecoding as part of the loss function to train the neural network and has several practical advantages compared to the two-stage approach: it neither suffers from an oscillation effect nor requires a frame-wise labeling as initialization or any kind of pseudo ground-truth, models are learned incrementally, and the accuracy is improved due to direct optimization of the loss function of interest.</p><p>As a second contribution, we also propose to use an explicit length model instead of the widely used HMMs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, allowing to learn the action classes directly rather than intermediate HMM states. In an extensive evaluation, we show an increase of up to 10% in accuracy compared to existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most existing work on temporal action segmentation focuses on a fully supervised task <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b27">28]</ref>. Purely CNN based approaches such as structured segment networks <ref type="bibr" target="#b36">[37]</ref> or temporal convolutional networks <ref type="bibr" target="#b17">[18]</ref> have recently shown convincing results on several action segmentation benchmarks. Similarly, LSTMbased approaches have been in focus <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28]</ref>. However, formulated in a classical deep learning setting, these approaches all rely on fully supervised, i.e. framewise annotated training data. <ref type="bibr">Richard and Gall [23]</ref> propose the use of an explicit statistical language model as well as a length model in a fully supervised formulation. Note that a framelevel annotation is available, so the estimation of the length model is straightforward using the ground-truth length from the training set. Moreover, the length model is learned prior to the actual action classifier. In our approach, on the contrary, no frame-level annotation is provided, so the length model changes over time during the training process and is dependent on all other models.</p><p>First attempts on weakly supervised learning have been made by <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21]</ref>, who try to to obtain training examples based on movie scripts. Duchenne et al. <ref type="bibr" target="#b4">[5]</ref> first addressed the problem of segmenting actions within videos, assuming that a clip contains not only frames of an action but also background frames. Current approaches go much further and try to infer an exact temporal segmentation of multiple action classes within a single video using weakly annotated training data only. In <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, web images are used to guide video learning in a weakly supervised fashion. Wang et al. <ref type="bibr" target="#b33">[34]</ref> use a purely CNN-based approach to weakly supervised action detection, where they use a different type of supervision, namely short unordered action lists. This approach is well suited to detect action occurrences in a video with large background portions but not designed for videos that contain a huge amount of different action classes as in our case. A similar kind of supervision using unordered sets of actions is proposed in <ref type="bibr" target="#b24">[25]</ref>, who also rely on the model factorization proposed by <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b0">[1]</ref>, the task of temporal action segmentation is relaxed to an alignment task: it is assumed that an ordered list of occurring actions is also given for inference, thus it only remains to align those ac-tion classes to the video frames.</p><p>Kuehne et al. <ref type="bibr" target="#b15">[16]</ref> interpret the task of learning an action segmentation system just given an ordered list of occurring actions as supervision as an instance of a speech recognition problem, where the videos correspond to the audio signal and the action classes correspond to words. They apply a standard HMM-GMM system using a speech recognition toolkit. Building upon this idea, Richard et al. <ref type="bibr" target="#b23">[24]</ref> replace the GMM by a recurrent neural network but still rely on an HMM for a coarse temporal modeling. A similar approach has been proposed by Koller et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> for sign language recognition, which is a problem closely related to temporal action segmentation, but with the most significant difference that the actual temporal boundaries of the recognized words/classes are not relevant. Note that in contrast to our proposed method, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] use a two-step optimization scheme that does not allow for direct, sequence-wise training.</p><p>Lin et al. <ref type="bibr" target="#b19">[20]</ref> use the CTC approach in combination with a statistical language model for weakly supervised video learning. However, they only infer the sequence of actions occurring in the video. As an extension of the CTC approach, <ref type="bibr" target="#b10">[11]</ref> propose ECTC that takes visual similarities between the frames into account to avoid degenerate segmentations. In contrast to our method, this approach does not allow to include explicit context and length models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Action Segmentation</head><p>We address the problem of temporally localizing activities in a video x T 1 = (x 1 , . . . , x T ) with T frames. The task is to find a segmentation of a video into an unknown number of N segments and to output class labels c N 1 = (c 1 , . . . , c N ) and lengths l N 1 = ( 1 , . . . , N ) for each of the N segments. Using a background class for uninteresting frames, each frame can be assigned to a segment. For terms of simplicity, we refer to the label assigned to frame x t as c n(t) , where n(t) is the number of the segment frame t belongs to. Putting the task in a probabilistic setting, we aim to find the most likely video labeling given the video frames, i.e.</p><formula xml:id="formula_0">(? N 1 ,l N 1 ) = arg max c N 1 ,l N 1 p(c N 1 , l N 1 |x T 1 ) .<label>(1)</label></formula><p>State-of-the-art methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref> formulate p(c N 1 , l N 1 |x T 1 ) in such a way such that the arg max can be efficiently computed using a Viterbi-like algorithm. Depending on the approach, the models are either trained in a fully supervised setting <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref>, which requires a very timeconsuming frame-wise labeling of the training videos, or in a weakly supervised setting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>. In the latter case, the training videos are annotated only by an ordered sequence of action classes that occur in the video. This means each training instance is a tuple (x T 1 , c N 1 ) consisting of a video x T 1 and a transcript sequence c 1 ? ? ? ? ? c N that defines the ordering of occurring actions. In contrast to the fully supervised setting, l N 1 and accordingly the framelevel annotation of the training data is unknown.</p><p>In this work, we focus on the problem of weakly supervised learning and propose two contributions. The first contribution addresses the modeling of p(c N 1 , l N 1 |x T 1 ). Instead of using a hidden Markov model as in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>, we explicitly model the length of each action class. The model is described in Section 5 and in our experiments we show that the proposed length model outperforms an HMM. The second contribution is a more principled approach for weakly supervised learning. This approach is described in Section 4 and can be used to train any model that uses neural networks and Viterbi decoding such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NeuralNetwork-Viterbi</head><p>Before we describe the proposed learning approach in Section 4.1, we briefly summarize the training in a fully supervised setting and the training procedure that is used in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref> for weakly supervised learning.</p><p>In a classical fully supervised training setup, frame-wise ground-truth annotation is provided for the training data, i.e. each training video comprises the triple (x T 1 , c N 1 , l N 1 ). Since l N 1 and therefore the label c n(t) for each frame x t is known, the underlying model for Equation <ref type="formula" target="#formula_0">(1)</ref>, which is typically a neural network, is trained using the frame-level annotations and, for instance, the cross-entropy loss.</p><p>If only the transcript of a training video, i.e. an ordered sequence of classes that occur in the video, is given, l N 1 is unknown and only (x T 1 , c N 1 ) is provided. Most existing weakly supervised approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref> reduce the problem to the fully supervised case by generating a pseudo ground-truth c pseudo n(t) for all training sequences. A neural network is then trained using a pseudo cross-entropy loss that is based on the pseudo ground-truth c pseudo n(t) . This approach comes with a major problem: The model learning and transcript decoding (i.e. pseudo ground-truth generation) are separated and the transcripts c N 1 are only used for the pseudo ground-truth c pseudo n(t) generation. In other words, the model learning does not explicitly include the transcripts. As a workaround, the two steps pseudo groundtruth generation and model learning are repeated several times, where the pseudo ground-truth in the first iteration is a uniform alignment of transcripts to sequence frames. In later repetitions, the pseudo ground-truth is generated using a Viterbi decoding on Equation (1) with the previously trained network. From a practical point, this results in several major limitations. As it was reported in <ref type="bibr" target="#b23">[24]</ref>, the approach is sensitive to the initialization of the pseudo groundtruth and the accuracy tends to oscillate between the iterations. Furthermore, the approach processes in each step the entire dataset, which prevents its use for incremental learning.</p><p>In this work, we propose a new framework that allows to learn directly from the transcripts. Therefore, we define a loss that can be computed solely based on the current model and a single training example (x N 1 , c N 1 ). The loss is designed to be zero if</p><formula xml:id="formula_1">p(c N 1 , l N 1 |x T 1 ) = p(c N 1 , l N 1 |x T 1 , c N 1 ),<label>(2)</label></formula><p>i.e. if the prediction without given transcripts (left hand side) is equal to the prediction with given transcripts (right hand side). Particularly, our approach does not require a precomputed pseudo ground-truth and works directly on the weakly annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Viterbi-based Network Training</head><p>Our new training procedure is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. The training algorithm randomly draws a sequence x T 1 and its annotation c N 1 from the training set. The sequence is then forwarded through a neural network. Note that there are no constraints on the network architecture, all commonly used feed-forward networks, CNNs, and recurrent networks can be used. The optimal segmentation by means of Equation <ref type="formula" target="#formula_0">(1)</ref> is then computed by application of a Viterbi decoding on the network output, see Section 5.1 for details. Since c N 1 is provided as annotation, only l N 1 needs to be inferred during training. We switch notation and write the Viterbi segmentation (c N 1 , l N 1 ) as framewise labels c n(1) , . . . , c n(T ) , with which the cross-entropy loss over all aligned frames is accumulated:</p><formula xml:id="formula_2">L = ? T t=1 log p(c n(t) |x t ).<label>(3)</label></formula><p>We chose the cross-entropy loss as it is most common in neural network optimization. However, our framework is not bound to a specific loss function. Once the Viterbi segmentation of the input sequence is computed, any other loss such as squared-error can as well be used. Based on the sequence loss L, the network parameters are updated using stochastic gradient descent with the gradient ?L of the loss. We would like to emphasize that the algorithm operates in an online fashion, i.e. in each iteration, the loss L is computed with respect to a single randomly drawn training sequence (x T 1 , c N 1 ) only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Enhancing the Robustness</head><p>In practice, a sequence x T 1 can easily be a few thousand frames long. Backpropagating all frames at once can thus raise problems with the limited GPU memory. Moreover, online learning algorithms generally benefit from making a large number of model updates. Therefore, we split the sequence into multiple mini-batches after the Viterbi segmentation c n(1) , . . . , c n(T ) has been computed. These minibatches are then backpropagated one-by-one through the network.</p><p>However, traditional online learning algorithms such as stochastic gradient descent rely on the assumption that</p><formula xml:id="formula_3">L * (w) = E x L(x, w) = L(x, w)dP(x),<label>(4)</label></formula><p>where w denotes the model parameters, L * (w) is the true loss that is to be optimized, and L(x, w) is the loss of a single observation x, see e.g. <ref type="bibr" target="#b1">[2]</ref>. In each iteration, the observations x are usually assumed to be drawn independently from the distribution P(x). In our setting, on the contrary, all frames in an iteration belong to the same sequence x T 1 , so they are not independent. Further subdividing long sequences into smaller mini-batches enhances the problem: multiple updates are made with a strong bias towards (a) the characteristics of the sequence frames and (b) the limited amount of classes occurring in the sequence.</p><p>We therefore propose to use a buffer B and store recently processed sequences and their inferred frame labels. In order to make the gradient in each iteration more robust, K frames from the buffer are sampled and added to the loss function,</p><formula xml:id="formula_4">L = ? T t=1 log p(c n(t) |x t ) + K k=1 log p(c k |x k ) .<label>(5)</label></formula><p>Since the neural network is updated gradually in small steps, most of the frame/label pairs in the buffer still agree with the current model. However, sampling random frames from the buffer lessens the above-mentioned sequence bias from the loss function and increases the robustness of the optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The Model</head><p>We now introduce the specific model used in this paper. Starting from Equation <ref type="formula" target="#formula_0">(1)</ref>, we factorize the overall probability p(c N 1 , l N 1 |x T 1 ),</p><formula xml:id="formula_5">(? N 1 ,l N 1 ) = arg max c N 1 ,l N 1 p(c N 1 , l N 1 |x T 1 ) = arg max c N 1 ,l N 1 p(x T 1 |c N 1 , l N 1 ) ? p(l N 1 |c N 1 ) ? p(c N 1 ) .<label>(6)</label></formula><p>Assuming conditional independence of the frames, the arg max term can be further decomposed into arg max</p><formula xml:id="formula_6">c N 1 ,l N 1 T t=1 p(x t |c n(t) ) ? N n=1 p( n |c n ) ? p(c n |c n?1 1</formula><p>) .</p><p>We refer to p(x t |c n(t) ) as visual model, to p( n |c n ) as length model, and to p(c n |c n?1 1 ) as context model.</p><p>The visual model is a neural network as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. We use a recurrent network with a single layer of 256 gated recurrent units and a softmax output. Similar recurrent networks have also been used in other recent methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, but we train the network as described in Section 4. Since the outputs of the neural network are posterior probabilities p(c|x t ), we follow the hybrid approach <ref type="bibr" target="#b2">[3]</ref> and refactor</p><formula xml:id="formula_8">p(x t |c) ? p(c|x t ) p(c) ,<label>(8)</label></formula><p>where p(c) is a class prior. During training, we count the amount of frames that have been labeled with a class c for all sequences that have been processed so far. Normalizing these counts to sum up to one finally results in our estimate of p(c). The prior is updated after every iteration, i.e. after every new training sequence. If a sequence annotation c N 1 contains a class that has not been seen before, 1/#classes is used.</p><p>As length model, we use a class-dependent Poisson distribution:</p><formula xml:id="formula_9">p( |c) = ? c exp (?? c ) ! .<label>(9)</label></formula><p>After each iteration, we update ? c , which is the mean length of a segment for class c. If the training sample (x T 1 , c N 1 ) contains a class that has not been seen before, we set ? c = N/T . Previous works using context models either rely on an ngram language model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref> or a finite set of allowed class sequences <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>. In order to capture both possibilities, we use a right-regular stochastic grammar, where all rules are of the formh ? c h withh, h denoting nonterminal symbols and a class c that acts as terminal symbol. Such a grammar is a superclass of n-grams and finite grammars. Therefore, we decode the possible contexts c n?1 1 as non-terminal symbols h of the grammar and denote the probability to hypothesize class c given the context h as p(c|h). During training, the grammar for a sequence is defined by the transcript sequence c N 1 . For evaluation, we consider two tasks, namely action alignment and action segmentation. While for action alignment a transcript sequence, which defines the grammar, is also provided for each test sequence, transcripts are not provided for the more difficult task of action segmentation. In this case, we estimate the grammar from the transcript annotation of all training videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Viterbi Algorithm Revisited</head><p>Finding the best segmentation in terms of Equation <ref type="formula" target="#formula_7">(7)</ref> is a challenging problem given the exponentially large search space over all possible class sequences and lengths. Most works optimizing a similar quantity rely on the Viterbi algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref> that is based on dynamic programming and is usually used to find the best label sequence of a hidden Markov model.</p><p>In contrast to the standard applications of the Viterbi algorithm, our model additionally features a length model that makes the optimization more complex. To find the best sequence by means of Equation <ref type="formula" target="#formula_7">(7)</ref>, we define an auxiliary function Q(t, , c, h) that yields the best probability score for a segmentation up to frame t meeting the following conditions:</p><p>1. the length of the last segment is , 2. the class label of the last segment is c, 3. the context (the nonterminal symbol) of the stochastic grammar is h.</p><p>The function can be computed recursively. We distinguish two cases. The first case is when no new segment is hypothesized, i.e. &gt; 1. Then,</p><formula xml:id="formula_10">Q(t, , c, h) = Q(t ? 1, ? 1, c, h) ? p(x t |c),<label>(10)</label></formula><p>so the score of the current frame is multiplied with the auxiliary function's value at the previous frame. The second case is a new segment being hypothesized at frame t, i.e. = 1. Then,</p><formula xml:id="formula_11">Q(t, = 1, c, h) = max ,c,h: h?c h ? ?h :h ?ch Q(t ? 1,? ,c,h) ? p(x t |c) ? p(? |c) ? p(c|h) ,<label>(11)</label></formula><p>i.e. the maximization is carried out over all possible lengths and over allc,h that are a right-hand side of a rule in the grammar and there is another rule that allows a transition fromh to h by hypothesizing class c.</p><p>The most likely segmentation of the complete video is then given by max ,c,h Q(T, , c, h) ? p( |c) .</p><p>The optimal class labels c N 1 and lengths l N 1 can be obtained by keeping track of the maximizing argumentsc and? from Equation <ref type="bibr" target="#b10">(11)</ref>. Additional details and code are available online. <ref type="bibr" target="#b0">1</ref> Complexity. The maximization over is bounded by the length T of the video and the possibilities forc,h pairs are limited by the number of rules in the grammar, so the cost to compute Q for a frame t is linear in the video length and the grammar size. Since Q needs to be computed for all frames, the overall complexity is quadratic in the video length. This can be prohibitive for long videos. Thus, in practice, we limit the maximal allowed length to a constant L, so the runtime of the Viterbi decoding is linear in both, video length and grammar size. Throughout this work, we use L = 2, 000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We provide results on three different datasets. The main evaluation (Sections 6.1 to 6.3) is conducted on Breakfast <ref type="bibr" target="#b13">[14]</ref>, a large-scale dataset for action segmentation. It comprises 1, 712 videos (around 3.6 million frames) of persons making breakfast. There are ten dishes such as pancakes or cereals, all with fine-grained annotations like stir or pour. Overall, there are 48 action classes and an average of 6.9 action instances per video. The videos range from some seconds to several minutes in length. We follow <ref type="bibr" target="#b13">[14]</ref> and report frame accuracy averaged over four splits.</p><p>The 50 Salads <ref type="bibr" target="#b28">[29]</ref> dataset is another video dataset for action segmentation. Although it only contains 50 videos, each video is very long and the dataset still has nearly 600, 000 frames annotated with 17 classes, which amount to an average of 20 action instances per video. As evaluation metric, we report frame accuracy averaged over five splits.</p><p>Hollywood Extended has been introduced in [1] for the task of action alignment, which we address with our method in Section 6.5. The dataset comprises 937 videos (nearly 800, 000 frames) and 16 different classes. Each video contains 2.5 action instances on average. As evaluation metric, we follow <ref type="bibr" target="#b0">[1]</ref> and report the Jaccard index as intersection over detection.</p><p>Setup. In accordance with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11]</ref>, we extract Fisher vectors of improved dense trajectories <ref type="bibr" target="#b32">[33]</ref> over a temporal window of length 20 for each frame and reduce the result to 64 dimensions using PCA. In all experiments, the recurrent network is trained for 10, 000 iterations with a learning rate of 0.01 for the first 2, 500 iterations and 0.001 afterwards. The minibatch size for backpropagation of the frames of a training sequence (cf . Section 4.2) is set to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Robustness</head><p>We start with an evaluation of our proposed end-to-end learning algorithm. As discussed in Section 4.2, we enhance the loss function (5) by sampling additional frames from a buffer. In the following, we evaluate the impact of this enhancement and its parameters, namely number of sampled frames and buffer size.</p><p>Impact of Old Data Sampling. The first proposition to enhance the robustness of our algorithm is to maintain some recently seen sequences and their inferred labeling in a buffer and to sample a certain amount K of additional frames from this buffer. This way, we want to ensure that in each iteration, the overall data and class distribution are sufficiently well captured. For the purpose of analyzing which value for K is necessary, we assume an unlimited buffer size, i.e. all previously processed sequences are maintained in memory. The results are illustrated in <ref type="figure">Figure 2</ref>. If we do not sample from previously seen sequences, the model is learned on-line, i.e. the training sequences are directly processed and not stored in a buffer. In this case, our approach achieves a frame accuracy of 27.2%. If we use a buffer and sample frames from it, the accuracy is greatly increased. Without sampling from the buffer, the model learns a strong bias towards the characteristics and class distributions of the current video only. This can be avoided by adding other frames from different classes and sequences to the loss function. While a 1:1 sampling, i.e. for each frame in the sequence one buffered frame is sampled, already shows a huge improvement, we find the optimization to stabilize at a sampling rate of 1:25. Thus, we stick to this value in all remaining experiments. Impact of the Buffer Size. For the above evaluation, we assumed an unlimited buffer size, i.e. every processed sequence could be stored. This may be undesirable in case of large datasets for two reasons: first, depending on the amount of training data, it can be prohibitive to maintain all videos in memory at the same time. Second, the underlying assumption when using the buffer is that the frame/label pairs that are sampled are still more or less consistent with the current model. While this assumption is reasonable if all buffered sequences have been processed only a few iterations ago, it will certainly be wrong if there are frame/label pairs that have been generated by a model a few thousand iterations ago. Hence, we evaluate the impact of the buffer size on the performance, see <ref type="figure" target="#fig_3">Figure 3</ref>. Since we already fixed a sampling ratio of 1:25, a buffer size of less than 25 sequences is not reasonable. A too small buffer of less than 100 sequences does not reflect the overall data and class distributions well enough, resulting in a poor segmentation performance, cf . <ref type="figure" target="#fig_3">Figure 3</ref>. With more than 200 buffered sequences, however, the system stabilizes. Considering the size of the datasets we use (less than 2, 000 sequences each), old frame/label pairs being inconsistent with the current model are not an issue here. Hence, we leave the buffer size unlimited for the remainder of this work.</p><p>Batch Size. In all experiments, we use a batch size of     one. <ref type="figure" target="#fig_5">Figure 4</ref> shows that with larger batch sizes the accuracy slowly drops. Our model is continuously updated, i.e. segmentation information from previous iterations enters the parameter updates, via a running length and prior estimate as well as through buffered data. Thus, a small batch size allows for a rapid adaptation of the length model and prior.</p><p>Convergence Behaviour. <ref type="figure" target="#fig_6">Figure 5</ref> shows the convergence behaviour of our algorithm as a pure online learning approach (no buffered data sampling) and with the robustness enhancements, i.e. with a 1:25 data sampling and an unlimited buffer size. While both variants of our algorithm start to converge after 2, 000 to 3, 000 iterations, the robustness enhancement is particularly advantageous at the beginning of training, adding a huge margin in terms of accuracy (%) runtime (h) pseudo ground-truth <ref type="bibr" target="#b23">[24]</ref> 23.9 03:45 pseudo gr.-tr. + HMM <ref type="bibr" target="#b23">[24]</ref> 33. frame accuracy compared to the pure online variant. Note that <ref type="bibr" target="#b23">[24]</ref> report an oscillating accuracy over the iterations of their two-step scheme. Our NN-Viterbi, in contrast, has a smooth and stable convergence behaviour for both variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Impact of Direct Learning and Model</head><p>In this section, we evaluate the impact of our proposed algorithm compared to the state-of-the-art methods for weakly supervised learning which generate pseudo groundtruth instead of using the transcript annotations directly for learning as discussed in Section 4, and the advantages of temporal modeling using an explicit length model rather than an HMM as discussed in Section 5. The results are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Temporal Modeling: HMM vs. Length Model</head><p>Since most recent methods use a hidden Markov model for the temporal progression throughout the sequence <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b12">13]</ref>, we first show the benefits of modeling the temporal progression directly with a length model. Although the Viterbi decoding is more involved in this case, it allows to train a model directly on the action classes rather than on hidden Markov model states. First, note the impact of temporal modeling in general: if we neither use an HMM nor an explicit length model, the accuracy drastically drops, see first row of <ref type="table">Table 1</ref>. When introducing an HMM as in <ref type="bibr" target="#b23">[24]</ref>, nearly +10% improvement can be observed. Using our factorization from Equation <ref type="formula" target="#formula_7">(7)</ref> with the explicit length model, however, a further gain of +6% is achieved, see fourth row of <ref type="table">Table 1</ref>. The reason for the latter is twofold: First, the training data is aligned to the actual classes rather than to a huge number of HMM states, so for each class more training examples are available. Second, the number of HMM states is fix during network training, while the length model can dynamically adopt to the learned model during training. Notably, using a length model on HMM states is not recommendable since HMM states are typically of very short duration and the state-wise length model has no major impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Pseudo Ground-Truth vs. Direct Learning</head><p>Note that so far, the model is still trained according to the two-step paradigm of repeatedly generating a pseudo ps-gt + len NN-Viterbi ground truth ps-gt + len NN-Viterbi ground truth <ref type="figure">Figure 6</ref>. Example segmentations of two videos from the Breakfast dataset. The two-step scheme with pseudo ground truth and length model has a bias towards uniform lengths, which prevents short actions from being detected accurately. The NN-Viterbi approach is much more robust.</p><p>ground-truth and optimizing the network. Using our proposed algorithm, on the contrary, leads to much better results of 43.0% accuracy, which can be attributed to the direct loss, see <ref type="table">Table 1</ref>. In case of the two-step scheme, the model is encouraged to learn the errors that are present in the generated pseudo ground-truth. Including the transcripts directly into the model learning, this can be avoided.</p><p>In <ref type="figure">Figure 6</ref>, two example segmentations are shown. Recall that for the two-step scheme, the initial pseudo groundtruth is a uniform segmentation. Even after several iterations, a bias towards uniform sequence lengths can be observed. This leads to inaccurate detections of short segments (upper example segmentation) or even completely missed segments (lower example segmentation). Our proposed NN-Viterbi learning is much more accurate, specifically when the segment lengths vary strongly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Incremental Learning</head><p>In a classical learning setup, usually a fixed training set is provided. In this case, it is convenient to process all data in random order. For algorithms working in an online or incremental fashion, however, an interesting practical question is what happens if not all training data is available right at the beginning. For instance, video data from different actors is added to the training data over time. Or, training data for some classes is only available at a later point in time.</p><p>We therefore analyze our algorithm under such conditions. To this end, we sorted the training set (a) by the ten coarse Breakfast activities 2 and (b) by the actors, see Table 2. In the first case, coarse activities that have been observed in the beginning, e.g. cereals and coffee, hardly lose any accuracy compared to training with randomly shuffled data, see <ref type="figure" target="#fig_7">Figure 7</ref>. Later coarse activities are usually not learned well and experience a relative drop of about 50% compared to random shuffling. The comparably small performance drop for milk and tea is due to the fact that these activities share a lot of fine-grained action classes with cereals and coffee, for instance take cup or pour milk.</p><p>Compared to the case where all data is available right at the beginning and random shuffling is possible, sorting the frame accuracy (%) sorted by activity 27.9 sorted by actor 41.5 randomly shuffled 43.0 <ref type="table">Table 2</ref>. Impact of the sequence input order on the robustness of the algorithm. The videos are sorted (a) by the ten coarse activities of the Breakfast dataset, (b) by the performing actor, and (c) randomly. data by actor still results in a very good performance. Apparently, learning the correct class distributions right at the beginning is very important, while changes in appearance over time -such as changing actors -still allows to robustly learn the underlying concepts of the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Comparison to State of the Art</head><p>In this section, we compare our approach to state-of-theart methods for the same task, see <ref type="table">Table 3</ref>. While OCDC <ref type="bibr" target="#b0">[1]</ref> is based on a discriminative clustering, <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b23">[24]</ref> rely on hidden Markov models and train their systems with the classical repeated two-step scheme. Their model formulation is comparable to our factorization from Equation <ref type="formula" target="#formula_7">(7)</ref>. Still, NN-Viterbi outperforms the methods by a large margin. CTC and ECTC allow to optimize the posterior probabilities p(c N 1 |x T 1 ) directly. However, the criterion does not include explicit models such as a stochastic grammar or a length model. The assumption is that the underlying recurrent network can learn all temporal dependencies on its own. As also shown in <ref type="bibr" target="#b10">[11]</ref>, this can lead to degenerate segmentations particularly when videos are long, since even LSTMs usually struggle to memorize context over multiple hundred frames. Human actions typically are rather long, hence modeling context and length explicitly is very important and purely CTC based methods struggle to achieve comparable performance. Lin et al. <ref type="bibr" target="#b19">[20]</ref> also use a CTC based model on Breakfast to infer the sequence of actions in a video. They evaluate the unit accuracy, i.e. the edit distance between the inferred action transcript and the ground truth transcript, and obtain 43.4% unit accuracy. With our approach, we obtain 55.5% unit accuracy.  <ref type="table">Table 3</ref>. Comparison of our method to several state-of-the-art methods for the task of temporal action segmentation. Results are reported as frame accuracy (%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hollywood Extended</head><p>ECTC <ref type="bibr" target="#b10">[11]</ref> 41.0 HTK <ref type="bibr" target="#b14">[15]</ref> 42.4 OCDC <ref type="bibr" target="#b0">[1]</ref> 43.9 HMM/RNN <ref type="bibr" target="#b23">[24]</ref> 46.3 NN-Viterbi 48.7 <ref type="table">Table 4</ref>. Comparison of our method to several state-of-the-art methods for the task of action alignment. Results are reported as a variant of the Jaccard Index (intersection over detection).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Action Alignment</head><p>The task of action alignment has first been addressed by Bojanowski et al. <ref type="bibr" target="#b0">[1]</ref>. In contrast to the previous task, the ordered action sequences c N 1 are now also given for inference. Thus, only the alignment of actions to frames, or in other words, the lengths l N 1 of each segment, need to be inferred. The training procedure is exactly the same as before.</p><p>The results are shown in <ref type="table">Table 4</ref>. Our method outperforms the current state-of-the art by +2.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have proposed a direct learning algorithm that can handle weakly labeled video sequences. The algorithm is generic and can be applied to any kind of model whose best segmentation can be inferred using a Viterbi-like algorithm. Unlike the CTC criterion, our approach allows to include multiple explicitly modeled terms such as a context model and a length model, what has been proven crucial for good performance. Moreover, we showed that using an explicit length model and optimizing the video classes directly leads to a huge improvement over related HMM-based methods that use a pseudo ground-truth. Overall, our method outperforms the current state-of-the-art by a large margin and shows a robust and stable convergence behaviour.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(input video) x 1</head><label>1</label><figDesc>, . . . , x T Neural Network Forward p(c|x 1 ), . . . , p(c|x T ) c 1 ? . . . ? c N p(c n(t) |x t ) (loss) Backprop</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>25 Figure 2 .</head><label>252</label><figDesc>Impact of buffered data sampling. A sampling ratio of 1:K means that for each frame of the current sequence, K buffered frames are sampled. The first column shows the result for on-line learning, i.e., without a buffer. Runtime is measured on a K80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Impact of the buffer size for a buffered data sampling ratio of 1:25. Only a few hundred buffered sequences are already sufficient for robust learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Effect of the batch size. A small batch and frequent updates are beneficial for better accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Convergence behaviour of our NN-Viterbi algorithm in both variants, online (red) and with enhanced robustness (blue), over 10, 000 training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Accuracy per coarse activity for randomly shuffled training data and training data sorted by coarse activities. Left activities have been seen early during training, right activities later.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://alexanderrichard.github.io</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Each video in the Breakfast dataset belongs to one of ten coarse activities. The activities are compositions of 48 fine-grained action classes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Online learning and stochastic approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online learning and neural networks</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Connectionist speech recognition: a hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">247</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic annotation of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting social actions of fruit flies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eyjolfsdottir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Hoopfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="772" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal residual networks for dynamic scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep hand: How to train a CNN on 1 million hand images when your data is continuous and weakly labelled</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Re-sign: Re-aligned end-to-end sequence modelling with deep recurrent CNN-HMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An end-to-end generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised learning of actions from transcripts. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal CNNs for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CTC network with statistical language modeling for action sequence recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thematic Workshops of the ACM Conf. on Multimedia</title>
		<meeting>the Thematic Workshops of the ACM Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with RNN based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multi-stream bi-directional recurrent neural network for finegrained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Joint Conf. on Pervasive and Ubiquitous Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From stochastic grammar to bayes network: Probabilistic parsing of complex activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2641" to="2648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Endto-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
