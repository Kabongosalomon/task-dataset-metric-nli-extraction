<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPAct: Self-supervised Privacy Preservation for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><forename type="middle">Rajendrakumar</forename><surname>Dave</surname></persName>
							<email>ishandave@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chen.chen@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPAct: Self-supervised Privacy Preservation for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code available at: https://github.com/DAVEISHAN/SPAct</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual private information leakage is an emerging key issue for the fast growing applications of video understanding like activity recognition. Existing approaches for mitigating privacy leakage in action recognition require privacy labels along with the action labels from the video dataset. However, annotating frames of video dataset for privacy labels is not feasible. Recent developments of self-supervised learning (SSL) have unleashed the untapped potential of the unlabeled data. For the first time, we present a novel training framework which removes privacy information from input video in a self-supervised manner without requiring privacy labels. Our training framework consists of three main components: anonymization function, self-supervised privacy removal branch, and action recognition branch. We train our framework using a minimax optimization strategy to minimize the action recognition cost function and maximize the privacy cost function through a contrastive selfsupervised loss. Employing existing protocols of knownaction and privacy attributes, our framework achieves a competitive action-privacy trade-off to the existing stateof-the-art supervised methods. In addition, we introduce a new protocol to evaluate the generalization of learned the anonymization function to novel-action and privacy attributes and show that our self-supervised framework outperforms existing supervised methods. Code available at: https://github.com/DAVEISHAN/SPAct</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in action recognition have enabled a wide range of real-world applications, e.g. video surveillance camera <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>, smart shopping systems like Amazon Go, elderly person monitor systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">52]</ref>. Most of these video understanding applications involve extensive computation, for which a user needs to share the video data to the cloud computation server. While sharing the videos to the cloud server for the utility action recognition task, the user also ends up sharing the private visual information like gender, skin color, clothing, background objects etc. in the videos as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, there is a pressing need for solutions to privacy preserving action recognition.</p><p>A simple-yet-effective solution for privacy preservation in action recognition is to utilize very low resolution videos ( <ref type="figure" target="#fig_0">Fig. 1a)</ref>  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref>. Although this downsampling method does not require any specialized training to remove privacy features, it does not provide a good trade-off between action recognition performance and privacy preservation.</p><p>Another set of methods use pretrained object-detectors to detect the privacy regions and then remove or modify the detected regions using synthesis <ref type="bibr" target="#b37">[38]</ref> or blurring <ref type="bibr" target="#b53">[54]</ref> as shown in <ref type="figure" target="#fig_0">Fig. 1b</ref>. The detection-based approaches require the bounding-box level annotations for the privacy attributes, and removing the privacy features without an endto-end learning framework may result in the performance drop of the action recognition task.</p><p>Wu et al. <ref type="bibr" target="#b46">[47]</ref> propose a novel approach to remove the privacy features via learning an anonymization function through an adversarial training framework, which requires both action and privacy labels from the video. Although the method is able to get a good trade-off of action recognition and privacy preservation, it has two main problems. First, it is not feasible to annotate a video dataset for privacy attributes. For instance, Wu et al. <ref type="bibr" target="#b46">[47]</ref> acknowledge the issue of privacy annotation time, where it takes immense efforts for them to annotate privacy attributes for even a small-scale (515 videos) video dataset PA-HMDB. Second, the learned anonymization function from the known privacy attributes may not generalize in anonymizing the novel privacy attributes. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref> the learned anonymization function for human-related privacy attributes (e.g. gender, skin color, clothing) may still leave other privacy information like scene or background objects un-anonymized.</p><p>The performance of the action recognition task depends on the spatio-temporal cues of the input video. Wu et al. <ref type="bibr" target="#b46">[47]</ref> show that anonymizing the privacy features like face, gender, etc. in the input video does not lead to any reduction in the action recognition performance. Instead of just focusing on the cues based on the privacy annotations, our goal is twofold: 1) learning an anonymization function that can remove all spatial cues in all frames without significantly degrading action recognition performance; and 2) learning the anonymization function without any privacy annotations.</p><p>Recently, self-supervised learning (SSL) methods have been successfully used to learn the representative features which are suitable for numerous downstream tasks including classification, segmentation, detection, etc. Towards our goal, we propose a novel frame-level SSL method to remove the semantic information from the input video, while maintaining the information that is useful for the action recognition task. We show that our proposed Self-supervised Privacy-preserving Action recognition (SPAct) framework is able to anonymize the video without requiring any privacy annotations in the training.</p><p>The learned anonymization function should provide a model-agnostic privacy preservation, hence, we first adopt the protocol from <ref type="bibr" target="#b46">[47]</ref> to show the transferability of the anonymization function across different models. However, there are two aspects in terms of evaluating the generalization ability of the anonymization function, which are overlooked in previous works.</p><p>First, in the real-world scenario, the anonymization function is expected to have generalization capability with domain shift in action and privacy classes. To evaluate the generalization capabilities of the anonymization function across novel action and privacy attributes, we propose new protocols. In our experiments, we show that since our model is not dependent on the predefined privacy features like existing supervised methods, and it achieves state-ofthe-art generalization across novel privacy attributes.</p><p>Second, prior privacy-preserving action recognition works have solely focused on privacy attributes of hu-mans. In practical scenarios, privacy leakage can happen in terms of scene and background objects as well, which could reveal personal identifiable information. Therefore, the generalization ability of anonymization function to preserve privacy attributes beyond humans (e.g. scene and object privacy) is of paramount importance as well. To evaluate such ability, we propose P-HVU dataset, a subset of LSHVU dataset <ref type="bibr" target="#b9">[10]</ref>, which has multi-label annotations for actions, objects and scenes. Compared to existing same-dataset privacy-action evaluation protocol on PA-HMDB <ref type="bibr" target="#b46">[47]</ref>, which consists of only 515 test videos, the proposed P-HVU dataset has about 16,000 test videos for robust evaluation of privacy-preserving action recognition.</p><p>The contributions of this work are summarized as follows:</p><p>? We introduce a novel self-supervised learning framework for privacy preserving action recognition without requiring any privacy attribute labels. ? On the existing UCF101-VISPR and PA-HMDB evaluation protocols, our framework achieves a competitive performance compared to the state-of-the-art supervised methods which require privacy labels. ? We propose new evaluation protocols for the learned anonymization function to evaluate its generalization capability across novel action and novel privacy attributes. For these protocols, we also show that our method outperforms state-of-the art supervised methods. Finally, we propose a new dataset split P-HVU to resolve the issue of smaller evaluation set and extend the privacy evaluation to non-human attributes like action scene and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent approaches for the privacy preservation can be categorized in three major groups: (1) Downsampling based approaches; (2) Obfuscation based approaches; and (3) Adversarial training based approaches. An overview of the existing privacy preserving approaches can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Downsampling based approaches utilized a very low resolution input to anonymize the personal identifiable information. Chou et al. <ref type="bibr" target="#b4">[5]</ref> utilize low resolution depth images to preserve privacy in the hospital environment. Srivastava et al. <ref type="bibr" target="#b42">[43]</ref> utilize low resolution images to mitigate privacy leakage in human pose estimation. Butler et al. <ref type="bibr" target="#b0">[1]</ref> use operations like blurring and superpixel clustering to anonymize videos. There are some works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref> utilizing a downsampling based solution for privacy preserving action recognition. An example of anonymization by downsampling is shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. Although it is a simple method and does not require privacy-labels for training, one major drawback of the method is its suboptimal tradeoff between action recognition and privacy preservation.</p><p>Obfuscation based approaches mainly involve using an off-the-shelf object detector to first detect the privacy attributes and then remove or modify the detected regions to make it less informative in terms of privacy features. An interesting solution is proposed by Ren et al. <ref type="bibr" target="#b37">[38]</ref> for anonymizing faces in the action detection utility. They synthesise a fake image in place of the detected face. A similar approach was taken for the video domain privacy by Zhang et al. <ref type="bibr" target="#b53">[54]</ref>, where first the semantic segmentation is employed to detect the regions of interest, which is followed by a blurring operation to reduce the privacy content of a video. Although the obfuscation based methods work well in preserving the privacy, there are two main problems associated with them: (1) there is domain knowledge required to know the regions of interests, and (2) the performance of the utility task is significantly reduced since this approach is not end-to-end and involves two separate steps: privateobject detection/segmentation and object removal.</p><p>Recently, Hinojosa et al. <ref type="bibr" target="#b18">[19]</ref> tackle the privacy preserving human pose estimation problem by optimizing an optical encoder (hardware-level protection) with a software decoder. In addition, some more work focus on hardware level protection in the image based vision systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46]</ref>, however, they are not within scope of this paper.</p><p>Pittaluga et al. <ref type="bibr" target="#b30">[31]</ref> and Xiao et al. <ref type="bibr" target="#b48">[49]</ref> propose adversarial optimization strategies for the privacy preservation in the images. Authors in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> introduce a novel adversarial training framework for privacy preserving action recognition. Their framework adopts a minimax optimization strategy, where action classification cost function is minimized, while privacy classification cost is maximized. Their adversarial framework remarkably outperforms prior works which are based on obfuscation and downsampling.</p><p>Recently, self-supervised learning (SSL) based methods have demonstrated learning powerful representations for images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref> and videos <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>, which are useful for multiple image and video understanding downstream tasks. In this paper, we propose selfsupervised privacy preservation method. Instead of using a privacy classifier to remove only the privacy attributes from the input data like <ref type="bibr" target="#b46">[47]</ref>, our approach is to remove all spatial semantic information from the video, along with keeping the useful utility action recognition information by training an anonymization function in an minimax optimization manner. To the bes of our best knowledge, there is no other self-supervised privacy preserving action recognition method, which learns in an end-to-end fashion, without requiring privacy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The key idea of our proposed framework is to learn an anonymization function such that it deteriorates the privacy attributes without requiring any privacy labels in the training, and maintains the performance of action recognition task. We build our self-supervised framework upon the existing supervised adversarial training framework of <ref type="bibr" target="#b46">[47]</ref>. A schematic of our framework is depicted in <ref type="figure">Fig. 2</ref>. In Sec 3.1, we first formulate the problem by explaining our objective. In Sec 3.2 we present details of each component of our framework, and in Sec 3.3 we explain the optimization algorithm employed in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Let's consider a video dataset X with action recognition as an utility task, T , and privacy attribute classification as a budget task, B. The goal of a privacy preserving action recognition system is to maintain performance of T , while cutting the budget B. This goal is achieved by learning an anonymization function, f A , which transforms (anonymize) the original raw data X. Assume that the final system has any action classification target model f ? T and any privacy target model f ? B . The goal of a privacy preserving training is to find an optimal point of f A called f * A , which is achieved by the following two criteria: C1: f * A should minimally affect the cost function of target model, f ? T , on raw data i.e.</p><formula xml:id="formula_0">L T (f ? T (f * A (X)), Y T ) ? L T (f ? T (X), Y T ),<label>(1)</label></formula><p>where T denotes utility Task, L T is the loss function which is the standard cross entropy in case of single action label Y T or binary cross entropy in case of multi-label actions Y T . C2: Cost of privacy target model, f ? B , should increase on the transformed (anonymized) data compared to raw data i.e.</p><formula xml:id="formula_1">L B (f ? B (f * A (X))) ? L B (f ? B (X)),<label>(2)</label></formula><p>Update f A to minimize L T and Maximize L B</p><formula xml:id="formula_2">Input Video Batch (X i ) Action label (y i ) Sigmoid Action Recognition Loss (L T ) S F NT-Xent Contrastive Loss (L B ) f T Frozen f B frozen Frozen f A Input Video Batch (X i+1 ) Action label (y i ) Sigmoid Action Recognition Loss (L T ) S F NT-Xent Contrastive Loss (L B ) update f T to minimize L T update f B to minimize L B</formula><p>Step-1: f A update</p><p>Step-2: f B , f T update <ref type="figure">Figure 2</ref>. Minimax optimization in the proposed SPAct framework. fA is anonymization function, fT is 3D-CNN based action classifier, fB is 2D-CNN based self-supervised learning model, and SF is temporal sampler. Details of each component can be found in Sec 3.2. We first initialize fA to identity function and fT and fB to pretrained checkpoints optimized on raw video. The proposed minimax optimization strategy is an iterative process including two-steps per iteration. In the left figure, we first update the weights of fA to minimize action classification loss, LT , and maximize NT-Xent contrastive self-supervised loss <ref type="bibr" target="#b3">[4]</ref> LB, keeping fT and fB frozen.</p><p>After that as shown in the right figure, for the next batch of videos, we keep fA frozen and update parameters of fT and fB to minimize LT and LB, respectively. For more details see <ref type="bibr">Sec 3.3.</ref> where B denotes privacy Budget, L B is the self-supervised loss for our framework, and binary cross entropy in case of a supervised framework, which requires privacy label annotations Y B . Increasing a self-supervised loss L B results in deteriorating all useful information regardless of if it is about privacy attributes or not. However, the useful information for the action recognition is preserved via criterion C1. Combining criteria C1 and C2, we can mathematically write the privacy preserving optimization equation as follows, where negative sign before L B indicates it is optimized by maximizing it:</p><formula xml:id="formula_3">f * A = argmin f A [L T (f ? T (f A (X)), Y T ) ? L B (f ? B (f A (X)))].<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Framework</head><p>The proposed framework mainly consists of three components as shown in <ref type="figure">Fig 2:</ref> (1) Anonymization function (f A ); (2) Self-supervised privacy removal branch; and (3) Action recognition or utility branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Anonymization Function (f A )</head><p>The anonymization function is a learnable transformation function, which transforms the video in such a way that the transformed information can be useful to learn action classification on any target model, f ? T , and not useful to learn any privacy target model, f ? B . We utilize an encoder-decoder neural network as the anonymization function. f A is initialized as an identity function by training it using L L1 reconstruction loss as given below:</p><formula xml:id="formula_4">L L1 = C c=1 H h=1 W w=1 |x c,h,w ?x c,h,w |,<label>(4)</label></formula><p>where, x is input image,x is sigmoid output of f A logits, C = input channels, H = input height, and W = input width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Self-supervised privacy removal branch</head><p>A schematic of self-supervised privacy removal branch is shown in <ref type="figure">Fig. 3</ref>. First the video x i is passed through f A to get the anonymized video f A (x i ), which is further passed through a temporal Frame sampler S F . S F samples 2 frames out of the video with various S F strategies, which are studied in Section 5.5. The sampled pair of frames (S F (f A (x i ))) are projected into the representation space through 2D-CNN backbone f B and a non-linear projection head g(?). The pair of frames of video x i corresponds to projection Z i and Z ? i in the representation space. The goal of the contrastive loss is to maximize the agreement between projection pair (Z i , Z ? i ) of the same video x i , while maximizing the disagreement between projection pairs of different videos (Z i , Z j ), where j ? = i. The NT-Xent contrastive loss <ref type="bibr" target="#b3">[4]</ref> for a batch of N videos is given as follows:</p><formula xml:id="formula_5">L i B = ? log h (Z i , Z ? i ) N j=1 [1 [j? =i] h(Z i , Z j ) + h(Z i , Z ? j )] ,<label>(5)</label></formula><p>where h(u, v) = exp u T v/(?u??v?? ) is used to compute the similarity between u and v vectors with an adjustable parameter temperature, ? . 1 [j? =i] ? {0, 1} is an indicator function which equals 1 iff j ? = i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Minimax optimization</head><p>In order to optimize the proposed self-supervised framework with the objective of Eq. 3, let's consider anonymization function f A parameterized by ? A , and auxiliary models f B and f T respectively parameterized by ? B and ? T . Assume, ? A , ? B , ? T respectively be the learning rates for ? A , ? B , ? T . First of all, ? A is initialized as given below ( Eq. 6),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head><p>Space</p><formula xml:id="formula_6">Anonymized Video f A (x i ) Temporal Sampler (S F ) 2D-CNN Backbone (f B ) Multilayer- Projection head (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected 2 frames S F (f A (x i ))</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anonymized Video f A (x j )</head><formula xml:id="formula_7">Temporal Sampler (S F ) g f B Z i Z' i Z j Z' j</formula><p>Attract Repel <ref type="figure">Figure 3</ref>. Contrastive Self-supervised Loss is used to maximize the agreement between two frames of a video and maximize disagreement between frames of different videos. Please refer to Sec 3.2.2 for more details.</p><p>unless f A reaches to threshold th A0 reconstruction performance ( Eq. 4) on validation set:</p><formula xml:id="formula_8">? A ? ? A ? ? A ? ? A (L L1 (? A )).<label>(6)</label></formula><p>Once ? A is initialized, it is utilized for initialization of ? T and ? B as shown in the following equations unless their performance reaches to the loss values of th B0 and th T 0 :</p><formula xml:id="formula_9">? T ? ? T ? ? T ? ? T (L T (? T , ? A )),<label>(7)</label></formula><formula xml:id="formula_10">? B ? ? B ? ? B ? ? B (L B (? B , ? A )).<label>(8)</label></formula><p>After the initialization, two step iterative optimization process takes place. The first step is depicted in the left side of <ref type="figure">Fig. 2</ref>, where ? A is updated using the following equation:</p><formula xml:id="formula_11">? A ? ? A ? ? A ? ? A (L T (? A , ? T ) ? ?L B (? A , ? B )),<label>(9)</label></formula><p>where ? ? (0, 1) is the relative weight of SSL loss, L B , with respect to supervised action classification loss, L T .</p><p>Here the negative sign before L B indicates that we want to maximize it. In implementation, it can be simply achieved by using negative gradients <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the second step, as shown in the right part of the <ref type="figure">Fig. 2</ref>, ? T and ? B are updated using Eq. 7 and 8, respectively. We update ? B to get powerful negative gradients in the next iteration's step-1. Note that there is a similarity with GAN training here; we can think of f A as the a generator who tries to fool f B in the first step and, in the second step f B tries to get stronger through update of Eq. 8. This two step iterative optimization process continues until L B reaches to a maximum value th Bmax .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Intuition: SSL Branch and Privacy removal</head><p>Take a model f b initialized with self-supervised contrastive loss (SSL) pretraining. Now keeping f b frozen, when we try to maximize the contrastive loss, it changes the input to f b in such a way that it decreases agreement between frames of the same video. We know that frames of the same video share a lot of semantic information, and minimizing the agreement between them results in destroying (i.e. unlearning) most of the semantic info of the input video. In simple terms, maximizing contrastive loss results in destroying all highlighted attention map parts of Supp. In this process, we also ensure that semantics related to action reco remains in video, through the action reco branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training and Evaluation Protocols</head><p>The existing training and evaluation protocols are discussed in Sec 4.1, 4.2 and a new proposed generalization protocol is introduced in Sec 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Same-dataset training and evaluation protocol</head><p>Training of supervised privacy preserving action recognition method requires a video dataset X t with action labels Y t T , and privacy labels Y t B , where t denotes training set. Since, our self-supervised privacy removal framework does not require any privacy labels, we do not utilize Y t B . Once the training is finished, the anonymization function is now frozen, called f * A , and auxiliary models f T and f B are discarded. To evaluate the quality of the learned anonymization, f * A is utilized to train: (1) a new action classifier f ? T over the train set (f * A (X t ), Y t T ); and (2) a new privacy clas- </p><formula xml:id="formula_12">sifier f ? B to train over (f * A (X t ), Y t B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cross-dataset training and evaluation protocol</head><p>In practice, a trainable-scale video dataset with action and privacy labels doesn't exist. The authors of <ref type="bibr" target="#b46">[47]</ref> remedy the supervised training process by a cross-dataset training and/or evaluating protocol. Two different datasets were utilized in <ref type="bibr" target="#b46">[47]</ref>: action annotated dataset (X t action , Y t T ) to optimize f A and f T ; and privacy annotated dataset (X t privacy , Y t B ) to optimize f A and f B . Again, note that in this protocol, our self-supervised framework does not uti-</p><formula xml:id="formula_13">lize Y t B .</formula><p>After learning the f A through the different train sets, it is frozen and we call it f *</p><formula xml:id="formula_14">A . A new action classi- fier f ? T is trained on anonymized version of action anno- tated dataset (f * A (X t action ), Y t T )</formula><p>, and a new privacy classifier f ? B is trained on the anonymized version of the privacy annotated dataset (f *</p><formula xml:id="formula_15">A (X t privacy ), Y t B ).</formula><p>Once the target models f ? T and f ? B finish training on the anonymized version of train sets, they are respectively evaluated on test sets</p><formula xml:id="formula_16">(f * A (X e action ), Y e T ) and (f * A (X e privacy ), Y e B ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Novel action and privacy attributes protocol</head><p>For the prior two protocols discussed above, the same training set X t (X t action and X t privacy ) is used for the target models f ? T , f ? B and learning the anonymization function f A . However, a learned anonymization function f * A is expected to generalize on any action or privacy attributes. To evaluate the generalization on novel actions, an anonymized verion of novel action set f *</p><formula xml:id="formula_17">A (X nt action ), such that Y nt T ? Y t T = ?,</formula><p>is used to train the target action model f ? T , and its performance is measured on the anonymized test set of novel action set f * A (X ne action ). For privacy generalization evaluation, a novel privacy set f * </p><formula xml:id="formula_18">A (X nt privacy ) (s.t. Y nt B ? Y t B = ?) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>For default experiment setting, we utilize UNet <ref type="bibr" target="#b39">[40]</ref> as f A , R3D-18 <ref type="bibr" target="#b15">[16]</ref> as f T , and ResNet-50 <ref type="bibr" target="#b17">[18]</ref> as f B . For a fair evaluation we report results of different methods with the exact same training augmentations and model architectures. Implementation details related to training setting, hyperparmeters, and model architectures can be found in Supp.Sec.C. Visualization of the learned anonymization from different methods can be seen in Supp.Sec.F. Downsampling methods are adopted with a down-sampled versions of input resolution with a factor of 2? and 4? used in training and testing. Obfuscation methods are carried out using a MS-COCO <ref type="bibr" target="#b24">[25]</ref> pretrained Yolo <ref type="bibr" target="#b36">[37]</ref> object detector to detect person category. The detected persons are removed using two different obfuscation strategies: (1) blackening the detected bounding boxes; <ref type="bibr" target="#b1">(2)</ref> applying Gaussian blur in the detected bounding boxes at two different strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluating learned anonymization on known action and privacy attributes</head><p>For known action and privacy attributes, we follow Section 4.2 to evaluate on 2 existing protocols from <ref type="bibr" target="#b46">[47]</ref>, and follow Section 4.1 to present a new protocol using P-HVU dataset for same-domain training and testing. Results are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCF101-VISPR cross dataset training and evaluation</head><p>In this protocol, X t action = UCF101 trainset and X e action = UCF101 testset; X t privacy = VISPR trainset and X e privacy = VISPR testset. HMDB51-VISPR cross dataset training and PA-HMDB evaluation In this protocol, X t action = HMDB51 trainset, and X e action = PA-HMDB, X t privacy = VISPR trainset, and X e privacy = PA-HMDB. P-HVU same dataset training and evaluation In this protocol, utility task is multi-label action recognition and privacy is defined in terms of object and scene multilabel classification. In this protocol, X t = P-HVU trainset, and X e = P-HVU testset.</p><p>We can observe in <ref type="table" target="#tab_2">Table 1</ref> that our proposed selfsupervised framework achieves a comparable actionprivacy trade-off in case of known action and privacy attributes. Other methods like Downsample-4?, Obf-blackening and Obf-StrongBlur get a commendable privacy removal, however, at a cost of action recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluating learned anonymization on Novel action and privacy attributes</head><p>Following Sec. 4.3, we propose 2 protocols for the novel actions and 2 protocols for the novel privacy attributes. Novel action and privacy attributes In this protocol, for actions X t action = UCF101 trainset, X nt action = HMDB51  <ref type="table">Table 2</ref>. Comparison of existing privacy preserving action recognition method on novel action and privacy attributes protocol. Our framework outperforms the supervised method <ref type="bibr" target="#b46">[47]</ref>. ?% denotes relative drop from raw data.</p><p>trainset, X ne action = HMDB51 testset/ PA-HMDB and for privacy, X t privacy = VISPR-1 trainset, X nt privacy = VISPR-2 trainset and X ne privacy = VISPR-2 testset. From the left part of <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref>, we can observe that our method outperforms the supervised method <ref type="bibr" target="#b46">[47]</ref> in both action and privacy attribute generalization.</p><p>Novel privacy attributes from Scenes to Objects In this protocol, we take known action set X t action = P-HVU trainset, and X e action = P-HVU testset, X t privacy = P-HVU trainset Object, X nt privacy = P-HVU trainset Scene and X ne privacy = P-HVU testset Scene. We can observe from the right most part of <ref type="table">Table 2</ref> that while testing the learned anononymization from scenes to objects, supervised method [47] gets a similar results like Obf-StrongBlur and removes only ?46% of the raw data's privacy, whereas our method removes ?88% object privacy of the raw data. Main reason for difference in our method's performance gain over <ref type="bibr" target="#b46">[47]</ref> in <ref type="table">Table 2</ref> is due to the amount of domain shift in novel privacy attributes. In VISPR1?2, domain shift is very small eg SkinColor(V1)?Tattoo(V2) (Supp.  . Trade-off between action classification and privacy removal while generalizing from UCF101?HMDB51 for action and VISPR1?VISPR2 for privacy attributes. Our self-supervised method achieves the best trade-off among other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Study</head><p>Experiments with different privacy removal branches Second row in <ref type="table">Table 3</ref> shows the results just using an encoder-decoder based model f A without using any privacy removal branch f B . However, the style changing fails to anonymize privacy information. In our next attempt, we utilize a pretrained SSL frozen model to anonymize the privacy information by Eq. 9. This method of frozen f B is able to remove the privacy information by a small extent (&lt; 2%), however, our biggest boost in privacy removal (7%) comes from updating f B with every update in f A as can be seen in the second last row of <ref type="table">Table 3</ref>. This observation shows the importance of updating the f B in step-2 (Eq. 8) of minimax optimization. In other words, we can say that if f B is not updated with f A , then it becomes very easy for f A to fool f B i.e. maximize L B , which ultimately leads to a poor privacy removal. Additionally, we also experiment with a spatio-temporal SSL framework as privacy removal branch. Details are given in Supp.Sec.C. Note that removing spatiotemporal semantics from the input video leads to severe degradation in action recognition performance, which is the main reason of choosing 2D SSL privacy removal branch in our framework in order to remove only spatial semantics from the input video. Temporal sampling strategies for SSL In order to experiment with various Temporal sampler (S F ) for choosing a pair of frames from a video, we change the duration (distance) between the two frames as shown in <ref type="table">Table 4</ref>. The chosen pair of frames from a video is considered for the positive term of contrastive loss (Eq. 5). In our default setting of experiments, we randomly select a pair of frames from a video as shown in the first row. We observe that mining positive frames from further distance decreases the anonymization capability. This is because mining the very dissimilar positives in contrastive loss leads to poorly learned representation, which is also observed while taking temporally distant positive pair in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>. Effect of different SSL frameworks As shown in <ref type="table" target="#tab_6">Table 5</ref>, we experiment with three different 2D SSL schemes in Eq. 5. We can observe that NT-Xent <ref type="bibr" target="#b3">[4]</ref> and MoCo <ref type="bibr" target="#b16">[17]</ref> achieve comparable performances, however, RotNet <ref type="bibr" target="#b13">[14]</ref> framework provides a suboptimal performance in both utility and privacy. Our conjecture is that this is because Rot-Net mainly encourages learning global representation, and heavily removing the global information from the input via privacy removal branch leads to drop in action recognition performance as well. <ref type="bibr" target="#b3">[4]</ref> 62.1 57.4 0.473 MoCo <ref type="bibr" target="#b16">[17]</ref> 61.4 57.1 0.462 RotNet <ref type="bibr" target="#b13">[14]</ref> 58.1 60.2 0.504  <ref type="table">Table 6</ref>. Effect of different fB in minimax optimization</p><formula xml:id="formula_19">fA fB UCF101 VISPR1 Top-1 (?) cMAP (?) F1 (?) ? ? 62.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSL Loss</head><formula xml:id="formula_20">UCF101 VISPR1 Top-1(%) (?) cMAP(%) (?) F1 (?) NT-Xent</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitation</head><p>One limitation of our work is that it utilizes the basic frameworks for self-supervised learning, and which may be suitable only for the action recognition, and not directly suitable for other video understanding tasks like actions detection or action anticipation. Additionally, there is still room of improvement to match the supervised baseline in case of known action-privacy attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduced a novel self-supervised privacy preserving action recognition framework which does not require privacy labels for the training. Our extensive experiments show that our framework achieves competitive performance compared to the supervised baseline for the known actionprivacy attributes. We also showed that our method achieves better generalization to novel action-privacy attributes compared to the supervised baseline. Our paper underscores the benefits of contrastive self-supervised learning in privacy preserving action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Overview</head><p>The supplementary material is organized into the following sections:</p><p>?  <ref type="bibr" target="#b28">[29]</ref> is an image dataset with a diverse set of personal information in an image like skin color, face, gender, clothing, document information etc. We use two subsets of privacy attributes of VISPR dataset as shown in <ref type="table">Table 7</ref>. Each of the privacy attribute is a binary label, where 0 indicates absence of the attribute and 1 indicates presence of the attribute in the image. An image can have multiple privacy attributes, hence it is as a multi-label classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VISPR1 [47]</head><p>VISPR2 a17 color a6 hair color a4 gender a16 race a9 face complete a59 sports a10 face partial a1 age approx a12 semi nudity a2 weight approx a64 rel personal a73 landmark a65 rel soci a11 tattoo <ref type="table">Table 7</ref>. Privacy attributes of VISPR <ref type="bibr" target="#b28">[29]</ref> subsets.</p><p>PA-HMDB51 <ref type="bibr" target="#b46">[47]</ref> is subset of HMDB51 dataset with 51 action labels and 6 human privacy attributes which are annotated temporally. The privacy attributes are the same as VISPR-1 subset shown in <ref type="table">Table 7</ref> except a65 rel soci attribute. Each privacy attribute has a fine-grained class assigned as well, however, it is not considered in this paper.</p><p>Following <ref type="bibr" target="#b46">[47]</ref>, we use binary label for each privacy attribute i.e. if the privacy attribute is present in the image or not. P-HVU is a selected subset of LSHVU <ref type="bibr" target="#b9">[10]</ref>, which is a large-scale dataset of multi-label human action with a diverse set of auxiliary annotations provided for objects, scenes, concepts, events etc. We consider using this dataset to understand privacy leakage in terms of object or scene. P-HVU is prepared from LSHVU dataset such that each video has object and scene annotations along with the action label. A video of the LSHVU always has action labels, however, it does not necessarily have scene and object label. We consider following steps to prepare P-HVU dataset:</p><p>? Select all LSHVU validation set videos such that each video has object and scene annotation and call it P-HVU test set. ? Select LSHVU train set videos which has action, object and privacy class from the P-HVU test set, and filter out videos if either of the object or scene annotations are missing in the video and call it P-HVU train set. Each video of the P-HVU dataset has multi-label action, object and scene annotation. The dataset consists of 739 action classes, 1678 objects, and 248 scene categories. Train/test split of P-HVU consists of 245,212/16,012 videos to provide a robust evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Architectural details</head><p>For anonymization function we utilize PyTorch implementation 1 of UNet <ref type="bibr" target="#b39">[40]</ref> with three output channels. For 2D-CNN based ResNet <ref type="bibr" target="#b17">[18]</ref>, 3D-CNN models R3D-18 <ref type="bibr" target="#b15">[16]</ref>, and R2plus1D-18 <ref type="bibr" target="#b44">[45]</ref>, we utilize torchvision.models implementation 2 . Multi-layer projection head g(?) of self-supervised privacy removal branch consists of 2 layers: Linear(2048, 2048) with ReLU activation and Linear(2048, 128) followed by L2-Normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Augmentations</head><p>We apply two different sets of augmentation depending upon the loss function: (1) For supervised losses, we use standard augmentations like random crop, random scaling, horizontal flip and random gray-scale conversion with less strength.</p><p>(2) For self-supervised loss, in addition to the standard augmentations with with more strength, we use: random color jitter, random cut-out and random color drop. For more details on augmentation strengths in supervised and self-supervised losses refer SimCLR <ref type="bibr" target="#b3">[4]</ref>. In order to ensure temporal consistency in a clip, we apply the exact same augmentation on all frames of the clip. All video frames or images are resized to 112 ? 112. Input videos are of 16 frames with skip rate of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Hyperparameters</head><p>We use a base learning rate of 1e-3 with a learning rate scheduler which drops learning rate to its 1/10th value on the loss plateau.</p><p>For self-supervised privacy removal branch, we use the 128-D output as representation vector to compute contrastive loss of temperature ? = 0.1. For RotNet <ref type="bibr" target="#b13">[14]</ref> experiment we use 4 rotations: {0, 90, 180, 270}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Training details</head><p>To optimize parameters of different neural networks we use Adam optimizer <ref type="bibr" target="#b22">[23]</ref>. For initialization, we train f A for 100 epochs using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. Performance Metrics</head><p>To evaluate the performance of target privacy model f ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head><p>we use macro-average of classwise mean average precision (cMAP). The results are also reported in average F1 score across privacy classes. F1 score for each class is computed at confidence 0.5. For action recognition, we use top-1 accuracy computed from video-level prediction from the model and groundtruth. A video-level prediction is average prediction of 10 equidistant clips from a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6. Baselines</head><p>Supervised adversarial framework <ref type="bibr" target="#b46">[47]</ref>: we refer to official github repo <ref type="bibr" target="#b2">3</ref> and with the consultation of authors we reproduce their method. For fair comparison, we use exact same model architectures and training augmentations. For more details on hyperparameters refer <ref type="bibr" target="#b46">[47]</ref>.</p><p>Blurring based obfuscation baselines: we first detect the person using MS-COCO <ref type="bibr" target="#b24">[25]</ref> pretrained yolov5x <ref type="bibr" target="#b21">[22]</ref> model in each frame of the video. After detecting the person bounding boxes, we apply Gaussian blur filter on the bounding boxes regions. We utilize torchvision.transforms.GaussianBlur function with kernel size = 21 and sigma = 10.0 for Strong blur, and kernel size = 13, sigma = 10.0 for the Weak blur baselines. For VISPR dataset, we first downsample images such that smaller side of image = 512. Blackening based obfuscation baselines: we first detect person bounding boxes using yolov5x model and assign zero value to all RGB channels of the bounding box regions. <ref type="bibr" target="#b2">3</ref> https://github.com/VITA-Group/Privacy-AdversarialLearning Blackening based obfuscation baselines: we first detect person bounding boxes using yolov5x model and assign zero value to all RGB channels of the bounding box regions. Ablation with spatio-temporal privacy removal branch: For ablation of <ref type="table">Table 3</ref> of the main paper, we use naive extension of SimCLR <ref type="bibr" target="#b3">[4]</ref> to the domain of video, where we consider two clips from the same video as positive and clips from other videos as negatives in the contrastive loss. R3D-18 is chosen as 3D-CNN backbone and MLP g(?) consist of Linear(512, 512) with ReLU activation and Linear(512, 128) followed by L2-Normalization. Noisy Features baseline <ref type="bibr" target="#b52">[53]</ref>: Zhang et al. <ref type="bibr" target="#b52">[53]</ref> proposed non-visual privacy preservation in wearable device from 1D singal of mobile sensors. We extended this work to video privacy by replacing LossNet to R3D-18, TransNet to UNet and extended similarity losses to handle video input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional results</head><formula xml:id="formula_21">D.1. Evaluating f * A privacy target model with f ? B</formula><p>pretrained on a raw data</p><p>In a practical scenario, learned anonymization f * A is not accessible to a intruder, hence one can try to extract privacy information using a pretrained privacy classifier of raw data. In this protocol, instead of learning a target privacy model f ? B from the anonymized version of the training data, we directly evaluate f * A using a privacy target model which is pretrained on raw data. Results are shown in <ref type="table">Table 8</ref>. We use ResNet-50 model as privacy target model, which is pretrained on raw training data of the the respective evaluation set. There are two main observations in in this protocol: (1) Compared to other methods, supervised <ref type="bibr" target="#b46">[47]</ref> and our self-supervised method gets a remarkable amount of privacy classification drop, which is desired to prevent privacy leakage. (2) Our method gets a competitive cMAP performance to <ref type="bibr" target="#b46">[47]</ref>, and greatly outperforms it in terms of F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Evaluating learned f *</head><p>A on different utility target model f ? T A learned anonymization function, f * A , should allow learning any action recognition target model, f ? T , over the anonymized version of training data without significant drop in the performance. Using the R3D-18 as a auxiliary action recognition model, f T , in the training of anonymization function, we evaluate the learned f * A to train different action recognition (utility) target models like R3D-18, C3D <ref type="bibr" target="#b43">[44]</ref>, and R2plus1D-18 from scratch and Kinetics-400 <ref type="bibr" target="#b2">[3]</ref> pretraining. Results are shown in <ref type="table">Table 9</ref>. We can observe that our method maintains the action recognition performance on any utility action recognition model. Also, it is interesting to notice that the learned anonymization by our method and method in <ref type="bibr" target="#b46">[47]</ref> get benefit from a largescale raw data pretraining of Kinetics-400.  <ref type="table">Table 8</ref>. Evaluating learned anonymization function f * A to measure its privacy leakage from a raw-data pretrained privacy target model f ? B . Lower privacy classification score is better, ?% denotes relative drop from raw data. Our self-supervised gets a competitive performance to the supervised method <ref type="bibr" target="#b46">[47]</ref>.  <ref type="bibr" target="#b8">[9]</ref> pretraining. From <ref type="table" target="#tab_2">Table 10</ref>, we can observe that our method protects privacy leakage regardless of choice of target privacy model. Using ImageNet pretraining as shown in <ref type="table" target="#tab_2">Table 11</ref>, privacy leakage increases in all methods, however, the relative drop to the raw data baseline is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Evaluation protocol: Pretrained Action classifier and fixed privacy classifier</head><p>In a practical scenario, we can initialize an action recognition target model f ? T from the Kinetics400 raw data pretrained checkpoint. Also, an intruder has no direct access to the learned anonymization function in a practical setting, hence we can consider the raw-data pretrained privacy classifier as a target privacy model f ? B . Results are shown in Ta-ble 12. We use Kinetics400 pretrained R2Plus1D-18 model as the action recognition target model f ? T , and ResNet models with varying capacity as the target privacy model f ? B . Plotting the trade-off of <ref type="table" target="#tab_2">Table 12</ref> in <ref type="figure" target="#fig_4">Fig. 5</ref>, we can observe that at the cost of a small drop in action recognition performance our method obtains about 66% reduction in privacy leakage from the raw data baseline. This highlights the potential of our self-supervised privacy preserving framework in a practical scenario without adding cost of privacy annotation in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Plots for known and novel action and privacy attributes protocol</head><p>A trade-off plot for evaluating learned f * A for novel action-privacy attributes is shown in <ref type="figure">Fig. 6</ref> and known action-privacy attributes is shown in <ref type="figure">Fig 7,</ref> for more details see Sec. 5 of main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional ablations E.1. Effect of different f T architectures</head><p>To understand the effect of auxiliary model f T in the training process of f A , we experiment with different utility auxiliary model f T , and report the performance of their learned f * A in the same evaluation setting as shown in Table <ref type="bibr" target="#b12">13</ref>. We can observe that there is no significant effect of f T in learning the f A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Visualization of learned anonymization f * A at different stages of training</head><p>In order to visualize the transformation due to learned anonymization function f * A , we experiment with various test set videos of UCF101. The sigmoid function after the f * A ensure (0,1) range of the output image. We visualize output at different stages of anonymization training as shown in <ref type="figure" target="#fig_9">Fig. 8, 9</ref> is successfully able to achieve anonymization as the training progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Visualization of learned anonymization f * A for different methods</head><p>Apart from <ref type="figure" target="#fig_9">Fig. 8, 9</ref>, 10 visualization of our method, we show visualization for all methods, attached in the form of videos in the supplementary zip file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Attention map for supervised vs self-supervised privacy removal branch</head><p>A self-supervised model focuses on holistic spatial semantics, whereas a supervised privacy classifier focuses on specific semantics of the privacy attributes. To bolster this observation, we visualize the attention map of ResNet50 model which is trained in (1) Supervised manner using binary cross entropy loss using VISPR-1. (2) Selfsupervised manner using NT-Xent loss. We use the method of Zagoruyko and Komodakis <ref type="bibr" target="#b49">[50]</ref> to generate model attention from the third convolutional block of the ResNet model. As can be observed from the attention map visualization of <ref type="figure" target="#fig_0">Fig. 11</ref> that a self-supervised model focuses on semantics related to human and its surrounding scene, whereas, the supervised privacy classifier mainly focuses on the human semantics. In <ref type="figure" target="#fig_0">Fig. 12</ref>, we can see that the selfsupervised model attends to the semantics of object along with human, and supervised privacy classifier mainly learns semantics of human only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Visual Aid for training and evaluation protocols</head><p>In order to better understand protocols of Sec. 4 of main paper, we provide here some visual aids in <ref type="figure" target="#fig_0">Fig 13, 14,</ref>       <ref type="figure">Figure 6</ref>. Evaluating learned anonymization for novel action-privacy attributes. Our framework outperforms the supervised method <ref type="bibr" target="#b46">[47]</ref> and achieves robust generalization across novel action-privacy attributes. For more details refer Sec. 5.4 of main paper.  Action Rec (cMAP %) (higher is better)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw Data DownSample2X</head><p>Obf-Blackening Obf-StrongBlur Obf-WeakBlur Supervised <ref type="bibr" target="#b46">[47]</ref> Ours (d) Trade-off between action classification vs privacy-scene classification on P-HVU. <ref type="figure">Figure 7</ref>. Evaluating learned anonymization for known action-privacy attributes. Our framework achieves comparable performance to the supervised method <ref type="bibr" target="#b46">[47]</ref>. For more details refer Sec. 5.3 of main paper.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the existing privacy preserving action recognition approaches. The main goals of a framework include removing privacy information and maintaining action recognition performance at low cost of annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 7 , 8</head><label>78</label><figDesc>middle row. Since this unlearned generic semantic info contained privacy attributes related to human, scene, and objects; we end up removing private info in the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Trade-off between action classification and privacy removal while generalizing from UCF101?HMDB51 for action and VISPR1?VISPR2 for privacy attributes. Our self-supervised method achieves the best trade-off among other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>L 1 reconstruction loss, action recognition auxiliary model f T using cross-entropy loss for 150 epochs, and privacy auxiliary model f B using NT-Xent loss for 400 epochs. Training phase of anonymization function f A is carried out for 100 epochs, whereas target utility model f ? T and target privacy model f ? B are trained for 150 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Trade-off between action classification using pretrained action classifier and raw-data frozen privacy classifier. UCF101 is used as action classification dataset and VISPR is used as privacy dataset. Increasing size of the marker shows increasing size of privacy classifiers: ResNet18, ResNet50, ResNet101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>48</head><label>48</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) Trade-off between action classification and privacy removal while generalizing from Scenes?Objects for privacy attributes on P-HVU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Trade-off between action classification on UCF101 vs privacy classification on VISPR-1. Trade-off between action classification vs privacy classification on PA-HMDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Trade-off between action classification vs privacy-object classification on P-HVU.10.0 12.5 15.0 17.5 20.0 22.5 25.0 privacy cMAP % (lower is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Learned anonymization using our self-supervised privacy preservation framework on test set of UCF101. Groundtruth action label: FrisbeeCatch. First row: original video, from second to last row: anonymized version of video at epoch1, 3, 6, 9, 30.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Learned anonymization using our self-supervised privacy preservation framework on test set of UCF101. Groundtruth action label: ApplyLipstick. First row: original video, from second to last row: anonymized version of video at epoch1, 3, 6, 9, 30.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Learned anonymization using our self-supervised privacy preservation framework on test set of UCF101. Groundtruth action label: BreastStroke. First row: original video, from second to last row: anonymized version of video at epoch 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Attention map visualization: Top row: original video, middle-row: attention of a self-supervised model, bottom-row: attention of supervised privacy classifier. It can be observed that supervised privacy classifier mainly focuses on the semantics of human, whereas self-supervised model learns holistic spatial semantic features related to the scene (eg. track-field in (a) and tennis court in (b)) as well.(a) PlayingFlute (b) Skijet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Attention map visualization: Top row: original video, middle-row: attention of a self-supervised model, bottom-row: attention of supervised privacy classifier. It can be observed that supervised privacy classifier mainly learns semantics of human, whereas selfsupervised model learns holistic semantic spatial features related to the objects (eg. Flute in (a) and SkiJet in (b)) as well. First phase: Training of anonymization function f A . For our self-supervised method we do not require privacy labels Y t B . At the end of training, f A is frozen call it f * A , and auxiliary models f B and f T are discarded. Second phase :Target models training Target models are used to evaluate the performance of learned anonymization function f * A and are different from auxiliary models. Target utility model i.e. action classifier f ? T and Target privacy classifier f ? B are learned in supervised manner on the anonymized version of training data X t . Third phase: Target models testing: Once target models are trained from anonymized version of X t , they are are frozen and evaluated on anonymized version of test/evaluation set X e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Visual Aid for Same-dataset training and evaluation protocol Sec. 4.1 of main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). For clarification, we do not utilize privacy labels for training f A in any protocol. Privacy labels are used only for the evaluation purpose to train the target model f ? B . Once the target models f ? T and f ? B finish training on the anonymized version of train set, they are evaluated on test set (f * A (X e ), Y e T ) and (f * A (X e ), Y e B ), respectively, where e denotes evaluation/test set. Test set performance of the action classifier is denoted as A 1 T (Top-1 accuracy) or A 2 T (classwise-mAP), and the performance of privacy classifier is denoted as A 1 B (classwise-mAP) or A 2 B (classwise-F 1 ). Detailed figures explaining different training and evaluation protocols are provided in Supp.Sec.G.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where nt represents novel training) is used to train the privacy target model f ? B , and its performance is measured on novel privacy test set f * A (X ne privacy ) (where ne. represents novel evaluation) Please note that novel privacy attribute protocol may not be referred as a transfer protocol for the methods, which do not use privacy attributes Y t B in learning f A . HMDB51<ref type="bibr" target="#b23">[24]</ref> are two of the most commonly used datasets for the human action recognition. PA-HMDB<ref type="bibr" target="#b46">[47]</ref> is dataset of 515 videos annotated with video level action annotation and framewise human privacy annotations. The dataset consists of 51 different actions and 5 different human privacy attributes. P-HVU is a selected subset of LSHVU<ref type="bibr" target="#b9">[10]</ref>, which is a large-scale dataset of multi-label human actions, with a diverse set of auxiliary annotations provided for object, scenes, concepts, event etc. However, the all auxiliary annotations are not provided for all videos. We select a subset of action-object-scene labels based on their availability in the val set to create our train/test split. The dataset consists of 739 action classes, 1678 objects, and 248 scene categories. Train/test split of P-HVU consists of 245,212/16,012 videos to provide a robust evaluation.</figDesc><table><row><cell>5. Experiments</cell></row><row><cell>5.1. Datasets</cell></row><row><cell>UCF101 [42] and</cell></row></table><note>VISPR [29] is an image dataset with a diverse set of per- sonal information in an image like skin color, face, gender, clothing, document information etc. Further details are provided in Supp.Sec.B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison of existing privacy preserving action recognition method on known action and privacy attributes protocol. Our framework achieves a competitive performance to the supervised method<ref type="bibr" target="#b46">[47]</ref>. ?% denotes relative drop from raw data. For a graphical view, refer to Supp.Sec.D.</figDesc><table><row><cell>Method</cell><cell>UCF101 Action</cell><cell cols="2">VISPR1 Privacy</cell><cell>Action</cell><cell cols="2">PA-HMDB Privacy</cell><cell>Action</cell><cell cols="2">P-HVU Objects</cell><cell>Scenes</cell></row><row><cell></cell><cell cols="2">Top-1 (?) cMAP (?)</cell><cell>F1 (?)</cell><cell cols="2">Top-1 (?) cMAP (?)</cell><cell>F1 (?)</cell><cell cols="4">cMAP (?) cMAP (?) cMAP (?)</cell></row><row><cell>Raw data</cell><cell>62.33</cell><cell>64.41</cell><cell>0.555</cell><cell>43.6</cell><cell>70.1</cell><cell>0.401</cell><cell>20.1</cell><cell cols="2">11.90</cell><cell>25.8</cell></row><row><cell>Downsample-2?</cell><cell>54.11</cell><cell>57.23</cell><cell>0.483</cell><cell>36.1</cell><cell>61.2</cell><cell>0.111</cell><cell>10.9</cell><cell cols="2">2.45</cell><cell>8.6</cell></row><row><cell>Downsample-4?</cell><cell>39.65</cell><cell>50.07</cell><cell>0.379</cell><cell>25.8</cell><cell>41.4</cell><cell>0.081</cell><cell>0.78</cell><cell cols="2">0.89</cell><cell>1.76</cell></row><row><cell>Obf-Blackening</cell><cell>53.13</cell><cell>56.39</cell><cell>0.457</cell><cell>34.2</cell><cell>63.8</cell><cell>0.386</cell><cell>8.6</cell><cell cols="2">6.12</cell><cell>22.1</cell></row><row><cell>Obf-StrongBlur</cell><cell>55.59</cell><cell>55.94</cell><cell>0.456</cell><cell>36.4</cell><cell>64.4</cell><cell>0.243</cell><cell>11.3</cell><cell cols="2">6.89</cell><cell>22.8</cell></row><row><cell>Obf-WeakBlur</cell><cell>61.52</cell><cell>63.52</cell><cell>0.523</cell><cell>41.7</cell><cell>69.4</cell><cell>0.398</cell><cell>18.6</cell><cell cols="2">11.33</cell><cell>25.4</cell></row><row><cell>Noise-Features [53]</cell><cell>61.90</cell><cell>62.40</cell><cell>0.531</cell><cell>41.5</cell><cell>69.1</cell><cell>0.384</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>Supervised [47]</cell><cell>62.10</cell><cell cols="2">55.32?14% 0.461?17%</cell><cell>42.3</cell><cell cols="2">62.3?11% 0.194?51%</cell><cell>18.33</cell><cell cols="3">1.98?83% 9.5?63%</cell></row><row><cell>Ours</cell><cell>62.03</cell><cell cols="2">57.43?11% 0.473?15%</cell><cell>43.1</cell><cell cols="2">62.7?11% 0.176?56%</cell><cell>18.01</cell><cell cols="3">1.42?88% 9.91?62%</cell></row><row><cell></cell><cell cols="3">Transfer Evaluation: Action</cell><cell cols="3">Transfer Evaluation: Privacy</cell><cell cols="4">Transfer Evaluation P-HVU</cell></row><row><cell>Method</cell><cell cols="4">UCF?HMDB UCF?PA-HMDB</cell><cell cols="2">VISPR1?VISPR2</cell><cell>Action</cell><cell></cell><cell cols="2">Scenes ? Obj</cell></row><row><cell></cell><cell cols="2">Top-1(%) (?)</cell><cell>Top-1(%) (?)</cell><cell cols="2">cMAP(%) (?)</cell><cell>F1 (?)</cell><cell cols="2">cMAP(%) (?)</cell><cell cols="2">cMAP(%) (?)</cell></row><row><cell>Raw data</cell><cell cols="2">35.6</cell><cell>43.6</cell><cell></cell><cell>57.6</cell><cell>0.498</cell><cell>20.1</cell><cell></cell><cell></cell><cell>11.9</cell></row><row><cell>Downsample-2?</cell><cell cols="2">24.1</cell><cell>36.1</cell><cell></cell><cell>52.2</cell><cell>0.447</cell><cell>10.9</cell><cell></cell><cell></cell><cell>2.45</cell></row><row><cell>Downsample-4?</cell><cell cols="2">16.8</cell><cell>25.8</cell><cell></cell><cell>41.5</cell><cell>0.331</cell><cell>0.78</cell><cell></cell><cell></cell><cell>0.89</cell></row><row><cell>Obf-Blackening</cell><cell cols="2">26.2</cell><cell>34.2</cell><cell></cell><cell>53.6</cell><cell>0.46</cell><cell>8.6</cell><cell></cell><cell></cell><cell>6.12</cell></row><row><cell>Obf-StrongBlur</cell><cell cols="2">26.4</cell><cell>36.4</cell><cell></cell><cell>53.7</cell><cell>0.462</cell><cell>11.3</cell><cell></cell><cell></cell><cell>6.89</cell></row><row><cell>Obf-WeakBlur</cell><cell cols="2">33.7</cell><cell>41.7</cell><cell></cell><cell>55.8</cell><cell>0.486</cell><cell>18.6</cell><cell></cell><cell></cell><cell>11.33</cell></row><row><cell>Noisy Features [53]</cell><cell cols="2">31.2</cell><cell>41.5</cell><cell></cell><cell>53.7</cell><cell>0.458</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell>Supervised [47]</cell><cell cols="2">33.2</cell><cell>40.6</cell><cell cols="2">49.6?14%</cell><cell>0.399?20%</cell><cell>18.34</cell><cell></cell><cell cols="2">6.43?46%</cell></row><row><cell>Ours</cell><cell cols="2">34.1</cell><cell>42.8</cell><cell cols="2">47.1?18%</cell><cell>0.386?22%</cell><cell>18.01</cell><cell></cell><cell cols="2">1.42?88%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 )</head><label>1</label><figDesc></figDesc><table><row><cell>Action Rec (Top-1 Acc %) (higher is better)</cell><cell>24 26 28 30 32 34 36</cell><cell cols="2">Raw Data DownSample2X Obf-Blackening Obf-StrongBlur Obf-WeakBlur Supervised [47] Ours</cell></row><row><cell></cell><cell></cell><cell>48</cell><cell>50 privacy cMAP % (lower is better) 52 54</cell><cell>56</cell><cell>58</cell></row></table><note>, and hence [37] is still able to generalize and per- form only (&lt;5%) worse than our method. Whereas, in PHVU Scene?Obj, domain shift is huge eg TennisCourt (Scene)?TennisRacket (Obj), where [37] suffers in gener- alizing and performs significantly (&gt;40%) poor than ours. Additional experiments can be found in Supp.Sec.D and qualitative results can be found in Supp.Sec.F.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Effect of different SSL frameworksEffect of different f B and f T architectures To understand the effect of auxiliary model f B in the training process of f A , we experiment with different privacy auxiliary models f B , and report the performance of their learned f * A in the same evaluation setting as shown inTable 6. We can observe that using a better architecture of f B leads to better anonoymization. There is no significant effect of using different architectures of f T in learning f A (Supp.Sec.E).</figDesc><table><row><cell>fB architecture</cell><cell cols="3">UCF101 Top-1 (?) cMAP (?) F1 (?) VISPR1</cell></row><row><cell>MobileNetV1 (MV1)</cell><cell>62.1</cell><cell>58.14</cell><cell>0.488</cell></row><row><cell>ResNet50 (R50)</cell><cell>62.1</cell><cell>57.43</cell><cell>0.473</cell></row><row><cell>R50 + MV1</cell><cell>61.4</cell><cell>56.20</cell><cell>0.454</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Goal of this evaluation is to maintain the action recognition performance close to the raw data baseline regardless of choice of model f ? T . Our self-supervised method achieves model-agnostic action recognition performance which is also comparable to the supervised method<ref type="bibr" target="#b46">[47]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="2">R3D-18 R2Plus1D</cell><cell>R2Plus1D K400 pretraining</cell><cell>C3D</cell></row><row><cell>Raw data</cell><cell>62.3</cell><cell>64.33</cell><cell>88.76</cell><cell>58.51</cell></row><row><cell cols="2">Supervised [47] 62.1</cell><cell>62.58</cell><cell>85.33</cell><cell>56.30</cell></row><row><cell>Ours</cell><cell>62.03</cell><cell>62.71</cell><cell>85.14</cell><cell>56.10</cell></row><row><cell cols="5">Table 9. Evaluation with different architectures of action recog-</cell></row><row><cell cols="5">nition utility target model f ? T . Results shows Top-1 Accuracy</cell></row><row><cell cols="5">(%) on UCF101. D.3. Evaluating on different privacy target model</cell></row><row><cell>f ? B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">A learned anonymization function f  *  A is expected to pro-</cell></row><row><cell cols="5">vide protection against privacy leakage from any privacy</cell></row><row><cell cols="5">target model f ? B . In training of anonymizatoin function , we</cell></row><row><cell cols="5">use ResNet50 as the auxiliary privacy model f B and eval-</cell></row><row><cell cols="5">uate the learned anonymization f  *  A on target privacy clas-sifiers f ? B like ResNet18/50/34/101/152 and MobileNet-V1</cell></row><row><cell cols="3">with and without ImageNet</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .Table 12 .Table 13 .</head><label>111213</label><figDesc>, 10. We can see our self-supervised framework 65.30 0.5554 64.40 0.5553 60.70 0.5269 58.83 0.4852 61.21 0.5056 Supervised 53.84 0.4402 53.22 0.4283 53.97 0.4459 53.55 0.4257 51.05 0.4030 52.48 0.4013 Ours 54.83 0.4574 54.09 0.4226 57.43 0.4732 52.94 0.4096 53.27 0.4322 53.41 0.3974 Table 10. Evaluating f * A for privacy leakage against different architectures of privacy target model f ? B. Results shown on VISPR-1 dataset. Lower privacy classification score is better. Our self-supervised method gets a model-agnostic privacy anonymization performance which is also comparable to the supervised method<ref type="bibr" target="#b46">[47]</ref>. Similar setting asTable 10, but f ? B is initialized with ImageNet Pretraining. Trade-off between action classification and privacy classifier in a practical scenario where target utility model is taken from Kinetics400 checkpoint and target privacy model is raw-data pretrained. UCF101 is used as action classification dataset and VISPR is used as privacy dataset. ?% denotes relative drop from raw data. With a small drop in action recognition performance our method greatly reduce privacy leakage. Auxiliary utility model fT architecture has no significant effect on final action-privacy measures. Auxiliary models are just used to train the anonymization function and discarded after that. All results are reported on ResNet50 privacy target model f ?</figDesc><table><row><cell>Method</cell><cell cols="5">ResNet18 cMAP (%)(?) F1(?) cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 ResNet34 ResNet50 ResNet101 ResNet152 MobileNet-V1</cell></row><row><cell>Raw data</cell><cell cols="5">64.38 0.5385 Method ResNet18 cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 cMAP (%) F1 ResNet34 ResNet50 ResNet101 ResNet152</cell></row><row><cell></cell><cell cols="2">Raw data</cell><cell cols="3">69.82 0.6041 69.55 0.6447 70.66 0.6591 71.09 0.6330 69.50 0.6130</cell></row><row><cell></cell><cell cols="5">Supervised 58.05 0.5367 58.02 0.5463 62.01 0.5281 61.44 0.5553 61.88 0.5711</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell cols="3">59.10 0.5302 59.71 0.5227 60.73 0.5689 59.24 0.5601 60.51 0.5352</cell></row><row><cell>Method</cell><cell cols="3">Top-1 Acc (%) (?)</cell><cell cols="2">cMAP (%) (?) ResNet18 ResNet50 ResNet101</cell></row><row><cell>Raw data</cell><cell></cell><cell>88.76</cell><cell></cell><cell>64.38</cell><cell>64.40</cell><cell>60.70</cell></row><row><cell cols="3">Downsample-2x 77.45</cell><cell></cell><cell>49.37</cell><cell>51.23</cell><cell>50.72</cell></row><row><cell cols="3">Downsample-4x 63.53</cell><cell></cell><cell>36.22</cell><cell>38.82</cell><cell>40.68</cell></row><row><cell cols="2">Obf-Blackening</cell><cell>72.11</cell><cell></cell><cell>46.48</cell><cell>48.38</cell><cell>47.92</cell></row><row><cell cols="2">Obf-StrongBlur</cell><cell>74.10</cell><cell></cell><cell>53.30</cell><cell>54.44</cell><cell>52.39</cell></row><row><cell>Supervised</cell><cell></cell><cell cols="4">85.33 19.23?70% 22.81?64% 22.01?64%</cell></row><row><cell>Ours</cell><cell></cell><cell cols="4">85.01 22.16?66% 23.44?64% 22.64?63%</cell></row><row><cell cols="2">fT architecture</cell><cell cols="4">UCF101 Top-1(%) (?) cMAP(%) (?) F1 (?) VISPR1</cell></row><row><cell>R3D-18</cell><cell></cell><cell cols="3">62.03</cell><cell>57.43</cell><cell>0.4732</cell></row><row><cell cols="2">R2+1D-18</cell><cell cols="3">62.37</cell><cell>57.37</cell><cell>0.4695</cell></row><row><cell>R3D-50</cell><cell></cell><cell cols="3">62.58</cell><cell>57.51</cell><cell>0.4707</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B</cell></row><row><cell cols="6">and R3D-18 action recognition target model f ? T .</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/milesial/Pytorch-UNet 2 https://github.com/pytorch/vision/tree/main/torchvision/models</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Vishesh Kumar Tanvar, Tushar Sangam, Rohit Gupta, and Zhenyu Wu for constructive suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The privacy-utility tradeoff for remotely teleoperated robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cakmak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction</title>
		<meeting>the tenth annual ACM/IEEE international conference on human-robot interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A vision-based system for monitoring elderly people at home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Buzzelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Alb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluigi</forename><surname>Ciocca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">374</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020. 3, 4</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Privacypreserving action recognition for smart hospitals using lowresolution depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09950</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving recognition of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrouz</forename><surname>Saghafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tclr: Temporal contrastive learning for video representation. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamshad</forename><surname>Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="103406" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gabriellav2: Towards better generalization in surveillance videos for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zacchaeus</forename><surname>Scheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Shiraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale holistic video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Motion-augmented self-training for video recognition at smaller scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Gavrilyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Karmanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10429" to="10438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards good practice for action recognition with spatiotemporal 3d convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning privacy-preserving optics for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Hinojosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Arguello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Time-equivariant contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="9970" to="9980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using time-of-flight measurements for privacy-preserving tracking in a smart room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard J Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="689" to="696" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Jocher</surname></persName>
		</author>
		<ptr target="https://github.com/ultralytics/yolov5" />
		<title level="m">ultralytics/yolov5: v3.1 -Bug Fixes and Performance Improvements</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Privacy-preserving in-home fall detection using visual shielding sensing and private information-embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indoor privacy-preserving action recognition via partially coupled convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leilei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Argus: Efficient activity detection system for extended video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards a visual privacy advisor: Understanding and predicting privacy risks in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Videomoco: Contrastive video representation learning with temporally adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11205" to="11214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning privacy preserving encodings through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Pittaluga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Koppal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayan</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="791" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Privacy preserving optics for miniature vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Pittaluga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koppal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Precapture privacy for small vision sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Pittaluga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2215" to="2226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhancing selfsupervised video representation learning via multi-level feature optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huabin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangrui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-supervised video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchana</forename><surname>Ranasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01514,2021.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to anonymize faces for privacy preserving action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongzheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the european conference on computer vision (ECCV)</title>
		<meeting>the european conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gabriella: An online system for real-time activity detection in untrimmed security videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Mamshad Nayeem Rizve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Tirupattur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jung Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishan R Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4237" to="4244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Privacy-preserving human activity recognition from extreme low resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Jong</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human pose estimation on privacy-preserving low-resolution depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinkle</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="583" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Privacy-preserving action recognition using coded aperture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zihao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pittaluga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sing Bing</forename><surname>Cossairt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep action recognition: An adversarial learning framework and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving visual recognition via adversarial training: A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="606" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial learning of privacy-preserving and task-oriented representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taihong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12434" to="12441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno>PMLR, 2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Privacy preserving automatic fall detection for elderly using rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Capezuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers for Handicapped Persons</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="625" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Collective protection: Preventing sensitive inferences via integrative transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cilloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Fleming</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-scale, class-generic, privacypreserving video</title>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
