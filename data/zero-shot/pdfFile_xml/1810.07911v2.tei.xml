<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<email>zhidingy@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
								<address>
									<postCode>95051</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
							<email>jinsong.wang@gm.com</email>
							<affiliation key="aff2">
								<orgName type="department">General Motors R &amp; D</orgName>
								<address>
									<postCode>48092</postCode>
									<settlement>Warren</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>[GitHub] [Project Page] [Slides] [Poster]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world "wild tasks" where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as "domain gap", and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes in pseudo-label generation, and introduce spatial priors to refine the generated pseudo-labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world "wild tasks" where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as "domain gap", and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes in pseudo-label generation, and introduce spatial priors to refine the generated pseudo-labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a core computer vision task where one aims to densely assign labels to each pixel in the input image. In the past decade, significant amount of effort has been devoted to this area [1, <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, leading to considerable progress with the recent advance of deep representation learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. The competition on major open benchmark datasets <ref type="bibr" target="#b10">[11]</ref> have resulted in a number of more powerful models that tend to overfit to the benchmark data. While the boundaries of benchmark performance have been pushed to new limits, these models often encounter challenges in practical applications such as autonomous driving, where one needs ubiquitous good performance of the perception module. This is because benchmark datasets are usually biased to specific environments, while the testing scenario may encounter large indicates equal contribution.  domain differences caused by a number of factors, including change of geological position, illumination, camera, weather condition, etc. In this case, even the performance of a powerful model often drops dramatically, and such issue can not be easily remediated by further building up the model power <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>A natural idea to improve network's generalization ability is to collect and annotate data covering more diverse scenes. However, densely annotating image is time-consuming and labor-intensive. An example is the Cityscapes dataset, where each image on average takes about 90 minutes to annotate <ref type="bibr" target="#b10">[11]</ref>. To overcome such limitation, efforts were made to efficiently generate densely annotated images from rendered scenes, such as the SYNTHIA Dataset <ref type="bibr" target="#b27">[28]</ref>, the Grand Theft Auto V (GTA5) Dataset <ref type="bibr" target="#b25">[26]</ref> and the VIPER Dataset <ref type="bibr" target="#b24">[25]</ref>. However, the large appearance gap across the simulated domain and the real domain can significantly degrade the performance of synthetically trained models.</p><p>In light of the above issues, we focus on the challenging problem of unsupervised domain adaptation for semantic segmentation in this paper, aiming to unsupervisedly adapt a segmentation model trained on a labeled source domain to a target domain without knowing target labels. Recently, the problem of unsupervised domain adaptation has been widely explored on classification/segmentation/detection tasks. There exist a predominant trend to use adversarial training based methods to match image-level/feature-level/predictionlevel distributions of both source and target domains <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref>. In particular, these methods aim to minimize a domain adversarial loss to reduce the discrepancy between source and target feature distributions, while retaining good performance on source domain task by minimizing the task-specific loss.</p><p>While adversarial training based methods have recently achieved great success in domain adaptation, in this work we show that comparable or even better adaptation performance can be achieved by taking an alternative way without adversarial training. Our proposed method is a self-training based learning framework where one predicts in the target domain and in turn uses the predictions to update the model. In this way, class-wise feature space alignment and task-specific learning are implicitly unified together under a single unified loss. This differs from adversarial training based methods which seek to adapt by confusing the domain discriminator, with domain alignment standalone from task-specific learning under a separate loss.</p><p>The idea of self-training is not new. Traditional self-training methods have been commonly used in semi-supervised learning (SSL) problems as a framework towards learning better classifier decision boundaries with unlabeled data <ref type="bibr" target="#b14">[15]</ref>. However, most of these methods deal with handcrafted features which are fixed inputs to classifiers. A subtle difference between these methods and deep selftraining is that the latter allows learnable/flexible deep embeddings as input to a classifier. Upon minimizing the self-training loss, the limited representation power of classifier not only leads to better decision boundaries, but also drives the feature embeddings across domains to be similar. As a result, deep self-training can become a powerful class-wise domain adaptation method. The method also coincides with the recent trend of class-wise/conditional adversarial domain adaptation <ref type="bibr" target="#b9">[10]</ref> in the sense that they all can be broadly regarded as EM-like adaptation frameworks with network predictions being latent variables.</p><p>To the best of our knowledge, this is one of the early works applying deep self-training to the recent tasks of domain adaptation for semantic segmentation. We propose a deep self-training (ST) framework of which the workflow is shown in <ref type="figure" target="#fig_0">Figure 1</ref> with GTA5 ? Cityscapes being an example. Self-training is performed by alternately generating a set of pseudo-labels corresponding to confident 4 predictions in the target domain, and then fine-tuning the network model based on these pseudo-labels together with the labeled source data. Note that such framework implicitly assumes that target samples with more confident predictions tend to have higher prediction accuracies.</p><p>Vanilla ST treats every class equally when measuring the prediction confidence. However, the domain gaps caused by appearance difference and scale difference can vary significantly across different classes. For instance, different countries may have quite dissimilar construction layouts, but traffic lights and vehicles are often more or less similar. As a result, it is harder for a source models to transfer knowledge on the construction class than on traffic light and vehicle. Such issue leads to different difficulties for a network to learn transferable knowledge as well as non-unified confidence levels for various classes in the target domain. We observe vanilla ST tends to bias towards easy classes while ignoring other difficult ones, since ST universally chooses pseudo-labels with high confidence. The problem causes decreased adaptation performance as pseudo-labels of the difficult classes diminish during training. To address this problem, we also propose a class-balanced self-training (CBST) framework. Our contributions in this paper can be summarized as follows:</p><p>-We introduce a deep self-training framework with iterative self-paced learning policy towards domain adaptation. Specifically, we treat the unknown target domain labels as latent variables (pseudo-labels), and formulate pseudolabel estimation and network training as a unified loss minimization problem which can optimized end-to-end via mixed integer programming. -To address the issue of imbalanced pseudo-labels in ST, we propose a novel class-balanced self-training framework towards more balanced pseudo-label generation. The proposed framework performs class-wise confidence normalization, by dividing the confidence of each pseudo-label with a rank-based reference confidence from that particular class. -We also observe that traffic scenes often share similar spatial layouts. As a result we also introduce spatial priors (SP) to improve cross-domain adaptation. We incorporate spatial priors into the proposed CBST framework, leading to CBST with spatial priors (CBST-SP). -Our approaches are comprehensively evaluated under both synthetic-to-real (SYNTHIA/GTA5 to Cityscapes) and real-to-real (Cityscapes-to-NTHU) settings with currently state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Domain adaptation: UDA problems have been widely investigated for their importance in a number of real world tasks. A major idea to perform domain adaption is to learn domain invariant embeddings by reducing the difference between source and target domain distributions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Among them, Maximum Mean Discrepancy (MMD) and its kernel variants has been a popular target towards minimizing the cross-domain difference of feature distributions. More recently, there has been an increasing interest in using adversarial training based methods to reduce the domain gaps <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Self-training for SSL: Self-training methods have been widely studied in semisupervised learning <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b47">48]</ref>, with applications to vision and natural language processing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>. Given the inherent relation between UDA and SSL in their forms, Tang et al. <ref type="bibr" target="#b34">[35]</ref> addressed image-to-video detector adaptation, by incorporating self-paced sample selection into self-training with a weight shifting policy to gradually increase target domain samples. Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed a variant of co-training <ref type="bibr" target="#b1">[3]</ref> for domain adaptation, by jointly learning target predictor, view split, and subset selection in a single optimization problem. Semantic segmentation: Recent advances in deep learning have aroused broad interests in semantic segmentation using convolutional neural networks (CNNs). Long et al. <ref type="bibr" target="#b20">[21]</ref> proposed fully convolutional network (FCN) towards pixel-level dense prediction. Since then, several powerful segmentation networks have been proposed, including DeepLab v2/v3 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, ResNet-38 <ref type="bibr" target="#b40">[41]</ref>, PSPNet <ref type="bibr" target="#b45">[46]</ref> etc.</p><p>UDA for Semantic segmentation: It is commonly observed that the representation power of a segmentation model does not transfer well to its crossdomain performance. As a result, domain adaptation for semantic segmentation recently emerged as a hot topic. A number of adaptation methods are proposed, including adversarial training at input image level <ref type="bibr" target="#b16">[17]</ref>, feature level <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, and network output level <ref type="bibr" target="#b35">[36]</ref>. Specifically, <ref type="bibr" target="#b16">[17]</ref> seeks to reduce domain gap by first transferring source images to target style with a cycle consistency loss, and then aligning the cross-domain feature distributions of the task network through adversarial training. In addition, <ref type="bibr" target="#b29">[30]</ref> propose to detect non-discriminative samples near decision boundaries through a critic network, and let the generator learn to generate more discriminative features by fooling the critic network with adversarial training. <ref type="bibr" target="#b44">[45]</ref> proposed a curriculum adaption method to regularize the distributions of predicted labels in the target domain such that they follow the label distributions in source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning for supervised domain adaptation</head><p>Assuming that the labels in both source and target are available, possibly the most direct way to perform domain adaptation is supervised fine-tuning the model on both domains. For semantic segmentation nets with softmax output, the adaptation problem can be formulated as minimizing the following loss function:</p><formula xml:id="formula_0">min w L S (w) = ? S s=1 N n=1 y s,n log(p n (w, I s )) ? T t=1 N n=1 y t,n log(p n (w, I t )) (1)</formula><p>where I s denotes the image in source domain indexed by s = 1, 2, ..., S, y s,n the ground truth label for the n-th pixel (n = 1, 2, ..., N ) in I s , and w contains the network weights. p n (w, I s ) is the softmax output containing the class probabilities at pixel n. Similar definitions apply for I t , y t,n and p n (w, I t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-training for unsupervised domain adaptation</head><p>In the case of unsupervised domain adaptation, the target ground truth labels are not available. An alternate way to fine-tune the segmentation model is to consider the target labels as hidden variables that can be learned. Accordingly, the problem can be formulated as follows:</p><formula xml:id="formula_1">min w,? L U (w,?) = ? S s=1 N n=1 y s,n log(p n (w, I s )) ? T t=1 N n=1? t,n log(p n (w, I t )) s.t.? t,n ? {e (i) |e (i) ? R C }, ?t, n<label>(2)</label></formula><p>where? indicates the set of target labels, C is the number of classes, and e (i) indicates a one-hot vector whose i-th entry is 1. By minimizing the loss in Eq.</p><p>(2) with respect to?, the optimized? should approximate the underlying true target ground truth. Domain adaptation can then be performed similarly to Eq.</p><p>(1). We call? "pseudo-labels", and regard such training strategy as self-training.</p><p>4 Proposed methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Self-training (ST) with self-paced learning</head><p>Jointly learning the model and optimizing pseudo-labels on unlabeled data is naturally difficult as it is not possible to completely guarantee the correctness of the generated pseudo-labels. A better strategy is to follow an "easy-to-hard" scheme via self-paced curriculum learning, where one seeks to generate pseudolabels from the most confident predictions and hope they are mostly correct. Once the model is updated and better adapted to the target domain, the scheme then explores the remaining pseudo-labels with less confidence. To incorporate curriculum learning, we consider the following revised self-training formulation:</p><formula xml:id="formula_2">min w,? L ST (w,?) = ? S s=1 N n=1 y s,n log(p n (w, I s )) ? T t=1 N n=1 ? t,n log(p n (w, I t )) + k|? t,n | 1 s.t.? t,n ? {{e (i) |e (i) ? R C } ? 0}, ?t, n k &gt; 0<label>(3)</label></formula><p>where assigning y s,n as 0 leads to ignoring this pseudo-label in model training, and the L 1 regularization serves as a negative sparse promoting term to prevent the trivial solution of ignoring all pseudo-labels. k is a hyperparameter controlling the amount of ignored pseudo-labels. A larger k encourages the selection of more pseudo-labels for model training. To minimize the loss in Eq. <ref type="formula" target="#formula_2">(3)</ref>, we take the following alternative block coordinate descent algorithm:</p><p>a) Fix (initialize) w and minimize the loss in Eq. 3 with respect to? t,n .</p><p>b) Fix? t,n and optimize the objective in Eq. 3 with respect to w.</p><p>We call one step of a) followed by one step of b) as one round. In this work, we propose a self-training algorithm where step a) and step b) are alternately repeated for multiple rounds. Intuitively, step a) selects a certain portion of most confident pseudo-labels from the target domain, while step b) trains the network model given the pseudo-labels selected in step a). <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the proposed algorithm flow in the domain adaptation example of GTA5 ? Cityscapes. Solving step b) leads to network learning with stochastic gradient descent. However, solving step a) requires a nonlinear integer programming given the optimization over discrete variables. Given k &gt; 0, step a) can be rewritten as:</p><formula xml:id="formula_3">min y ? T t=1 N n=1 C c=1? (c) t,n log(p n (c|w, I t )) + k|? t,n | 1 s.t.? t,n = ? (1) t,n , ...,? (C) t,n ? {{e (i) |e (i) ? R C } ? 0}, ? t, n k &gt; 0 (4)</formula><p>Since? t,n is required to be either a discrete one-hot vector or a zero vector, the pseudo-label configuration can be optimized via the following solver:</p><formula xml:id="formula_4">y (c) * t,n = ? ? ? ? ? ? ? 1, if c = arg max c p n (c|w, I t ), p n (c|w, I t ) &gt; exp(?k) 0, otherwise<label>(5)</label></formula><p>Unlike traditional self-training adaptation with handcrafted features that learn a domain-invariant classifier, CNN based self-training can learn not only domaininvariant classifier but also domain-invariant features. The softmax loss implicitly tries to reduce the domain difference in feature space. In addition, the selftraining also has the missing value (pseudo-label) problem, similar to EM algorithm. The proposed alternate optimization method can learn the weights of models without prior observation of target domain labels.</p><p>One may note that the proposed framework is similar to <ref type="bibr" target="#b34">[35]</ref> and several other related works. However, the proposed method presents a more unified model for self-training and self-paced learning, in the sense that pseudo-label generation is unified with curriculum learning under a single learning framework. More importantly, in terms of the specific application, the above self-training framework sheds light on a relatively new direction for adapting semantic segmentation models. We will show that self-training based methods lead to considerably better or competitive performance compared to many current state of the art methods that are predominantly based on adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Class-balanced self-training (CBST)</head><p>As mentioned in section 1, the difference in visual domain gap and class distribution can cause different levels of domain-transfer difficulties among different classes, which on average results in relatively higher prediction confidence scores for easy-to-transfer classes in the target domain. A problem with vanilla self-training is that it does not take such issue into consideration, but selects pseudo-labels by referring to their confidence universally across different classes. A consequent issue is that the model tends to bias towards some initially welltransferred classes while ignoring other hard classes along the training process. It is thus difficult for ST to perform well in certain domain adaptation scenarios. To overcome this issue, we propose a framework towards class-balanced self-training, where class-wise confidence levels are normalized to cancel the influence from diverse confidence levels:</p><formula xml:id="formula_5">min w,? L CB (w,?) = ? S s=1 N n=1 y s,n log(p n (w, I s )) ? T t=1 N n=1 C c=1 ? (c) t,n log(p n (c|w, I t )) + k c? (c) t,n s.t.? t,n = ? (1) t,n , ...,? (C) t,n ? {{e (i) |e (i) ? R C } ? 0}, ?t, n k c &gt; 0, ?c<label>(6)</label></formula><p>where each k c is a separate parameter determining the proportion of selected pseudo-labels in class c. As one may observe, it is the difference between k c that introduces different levels of class-wise bias for pseudo-label selection, and addresses the issue of inter-class balance.</p><p>The optimization flow of class-balanced self-training is the same as in Eq. (3) except for the generation of pseudo-labels. Again, we can rewrite the step of pseudo-label optimization as:</p><formula xml:id="formula_6">min y ? T t=1 N n=1 C c=1 ? (c) t,n log(p n (c|w, I t )) + k c? (c) t,n s.t.? t,n = ? (1) t,n , ...,? (C) t,n ? {{e|e ? R C } ? 0}, ? t, n k c &gt; 0, ? c<label>(7)</label></formula><p>Note that the loss function in Eq. <ref type="bibr" target="#b6">(7)</ref> can not be trivially minimized by the solver of Eq. (3). Instead, optimizing Eq. (7) leads to the following solver:</p><formula xml:id="formula_7">y (c) * t,n = ? ? ? ? ? ? ? ? ? ? ? ? ? 1, if c = arg max c p n (c|w, I t ) exp(?k c ) , p n (c|w, I t ) exp(?k c ) &gt; 1 0, otherwise<label>(8)</label></formula><p>From Eq. (8), one can see that pseudo-label generation in Eq. (6) is no longer dependent on the output p n (c|w, I t ), but hinges on the normalized output pn(c|w,It) exp(?kc) . Pseudo-label assignment using this normalized output owns the benefit of balancing towards the class with relatively low score but having high within-class confidence. As a result, k c should be set in a way that exp(?k c ) encodes the response strength of each class to balance different classes. In addition, for CBST, the pseudo-label of any pixel is only filtered when all the balanced responses are smaller than 1. There could also be multiple classes with pn(c|w,It) exp(?kc) &gt; 1. In this case, the class with the maximum balanced response is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Self-paced learning policy design</head><p>Determination of k in ST From the previous section, we know that k essentially plays a key role in pseudo-label selection, by filtering out those with confidence lower than exp(?k). We can design the following policy on k such that we gradually increase the proportion of selected pseudo-labels in each round:</p><p>We take the confidence of all pixels from the whole target set, and sort their confidence in a descending order. We then set the value of k such that exp(?k) equals to the probability ranked at round(p * T * N ), where p indicates the pseudolabel proportion and is between [0, 1]. In this case, pseudo-label optimization produces p ? 100% most confident pseudo-labels for network training. The above pseudo-label selection policy is summarized in Algorithm 1. We design the self-paced learning policy such that more pseudo-labels are incorporated for each additional round. In particular, we start p from 20%, and empirically add 5% to p in each additional round of pseudo-label generation. The maximum portion is set to be 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Determination of k in ST</head><p>Determination of k c in CBST The policy of k c in CBST can be defined similar to ST. Although CBST seemingly introduce much more parameters than ST, we propose a strategy to determine k c for every class with a single parameter p, while effectively encoding the class-wise confidence levels. The proposed strategy is described in Algorithm 2.</p><p>Note that Algorithm 2 determines k c by ranking the class c probabilities on all pixels predicted as class c, and setting k c such that exp(?k c ) equals to the probability ranked at round(p * N c ), where N c indicates the number of pixels predicted as class c. Such a strategy basically takes the probability ranked at p ? 100% separately from each class as a reference for both thresholding and confidence normalization. The pseudo-label proportion p and its increasing policy is defined exactly the same to ST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Incorporating spatial priors</head><p>For adapting models in the case of street scenes, we could take advantage of the spatial prior knowledge. Traffic scenes have common structures. For example, sky is not likely to appear at the bottom and road is not likely to appear at the top. If the image views in source domain and target domain are similar, we believe this knowledge can help to adapt source model. Thus we introduce spatial priors, similar to <ref type="bibr" target="#b31">[32]</ref>, by counting the class frequencies in the source domain, followed by smoothing with a 70 ? 70 Gaussian kernel. In particular, we use q n (c) to indicate the frequency of class c at pixel n. Upon obtaining the class frequencies, we also normalize them by requiring N i=1 q n (c) = 1. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the heat map of spatial priors, calculated from GTA5 dataset, where yellow color indicates higher energy and blue color indicates lower energy.</p><p>To incorporate spatial priors into proposed CBST, we multiply the softmax output with the spatial priors, and consider the resulting potential as selection metric in pseudo-label generation:</p><formula xml:id="formula_8">min w,? L SP (w,?) = ? S s=1 N n=1 y s,n log(p n (w, I s )) ? T t=1 N n=1 C c=1 ? (c) t,n log(q n (c)p n (c|w, I t )) + k c? (c) t,n s.t.? t,n ? {{e|e ? R C } ? 0}, ?t, n k c &gt; 0, ?c<label>(9)</label></formula><p>We denote this as CBST-SP. The self-training and self-paced learning policy are identical to CBST, except that the potential q n (c)p n (c|w, I t ) is used to replace p n (c|w, I t ) in CBST. It should be noted that incorporating the spatial prior does not change network training, since q n (c) can be taken out of log(?) as a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we provide comprehensive evaluations of the proposed methods by performing experiments on three domain adaptation settings. We first consider a cross-city adaptation scenario by transferring segmentation models from Cityscapes training set to the NTHU Dataset <ref type="bibr" target="#b9">[10]</ref>, where the dataset contains 400 images of size 1, 024 ? 2, 048 from 4 different cities: Rome, Rio, Tokyo and Taipei. We also consider another two challenging synthetic-to-real scenarios: SYNTHIA <ref type="bibr" target="#b27">[28]</ref> to Cityscapes and GTA5 <ref type="bibr" target="#b25">[26]</ref> to Cityscapes. We use the SYNTHIA-RAND-CITYSCAPES subset which includes 9,400 labeled images of size 760 ? 1280. The GTA5 Dataset includes 24,966 annotated images of size 1, 052 ? 1, 914 rendered by the GTA5 game engine. In both of the above settings, the validation set of Cityscapes is treated as target domain.</p><p>We use FCN8s-VGG16 <ref type="bibr" target="#b20">[21]</ref> as one of the backbone networks in SYNTHIA to Cityscapes and GTA5 to Cityscapes to give a fair comparison with other methods using the same backbone. In addition, we boost our method performance with a better backbone network ResNet-38 <ref type="bibr" target="#b40">[41]</ref>. Our implementations are based on MXNet <ref type="bibr" target="#b8">[9]</ref>, where we pretrain the networks on ImageNet <ref type="bibr" target="#b28">[29]</ref> and fine-tune on source datasets with SGD. We also apply a hard sample mining strategy which mines the smallest classes according to target domain predictions. In particular, during random cropping on each target image for network input, priorities are given to classes whose predicted portions are less than 0.1%, by selectively cropping at locations containing pixels predicted to those classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Small domain shift: Cross-city adaptation</head><p>The NTHU dataset <ref type="bibr" target="#b9">[10]</ref> contains 13 overlapping classes with Cityscapes. It should be noted that we train a 19 class model on Cityscapes and then evaluate it on the 13 NTHU classes. Similar to the rule of NTHU dataset, we consider pole, fence, wall as buildings, truck as car and terrain as vegetations. Following <ref type="bibr" target="#b9">[10]</ref>, we split 100 images into 10 folds for each city, and report the cross-validation results by each time self-training with 90 unlabeled images and testing on the remaining 10. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>, which indicates that CBST achieves comparable or even better performance compared with state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large domain shift: Synthetic-to-real adaptation</head><p>SYNTHIA to Cityscapes We follow the same evaluation protocol as other works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref>, where we choose the 16 overlapping classes between SYNTHIA and Cityscapes as valid classes for evaluation. There is another setting which only considers 13 classes excluding wall, fence and pole <ref type="bibr" target="#b35">[36]</ref>. <ref type="table">Table 2</ref> reports the results, where mIoU* is the mean IoU of 13 classes, excluding the classes with *. With FCN8s-VGG16 as backbone, our CBST provides competitive performance compared with other methods. Equipped with a better backbone ResNet-38, CBST achieves the superior performance outperforming state-of-the-art by 1.7. Compared with ST, CBST with either FCN8s-VGG16 or ResNet-38 achieves better performance for mIoU and IoU of these initially  <ref type="figure">Figure 3</ref> gives the visualization segmentation results in Cityscapes. <ref type="table">Table 3</ref> gives the results on the 19 valid classes. For models with FCN8s-VGG16 as backbone, the performance of ST demonstrates that the adapted model can easily bias to easy-to-transfer classes. However, the CBST not only achieves better mIoU than ST, but also better IoU for these initial hard-to-transfer classes. Since images from GTA5 and Cityscapes have similar structure layouts, we evaluate the performance of CBST-SP, and achieves an mIoU of 36.1% better than some methods using powerful backbones such as ResNet-50 <ref type="bibr" target="#b29">[30]</ref> and DenseNet <ref type="bibr" target="#b23">[24]</ref>. With a more powerful model ResNet-38, our framework achieves an mIoU of 46.2%. Finally, multi-scale testing (MST) at scales 0.5, 0.75, and 1.0 boosts the mIoU to 47.0%. Qualitative results on Cityscapes from different comparing methods are visualized in <ref type="figure" target="#fig_4">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5 to Cityscapes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a deep self-training frameworks towards unsupervised domain adaptation for semantic segmentation. The framework is formulated as a loss minimization problem and can be learned end-to-end. We also introduce a class-balanced self-training (CBST) framework to overcome the imbalance issue of transferring difficulty among classes via generating pseudo-labels with balanced class distribution. Finally, if both the source and target domains share similar spatial layouts, we can incorporate spatial priors in self-training, which also improves the adaptation quality. In the experiment, we demonstrate that our method achieves good results which outperform some state-of-the-art methods with considerable margins. This empirically suggests that self-training based approaches may provide an effective alternative towards domain adaptation besides adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional implementation details</head><p>For training FCN8s-VGG16 in our experiments, we use SGD with learning rate of 1 ? 10 ?6 , batch size of 1, and input image patch with 500 ? 500 crop size.</p><p>For training ResNet-38 in our experiments, we use SGD with learning rate of 1?10 ?4 and a mini-batch of two image patches with 500?500 crop size, plus the augmentation of random multi-scale resizing (0.7 ? 1.3) and horizontal flipping. Fine-tuning in each self-training round contains two target epochs. Note that for all experiments in the main paper, we start the pseudo-label selection portion p from 20%, and incrementally add 5% to p in each additional round of pseudolabel generation until p reaches 50%. All results in the main paper are unified to report the mIoUs of the self-trained models at the 3 round (6 epochs).</p><p>B Additional results on GTA5 ? BDD-V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Ablation Study</head><p>We show additional ablation studies on GTA5 to Berkeley DeepDrive Video Dataset (BDD-V) [1], where we evaluate all self-training based methods, including ST, CBST and CBST-SP. In addition, we also evaluate the results of ST-SP which is self-training with spatial priors. The BDD-V Dataset is a recently released dataset containing 5,561 1280?720 images, where the dataset is divided into 3,333 annotated training images, 745 annotated validation images and 1,483 unlabeled test images. The dataset was collected using the NEXAR dashcam interface, and has 41 classes where 19 valid evaluation classes overlap with those in Cityscapes and GTA5. <ref type="table" target="#tab_4">Table 4</ref> shows the  quantitative results of GTA5 to BDD-V validation. The experiment consistently shows the effectiveness of the proposed self-training frameworks.</p><p>One could see that both CBST-SP, CBST are better than ST, with CBST-SP being the best among them. Moreover, the class-balanced pseudo label generation strategies particularly benefits the adaptation of classes with small object scales, such as pols, traffic light, pedestrian and motorcycle, etc. We additionally evaluate self-training with spatial priors (ST-SP), and found that the priors also benefit the vanilla self-training but the performance is lower than CBST-SP. This demonstrates the effectiveness of both CBST and SP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Parameter analysis</head><p>We also conduct sensitivity analysis for ST-SP and CBST-SP on the policy parameters of k and k c . Recap that in ST-SP/CBST-SP, both k and k c are determined by a single policy parameter p which is the portion of pseudo-labels. We conduct analysis by changing both the initial value p 0 and the per round increment size p. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the system performance curves of the comparing methods at different self-training rounds with different (p 0 / p) configurations. It can be observed that CBST-SP is not very sensitive to k c and shows convergence behavior better than ST-SP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the proposed itertive self-training framework for unsupervised domain adaptation. Left: algorithm workflow. Right figure: semantic segmentation results on Cityscapes before and after adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Input: 2 P 3 MP 4 M</head><label>234</label><figDesc>Neural network P (w), target images It, pseudo-label portion p Output: k 1 for t=1 to T do I t = P(w,It) I t = max(P I t ,axis=0) = [M, matrix to vector(MP I t )] 5 end 6 M = sort(M,order=descending) 7 len th = length(M) ? p 8 k = -log(M[len th ]) 9 return k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 : 2 P 3 LP 4 MP 5 for c=1 to C do 6 MP</head><label>223456</label><figDesc>Determination of k c in CBST Input : Neural network f (w), target images It, pseudo-label portion p Output: kc 1 for t=1 to T do I t = P(w,It) I t = argmax(P,axis=0) I t = max(P,axis=0) c,I t = MP I t (LP I t == c) 7 Mc = [Mc, matrix to vector(MP c,I t )] 8 end 9 end 10 for c=1 to C do 11 Mc = sort(Mc,order=descending) 12 len c,th = length(Mc) ? p 13 kc = -log(Mc[len c,th ]) 14 end 15 return kc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization of class-wise spatial priors on GTA5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Adaptation results on GTA5 ? Cityscapes. Rows correspond to sampled images and predictions. Columns correspond to original images, ground truth, and results of source ResNet-38, ST, CBST and CBST-SP. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Parameter analysis on GTA5 ? BDD-V. Top: ST-SP. Bottom: CBST-SP. Left: Different p0. Right: Different p. Legends: p0/ p. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1810.07911v2 [cs.CV] 25 Oct 2018</figDesc><table><row><cell>road</cell><cell cols="2">sidewalk building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic lgt</cell><cell>traffic sgn</cell><cell>vegetation</cell><cell></cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results for Cityscapes ? NTHU dataset City Method Road SW Build TL TS Veg. Sky PR Rider Car Bus Motor Bike Mean Rome Source Dilation-Frontend [10] 77.7 21.9 83.5 0.1 10.7 78.9 88.1 21.6 10.0 67.2 30.4 6.1 0.6 38.2 GCAA [10] 79.5 29.3 84.5 0.0 22.2 80.6 82.8 29.5 13.0 71.7 37.5 25.9 1.0 42.9 DeepLab-v2 [36] 83.9 34.3 87.7 13.0 41.9 84.6 92.5 37.7 22.4 80.8 38.1 39.1 5.3 50.9 MAA [36] 83.9 34.2 88.3 18.8 40.2 86.2 93.1 47.8 21.7 80.9 47.8 48.3 8.6 53.8 Source Resnet-38 86.0 21.4 81.5 14.3 47.4 82.9 59.8 30.8 20.9 83.1 20.2 40.0 5.6 45.7 ST 85.9 20.2 84.3 15.0 46.4 84.9 73.5 48.5 21.6 84.6 17.6 46.2 6.7 48.9 CBST 87.1 43.9 89.7 14.8 47.7 85.4 90.3 45.4 26.6 85.4 20.5 49.8 10.3 53.6 85.2 26.4 15.6 76.7 91.7 31.0 12.5 71.5 41.1 47.3 27.7 49.1 Source Resnet-38 84.9 26.0 80.1 8.3 28.0 73.9 54.4 18.9 26.8 71.6 26.0 48.2 14.7 43.2 ST 83.1 23.5 78.2 9.6 25.4 74.8 35.9 33.2 27.3 75.2 32.3 52.2 28.8 44.6 CBST 86.1 35.2 84.2 15.0 22.2 75.6 74.9 22.7 33.1 78.0 37.6 58.0 30.9 50.3</figDesc><table><row><cell></cell><cell cols="2">Source Dilation-Frontend [10] 69.0 31.8 77.0 4.7 3.7 71.8 80.8 38.2 8.0 61.2 38.9 11.5 3.4 38.5</cell></row><row><cell></cell><cell>GCAA [10]</cell><cell>74.2 43.9 79.0 2.4 7.5 77.8 69.5 39.3 10.3 67.9 41.2 27.9 10.9 42.5</cell></row><row><cell>Rio</cell><cell>DeepLab-v2 [36]</cell><cell>76.6 47.3 82.5 12.6 22.5 77.9 86.5 43.0 19.8 74.5 36.8 29.4 16.7 48.2</cell></row><row><cell></cell><cell>MAA [36]</cell><cell>76.2 44.7 84.6 9.3 25.5 81.8 87.3 55.3 32.7 74.3 28.9 43.0 27.6 51.6</cell></row><row><cell></cell><cell>Source Resnet-38</cell><cell>80.6 36.0 81.8 21.0 33.1 79.0 64.7 36.0 21.0 73.1 33.6 22.5 7.8 45.4</cell></row><row><cell></cell><cell>ST</cell><cell>80.1 41.4 83.8 19.1 39.1 80.8 71.2 56.3 27.7 79.9 32.7 36.4 12.2 50.8</cell></row><row><cell></cell><cell>CBST</cell><cell>84.3 55.2 85.4 19.6 30.1 80.5 77.9 55.2 28.6 79.7 33.2 37.6 11.5 52.2</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend [10] 81.2 26.7 71.7 8.7 5.6 73.2 75.7 39.3 14.9 57.6 19.0 1.6 33.8 39.2</cell></row><row><cell></cell><cell>GCAA [10]</cell><cell>83.4 35.4 72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 42.8</cell></row><row><cell>Tokyo</cell><cell>DeepLab-v2 [36]</cell><cell>83.4 35.4 72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 42.8</cell></row><row><cell></cell><cell>MAA [36]</cell><cell>81.5 26.0 77.8 17.8 26.8 82.7 90.9 55.8 38.0 72.1 4.2 24.5 50.8 49.9</cell></row><row><cell></cell><cell>Source Resnet-38</cell><cell>83.8 26.4 73.0 6.5 27.0 80.5 46.6 35.6 22.8 71.3 4.2 10.5 36.1 40.3</cell></row><row><cell></cell><cell>ST</cell><cell>83.1 27.7 74.8 7.1 29.4 84.4 48.5 57.2 23.3 73.3 3.3 22.7 45.8 44.6</cell></row><row><cell></cell><cell>CBST</cell><cell>85.2 33.6 80.4 8.3 31.1 83.9 78.2 53.2 28.9 72.7 4.4 27.0 47.0 48.8</cell></row><row><cell></cell><cell cols="2">Source Dilation-Frontend [10] 77.2 20.9 76.0 5.9 4.3 60.3 81.4 10.9 11.0 54.9 32.6 15.3 5.2 35.1</cell></row><row><cell></cell><cell>GCAA [10]</cell><cell>78.6 28.6 80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 39.6</cell></row><row><cell>Taipei</cell><cell>DeepLab-v2 [36]</cell><cell>78.6 28.6 80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 39.6</cell></row><row><cell></cell><cell>MAA [36]</cell><cell>81.7 29.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>Experimental results of SYNTHIA ? Cityscapes Method Base Net Road SW Build Wall* Fence* Pole* TL TS Veg. Sky PR Rider Car Bus Motor Bike mIoU mIoU* Source only [18] Dilation-Frontend 6.4 17.7 29.7 1.2 0.0 15.1 0.0 7.2 30.3 66.8 51.1 1.5 47.3 3.9 0.1 0.0 17.4 20.2 FCN wild [18] [43] 11.5 19.6 30.8 4.4 0.0 20.3 0.1 11.7 42.3 68.7 51.2 3.8 54.0 3.2 0.2 0.6 20.2 22.1 Source only [45] FCN8s-VGG16 5.6 11.2 59.6 8.0 0.5 21.5 8.0 5.3 72.4 75.6 35.1 9.0 23.6 4.5 0.5 18.0 22.0 27.6 Curr. DA [45] [21] 65.2 26.1 74.9 0.1 0.5 10.7 3.5 3.0 76.1 70.6 47.1 8.2 43.2 20.7 0.7 13.1 29.0 34.8 Source only FCN8s-VGG16 24.1 19.1 68.5 0.9 0.3 16.4 5.7 10.8 75.2 76.3 43.2 15.2 26.7 15.0 5.9 8.5 25.7 30.3 12.5 0.3 36.4 23.5 26.3 84.8 74.7 67.2 17.5 84.5 28.4 15.2 55.8 42.5 48.4 Experimental results for GTA5 ? Cityscapes 28.5 23.0 41.5 45.2 39.6 84.8 26.4 49.2 59.0 27.4 82.3 39.7 45.6 20.9 34.8 46.2 41.5 CBST 86.8 46.7 76.9 26.3 24.8 42.0 46.0 38.6 80.7 15.7 48.0 57.3 27.9 78.2 24.5 49.6 17.7 25.5 45.1 45.2 CBST-SP 88.0 56.2 77.0 27.4 22.4 40.7 47.3 40.9 82.4 21.6 60.3 50.2 20.4 83.8 35.0 51.0 15.2 20.6 37.0 46.2 CBST-SP+MST 89.6 58.9 78.5 33.0 22.3 41.4 48.2 39.2 83.6 24.3 65.4 49.3 20.2 83.3 39.0 48.6 12.5 20.3 35.3 47.0 not well-transfered classes, such as wall, rider, motorcycle and bike. The appearance of fence in SYNTHIA (car barriers) is extremely different from the fence in Cityscapes (pedestrian barriors) and it's very hard for the model to learn transferable knowledge for fence from SYNTHIA to Cityscapes.</figDesc><table><row><cell>GAN DA</cell><cell>[21]</cell><cell cols="2">79.1 31.1 77.1 3.0</cell><cell cols="4">0.2 22.8 6.6 15.2 77.4 78.9 47.0 14.8 67.5 16.3 6.9 13.0 34.8 40.8</cell></row><row><cell>Source only</cell><cell cols="2">DeepLab-v2 [36] 55.6 23.8 74.6</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0 ?</cell><cell>38.6</cell></row><row><cell>MAA</cell><cell>[36]</cell><cell cols="2">84.3 42.7 77.5 ?</cell><cell>?</cell><cell>?</cell><cell cols="2">4.7 7.0 77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3 ?</cell><cell>46.7</cell></row><row><cell>Source only</cell><cell cols="3">FCN8s-VGG16 17.2 19.7 47.3 1.1</cell><cell cols="4">0.0 19.1 3.0 9.1 71.8 78.3 37.6 4.7 42.2 9.0 0.1 0.9 22.6 26.2</cell></row><row><cell>ST</cell><cell>[21]</cell><cell cols="2">0.2 14.5 53.8 1.6</cell><cell cols="4">0.0 18.9 0.9 7.8 72.2 80.3 48.1 6.3 67.7 4.7 0.2 4.5 23.9 27.8</cell></row><row><cell>CBST</cell><cell></cell><cell cols="2">69.6 28.7 69.5 12.1</cell><cell cols="4">0.1 25.4 11.9 13.6 82.0 81.9 49.1 14.5 66.0 6.6 3.7 32.4 35.4 36.1</cell></row><row><cell>Source only</cell><cell>ResNet-38</cell><cell cols="2">32.6 21.5 46.5 4.8</cell><cell cols="4">0.1 26.5 14.8 13.1 70.8 60.3 56.6 3.5 74.1 20.4 8.9 13.1 29.2 33.6</cell></row><row><cell>ST</cell><cell>[41]</cell><cell cols="2">38.2 19.6 70.2 3.9</cell><cell cols="4">0.0 31.9 17.6 17.2 82.4 68.3 63.1 5.3 78.4 11.2 0.8 7.5 32.2 36.9</cell></row><row><cell cols="8">CBST 53.6 23.7 75.0 Method Base Net Road SW Build Wall Fence Pole TL TS Veg. Terrain Sky PR Rider Car Truck Bus Train Motor Bike mIoU</cell></row><row><cell cols="7">Source only [18] Dilation-Frontend 31.9 18.9 47.7 7.4 3.1 16.0 10.4 1.0 76.5 13.0 58.9 36.0 1.0 67.1 9.5 3.7 0.0</cell><cell>0.0 0.0 21.2</cell></row><row><cell>FCN wild [18]</cell><cell>[43]</cell><cell cols="5">70.4 32.4 62.1 14.9 5.4 10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0</cell><cell>3.5 0.0 27.1</cell></row><row><cell cols="7">Source only [45] FCN8s-VGG16 18.1 6.8 64.1 7.3 8.7 21.0 14.9 16.8 45.9 2.4 64.4 41.6 17.5 55.3 8.4 5.0 6.9</cell><cell>4.3 13.8 22.3</cell></row><row><cell>Curr. DA [45]</cell><cell>[21]</cell><cell cols="6">74.9 22.0 71.7 6.0 11.9 8.4 16.3 11.1 75.7 13.3 66.5 38.0 9.3 55.2 18.8 18.9 0.0 16.8 16.6 28.9</cell></row><row><cell cols="7">Source only [17] FCN8s-VGG16 26.0 14.9 65.1 5.5 12.9 8.9 6.0 2.5 70.0 2.9 47.0 24.5 0.0 40.0 12.1 1.5 0.0</cell><cell>0.0 0.0 17.9</cell></row><row><cell>CyCADA [17]</cell><cell>[21]</cell><cell cols="5">85.2 37.2 76.5 21.8 15.0 23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5</cell><cell>9.8 0.0 35.4</cell></row><row><cell cols="7">Source only [17] Dilated ResNet-26 42.7 26.3 51.7 5.5 6.8 13.8 23.6 6.9 75.5 11.5 36.8 49.3 0.9 46.7 3.4 5.0 0.0</cell><cell>5.0 1.4 21.7</cell></row><row><cell>CyCADA [17]</cell><cell>[44]</cell><cell cols="6">79.1 33.1 77.9 23.4 17.3 32.1 33.3 31.8 81.5 26.7 69.0 62.8 14.7 74.5 20.9 25.6 6.9 18.8 20.4 39.5</cell></row><row><cell>Source only [30]</cell><cell>ResNet-50</cell><cell cols="5">64.5 24.9 73.7 14.8 2.5 18.0 15.9 0 74.9 16.4 72.0 42.3 0.0 39.5 8.6 13.4 0.0</cell><cell>0.0 0.0 25.3</cell></row><row><cell>ADR [30]</cell><cell>[16]</cell><cell cols="5">87.8 15.6 77.4 20.6 9.7 19.0 19.9 7.7 82.0 31.5 74.3 43.5 9.0 77.8 17.5 27.7 1.8</cell><cell>9.7 0.0 33.3</cell></row><row><cell>Source only [24]</cell><cell>DenseNet</cell><cell cols="5">67.3 23.1 69.4 13.9 14.4 21.6 19.2 12.4 78.7 24.5 74.8 49.3 3.7 54.1 8.7 5.3 2.6</cell><cell>6.2 1.9 29.0</cell></row><row><cell>I2I Adapt [24]</cell><cell>[19]</cell><cell cols="6">85.8 37.5 80.2 23.3 16.1 23.0 14.5 9.8 79.2 36.5 76.4 53.4 7.4 82.8 19.1 15.7 2.8 13.4 1.7 35.7</cell></row><row><cell>Source only [36]</cell><cell>DeepLab-v2</cell><cell cols="6">75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6</cell></row><row><cell>MAA [36]</cell><cell>[19]</cell><cell cols="6">86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4</cell></row><row><cell>Source only</cell><cell cols="7">FCN8s-VGG16 64.0 22.1 68.6 13.3 8.7 19.9 15.5 5.9 74.9 13.4 37.0 37.7 10.3 48.2 6.1 1.2 1.8 10.8 2.9 24.3</cell></row><row><cell>ST</cell><cell>[18]</cell><cell cols="6">83.8 17.4 72.1 14.3 2.9 16.5 16.0 6.8 81.4 24.2 47.2 40.7 7.6 71.7 10.2 7.6 0.5 11.1 0.9 28.1</cell></row><row><cell>CBST</cell><cell></cell><cell cols="6">66.7 26.8 73.7 14.8 9.5 28.3 25.9 10.1 75.5 15.7 51.6 47.2 6.2 71.9 3.7 2.2 5.4 18.9 32.4 30.9</cell></row><row><cell>CBST-SP</cell><cell></cell><cell cols="6">90.4 50.8 72.0 18.3 9.5 27.2 28.6 14.1 82.4 25.1 70.8 42.6 14.5 76.9 5.9 12.5 1.2 14.0 28.6 36.1</cell></row><row><cell>Source only</cell><cell>ResNet-38</cell><cell cols="6">70.0 23.7 67.8 15.4 18.1 40.2 41.9 25.3 78.8 11.7 31.4 62.9 29.8 60.1 21.5 26.8 7.7 28.1 12.0 35.4</cell></row><row><cell>ST</cell><cell>[41]</cell><cell>90.1 56.8 77.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Adaptation results on SYNTHIA ? Cityscapes. Rows correspond to sampled images and predictions. Columns correspond to original images, ground truths, and results of source ResNet-38, ST, CBST. Best viewed in color.</figDesc><table><row><cell>road</cell><cell cols="2">sidewalk building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic lgt</cell><cell>traffic sgn</cell><cell>vegetation</cell><cell></cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row><row><cell>Fig. 3. road</cell><cell cols="2">sidewalk building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>traffic lgt</cell><cell>traffic sgn</cell><cell>vegetation</cell><cell></cell></row><row><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorcycle</cell><cell>bike</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on GTA V ? BDD-V Method Road SW Build Wall Fence Pole TL TS Veg. Terrain Sky PR Rider Car Truck Bus Train Motor Bike Mean Source Resnet-38 76.7 34.1 53.8 10.2 28.3 29.1 34.1 33.9 73.4 17.5 60.8 52.8 15.2 63.8 40.78 28.8 0.0 21.3 2.6 35.0 ST 83.5 26.1 72.5 14.1 27.3 26.5 32.5 28.5 74.5 35.7 88.1 51.4 15.9 67.4 26.6 35.9 0.0 8.9 2.9 37.8 ST-SP 88.2 40.8 74.1 14.8 27.1 25.8 33.1 36.1 72.2 37.4 88.8 53.8 21.2 74.2 24.5 22.9 0.0 12.9 1.5 39.5 CBST 84.1 26.6 75.0 15.3 28.8 28.0 33.8 29.8 76.2 35.6 90.4 54.2 18.2 69.4 28.6 36.7 0.0 13.0 3.8 39.3 CBST-SP 89.9 39.3 73.9 14.9 28.0 28.7 34.1 35.6 76.7 34.9 89.6 57.4 19.8 77.3 27.1 28.1 0.0 13.8 1.7 40.6</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In this paper, we define "confidence" as the maximum class probability at each pixel.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The main paper presents our ECCV 2018 camera ready submission. In the appendix, we further present the additional details and results that are not covered by the camera ready paper due to space constraints. We believe these details will benefit successful reproduction of the reported experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<title level="m">Domainadversarial neural networks. In: NIPS Workshop on Transfer and Multi-task learning: Theory Meets Practice</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>COLT 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semi-supervised learning (chapelle, o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<editor>. et al.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Frank Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Densely connected convolutional networks 13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2015) 1, 4</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Co-training for predicting emotions with spoken dialogue data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maeireizo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interactive poster and demonstration sessions</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Playing for benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning subjective nouns using extraction pattern bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh conference on Natural language learning, HLT-NAACL Workshop</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01575</idno>
		<title level="m">Adversarial dropout regularization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06969</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shifting weights: Adapting object detectors from image to video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>ACL</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<title level="m">Semantic understanding of scenes through the ade20k dataset</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science, University of Wisconsin-Madison</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
